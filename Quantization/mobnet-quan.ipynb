{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":2546969,"sourceType":"datasetVersion","datasetId":1544742},{"sourceId":6810129,"sourceType":"datasetVersion","datasetId":3917752},{"sourceId":1663377,"sourceType":"datasetVersion","datasetId":918039}],"dockerImageVersionId":30674,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Imports here\nimport torch\nimport PIL\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nfrom torchvision import datasets, transforms, models\nfrom torch import nn\nfrom torch import optim\nfrom collections import OrderedDict\nfrom time import time\nimport copy\nfrom sklearn.metrics import confusion_matrix,classification_report\nimport numpy as np\nimport skimage.io as io\nimport torch.nn.functional as F  \n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-07T11:45:09.820248Z","iopub.execute_input":"2024-04-07T11:45:09.821059Z","iopub.status.idle":"2024-04-07T11:45:20.955225Z","shell.execute_reply.started":"2024-04-07T11:45:09.821029Z","shell.execute_reply":"2024-04-07T11:45:20.954256Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"data_dir = '/kaggle/input/eurosat-dataset/EuroSAT'\ntrain_dir = data_dir \n\n# Define your transformations\ntrain_transforms = transforms.Compose([\n    #transforms.RandomRotation(30),\n    transforms.RandomResizedCrop(224),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\n# Load all the images from the train folder\nall_data = datasets.ImageFolder(train_dir, transform=train_transforms)\n\n# Calculate the sizes for train, validation, and test sets\ntotal_size = len(all_data)\ntrain_size = int(0.7 * total_size)\ntest_size = int(0.2 * total_size)\nvalid_size = total_size - train_size - test_size\n\n# Use random_split to split the dataset\ntrain_data, valid_data, test_data = torch.utils.data.random_split(all_data, [train_size, valid_size, test_size])\n\n# Create data loaders\ntrainloader = torch.utils.data.DataLoader(train_data, batch_size=50, shuffle=False)\nvalidloader = torch.utils.data.DataLoader(valid_data, batch_size=50)\ntestloader = torch.utils.data.DataLoader(test_data, batch_size=50)","metadata":{"execution":{"iopub.status.busy":"2024-04-07T11:45:20.956958Z","iopub.execute_input":"2024-04-07T11:45:20.957425Z","iopub.status.idle":"2024-04-07T11:45:29.902417Z","shell.execute_reply.started":"2024-04-07T11:45:20.957398Z","shell.execute_reply":"2024-04-07T11:45:29.901646Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# # Load a pre-trained network \n# # model = models.vgg16(pretrained=True)\n# # model.name = \"vgg16\"\n# # model\n# model = models.mobilenet_v2(pretrained=True)\n# model.name='MobileNet'","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-04-07T11:45:29.903545Z","iopub.execute_input":"2024-04-07T11:45:29.903886Z","iopub.status.idle":"2024-04-07T11:45:29.908166Z","shell.execute_reply.started":"2024-04-07T11:45:29.903856Z","shell.execute_reply":"2024-04-07T11:45:29.907235Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# for param in model.parameters():\n#     param.requires_grad = False","metadata":{"execution":{"iopub.status.busy":"2024-04-07T11:45:29.910678Z","iopub.execute_input":"2024-04-07T11:45:29.910921Z","iopub.status.idle":"2024-04-07T11:45:29.925513Z","shell.execute_reply.started":"2024-04-07T11:45:29.910900Z","shell.execute_reply":"2024-04-07T11:45:29.924637Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# num_features = model.classifier[1].out_features","metadata":{"execution":{"iopub.status.busy":"2024-04-07T11:45:29.926604Z","iopub.execute_input":"2024-04-07T11:45:29.926863Z","iopub.status.idle":"2024-04-07T11:45:29.937316Z","shell.execute_reply.started":"2024-04-07T11:45:29.926835Z","shell.execute_reply":"2024-04-07T11:45:29.936491Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# num_features","metadata":{"execution":{"iopub.status.busy":"2024-04-07T11:45:29.938438Z","iopub.execute_input":"2024-04-07T11:45:29.938783Z","iopub.status.idle":"2024-04-07T11:45:29.948204Z","shell.execute_reply.started":"2024-04-07T11:45:29.938761Z","shell.execute_reply":"2024-04-07T11:45:29.947355Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# classifier = nn.Sequential(OrderedDict([\n#                           ('fc1', nn.Linear(25088, 4096, bias=True)),\n#                           ('relu1', nn.ReLU()),\n#                           ('dropout1', nn.Dropout(p=0.5)),\n#                           ('fc2', nn.Linear(4096, 512, bias=True)),\n#                           ('relu2', nn.ReLU()),\n#                           ('dropout2', nn.Dropout(p=0.5)),\n#                           ('fc3', nn.Linear(512, 256, bias=True)),\n#                           ('relu3', nn.ReLU()),\n#                           ('dropout3', nn.Dropout(p=0.5)),\n#                           ('fc4',nn.Linear(256,4,bias=True)),\n#                           ('output', nn.Softmax(dim=1))\n#                         ]))\n    \n# # model.classifier = classifier\n# classifier = nn.Sequential(\n#     nn.AdaptiveAvgPool2d((1, 1)),\n#     nn.Flatten(),\n#     nn.Linear(1280, 1024),\n#     nn.ReLU(),\n#     nn.BatchNorm1d(1024),\n#     nn.Linear(1024, 128),\n#     nn.ReLU(),\n#     nn.Linear(128, 4),\n#     nn.Softmax(dim=1)\n# )\n# model.classifier=classifier","metadata":{"execution":{"iopub.status.busy":"2024-04-07T11:45:29.951219Z","iopub.execute_input":"2024-04-07T11:45:29.951504Z","iopub.status.idle":"2024-04-07T11:45:29.960999Z","shell.execute_reply.started":"2024-04-07T11:45:29.951482Z","shell.execute_reply":"2024-04-07T11:45:29.960186Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"import torch.nn as nn\nfrom torchvision.models import mobilenet_v2\n\nclass CustomMobileNetv2(nn.Module):\n    def __init__(self, output_size):\n        super().__init__()\n        self.mnet = mobilenet_v2(pretrained=True)\n        self.freeze()\n\n        self.mnet.classifier = nn.Sequential(\n            nn.Linear(1280, 1024),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm1d(1024),\n            nn.Linear(1024, 512),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm1d(512),\n            nn.Linear(512, output_size),\n            nn.LogSoftmax(1)\n        )\n\n    def forward(self, x):\n        return self.mnet(x)\n\n    def freeze(self):\n        for param in self.mnet.parameters():\n            param.requires_grad = False\n\n    def unfreeze(self):\n        for param in self.mnet.parameters():\n            param.requires_grad = True\n","metadata":{"execution":{"iopub.status.busy":"2024-04-07T11:45:29.961991Z","iopub.execute_input":"2024-04-07T11:45:29.962276Z","iopub.status.idle":"2024-04-07T11:45:29.980226Z","shell.execute_reply.started":"2024-04-07T11:45:29.962253Z","shell.execute_reply":"2024-04-07T11:45:29.979260Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"model = CustomMobileNetv2(10)","metadata":{"execution":{"iopub.status.busy":"2024-04-07T11:45:29.981317Z","iopub.execute_input":"2024-04-07T11:45:29.981665Z","iopub.status.idle":"2024-04-07T11:45:30.396227Z","shell.execute_reply.started":"2024-04-07T11:45:29.981643Z","shell.execute_reply":"2024-04-07T11:45:30.395342Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/mobilenet_v2-b0353104.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v2-b0353104.pth\n100%|██████████| 13.6M/13.6M [00:00<00:00, 91.7MB/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ndevice","metadata":{"execution":{"iopub.status.busy":"2024-04-07T11:45:30.399690Z","iopub.execute_input":"2024-04-07T11:45:30.399965Z","iopub.status.idle":"2024-04-07T11:45:30.459673Z","shell.execute_reply.started":"2024-04-07T11:45:30.399941Z","shell.execute_reply":"2024-04-07T11:45:30.458624Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"device(type='cuda', index=0)"},"metadata":{}}]},{"cell_type":"code","source":"model.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-04-07T11:45:30.461126Z","iopub.execute_input":"2024-04-07T11:45:30.461764Z","iopub.status.idle":"2024-04-07T11:45:30.675059Z","shell.execute_reply.started":"2024-04-07T11:45:30.461729Z","shell.execute_reply":"2024-04-07T11:45:30.674166Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"CustomMobileNetv2(\n  (mnet): MobileNetV2(\n    (features): Sequential(\n      (0): Conv2dNormActivation(\n        (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU6(inplace=True)\n      )\n      (1): InvertedResidual(\n        (conv): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (2): InvertedResidual(\n        (conv): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (3): InvertedResidual(\n        (conv): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (4): InvertedResidual(\n        (conv): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (2): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (5): InvertedResidual(\n        (conv): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (6): InvertedResidual(\n        (conv): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (7): InvertedResidual(\n        (conv): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (8): InvertedResidual(\n        (conv): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (9): InvertedResidual(\n        (conv): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (10): InvertedResidual(\n        (conv): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (11): InvertedResidual(\n        (conv): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (12): InvertedResidual(\n        (conv): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (13): InvertedResidual(\n        (conv): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (14): InvertedResidual(\n        (conv): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)\n            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (2): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (15): InvertedResidual(\n        (conv): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (16): InvertedResidual(\n        (conv): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (17): InvertedResidual(\n        (conv): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (2): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (18): Conv2dNormActivation(\n        (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU6(inplace=True)\n      )\n    )\n    (classifier): Sequential(\n      (0): Linear(in_features=1280, out_features=1024, bias=True)\n      (1): ReLU(inplace=True)\n      (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (3): Linear(in_features=1024, out_features=512, bias=True)\n      (4): ReLU(inplace=True)\n      (5): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (6): Linear(in_features=512, out_features=10, bias=True)\n      (7): LogSoftmax(dim=1)\n    )\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"# criterion = nn.CrossEntropyLoss()\n# optimizer = optim.Adam(model.classifier.parameters(), lr=0.001)\ncriterion = nn.NLLLoss()\noptimizer = optim.AdamW(model.parameters(), lr=0.001)\n\n\nepochs = 5\nprint_every = 30 # Prints every 30 images out of batch of 50 images\nsteps = 0","metadata":{"execution":{"iopub.status.busy":"2024-04-07T11:45:30.676348Z","iopub.execute_input":"2024-04-07T11:45:30.676697Z","iopub.status.idle":"2024-04-07T11:45:30.683321Z","shell.execute_reply.started":"2024-04-07T11:45:30.676666Z","shell.execute_reply":"2024-04-07T11:45:30.682286Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def validation(model, testloader, criterion):\n    test_loss = 0\n    accuracy = 0\n    \n    for ii, (inputs, labels) in enumerate(testloader):\n        \n        inputs, labels = inputs.to(device), labels.to(device)\n        \n        output = model.forward(inputs)\n        test_loss += criterion(output, labels).item()\n        \n        ps = torch.exp(output)\n        equality = (labels.data == ps.max(dim=1)[1])\n        accuracy += equality.type(torch.FloatTensor).mean()\n    \n    return test_loss, accuracy","metadata":{"execution":{"iopub.status.busy":"2024-04-07T11:45:30.684786Z","iopub.execute_input":"2024-04-07T11:45:30.685438Z","iopub.status.idle":"2024-04-07T11:45:30.696087Z","shell.execute_reply.started":"2024-04-07T11:45:30.685406Z","shell.execute_reply":"2024-04-07T11:45:30.695182Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def train(model,steps,print_every,epochs):\n    print(\"Training process initializing .....\\n\")\n\n    for e in range(epochs):\n        running_loss = 0\n        model.train() \n    \n        for ii, (inputs, labels) in enumerate(trainloader):\n            steps += 1\n        \n            inputs, labels = inputs.to(device), labels.to(device)\n        \n            optimizer.zero_grad()\n        \n            # Forward and backward passes\n            outputs = model.forward(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n        \n            running_loss += loss.item()\n        \n            if steps % print_every == 0:\n                model.eval()\n\n                with torch.no_grad():\n                    valid_loss, accuracy = validation(model, validloader, criterion)\n            \n                print(\"Epoch: {}/{} | \".format(e+1, epochs),\n                      \"Training Loss: {:.4f} | \".format(running_loss/print_every),\n                      \"Validation Loss: {:.4f} | \".format(valid_loss/len(validloader)),\n                      \"Validation Accuracy: {:.4f}\".format(accuracy/len(validloader)))\n            \n                running_loss = 0\n                model.train()\n\n    print(\"\\nTraining process is now complete!!\")\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-04-07T11:45:30.697197Z","iopub.execute_input":"2024-04-07T11:45:30.697531Z","iopub.status.idle":"2024-04-07T11:45:30.708229Z","shell.execute_reply.started":"2024-04-07T11:45:30.697506Z","shell.execute_reply":"2024-04-07T11:45:30.707495Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# def train(model, train_loader, valid_loader, optimizer, criterion, print_every, epochs):\n#     print(\"Training process initializing .....\\n\")\n    \n#     model.train()\n\n#     for e in range(epochs):\n#         running_loss = 0\n#         steps = 0\n        \n#         for ii, (inputs, labels) in enumerate(train_loader):\n#             steps += 1\n        \n#             inputs, labels = inputs.to(device), labels.to(device)\n        \n#             optimizer.zero_grad()\n        \n#             # Forward and backward passes\n#             outputs = model(inputs)\n#             loss = criterion(outputs, labels)\n#             loss.backward()\n#             optimizer.step()\n        \n#             running_loss += loss.item()\n        \n#             if steps % print_every == 0:\n#                 model.eval()\n#                 valid_loss, accuracy = validation(model, valid_loader, criterion)\n            \n#                 print(\"Epoch: {}/{} | \".format(e+1, epochs),\n#                       \"Training Loss: {:.4f} | \".format(running_loss/print_every),\n#                       \"Validation Loss: {:.4f} | \".format(valid_loss/len(valid_loader)),\n#                       \"Validation Accuracy: {:.4f}\".format(accuracy/len(valid_loader)))\n            \n#                 running_loss = 0\n#                 model.train()\n\n#     print(\"\\nTraining process is now complete!!\")\n#     return model\n\n\n\n# # Define the optimizer and loss function\n# optimizer = torch.optim.Adam(model.classifier.parameters(), lr=0.001)\n# criterion = nn.CrossEntropyLoss()\n\n# # Define print_every, epochs\n# print_every = 10\n# epochs = 10\n\n# # Train the model\n# trained_model = train(model, trainloader, validloader, optimizer, criterion, print_every, epochs)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-07T11:45:30.709288Z","iopub.execute_input":"2024-04-07T11:45:30.709562Z","iopub.status.idle":"2024-04-07T11:45:30.724129Z","shell.execute_reply.started":"2024-04-07T11:45:30.709541Z","shell.execute_reply":"2024-04-07T11:45:30.723403Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"model = train(model,steps,print_every,epochs)","metadata":{"execution":{"iopub.status.busy":"2024-04-07T11:45:30.725012Z","iopub.execute_input":"2024-04-07T11:45:30.725254Z","iopub.status.idle":"2024-04-07T12:03:48.834482Z","shell.execute_reply.started":"2024-04-07T11:45:30.725233Z","shell.execute_reply":"2024-04-07T12:03:48.833398Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Training process initializing .....\n\nEpoch: 1/5 |  Training Loss: 0.8579 |  Validation Loss: 0.6581 |  Validation Accuracy: 0.7733\nEpoch: 1/5 |  Training Loss: 0.6622 |  Validation Loss: 0.5994 |  Validation Accuracy: 0.7889\nEpoch: 1/5 |  Training Loss: 0.6163 |  Validation Loss: 0.5655 |  Validation Accuracy: 0.8078\nEpoch: 1/5 |  Training Loss: 0.6083 |  Validation Loss: 0.5031 |  Validation Accuracy: 0.8185\nEpoch: 1/5 |  Training Loss: 0.5570 |  Validation Loss: 0.5000 |  Validation Accuracy: 0.8278\nEpoch: 1/5 |  Training Loss: 0.5489 |  Validation Loss: 0.4582 |  Validation Accuracy: 0.8411\nEpoch: 1/5 |  Training Loss: 0.5771 |  Validation Loss: 0.4472 |  Validation Accuracy: 0.8589\nEpoch: 1/5 |  Training Loss: 0.5444 |  Validation Loss: 0.4979 |  Validation Accuracy: 0.8267\nEpoch: 1/5 |  Training Loss: 0.5470 |  Validation Loss: 0.4613 |  Validation Accuracy: 0.8333\nEpoch: 1/5 |  Training Loss: 0.5294 |  Validation Loss: 0.4465 |  Validation Accuracy: 0.8485\nEpoch: 1/5 |  Training Loss: 0.4960 |  Validation Loss: 0.5060 |  Validation Accuracy: 0.8326\nEpoch: 1/5 |  Training Loss: 0.5084 |  Validation Loss: 0.4221 |  Validation Accuracy: 0.8559\nEpoch: 2/5 |  Training Loss: 0.1939 |  Validation Loss: 0.4372 |  Validation Accuracy: 0.8511\nEpoch: 2/5 |  Training Loss: 0.4924 |  Validation Loss: 0.4064 |  Validation Accuracy: 0.8707\nEpoch: 2/5 |  Training Loss: 0.4935 |  Validation Loss: 0.4124 |  Validation Accuracy: 0.8604\nEpoch: 2/5 |  Training Loss: 0.4881 |  Validation Loss: 0.4493 |  Validation Accuracy: 0.8356\nEpoch: 2/5 |  Training Loss: 0.5071 |  Validation Loss: 0.4279 |  Validation Accuracy: 0.8585\nEpoch: 2/5 |  Training Loss: 0.4756 |  Validation Loss: 0.3995 |  Validation Accuracy: 0.8619\nEpoch: 2/5 |  Training Loss: 0.5295 |  Validation Loss: 0.4500 |  Validation Accuracy: 0.8467\nEpoch: 2/5 |  Training Loss: 0.4808 |  Validation Loss: 0.4100 |  Validation Accuracy: 0.8611\nEpoch: 2/5 |  Training Loss: 0.4889 |  Validation Loss: 0.4129 |  Validation Accuracy: 0.8607\nEpoch: 2/5 |  Training Loss: 0.4752 |  Validation Loss: 0.4120 |  Validation Accuracy: 0.8585\nEpoch: 2/5 |  Training Loss: 0.4443 |  Validation Loss: 0.4024 |  Validation Accuracy: 0.8544\nEpoch: 2/5 |  Training Loss: 0.4853 |  Validation Loss: 0.3833 |  Validation Accuracy: 0.8659\nEpoch: 2/5 |  Training Loss: 0.4360 |  Validation Loss: 0.3996 |  Validation Accuracy: 0.8633\nEpoch: 3/5 |  Training Loss: 0.3428 |  Validation Loss: 0.4107 |  Validation Accuracy: 0.8593\nEpoch: 3/5 |  Training Loss: 0.4506 |  Validation Loss: 0.4018 |  Validation Accuracy: 0.8600\nEpoch: 3/5 |  Training Loss: 0.4626 |  Validation Loss: 0.4296 |  Validation Accuracy: 0.8485\nEpoch: 3/5 |  Training Loss: 0.4746 |  Validation Loss: 0.3949 |  Validation Accuracy: 0.8596\nEpoch: 3/5 |  Training Loss: 0.4557 |  Validation Loss: 0.3770 |  Validation Accuracy: 0.8648\nEpoch: 3/5 |  Training Loss: 0.4324 |  Validation Loss: 0.4004 |  Validation Accuracy: 0.8604\nEpoch: 3/5 |  Training Loss: 0.5073 |  Validation Loss: 0.4239 |  Validation Accuracy: 0.8515\nEpoch: 3/5 |  Training Loss: 0.4106 |  Validation Loss: 0.3794 |  Validation Accuracy: 0.8693\nEpoch: 3/5 |  Training Loss: 0.4524 |  Validation Loss: 0.3709 |  Validation Accuracy: 0.8678\nEpoch: 3/5 |  Training Loss: 0.4625 |  Validation Loss: 0.3855 |  Validation Accuracy: 0.8630\nEpoch: 3/5 |  Training Loss: 0.4490 |  Validation Loss: 0.3880 |  Validation Accuracy: 0.8567\nEpoch: 3/5 |  Training Loss: 0.4826 |  Validation Loss: 0.3665 |  Validation Accuracy: 0.8767\nEpoch: 4/5 |  Training Loss: 0.0896 |  Validation Loss: 0.3944 |  Validation Accuracy: 0.8596\nEpoch: 4/5 |  Training Loss: 0.4487 |  Validation Loss: 0.3851 |  Validation Accuracy: 0.8637\nEpoch: 4/5 |  Training Loss: 0.4240 |  Validation Loss: 0.4085 |  Validation Accuracy: 0.8563\nEpoch: 4/5 |  Training Loss: 0.4432 |  Validation Loss: 0.3619 |  Validation Accuracy: 0.8719\nEpoch: 4/5 |  Training Loss: 0.4314 |  Validation Loss: 0.3865 |  Validation Accuracy: 0.8644\nEpoch: 4/5 |  Training Loss: 0.4191 |  Validation Loss: 0.3946 |  Validation Accuracy: 0.8681\nEpoch: 4/5 |  Training Loss: 0.4379 |  Validation Loss: 0.3915 |  Validation Accuracy: 0.8711\nEpoch: 4/5 |  Training Loss: 0.4644 |  Validation Loss: 0.3878 |  Validation Accuracy: 0.8678\nEpoch: 4/5 |  Training Loss: 0.4228 |  Validation Loss: 0.3720 |  Validation Accuracy: 0.8730\nEpoch: 4/5 |  Training Loss: 0.4081 |  Validation Loss: 0.3578 |  Validation Accuracy: 0.8789\nEpoch: 4/5 |  Training Loss: 0.4090 |  Validation Loss: 0.4015 |  Validation Accuracy: 0.8559\nEpoch: 4/5 |  Training Loss: 0.4138 |  Validation Loss: 0.3812 |  Validation Accuracy: 0.8663\nEpoch: 4/5 |  Training Loss: 0.3973 |  Validation Loss: 0.3584 |  Validation Accuracy: 0.8789\nEpoch: 5/5 |  Training Loss: 0.2441 |  Validation Loss: 0.3940 |  Validation Accuracy: 0.8674\nEpoch: 5/5 |  Training Loss: 0.3859 |  Validation Loss: 0.3747 |  Validation Accuracy: 0.8733\nEpoch: 5/5 |  Training Loss: 0.4486 |  Validation Loss: 0.3530 |  Validation Accuracy: 0.8778\nEpoch: 5/5 |  Training Loss: 0.4466 |  Validation Loss: 0.3796 |  Validation Accuracy: 0.8589\nEpoch: 5/5 |  Training Loss: 0.4245 |  Validation Loss: 0.4035 |  Validation Accuracy: 0.8641\nEpoch: 5/5 |  Training Loss: 0.3914 |  Validation Loss: 0.3867 |  Validation Accuracy: 0.8641\nEpoch: 5/5 |  Training Loss: 0.4715 |  Validation Loss: 0.3466 |  Validation Accuracy: 0.8707\nEpoch: 5/5 |  Training Loss: 0.4069 |  Validation Loss: 0.3823 |  Validation Accuracy: 0.8607\nEpoch: 5/5 |  Training Loss: 0.4074 |  Validation Loss: 0.3713 |  Validation Accuracy: 0.8785\nEpoch: 5/5 |  Training Loss: 0.4318 |  Validation Loss: 0.3503 |  Validation Accuracy: 0.8804\nEpoch: 5/5 |  Training Loss: 0.4020 |  Validation Loss: 0.3438 |  Validation Accuracy: 0.8785\nEpoch: 5/5 |  Training Loss: 0.3974 |  Validation Loss: 0.3460 |  Validation Accuracy: 0.8815\nEpoch: 5/5 |  Training Loss: 0.3922 |  Validation Loss: 0.3775 |  Validation Accuracy: 0.8696\n\nTraining process is now complete!!\n","output_type":"stream"}]},{"cell_type":"code","source":"def pred(Model,Testloader):\n    all_labels = []\n    all_predictions = []\n    correct = 0\n    total = 0\n    start_time = time()\n    with torch.no_grad():\n        Model.eval()\n        for images, labels in Testloader:\n            all_labels.extend(labels.numpy())\n            images, labels = images.to(device), labels.to(device)\n            outputs = Model(images)\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n            predicted_tensor_cpu = predicted.to('cpu')\n            all_predictions.extend(predicted_tensor_cpu.numpy())\n    end_time = time()\n    print(\"Time: \",end_time - start_time)\n    print('Accuracy achieved by the network on test images is: %d%%' % (100 * correct / total))\n    \n    return all_labels,all_predictions","metadata":{"execution":{"iopub.status.busy":"2024-04-07T12:03:48.835688Z","iopub.execute_input":"2024-04-07T12:03:48.835979Z","iopub.status.idle":"2024-04-07T12:03:48.843521Z","shell.execute_reply.started":"2024-04-07T12:03:48.835956Z","shell.execute_reply":"2024-04-07T12:03:48.842666Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"labels_fp32,predictions_fp32 = pred(model,testloader)","metadata":{"execution":{"iopub.status.busy":"2024-04-07T12:03:48.844680Z","iopub.execute_input":"2024-04-07T12:03:48.845084Z","iopub.status.idle":"2024-04-07T12:04:31.320586Z","shell.execute_reply.started":"2024-04-07T12:03:48.845054Z","shell.execute_reply":"2024-04-07T12:04:31.319474Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Time:  42.45575714111328\nAccuracy achieved by the network on test images is: 86%\n","output_type":"stream"}]},{"cell_type":"code","source":"import seaborn as sns\ndef metrics(labels,predictions):\n    classes = train_data.dataset.classes\n    cm = confusion_matrix(np.array(labels), np.array(predictions))\n    print(\"Confusion Matrix:\")\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n    plt.xlabel('Predicted')\n    plt.ylabel('Actual')\n    plt.title('Confusion Matrix')\n    plt.show()\n    print('----------------------------------------------------------------')\n    print(\"Classification Report:\")\n    report = classification_report(np.array(labels),np.array(predictions))\n    print(report)","metadata":{"execution":{"iopub.status.busy":"2024-04-07T12:04:31.322216Z","iopub.execute_input":"2024-04-07T12:04:31.322509Z","iopub.status.idle":"2024-04-07T12:04:31.329600Z","shell.execute_reply.started":"2024-04-07T12:04:31.322484Z","shell.execute_reply":"2024-04-07T12:04:31.328564Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"metrics(labels_fp32,predictions_fp32)","metadata":{"execution":{"iopub.status.busy":"2024-04-07T12:04:31.330824Z","iopub.execute_input":"2024-04-07T12:04:31.331081Z","iopub.status.idle":"2024-04-07T12:04:32.187224Z","shell.execute_reply.started":"2024-04-07T12:04:31.331059Z","shell.execute_reply":"2024-04-07T12:04:32.186273Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Confusion Matrix:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 800x600 with 2 Axes>","image/png":"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"},"metadata":{"image/png":{"width":793,"height":689}}},{"name":"stdout","text":"----------------------------------------------------------------\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.85      0.92      0.88       635\n           1       0.94      0.94      0.94       571\n           2       0.78      0.92      0.85       597\n           3       0.90      0.58      0.71       493\n           4       0.90      0.91      0.91       492\n           5       0.86      0.82      0.84       400\n           6       0.86      0.69      0.77       517\n           7       0.90      0.98      0.93       618\n           8       0.74      0.83      0.79       490\n           9       0.94      0.96      0.95       587\n\n    accuracy                           0.86      5400\n   macro avg       0.87      0.86      0.86      5400\nweighted avg       0.87      0.86      0.86      5400\n\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\ndef print_model_size(mdl):\n    torch.save(mdl.state_dict(), \"tmp.pt\")\n    print(\"%.2f MB\" %(os.path.getsize(\"tmp.pt\")/1e6))\n    os.remove('tmp.pt')","metadata":{"execution":{"iopub.status.busy":"2024-04-07T12:04:32.188510Z","iopub.execute_input":"2024-04-07T12:04:32.188859Z","iopub.status.idle":"2024-04-07T12:04:32.194181Z","shell.execute_reply.started":"2024-04-07T12:04:32.188828Z","shell.execute_reply":"2024-04-07T12:04:32.193203Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"print(\"Size of fp32 model:\",end='')\nprint_model_size(model)","metadata":{"execution":{"iopub.status.busy":"2024-04-07T12:04:32.195408Z","iopub.execute_input":"2024-04-07T12:04:32.195714Z","iopub.status.idle":"2024-04-07T12:04:32.264483Z","shell.execute_reply.started":"2024-04-07T12:04:32.195685Z","shell.execute_reply":"2024-04-07T12:04:32.263584Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Size of fp32 model:16.52 MB\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Quantization","metadata":{}},{"cell_type":"markdown","source":"# FP-16","metadata":{}},{"cell_type":"code","source":"model_fp16 = copy.deepcopy(model)","metadata":{"execution":{"iopub.status.busy":"2024-04-07T12:04:32.265925Z","iopub.execute_input":"2024-04-07T12:04:32.266282Z","iopub.status.idle":"2024-04-07T12:04:32.319750Z","shell.execute_reply.started":"2024-04-07T12:04:32.266250Z","shell.execute_reply":"2024-04-07T12:04:32.319012Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"model_fp16.half()\nmodel_fp16.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-04-07T12:04:32.320766Z","iopub.execute_input":"2024-04-07T12:04:32.321056Z","iopub.status.idle":"2024-04-07T12:04:32.342555Z","shell.execute_reply.started":"2024-04-07T12:04:32.321032Z","shell.execute_reply":"2024-04-07T12:04:32.341684Z"},"trusted":true},"execution_count":24,"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"CustomMobileNetv2(\n  (mnet): MobileNetV2(\n    (features): Sequential(\n      (0): Conv2dNormActivation(\n        (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU6(inplace=True)\n      )\n      (1): InvertedResidual(\n        (conv): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (2): InvertedResidual(\n        (conv): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (3): InvertedResidual(\n        (conv): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (4): InvertedResidual(\n        (conv): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (2): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (5): InvertedResidual(\n        (conv): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (6): InvertedResidual(\n        (conv): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (7): InvertedResidual(\n        (conv): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (8): InvertedResidual(\n        (conv): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (9): InvertedResidual(\n        (conv): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (10): InvertedResidual(\n        (conv): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (11): InvertedResidual(\n        (conv): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (12): InvertedResidual(\n        (conv): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (13): InvertedResidual(\n        (conv): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (14): InvertedResidual(\n        (conv): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)\n            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (2): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (15): InvertedResidual(\n        (conv): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (16): InvertedResidual(\n        (conv): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (17): InvertedResidual(\n        (conv): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (2): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (18): Conv2dNormActivation(\n        (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU6(inplace=True)\n      )\n    )\n    (classifier): Sequential(\n      (0): Linear(in_features=1280, out_features=1024, bias=True)\n      (1): ReLU(inplace=True)\n      (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (3): Linear(in_features=1024, out_features=512, bias=True)\n      (4): ReLU(inplace=True)\n      (5): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (6): Linear(in_features=512, out_features=10, bias=True)\n      (7): LogSoftmax(dim=1)\n    )\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"def pred_fp16(Model,Testloader):\n    all_labels = []\n    all_predictions = []\n    correct = 0\n    total = 0\n    start_time = time()\n    with torch.no_grad():\n        Model.eval()\n        for images, labels in Testloader:\n            all_labels.extend(labels.numpy())\n            images, labels = images.to(device), labels.to(device)\n            outputs = Model(images.half())\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n            predicted_tensor_cpu = predicted.to('cpu')\n            all_predictions.extend(predicted_tensor_cpu.numpy())\n    end_time = time()\n    print(\"Time: \",end_time - start_time)\n    print('Accuracy achieved by the network on test images is: %d%%' % (100 * correct / total))\n    \n    return all_labels,all_predictions","metadata":{"execution":{"iopub.status.busy":"2024-04-07T12:04:32.343834Z","iopub.execute_input":"2024-04-07T12:04:32.344186Z","iopub.status.idle":"2024-04-07T12:04:32.352260Z","shell.execute_reply.started":"2024-04-07T12:04:32.344155Z","shell.execute_reply":"2024-04-07T12:04:32.351345Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"labels_fp16,predictions_fp16 = pred_fp16(model_fp16,testloader)","metadata":{"execution":{"iopub.status.busy":"2024-04-07T12:04:32.353743Z","iopub.execute_input":"2024-04-07T12:04:32.354010Z","iopub.status.idle":"2024-04-07T12:04:52.879607Z","shell.execute_reply.started":"2024-04-07T12:04:32.353988Z","shell.execute_reply":"2024-04-07T12:04:52.878658Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"Time:  20.51145911216736\nAccuracy achieved by the network on test images is: 85%\n","output_type":"stream"}]},{"cell_type":"code","source":"metrics(labels_fp16,predictions_fp16)","metadata":{"execution":{"iopub.status.busy":"2024-04-07T12:04:52.880937Z","iopub.execute_input":"2024-04-07T12:04:52.881329Z","iopub.status.idle":"2024-04-07T12:04:53.576493Z","shell.execute_reply.started":"2024-04-07T12:04:52.881270Z","shell.execute_reply":"2024-04-07T12:04:53.575523Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"Confusion Matrix:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 800x600 with 2 Axes>","image/png":"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"},"metadata":{"image/png":{"width":793,"height":689}}},{"name":"stdout","text":"----------------------------------------------------------------\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.83      0.90      0.86       635\n           1       0.94      0.95      0.94       571\n           2       0.76      0.93      0.84       597\n           3       0.88      0.58      0.70       493\n           4       0.88      0.90      0.89       492\n           5       0.87      0.82      0.84       400\n           6       0.90      0.66      0.76       517\n           7       0.89      0.97      0.92       618\n           8       0.74      0.83      0.78       490\n           9       0.94      0.96      0.95       587\n\n    accuracy                           0.86      5400\n   macro avg       0.86      0.85      0.85      5400\nweighted avg       0.86      0.86      0.85      5400\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## INT-8","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.ao.quantization import (\n  get_default_qconfig_mapping,\n  get_default_qat_qconfig_mapping,\n  QConfigMapping,\n)\nimport torch.ao.quantization.quantize_fx as quantize_fx","metadata":{"execution":{"iopub.status.busy":"2024-04-07T12:04:53.583553Z","iopub.execute_input":"2024-04-07T12:04:53.583826Z","iopub.status.idle":"2024-04-07T12:04:53.644993Z","shell.execute_reply.started":"2024-04-07T12:04:53.583804Z","shell.execute_reply":"2024-04-07T12:04:53.644273Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"### PTQ","metadata":{}},{"cell_type":"code","source":"model.to('cpu')","metadata":{"execution":{"iopub.status.busy":"2024-04-07T12:04:53.645972Z","iopub.execute_input":"2024-04-07T12:04:53.646233Z","iopub.status.idle":"2024-04-07T12:04:53.672859Z","shell.execute_reply.started":"2024-04-07T12:04:53.646210Z","shell.execute_reply":"2024-04-07T12:04:53.671964Z"},"trusted":true},"execution_count":29,"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"CustomMobileNetv2(\n  (mnet): MobileNetV2(\n    (features): Sequential(\n      (0): Conv2dNormActivation(\n        (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU6(inplace=True)\n      )\n      (1): InvertedResidual(\n        (conv): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (2): InvertedResidual(\n        (conv): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (3): InvertedResidual(\n        (conv): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (4): InvertedResidual(\n        (conv): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (2): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (5): InvertedResidual(\n        (conv): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (6): InvertedResidual(\n        (conv): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (7): InvertedResidual(\n        (conv): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (8): InvertedResidual(\n        (conv): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (9): InvertedResidual(\n        (conv): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (10): InvertedResidual(\n        (conv): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (11): InvertedResidual(\n        (conv): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (12): InvertedResidual(\n        (conv): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (13): InvertedResidual(\n        (conv): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (14): InvertedResidual(\n        (conv): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)\n            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (2): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (15): InvertedResidual(\n        (conv): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (16): InvertedResidual(\n        (conv): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (17): InvertedResidual(\n        (conv): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (2): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (18): Conv2dNormActivation(\n        (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU6(inplace=True)\n      )\n    )\n    (classifier): Sequential(\n      (0): Linear(in_features=1280, out_features=1024, bias=True)\n      (1): ReLU(inplace=True)\n      (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (3): Linear(in_features=1024, out_features=512, bias=True)\n      (4): ReLU(inplace=True)\n      (5): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (6): Linear(in_features=512, out_features=10, bias=True)\n      (7): LogSoftmax(dim=1)\n    )\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"input_data = next(iter(trainloader))[0] \ncalibrate_data = input_data.to(\"cpu\")","metadata":{"execution":{"iopub.status.busy":"2024-04-07T12:04:53.673871Z","iopub.execute_input":"2024-04-07T12:04:53.674139Z","iopub.status.idle":"2024-04-07T12:04:53.817089Z","shell.execute_reply.started":"2024-04-07T12:04:53.674116Z","shell.execute_reply":"2024-04-07T12:04:53.816251Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"model_int8 = copy.deepcopy(model)\n\nqconfig_mapping = get_default_qconfig_mapping(\"x86\")\nmodel_int8.eval()\n# prepare\nmodel_prepared = quantize_fx.prepare_fx(model_int8, qconfig_mapping, calibrate_data)\n# calibrate","metadata":{"execution":{"iopub.status.busy":"2024-04-07T12:04:53.818349Z","iopub.execute_input":"2024-04-07T12:04:53.818790Z","iopub.status.idle":"2024-04-07T12:04:54.834778Z","shell.execute_reply.started":"2024-04-07T12:04:53.818757Z","shell.execute_reply":"2024-04-07T12:04:54.833719Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/overrides.py:110: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n  torch.has_cuda,\n/opt/conda/lib/python3.10/site-packages/torch/overrides.py:111: UserWarning: 'has_cudnn' is deprecated, please use 'torch.backends.cudnn.is_available()'\n  torch.has_cudnn,\n/opt/conda/lib/python3.10/site-packages/torch/overrides.py:117: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n  torch.has_mps,\n/opt/conda/lib/python3.10/site-packages/torch/overrides.py:118: UserWarning: 'has_mkldnn' is deprecated, please use 'torch.backends.mkldnn.is_available()'\n  torch.has_mkldnn,\n/opt/conda/lib/python3.10/site-packages/torch/ao/quantization/observer.py:214: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"with torch.no_grad():\n    for i in range(20):\n        batch = next(iter(trainloader))[0]\n        output = model_prepared(batch.to('cpu'))","metadata":{"execution":{"iopub.status.busy":"2024-04-07T12:04:54.836132Z","iopub.execute_input":"2024-04-07T12:04:54.836877Z","iopub.status.idle":"2024-04-07T12:07:48.538997Z","shell.execute_reply.started":"2024-04-07T12:04:54.836841Z","shell.execute_reply":"2024-04-07T12:07:48.537944Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"model_quantized_static = quantize_fx.convert_fx(model_prepared)\nmodel_quantized_static.state_dict()","metadata":{"execution":{"iopub.status.busy":"2024-04-07T12:07:48.540334Z","iopub.execute_input":"2024-04-07T12:07:48.540652Z","iopub.status.idle":"2024-04-07T12:07:51.881513Z","shell.execute_reply.started":"2024-04-07T12:07:48.540626Z","shell.execute_reply":"2024-04-07T12:07:51.880654Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":33,"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"OrderedDict([('mnet_features_0_0_input_scale_0', tensor(0.0346)),\n             ('mnet_features_0_0_input_zero_point_0', tensor(51)),\n             ('mnet_features_3_scale_0', tensor(0.0937)),\n             ('mnet_features_3_zero_point_0', tensor(56)),\n             ('mnet_features_5_scale_0', tensor(0.0620)),\n             ('mnet_features_5_zero_point_0', tensor(62)),\n             ('mnet_features_6_scale_0', tensor(0.0736)),\n             ('mnet_features_6_zero_point_0', tensor(62)),\n             ('mnet_features_8_scale_0', tensor(0.0449)),\n             ('mnet_features_8_zero_point_0', tensor(65)),\n             ('mnet_features_9_scale_0', tensor(0.0472)),\n             ('mnet_features_9_zero_point_0', tensor(65)),\n             ('mnet_features_10_scale_0', tensor(0.0679)),\n             ('mnet_features_10_zero_point_0', tensor(69)),\n             ('mnet_features_12_scale_0', tensor(0.0571)),\n             ('mnet_features_12_zero_point_0', tensor(56)),\n             ('mnet_features_13_scale_0', tensor(0.1035)),\n             ('mnet_features_13_zero_point_0', tensor(63)),\n             ('mnet_features_15_scale_0', tensor(0.2005)),\n             ('mnet_features_15_zero_point_0', tensor(51)),\n             ('mnet_features_16_scale_0', tensor(0.4075)),\n             ('mnet_features_16_zero_point_0', tensor(66)),\n             ('mnet_classifier_2_scale_0', tensor(0.0679)),\n             ('mnet_classifier_2_zero_point_0', tensor(44)),\n             ('mnet_classifier_5_scale_0', tensor(0.1815)),\n             ('mnet_classifier_5_zero_point_0', tensor(4)),\n             ('mnet.features.0.0.weight',\n              tensor([[[[ 4.4009e-03, -1.4670e-03,  4.9877e-03],\n                        [ 1.0953e-02, -8.5084e-03,  2.2493e-03],\n                        [ 3.5207e-03, -1.2420e-02, -4.8899e-03]],\n              \n                       [[ 2.6405e-03, -1.9559e-03,  4.9877e-03],\n                        [ 6.6502e-03, -1.0953e-02, -6.8458e-04],\n                        [ 3.8141e-03, -1.1051e-02, -2.6405e-03]],\n              \n                       [[-8.4106e-03, -6.7480e-03, -3.3251e-03],\n                        [-3.7163e-03, -9.7797e-03, -5.0855e-03],\n                        [-8.8996e-03, -1.1149e-02, -8.1172e-03]]],\n              \n              \n                      [[[-1.6543e-01, -6.0156e-02,  6.0156e-02],\n                        [ 3.3086e-01,  9.7753e-01,  1.0527e-01],\n                        [-4.8125e-01, -7.2187e-01, -1.0527e-01]],\n              \n                       [[-1.6543e-01,  1.3535e-01,  0.0000e+00],\n                        [ 6.7675e-01,  1.9099e+00,  2.5566e-01],\n                        [-8.5722e-01, -1.5641e+00, -1.9551e-01]],\n              \n                       [[-4.5117e-02, -3.0078e-02,  6.0156e-02],\n                        [ 1.0527e-01,  4.3613e-01,  0.0000e+00],\n                        [-1.6543e-01, -4.0605e-01,  0.0000e+00]]],\n              \n              \n                      [[[-7.5091e-02,  6.6747e-02, -8.3434e-03],\n                        [-2.9202e-01,  5.3398e-01, -2.2527e-01],\n                        [-4.8392e-01,  6.6747e-01, -2.1693e-01]],\n              \n                       [[-8.3434e-02,  1.3349e-01, -3.3374e-02],\n                        [-5.9238e-01,  9.6783e-01, -3.8380e-01],\n                        [-7.5925e-01,  1.0596e+00, -3.4208e-01]],\n              \n                       [[-1.6687e-02, -1.6687e-02,  2.5030e-02],\n                        [-1.4184e-01,  2.6699e-01, -1.2515e-01],\n                        [-2.0024e-01,  3.0871e-01, -1.1681e-01]]],\n              \n              \n                      [[[ 0.0000e+00, -1.4224e-01,  2.6670e-01],\n                        [ 1.9558e-01,  6.4008e-01, -9.2456e-01],\n                        [ 7.1120e-02,  9.9568e-01, -1.0846e+00]],\n              \n                       [[-3.5560e-02, -2.1336e-01,  2.6670e-01],\n                        [ 1.6002e-01,  1.6713e+00, -1.9914e+00],\n                        [ 1.6002e-01,  2.0803e+00, -2.2758e+00]],\n              \n                       [[-5.3340e-02, -8.8900e-02,  8.8900e-02],\n                        [ 1.4224e-01,  2.8448e-01, -2.1336e-01],\n                        [ 0.0000e+00,  2.8448e-01, -3.9116e-01]]],\n              \n              \n                      [[[-8.7918e-03,  3.6201e-03,  3.6201e-03],\n                        [ 1.0343e-03, -1.0343e-02, -9.8261e-03],\n                        [ 1.4998e-02, -1.3963e-02, -3.9822e-02]],\n              \n                       [[ 2.5858e-03,  4.6545e-03, -1.5515e-03],\n                        [ 9.8261e-03, -2.6893e-02, -3.3099e-02],\n                        [ 2.5858e-02, -2.7410e-02, -6.6197e-02]],\n              \n                       [[ 0.0000e+00,  3.1030e-03, -4.6545e-03],\n                        [-1.0343e-03, -7.7575e-03, -7.7575e-03],\n                        [ 3.1030e-03, -7.2403e-03, -1.3963e-02]]],\n              \n              \n                      [[[ 2.5582e-02,  2.7287e-02, -3.9225e-02],\n                        [ 2.0465e-02,  1.0488e-01,  4.0078e-02],\n                        [-2.5582e-02,  1.0062e-01,  1.0830e-01]],\n              \n                       [[ 1.7054e-03, -2.2171e-02,  8.5272e-04],\n                        [-5.9690e-03, -4.0931e-02, -2.3876e-02],\n                        [-3.4109e-03, -3.2403e-02, -2.3876e-02]],\n              \n                       [[-2.3023e-02, -1.0233e-02,  3.8372e-02],\n                        [-5.9690e-03, -6.4807e-02, -1.8760e-02],\n                        [ 2.7287e-02, -6.3101e-02, -7.7598e-02]]],\n              \n              \n                      [[[-1.1466e-02, -1.1466e-02,  0.0000e+00],\n                        [ 2.3696e-01, -7.2616e-02, -1.1083e-01],\n                        [ 6.4972e-02, -6.4972e-02, -4.2041e-02]],\n              \n                       [[-3.4397e-02, -7.6438e-03,  1.1466e-02],\n                        [ 4.8538e-01, -1.4523e-01, -2.4460e-01],\n                        [ 1.2994e-01, -1.2994e-01, -4.9685e-02]],\n              \n                       [[ 7.6438e-03,  7.6438e-03,  0.0000e+00],\n                        [ 1.1848e-01, -3.8219e-02, -6.1150e-02],\n                        [ 3.0575e-02, -2.6753e-02, -1.1466e-02]]],\n              \n              \n                      [[[ 4.1948e-03, -4.1948e-03, -2.1813e-02],\n                        [-2.5169e-03,  1.5940e-02,  4.0270e-02],\n                        [-2.5169e-03,  2.7686e-02,  6.6278e-02]],\n              \n                       [[-3.3558e-03, -1.7618e-02, -3.2720e-02],\n                        [ 1.6779e-03,  3.4397e-02,  6.7117e-02],\n                        [ 8.3896e-04,  6.2922e-02,  1.0655e-01]],\n              \n                       [[-1.6779e-03, -4.1948e-03, -7.5507e-03],\n                        [ 8.3896e-04,  3.3558e-03,  1.9296e-02],\n                        [ 3.3558e-03,  1.3423e-02,  2.5169e-02]]],\n              \n              \n                      [[[ 3.1920e-02,  2.6600e-02, -7.9801e-02],\n                        [-1.1172e-01,  4.3093e-01, -3.2452e-01],\n                        [ 1.5960e-02,  4.5221e-01, -4.3093e-01]],\n              \n                       [[-2.1280e-02,  1.0640e-01, -1.2768e-01],\n                        [-7.9801e-02,  6.7565e-01, -5.7457e-01],\n                        [ 5.3201e-03,  6.5437e-01, -6.1181e-01]],\n              \n                       [[ 3.1920e-02, -1.0640e-02, -5.3201e-03],\n                        [-7.4481e-02,  2.5004e-01, -2.0748e-01],\n                        [ 5.3201e-03,  2.5004e-01, -2.4472e-01]]],\n              \n              \n                      [[[ 3.2331e-05,  1.0669e-03,  1.2124e-03],\n                        [ 3.8797e-04,  1.1316e-04,  7.1127e-04],\n                        [ 1.7782e-04, -2.4248e-04,  3.7180e-04]],\n              \n                       [[ 5.8195e-04,  7.9210e-04,  4.5263e-04],\n                        [ 5.8195e-04,  5.1729e-04,  8.7293e-04],\n                        [ 4.6879e-04,  1.9398e-04,  1.0184e-03]],\n              \n                       [[ 2.0530e-03,  1.6165e-03,  1.5842e-03],\n                        [ 1.1477e-03,  1.1316e-03,  1.9075e-03],\n                        [ 7.2744e-04,  9.5375e-04,  7.2744e-04]]],\n              \n              \n                      [[[-5.2633e-02, -1.5551e-01, -8.8519e-02],\n                        [-2.1292e-01, -2.3206e-01, -8.1342e-02],\n                        [ 2.3924e-02, -7.1772e-03, -5.9810e-02]],\n              \n                       [[ 2.2249e-01,  2.6077e-01,  2.6316e-01],\n                        [ 5.0240e-02,  1.3397e-01,  2.3924e-01],\n                        [ 2.3206e-01,  3.0384e-01,  1.6986e-01]],\n              \n                       [[ 2.3924e-02, -1.3876e-01, -2.2010e-01],\n                        [-2.2249e-01, -3.0623e-01, -2.7513e-01],\n                        [-4.3063e-02, -6.9380e-02, -1.6268e-01]]],\n              \n              \n                      [[[-1.0408e-02,  2.6021e-03,  6.3194e-03],\n                        [ 3.3456e-03,  1.0408e-02, -2.2304e-03],\n                        [-9.6650e-03,  3.3084e-02,  2.5278e-02]],\n              \n                       [[ 3.7173e-04,  5.9477e-03, -1.8587e-03],\n                        [ 7.4346e-04,  1.5241e-02, -6.6912e-03],\n                        [-1.4126e-02,  4.7210e-02,  4.6838e-02]],\n              \n                       [[-1.1152e-03, -3.7173e-04,  0.0000e+00],\n                        [ 2.6021e-03,  2.6021e-03, -4.0890e-03],\n                        [-1.2267e-02,  1.8958e-02,  1.4498e-02]]],\n              \n              \n                      [[[-2.4030e-01, -4.7284e-01, -1.2402e-01],\n                        [ 3.7207e-01,  4.9609e-01,  1.3177e-01],\n                        [-7.7515e-02, -6.9763e-02,  3.8757e-02]],\n              \n                       [[-4.6509e-01, -9.7668e-01, -3.1781e-01],\n                        [ 7.5189e-01,  9.8443e-01,  2.4805e-01],\n                        [-1.7828e-01, -1.5503e-01, -2.3254e-02]],\n              \n                       [[-8.5266e-02, -1.8603e-01, -4.6509e-02],\n                        [ 1.6278e-01,  2.0929e-01,  3.8757e-02],\n                        [-3.8757e-02, -4.6509e-02,  3.1006e-02]]],\n              \n              \n                      [[[ 1.8325e-01,  6.9809e-03,  3.4905e-03],\n                        [ 2.9669e-02, -2.7924e-02,  0.0000e+00],\n                        [ 6.9809e-03,  0.0000e+00, -8.7262e-03]],\n              \n                       [[ 2.2164e-01,  3.1414e-02,  1.5707e-02],\n                        [ 3.4905e-03, -1.9198e-02,  2.2688e-02],\n                        [-1.9198e-02,  0.0000e+00, -1.3962e-02]],\n              \n                       [[ 1.0297e-01, -3.1414e-02, -1.7452e-02],\n                        [-5.5847e-02, -3.1414e-02, -1.7452e-03],\n                        [ 6.9809e-03,  4.0140e-02, -1.3962e-02]]],\n              \n              \n                      [[[ 1.4963e-03,  5.5417e-04,  1.2377e-03],\n                        [ 3.6945e-04,  2.3460e-03,  9.2362e-05],\n                        [ 1.9211e-03,  1.2192e-03,  1.6071e-03]],\n              \n                       [[-7.3890e-05,  6.6501e-04, -2.5861e-04],\n                        [ 5.1723e-04, -2.5861e-04, -1.6625e-04],\n                        [ 1.4593e-03,  5.3570e-04, -1.6625e-04]],\n              \n                       [[ 8.3126e-04, -2.4014e-04, -1.6625e-04],\n                        [ 6.6501e-04,  5.5417e-05, -3.3250e-04],\n                        [ 7.3890e-04,  1.2746e-03, -1.1453e-03]]],\n              \n              \n                      [[[ 1.0269e-03,  1.6181e-03,  1.8048e-03],\n                        [ 1.9759e-03,  1.3380e-03,  1.3380e-03],\n                        [ 1.7892e-03,  3.1117e-04,  8.7127e-04]],\n              \n                       [[-6.6901e-04, -4.8231e-04, -7.7792e-04],\n                        [-5.7566e-04,  1.4003e-04,  4.9787e-04],\n                        [ 2.0226e-04,  2.8005e-04,  2.8005e-04]],\n              \n                       [[ 4.0452e-04, -4.6675e-04, -2.8005e-04],\n                        [ 1.5558e-04,  2.1782e-04,  1.5558e-05],\n                        [ 7.3124e-04,  7.1569e-04, -1.0891e-04]]],\n              \n              \n                      [[[-5.6199e-05,  1.1240e-04,  1.0116e-03],\n                        [ 2.8099e-04,  2.2479e-03,  5.0579e-03],\n                        [ 1.6860e-04, -6.1819e-04,  3.3719e-04]],\n              \n                       [[-5.6199e-04, -4.4959e-04,  1.7984e-03],\n                        [-3.3719e-04,  3.5967e-03,  7.1372e-03],\n                        [-1.1240e-03, -5.6199e-04, -5.6199e-05]],\n              \n                       [[ 3.3719e-04, -1.1802e-03, -2.8099e-04],\n                        [-2.8099e-04,  7.3058e-04,  4.3835e-03],\n                        [ 0.0000e+00, -1.7422e-03, -1.6860e-03]]],\n              \n              \n                      [[[ 1.5872e-04,  3.1744e-04,  2.9760e-04],\n                        [-1.9840e-04, -5.6544e-04,  2.9760e-05],\n                        [-3.1744e-04, -3.3728e-04,  4.9600e-05]],\n              \n                       [[-6.5472e-04, -3.6704e-04, -2.4800e-04],\n                        [-6.8448e-04, -1.1309e-03, -3.9680e-05],\n                        [-1.0218e-03, -7.8368e-04, -1.2698e-03]],\n              \n                       [[ 3.9680e-04,  1.0912e-04,  9.9200e-05],\n                        [ 1.1904e-04, -4.3648e-04,  5.9520e-05],\n                        [-3.4720e-04, -2.9760e-04, -1.3888e-04]]],\n              \n              \n                      [[[ 8.0743e-03, -1.8840e-03, -5.3829e-03],\n                        [-3.7680e-03,  5.3829e-04,  4.8446e-03],\n                        [-8.0743e-03,  1.0766e-02,  2.5299e-02]],\n              \n                       [[ 4.0371e-03, -3.7680e-03, -1.4803e-02],\n                        [-2.1531e-03,  1.5610e-02,  4.5754e-03],\n                        [-8.8817e-03,  2.9337e-02,  3.4181e-02]],\n              \n                       [[ 2.1531e-03,  1.0766e-03, -5.9211e-03],\n                        [-2.4223e-03, -3.2297e-03,  3.4989e-03],\n                        [-1.6149e-03,  1.8840e-03,  2.0455e-02]]],\n              \n              \n                      [[[ 4.8774e-03, -8.2916e-02, -1.0243e-01],\n                        [-8.6168e-02, -1.8534e-01, -2.0810e-01],\n                        [-4.5523e-02, -1.1381e-01, -1.3007e-01]],\n              \n                       [[ 4.8774e-02,  4.7149e-02,  3.7394e-02],\n                        [ 3.4142e-02,  2.9265e-02,  2.4387e-02],\n                        [ 5.0400e-02,  3.4142e-02,  1.9510e-02]],\n              \n                       [[-4.5523e-02,  3.9020e-02,  4.5523e-02],\n                        [ 3.4142e-02,  1.6746e-01,  1.7884e-01],\n                        [ 6.5033e-03,  1.0730e-01,  9.5923e-02]]],\n              \n              \n                      [[[-8.4671e-03, -1.6652e-01,  1.4112e-03],\n                        [ 9.8782e-03, -8.4671e-02, -4.2335e-03],\n                        [ 5.2214e-02,  4.7980e-02,  6.3503e-02]],\n              \n                       [[-1.1148e-01, -1.8063e-01, -6.9148e-02],\n                        [-4.6569e-02, -4.7980e-02, -4.0924e-02],\n                        [-6.0681e-02,  1.5523e-02, -2.6812e-02]],\n              \n                       [[ 1.0302e-01, -7.4792e-02,  6.7737e-02],\n                        [ 3.1046e-02, -6.6325e-02, -1.4112e-03],\n                        [ 5.6447e-03,  1.1289e-02,  1.9756e-02]]],\n              \n              \n                      [[[-3.9240e-02, -2.4852e-02,  7.8480e-02],\n                        [ 7.0632e-02, -8.7636e-02, -1.4388e-02],\n                        [-3.4008e-02,  1.8312e-02, -2.4852e-02]],\n              \n                       [[-3.2700e-02, -4.5780e-02,  1.3603e-01],\n                        [ 8.5020e-02, -1.6742e-01, -1.9620e-02],\n                        [-3.9240e-02, -2.6160e-03, -4.3164e-02]],\n              \n                       [[-2.4852e-02,  1.3080e-03,  3.0084e-02],\n                        [ 4.5780e-02, -4.9704e-02,  6.5400e-03],\n                        [-3.1392e-02,  3.9240e-02, -2.8776e-02]]],\n              \n              \n                      [[[ 5.3999e-02,  1.5369e-01,  1.2461e-01],\n                        [ 2.6584e-01,  2.8661e-01,  1.4954e-01],\n                        [-1.1215e-01, -1.6615e-02,  2.2430e-01]],\n              \n                       [[-2.0769e-01, -3.4061e-01, -2.6169e-01],\n                        [-9.5536e-02, -3.4061e-01, -4.3199e-01],\n                        [-2.6584e-01, -4.1122e-01, -2.5338e-01]],\n              \n                       [[-7.8921e-02,  2.0769e-01,  2.4507e-01],\n                        [ 3.4061e-01,  5.2753e-01,  3.3645e-01],\n                        [ 9.1383e-02,  1.3292e-01,  1.4538e-01]]],\n              \n              \n                      [[[ 6.5987e-04,  2.3096e-04,  3.2994e-05],\n                        [ 3.6293e-04,  1.1548e-04, -6.1038e-04],\n                        [ 1.4847e-04, -1.0063e-03, -1.6497e-03]],\n              \n                       [[ 1.2208e-03,  6.7637e-04,  1.4847e-04],\n                        [ 1.5837e-03,  9.0732e-04,  3.9592e-04],\n                        [ 2.3096e-04,  1.8146e-04, -2.6395e-04]],\n              \n                       [[ 1.1878e-03,  1.3197e-03,  4.7841e-04],\n                        [ 2.0951e-03,  7.5885e-04, -1.6497e-05],\n                        [ 0.0000e+00, -3.2994e-05, -1.2373e-03]]],\n              \n              \n                      [[[ 4.4960e-02,  5.1383e-02,  1.0705e-02],\n                        [-8.5638e-03, -6.2088e-02, -8.9920e-02],\n                        [-2.5691e-02,  6.4229e-03,  8.1356e-02]],\n              \n                       [[ 9.6343e-02, -6.4229e-03, -1.4773e-01],\n                        [ 4.4960e-02, -1.1561e-01, -2.0125e-01],\n                        [ 4.4960e-02,  6.4229e-02,  1.4344e-01]],\n              \n                       [[-1.1561e-01, -1.1989e-01, -2.7404e-01],\n                        [-1.7128e-02, -7.4933e-02, -1.7984e-01],\n                        [-8.5638e-02,  8.5638e-03,  2.3550e-02]]],\n              \n              \n                      [[[ 1.9587e-02,  4.8968e-02, -2.9381e-02],\n                        [-1.4690e-01,  4.0154e-01,  7.3452e-01],\n                        [ 1.1752e-01, -4.1133e-01, -6.7576e-01]],\n              \n                       [[ 3.9175e-02,  7.8349e-02,  1.9587e-02],\n                        [-2.4484e-01,  6.7576e-01,  1.2438e+00],\n                        [ 2.0567e-01, -7.7370e-01, -1.2242e+00]],\n              \n                       [[ 4.8968e-02, -2.9381e-02, -2.9381e-02],\n                        [-8.8143e-02,  1.9587e-01,  3.6237e-01],\n                        [ 6.8556e-02, -2.3505e-01, -2.8402e-01]]],\n              \n              \n                      [[[-3.3516e-03,  2.2344e-02, -2.4579e-02],\n                        [ 1.7875e-02, -3.5751e-02, -4.0219e-02],\n                        [ 3.3516e-03, -2.5696e-02, -1.0055e-01]],\n              \n                       [[ 2.4579e-02,  1.2289e-02, -6.7032e-03],\n                        [ 2.6813e-02, -1.0613e-01, -8.7142e-02],\n                        [ 5.6977e-02, -6.9267e-02, -1.4300e-01]],\n              \n                       [[-2.0110e-02,  2.7930e-02,  3.7985e-02],\n                        [ 4.4688e-03, -1.1172e-03,  2.2344e-03],\n                        [ 4.4688e-03,  6.7032e-03, -6.7032e-02]]],\n              \n              \n                      [[[ 1.3865e-01,  4.4859e-02, -2.4468e-02],\n                        [-1.5497e-01, -8.5640e-02,  9.7874e-02],\n                        [ 1.4273e-01,  1.4681e-01,  1.6312e-02]],\n              \n                       [[ 1.5089e-01, -1.2642e-01, -1.1826e-01],\n                        [-4.6898e-01, -5.2199e-01, -1.8759e-01],\n                        [ 1.6720e-01,  1.1011e-01, -8.1561e-02]],\n              \n                       [[ 7.3405e-02,  4.8937e-02,  8.1561e-03],\n                        [-1.0195e-01,  8.1561e-03,  7.7483e-02],\n                        [ 9.7874e-02,  1.4681e-01, -3.2625e-02]]],\n              \n              \n                      [[[ 2.2140e-02,  5.6200e-02,  1.3795e-01],\n                        [ 6.8122e-02,  6.4716e-02,  1.1240e-01],\n                        [ 2.8952e-02, -1.2432e-01, -5.2794e-02]],\n              \n                       [[-2.3843e-02, -4.4279e-02, -2.0437e-02],\n                        [ 3.5764e-02, -6.1310e-02, -8.5152e-03],\n                        [ 8.0043e-02, -1.0559e-01,  5.1091e-02]],\n              \n                       [[-8.1746e-02, -6.6419e-02, -8.8558e-02],\n                        [-9.0261e-02, -2.0437e-01, -1.8563e-01],\n                        [-3.2358e-02, -2.1799e-01, -8.8558e-02]]],\n              \n              \n                      [[[ 1.6112e-02, -8.0561e-03,  1.6112e-02],\n                        [ 4.8337e-02, -6.2435e-02, -1.1279e-01],\n                        [ 1.8328e-01, -4.2295e-02, -1.9335e-01]],\n              \n                       [[ 5.8407e-02,  2.2154e-02,  6.4449e-02],\n                        [ 4.2295e-02, -1.4702e-01, -2.0342e-01],\n                        [ 2.1147e-01, -8.8618e-02, -2.5780e-01]],\n              \n                       [[ 1.0070e-02,  3.0211e-02,  1.0070e-01],\n                        [ 7.6533e-02, -3.2225e-02, -5.0351e-02],\n                        [ 1.9939e-01, -7.8547e-02, -2.2759e-01]]],\n              \n              \n                      [[[-9.3704e-02,  1.4576e-02,  1.1036e-01],\n                        [ 8.9539e-02,  2.5404e-01,  2.6445e-01],\n                        [ 1.3535e-01,  1.8116e-01,  6.0387e-02]],\n              \n                       [[-8.3292e-03, -4.9975e-02, -5.4140e-02],\n                        [-5.8304e-02, -1.1453e-01, -1.3743e-01],\n                        [-7.2881e-02, -1.1869e-01, -1.1869e-01]],\n              \n                       [[ 8.7457e-02,  3.1235e-02, -5.4140e-02],\n                        [ 0.0000e+00, -7.0798e-02, -9.5786e-02],\n                        [-8.9539e-02, -7.2881e-02,  4.1646e-02]]],\n              \n              \n                      [[[ 5.2686e-04,  4.2149e-03, -7.9029e-03],\n                        [-2.2128e-02,  5.2686e-04,  3.3719e-02],\n                        [-8.9566e-03,  7.9029e-03,  5.1105e-02]],\n              \n                       [[ 6.3223e-03,  4.2149e-03, -6.3223e-03],\n                        [-4.2149e-03,  1.4225e-02,  4.8471e-02],\n                        [ 1.0537e-02,  2.1074e-02,  6.6911e-02]],\n              \n                       [[-6.3223e-03, -1.0537e-02, -3.5826e-02],\n                        [-1.4225e-02,  6.8492e-03,  1.6860e-02],\n                        [-2.1074e-03,  1.1591e-02,  3.0031e-02]]]], size=(32, 3, 3, 3),\n                     dtype=torch.qint8, quantization_scheme=torch.per_channel_affine,\n                     scale=tensor([9.7797e-05, 1.5039e-02, 8.3434e-03, 1.7780e-02, 5.1716e-04, 8.5272e-04,\n                      3.8219e-03, 8.3896e-04, 5.3201e-03, 1.6165e-05, 2.3924e-03, 3.7173e-04,\n                      7.7515e-03, 1.7452e-03, 1.8472e-05, 1.5558e-05, 5.6199e-05, 9.9200e-06,\n                      2.6914e-04, 1.6258e-03, 1.4112e-03, 1.3080e-03, 4.1538e-03, 1.6497e-05,\n                      2.1410e-03, 9.7937e-03, 1.1172e-03, 4.0781e-03, 1.7030e-03, 2.0140e-03,\n                      2.0823e-03, 5.2686e-04], dtype=torch.float64),\n                     zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0]),\n                     axis=0)),\n             ('mnet.features.0.0.bias',\n              Parameter containing:\n              tensor([-0.0886,  0.6104,  0.3289,  0.2440,  0.8963,  0.8255,  0.4903,  0.7134,\n                       0.6324,  0.0039, -0.2074,  1.0237,  0.4431, -0.1801,  0.0067,  0.0130,\n                      -0.0333, -0.0205,  0.3781, -0.3765, -0.5035,  0.4215,  0.1957, -0.0149,\n                      -0.4929,  0.5490, -0.5096,  0.4666,  0.0499, -0.2649,  0.7650,  0.3588],\n                     requires_grad=True)),\n             ('mnet.features.0.0.scale', tensor(0.0720)),\n             ('mnet.features.0.0.zero_point', tensor(44)),\n             ('mnet.features.1.conv.0.0.weight',\n              tensor([[[[-7.2174e-02, -9.1858e-02, -7.2174e-02],\n                        [-1.5091e-01,  3.2807e-02,  8.3329e-01],\n                        [-8.5297e-02, -6.5613e-02,  5.9052e-02]]],\n              \n              \n                      [[[ 8.0657e-02, -2.8230e-01,  4.8394e-01],\n                        [ 0.0000e+00, -9.2755e-01,  4.8797e+00],\n                        [ 2.4197e-01,  6.4525e-01, -5.1620e+00]]],\n              \n              \n                      [[[ 0.0000e+00, -2.6038e-01, -1.3887e+00],\n                        [ 8.6795e-02,  5.2077e-01,  1.1023e+01],\n                        [-8.6795e-02, -6.0756e-01, -3.4718e-01]]],\n              \n              \n                      [[[-4.8365e-01, -3.5468e-01, -4.5141e-01],\n                        [ 3.2244e-01,  4.0949e+00, -1.4832e+00],\n                        [-1.9346e-01, -1.9346e-01, -9.6731e-01]]],\n              \n              \n                      [[[ 1.5109e+00,  4.8781e+00,  1.1224e+00],\n                        [ 5.6119e-01,  1.0792e+00,  4.3169e-01],\n                        [-2.2016e+00, -5.5256e+00, -1.9426e+00]]],\n              \n              \n                      [[[-1.4259e-01,  4.2778e-01, -6.1790e-01],\n                        [ 6.1790e-01,  1.1407e+00, -6.0839e+00],\n                        [ 0.0000e+00,  4.0876e+00,  8.0802e-01]]],\n              \n              \n                      [[[ 0.0000e+00,  1.2085e+00, -1.2085e+00],\n                        [ 2.0142e-01,  0.0000e+00, -1.1481e+01],\n                        [-2.0142e-01, -8.0567e-01,  1.2790e+01]]],\n              \n              \n                      [[[-1.1681e+00, -6.0944e-01,  1.8791e+00],\n                        [-5.7897e+00, -7.6180e-01,  6.4499e+00],\n                        [-1.1681e+00, -4.0630e-01,  1.5744e+00]]],\n              \n              \n                      [[[-1.6550e-01,  7.1718e-01, -6.0685e-01],\n                        [ 1.1034e-01, -7.0063e+00,  1.1585e+00],\n                        [ 5.5168e-02,  1.6550e-01, -2.2067e-01]]],\n              \n              \n                      [[[-6.4355e-03, -1.8274e-03, -2.0657e-03],\n                        [-4.2109e-03,  2.0657e-03, -5.5615e-04],\n                        [-1.0090e-02, -4.5287e-03, -2.1452e-03]]],\n              \n              \n                      [[[ 9.9000e-01, -1.0560e+00, -4.2240e+00],\n                        [-1.6500e+00,  1.4850e+00,  3.4980e+00],\n                        [-3.9270e+00,  2.4090e+00,  3.7620e+00]]],\n              \n              \n                      [[[ 5.3799e-01,  4.5550e+00,  2.8693e+00],\n                        [ 2.5106e-01,  2.1520e-01, -3.5866e-01],\n                        [-9.3252e-01, -4.5191e+00, -2.7617e+00]]],\n              \n              \n                      [[[-9.9505e-02,  2.9852e-01, -1.9901e-01],\n                        [ 4.9753e-02,  1.9901e-01, -5.0748e+00],\n                        [-2.4876e-01,  1.4926e-01,  6.3186e+00]]],\n              \n              \n                      [[[-4.8428e-01,  0.0000e+00, -8.3649e-01],\n                        [ 8.8051e-02,  2.6415e-01, -9.2454e-01],\n                        [-9.2454e-01, -8.8051e-01,  5.5913e+00]]],\n              \n              \n                      [[[-3.7483e-04, -1.0174e-03,  2.6238e-03],\n                        [-6.8541e-03, -8.0321e-04, -6.4257e-04],\n                        [-6.1044e-03, -3.8554e-03, -3.5341e-03]]],\n              \n              \n                      [[[ 2.0814e-03, -6.1748e-03, -5.2035e-03],\n                        [-2.2895e-03, -7.0073e-03, -8.8806e-03],\n                        [-1.2488e-03, -7.1461e-03, -7.8399e-03]]],\n              \n              \n                      [[[-3.4942e-02, -2.6623e-02, -1.9967e-02],\n                        [ 1.2479e-02,  2.3295e-02, -2.6623e-02],\n                        [-1.6639e-03,  1.0566e-01, -1.0815e-02]]],\n              \n              \n                      [[[-7.1216e-03,  9.1303e-04,  1.6252e-02],\n                        [-2.3374e-02, -1.1687e-02,  1.3148e-02],\n                        [-9.4955e-03, -3.6521e-03,  1.2782e-02]]],\n              \n              \n                      [[[ 1.9210e+00, -7.5360e+00,  2.0687e+00],\n                        [ 1.3299e+01, -1.8914e+01,  1.0196e+01],\n                        [-1.7732e+00,  2.0687e+00, -1.4777e+00]]],\n              \n              \n                      [[[ 5.3752e-01, -7.5253e-01, -2.0963e+00],\n                        [-1.9082e+00,  1.3438e+00,  3.4133e+00],\n                        [-7.7941e-01,  4.8377e-01,  4.5689e-01]]],\n              \n              \n                      [[[-3.7792e-01,  7.5583e-02, -1.5872e+00],\n                        [-3.7792e-01, -6.8025e-01, -5.2908e-01],\n                        [-1.4361e+00, -1.2093e+00,  9.5991e+00]]],\n              \n              \n                      [[[ 7.0474e-02, -1.1746e-01, -1.1746e-01],\n                        [-2.3491e-01, -1.4799e+00, -7.0474e-01],\n                        [ 3.9935e-01,  2.9834e+00, -2.5370e+00]]],\n              \n              \n                      [[[ 2.6578e-01, -1.8862e-01, -1.0202e+00],\n                        [-4.3725e-01,  1.2860e-01,  1.0374e+00],\n                        [-7.2017e-01,  2.5721e-01,  1.0888e+00]]],\n              \n              \n                      [[[-4.8907e-03, -2.4453e-03, -7.4525e-03],\n                        [-1.0480e-03, -3.3769e-03, -9.6649e-03],\n                        [-1.4905e-02, -6.9867e-04,  1.4556e-02]]],\n              \n              \n                      [[[ 2.6758e-01,  1.7202e+00,  2.2171e+00],\n                        [ 9.1742e-01, -3.3257e+00, -8.4097e-01],\n                        [ 3.2492e+00, -1.4908e+00, -4.8929e+00]]],\n              \n              \n                      [[[-5.1234e-01,  1.0247e-01,  3.0740e-01],\n                        [ 4.0987e-01,  1.3013e+01, -3.7913e+00],\n                        [-1.0247e+00, -8.1974e-01, -1.8444e+00]]],\n              \n              \n                      [[[-2.4661e-01, -7.3983e-01, -7.3983e-01],\n                        [ 2.0962e+00,  1.5660e+01, -6.6585e+00],\n                        [-2.4661e-01, -2.0962e+00, -4.9322e-01]]],\n              \n              \n                      [[[ 9.9622e-02, -4.9811e-02, -1.3324e+00],\n                        [-3.7358e-01, -2.4906e-01,  1.5317e+00],\n                        [-7.5962e-01,  2.6151e-01,  1.5815e+00]]],\n              \n              \n                      [[[ 1.8557e-01,  3.0929e-01,  1.4227e+00],\n                        [ 5.7734e-01, -2.6805e-01, -2.6393e+00],\n                        [ 7.4230e-01, -2.6805e-01, -1.0310e+00]]],\n              \n              \n                      [[[ 9.6856e-02, -1.7526e-01, -3.2285e-02],\n                        [-1.0608e-01, -5.9036e-01, -2.8134e-01],\n                        [-5.5346e-02, -2.3983e-01, -2.2600e-01]]],\n              \n              \n                      [[[ 2.2882e-01, -4.4559e-01, -1.0116e+00],\n                        [-5.5398e-01, -8.4301e-02,  1.5295e+00],\n                        [-5.6602e-01,  5.4194e-01,  7.4667e-01]]],\n              \n              \n                      [[[ 1.3194e+00, -5.5822e-01, -9.6420e-01],\n                        [ 6.4449e+00, -4.5673e-01, -5.8360e+00],\n                        [ 1.2687e+00, -3.5523e-01, -8.1196e-01]]]], size=(32, 1, 3, 3),\n                     dtype=torch.qint8, quantization_scheme=torch.per_channel_affine,\n                     scale=tensor([6.5613e-03, 4.0328e-02, 8.6795e-02, 3.2244e-02, 4.3169e-02, 4.7531e-02,\n                      1.0071e-01, 5.0787e-02, 5.5168e-02, 7.9450e-05, 3.3000e-02, 3.5866e-02,\n                      4.9753e-02, 4.4026e-02, 5.3547e-05, 6.9380e-05, 8.3196e-04, 1.8261e-04,\n                      1.4777e-01, 2.6876e-02, 7.5583e-02, 2.3491e-02, 8.5735e-03, 1.1644e-04,\n                      3.8226e-02, 1.0247e-01, 1.2331e-01, 1.2453e-02, 2.0619e-02, 4.6122e-03,\n                      1.2043e-02, 5.0748e-02], dtype=torch.float64),\n                     zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0]),\n                     axis=0)),\n             ('mnet.features.1.conv.0.0.bias',\n              Parameter containing:\n              tensor([-1.1896e-02,  9.3624e-01, -1.7982e+00,  3.5723e-01,  2.6031e-01,\n                       8.1770e-01,  8.0415e-01,  4.1184e-01,  4.1695e+00, -4.6797e-03,\n                       2.9699e-01,  3.3220e-01,  4.4953e-01, -4.1597e-01, -5.6763e-03,\n                      -5.5587e-03,  8.3294e-03,  1.3609e-02,  2.2775e-02, -3.5948e-03,\n                      -3.0620e-01,  1.7185e+00,  8.2919e-01,  1.2252e-03,  7.5875e-01,\n                      -1.3282e+00, -3.6581e-01,  7.4491e-01,  6.8431e-01,  3.1922e-01,\n                       9.3604e-01, -2.0726e-01], requires_grad=True)),\n             ('mnet.features.1.conv.0.0.scale', tensor(0.2013)),\n             ('mnet.features.1.conv.0.0.zero_point', tensor(70)),\n             ('mnet.features.1.conv.1.weight',\n              tensor([[[[-0.0108]],\n              \n                       [[ 0.0649]],\n              \n                       [[-0.0649]],\n              \n                       [[ 0.1677]],\n              \n                       [[-0.0162]],\n              \n                       [[ 0.6872]],\n              \n                       [[ 0.2219]],\n              \n                       [[-0.0216]],\n              \n                       [[-0.0487]],\n              \n                       [[ 0.0054]],\n              \n                       [[ 0.0000]],\n              \n                       [[ 0.2597]],\n              \n                       [[ 0.0758]],\n              \n                       [[-0.1136]],\n              \n                       [[ 0.0000]],\n              \n                       [[-0.0054]],\n              \n                       [[-0.0054]],\n              \n                       [[ 0.0000]],\n              \n                       [[ 0.4329]],\n              \n                       [[-0.3247]],\n              \n                       [[-0.1948]],\n              \n                       [[-0.0054]],\n              \n                       [[-0.0812]],\n              \n                       [[-0.0054]],\n              \n                       [[ 0.1569]],\n              \n                       [[ 0.2597]],\n              \n                       [[ 0.0649]],\n              \n                       [[ 0.0325]],\n              \n                       [[ 0.0325]],\n              \n                       [[ 0.0812]],\n              \n                       [[ 0.2706]],\n              \n                       [[ 0.1786]]],\n              \n              \n                      [[[ 0.0321]],\n              \n                       [[ 0.0321]],\n              \n                       [[-1.0191]],\n              \n                       [[-0.1685]],\n              \n                       [[ 0.1765]],\n              \n                       [[-0.0241]],\n              \n                       [[-0.1043]],\n              \n                       [[ 0.1525]],\n              \n                       [[ 0.1926]],\n              \n                       [[ 0.0000]],\n              \n                       [[ 0.2488]],\n              \n                       [[-0.0642]],\n              \n                       [[-0.2167]],\n              \n                       [[ 0.3691]],\n              \n                       [[ 0.0000]],\n              \n                       [[ 0.0000]],\n              \n                       [[ 0.0080]],\n              \n                       [[ 0.0000]],\n              \n                       [[ 0.2327]],\n              \n                       [[ 0.7463]],\n              \n                       [[ 0.3852]],\n              \n                       [[-0.0401]],\n              \n                       [[-0.1926]],\n              \n                       [[ 0.0000]],\n              \n                       [[-0.2327]],\n              \n                       [[ 0.1685]],\n              \n                       [[-0.3691]],\n              \n                       [[ 0.5697]],\n              \n                       [[-0.2407]],\n              \n                       [[-0.0642]],\n              \n                       [[ 0.4253]],\n              \n                       [[-0.1685]]],\n              \n              \n                      [[[-0.0213]],\n              \n                       [[ 0.9024]],\n              \n                       [[ 0.1776]],\n              \n                       [[ 0.6608]],\n              \n                       [[-0.3979]],\n              \n                       [[-0.3411]],\n              \n                       [[-0.4477]],\n              \n                       [[ 0.1066]],\n              \n                       [[-0.2700]],\n              \n                       [[ 0.0000]],\n              \n                       [[-0.0568]],\n              \n                       [[ 0.5258]],\n              \n                       [[ 0.1350]],\n              \n                       [[-0.0568]],\n              \n                       [[ 0.0071]],\n              \n                       [[ 0.0071]],\n              \n                       [[ 0.0142]],\n              \n                       [[ 0.0071]],\n              \n                       [[ 0.0995]],\n              \n                       [[ 0.0782]],\n              \n                       [[ 0.0213]],\n              \n                       [[ 0.8811]],\n              \n                       [[ 0.1208]],\n              \n                       [[ 0.0000]],\n              \n                       [[ 0.0000]],\n              \n                       [[-0.5400]],\n              \n                       [[-0.0711]],\n              \n                       [[ 0.7745]],\n              \n                       [[ 0.1279]],\n              \n                       [[ 0.0284]],\n              \n                       [[-0.2061]],\n              \n                       [[ 0.0000]]],\n              \n              \n                      [[[ 0.0375]],\n              \n                       [[-0.0833]],\n              \n                       [[-0.1499]],\n              \n                       [[ 0.1415]],\n              \n                       [[ 0.5121]],\n              \n                       [[-0.5287]],\n              \n                       [[ 0.0583]],\n              \n                       [[ 0.0749]],\n              \n                       [[-0.3164]],\n              \n                       [[ 0.0000]],\n              \n                       [[ 0.0375]],\n              \n                       [[-0.4913]],\n              \n                       [[ 0.2873]],\n              \n                       [[ 0.3622]],\n              \n                       [[ 0.0042]],\n              \n                       [[ 0.0000]],\n              \n                       [[ 0.0083]],\n              \n                       [[ 0.0125]],\n              \n                       [[-0.1124]],\n              \n                       [[-0.4122]],\n              \n                       [[-0.3455]],\n              \n                       [[ 0.4663]],\n              \n                       [[-0.1166]],\n              \n                       [[ 0.0000]],\n              \n                       [[-0.1707]],\n              \n                       [[ 0.0291]],\n              \n                       [[-0.4455]],\n              \n                       [[-0.4746]],\n              \n                       [[-0.3081]],\n              \n                       [[ 0.3497]],\n              \n                       [[ 0.0125]],\n              \n                       [[-0.0666]]],\n              \n              \n                      [[[-0.0411]],\n              \n                       [[-0.7291]],\n              \n                       [[ 0.5648]],\n              \n                       [[-0.0719]],\n              \n                       [[ 0.3697]],\n              \n                       [[-1.3144]],\n              \n                       [[ 0.1438]],\n              \n                       [[-0.6161]],\n              \n                       [[ 0.3491]],\n              \n                       [[-0.0205]],\n              \n                       [[ 0.3081]],\n              \n                       [[-0.1951]],\n              \n                       [[-0.4313]],\n              \n                       [[-0.0821]],\n              \n                       [[-0.0103]],\n              \n                       [[ 0.0205]],\n              \n                       [[-0.0103]],\n              \n                       [[ 0.0000]],\n              \n                       [[ 0.8112]],\n              \n                       [[ 0.2670]],\n              \n                       [[-0.3697]],\n              \n                       [[ 0.6366]],\n              \n                       [[-0.4210]],\n              \n                       [[ 0.0000]],\n              \n                       [[ 0.1232]],\n              \n                       [[ 0.1232]],\n              \n                       [[ 0.4005]],\n              \n                       [[ 0.0719]],\n              \n                       [[ 0.1438]],\n              \n                       [[-0.0719]],\n              \n                       [[ 0.8626]],\n              \n                       [[ 0.6880]]],\n              \n              \n                      [[[ 0.0000]],\n              \n                       [[-0.2349]],\n              \n                       [[-0.0267]],\n              \n                       [[-0.6725]],\n              \n                       [[-0.3843]],\n              \n                       [[-0.2936]],\n              \n                       [[ 0.5124]],\n              \n                       [[ 0.5498]],\n              \n                       [[ 0.5284]],\n              \n                       [[ 0.0053]],\n              \n                       [[ 0.0587]],\n              \n                       [[ 0.5391]],\n              \n                       [[-0.0480]],\n              \n                       [[ 0.0854]],\n              \n                       [[ 0.0000]],\n              \n                       [[ 0.0053]],\n              \n                       [[-0.0107]],\n              \n                       [[-0.0053]],\n              \n                       [[ 0.0160]],\n              \n                       [[-0.0747]],\n              \n                       [[-0.1761]],\n              \n                       [[ 0.4750]],\n              \n                       [[-0.1014]],\n              \n                       [[-0.0053]],\n              \n                       [[ 0.1334]],\n              \n                       [[ 0.1815]],\n              \n                       [[-0.1334]],\n              \n                       [[ 0.3256]],\n              \n                       [[ 0.0000]],\n              \n                       [[ 0.0801]],\n              \n                       [[-0.6832]],\n              \n                       [[-0.0694]]],\n              \n              \n                      [[[ 0.0000]],\n              \n                       [[ 0.4188]],\n              \n                       [[-0.1224]],\n              \n                       [[-0.3802]],\n              \n                       [[-0.4575]],\n              \n                       [[ 0.0193]],\n              \n                       [[ 0.0451]],\n              \n                       [[-0.8247]],\n              \n                       [[ 0.1869]],\n              \n                       [[ 0.0000]],\n              \n                       [[-0.0709]],\n              \n                       [[ 0.1933]],\n              \n                       [[-0.1224]],\n              \n                       [[ 0.1675]],\n              \n                       [[ 0.0000]],\n              \n                       [[ 0.0000]],\n              \n                       [[ 0.0064]],\n              \n                       [[ 0.0064]],\n              \n                       [[-0.2771]],\n              \n                       [[ 0.0387]],\n              \n                       [[ 0.0000]],\n              \n                       [[ 0.3415]],\n              \n                       [[ 0.1482]],\n              \n                       [[-0.0064]],\n              \n                       [[ 0.1095]],\n              \n                       [[ 0.1933]],\n              \n                       [[-0.2577]],\n              \n                       [[ 0.0580]],\n              \n                       [[ 0.0966]],\n              \n                       [[ 0.2191]],\n              \n                       [[ 0.1675]],\n              \n                       [[ 0.2062]]],\n              \n              \n                      [[[-0.0813]],\n              \n                       [[ 0.1708]],\n              \n                       [[ 0.4067]],\n              \n                       [[-0.0813]],\n              \n                       [[ 0.8948]],\n              \n                       [[ 0.9761]],\n              \n                       [[-0.3823]],\n              \n                       [[ 0.3335]],\n              \n                       [[-0.2034]],\n              \n                       [[-0.0081]],\n              \n                       [[ 0.3010]],\n              \n                       [[-1.0412]],\n              \n                       [[-0.4149]],\n              \n                       [[ 0.0000]],\n              \n                       [[ 0.0000]],\n              \n                       [[ 0.0000]],\n              \n                       [[ 0.0000]],\n              \n                       [[ 0.0000]],\n              \n                       [[-0.3172]],\n              \n                       [[ 0.6507]],\n              \n                       [[-0.2847]],\n              \n                       [[ 0.9273]],\n              \n                       [[-0.4067]],\n              \n                       [[-0.0081]],\n              \n                       [[ 0.5369]],\n              \n                       [[ 0.0732]],\n              \n                       [[ 0.1057]],\n              \n                       [[ 0.5287]],\n              \n                       [[ 0.7077]],\n              \n                       [[ 0.1627]],\n              \n                       [[-0.3010]],\n              \n                       [[-0.2928]]],\n              \n              \n                      [[[ 0.0618]],\n              \n                       [[-0.0412]],\n              \n                       [[ 0.5302]],\n              \n                       [[-0.2574]],\n              \n                       [[ 0.1699]],\n              \n                       [[ 0.5302]],\n              \n                       [[-0.1750]],\n              \n                       [[-0.2008]],\n              \n                       [[ 0.2625]],\n              \n                       [[ 0.0154]],\n              \n                       [[-0.0051]],\n              \n                       [[-0.0412]],\n              \n                       [[ 0.4273]],\n              \n                       [[ 0.3294]],\n              \n                       [[-0.0103]],\n              \n                       [[ 0.0051]],\n              \n                       [[ 0.0257]],\n              \n                       [[-0.0051]],\n              \n                       [[ 0.0000]],\n              \n                       [[ 0.2008]],\n              \n                       [[ 0.0721]],\n              \n                       [[ 0.4221]],\n              \n                       [[ 0.0875]],\n              \n                       [[ 0.0206]],\n              \n                       [[-0.6280]],\n              \n                       [[ 0.0154]],\n              \n                       [[-0.2213]],\n              \n                       [[ 0.0154]],\n              \n                       [[-0.6589]],\n              \n                       [[-0.0669]],\n              \n                       [[-0.2883]],\n              \n                       [[ 0.4170]]],\n              \n              \n                      [[[-0.0141]],\n              \n                       [[-0.3944]],\n              \n                       [[-0.0563]],\n              \n                       [[-0.3099]],\n              \n                       [[-0.0141]],\n              \n                       [[ 0.0634]],\n              \n                       [[-0.1268]],\n              \n                       [[ 0.0986]],\n              \n                       [[ 0.2817]],\n              \n                       [[ 0.0000]],\n              \n                       [[-0.0282]],\n              \n                       [[ 0.0563]],\n              \n                       [[ 0.6198]],\n              \n                       [[-0.0352]],\n              \n                       [[ 0.0070]],\n              \n                       [[-0.0070]],\n              \n                       [[ 0.0141]],\n              \n                       [[ 0.0000]],\n              \n                       [[-0.1268]],\n              \n                       [[-0.0986]],\n              \n                       [[-0.1338]],\n              \n                       [[ 0.3029]],\n              \n                       [[ 0.0634]],\n              \n                       [[ 0.0070]],\n              \n                       [[ 0.1197]],\n              \n                       [[-0.2536]],\n              \n                       [[ 0.1197]],\n              \n                       [[ 0.4578]],\n              \n                       [[ 0.2043]],\n              \n                       [[ 0.0634]],\n              \n                       [[ 0.8946]],\n              \n                       [[-0.1620]]],\n              \n              \n                      [[[ 0.0167]],\n              \n                       [[ 0.1757]],\n              \n                       [[-0.1088]],\n              \n                       [[-0.1088]],\n              \n                       [[-0.0669]],\n              \n                       [[-0.0418]],\n              \n                       [[-0.1590]],\n              \n                       [[-0.2175]],\n              \n                       [[ 0.0920]],\n              \n                       [[ 0.0000]],\n              \n                       [[ 0.5355]],\n              \n                       [[ 0.0669]],\n              \n                       [[ 0.1088]],\n              \n                       [[-0.1590]],\n              \n                       [[ 0.0000]],\n              \n                       [[ 0.0000]],\n              \n                       [[ 0.0000]],\n              \n                       [[ 0.0000]],\n              \n                       [[-0.0669]],\n              \n                       [[-0.0837]],\n              \n                       [[-0.0335]],\n              \n                       [[-0.0753]],\n              \n                       [[-1.0710]],\n              \n                       [[ 0.0000]],\n              \n                       [[-0.2343]],\n              \n                       [[-0.1673]],\n              \n                       [[ 0.1841]],\n              \n                       [[ 0.1004]],\n              \n                       [[-0.0586]],\n              \n                       [[-0.1673]],\n              \n                       [[-0.0753]],\n              \n                       [[ 0.0586]]],\n              \n              \n                      [[[ 0.0154]],\n              \n                       [[-0.2768]],\n              \n                       [[ 0.4766]],\n              \n                       [[ 0.2306]],\n              \n                       [[-0.1461]],\n              \n                       [[-0.3306]],\n              \n                       [[-0.3383]],\n              \n                       [[ 0.1999]],\n              \n                       [[-0.9764]],\n              \n                       [[-0.0077]],\n              \n                       [[ 0.0231]],\n              \n                       [[ 0.0692]],\n              \n                       [[-0.4459]],\n              \n                       [[-0.0461]],\n              \n                       [[-0.0077]],\n              \n                       [[ 0.0077]],\n              \n                       [[-0.0231]],\n              \n                       [[ 0.0077]],\n              \n                       [[-0.5766]],\n              \n                       [[-0.3921]],\n              \n                       [[ 0.1768]],\n              \n                       [[ 0.1922]],\n              \n                       [[-0.1461]],\n              \n                       [[ 0.0000]],\n              \n                       [[-0.2306]],\n              \n                       [[ 0.4075]],\n              \n                       [[-0.0999]],\n              \n                       [[ 0.8687]],\n              \n                       [[-0.3075]],\n              \n                       [[-0.1538]],\n              \n                       [[ 0.3844]],\n              \n                       [[-0.1461]]],\n              \n              \n                      [[[-0.0054]],\n              \n                       [[ 0.6858]],\n              \n                       [[-0.0162]],\n              \n                       [[-0.1890]],\n              \n                       [[ 0.1404]],\n              \n                       [[-0.5400]],\n              \n                       [[-0.5670]],\n              \n                       [[ 0.2214]],\n              \n                       [[ 0.4374]],\n              \n                       [[ 0.0000]],\n              \n                       [[-0.1134]],\n              \n                       [[-0.4914]],\n              \n                       [[ 0.5508]],\n              \n                       [[-0.1512]],\n              \n                       [[ 0.0000]],\n              \n                       [[-0.0054]],\n              \n                       [[ 0.0000]],\n              \n                       [[-0.0108]],\n              \n                       [[ 0.1620]],\n              \n                       [[-0.1836]],\n              \n                       [[ 0.0702]],\n              \n                       [[-0.5832]],\n              \n                       [[ 0.1512]],\n              \n                       [[ 0.0054]],\n              \n                       [[ 0.1242]],\n              \n                       [[ 0.4698]],\n              \n                       [[ 0.1782]],\n              \n                       [[-0.2592]],\n              \n                       [[ 0.0918]],\n              \n                       [[-0.0486]],\n              \n                       [[ 0.2214]],\n              \n                       [[-0.0486]]],\n              \n              \n                      [[[-0.0292]],\n              \n                       [[-0.4374]],\n              \n                       [[-0.0583]],\n              \n                       [[ 0.0408]],\n              \n                       [[ 0.1925]],\n              \n                       [[-0.1108]],\n              \n                       [[ 0.1866]],\n              \n                       [[-0.5540]],\n              \n                       [[-0.4607]],\n              \n                       [[ 0.0000]],\n              \n                       [[-0.0642]],\n              \n                       [[ 0.0350]],\n              \n                       [[ 0.6999]],\n              \n                       [[-0.2158]],\n              \n                       [[ 0.0000]],\n              \n                       [[ 0.0000]],\n              \n                       [[-0.0117]],\n              \n                       [[ 0.0117]],\n              \n                       [[-0.0233]],\n              \n                       [[ 0.2858]],\n              \n                       [[ 0.2449]],\n              \n                       [[-0.3208]],\n              \n                       [[ 0.1983]],\n              \n                       [[ 0.0000]],\n              \n                       [[ 0.2391]],\n              \n                       [[ 0.2508]],\n              \n                       [[ 0.0233]],\n              \n                       [[ 0.2916]],\n              \n                       [[ 0.3791]],\n              \n                       [[-0.1225]],\n              \n                       [[-0.7465]],\n              \n                       [[ 0.3033]]],\n              \n              \n                      [[[-0.0123]],\n              \n                       [[-0.7143]],\n              \n                       [[-0.0616]],\n              \n                       [[ 0.3695]],\n              \n                       [[-0.8744]],\n              \n                       [[ 0.3202]],\n              \n                       [[-1.5765]],\n              \n                       [[ 0.1108]],\n              \n                       [[-0.0246]],\n              \n                       [[ 0.0000]],\n              \n                       [[ 0.1232]],\n              \n                       [[ 0.6651]],\n              \n                       [[ 0.0246]],\n              \n                       [[ 0.0616]],\n              \n                       [[ 0.0000]],\n              \n                       [[-0.0123]],\n              \n                       [[-0.0123]],\n              \n                       [[ 0.0000]],\n              \n                       [[ 0.2586]],\n              \n                       [[ 0.2710]],\n              \n                       [[-0.2094]],\n              \n                       [[ 0.0493]],\n              \n                       [[-0.1232]],\n              \n                       [[-0.0123]],\n              \n                       [[ 0.1478]],\n              \n                       [[ 0.1232]],\n              \n                       [[-0.0123]],\n              \n                       [[-0.8005]],\n              \n                       [[ 0.1108]],\n              \n                       [[ 0.0985]],\n              \n                       [[-0.1724]],\n              \n                       [[-0.1971]]],\n              \n              \n                      [[[ 0.0000]],\n              \n                       [[-0.3866]],\n              \n                       [[-0.4031]],\n              \n                       [[-0.2715]],\n              \n                       [[ 0.5429]],\n              \n                       [[-0.0329]],\n              \n                       [[-0.7897]],\n              \n                       [[-0.3373]],\n              \n                       [[ 0.6252]],\n              \n                       [[ 0.0000]],\n              \n                       [[-0.3702]],\n              \n                       [[-0.4360]],\n              \n                       [[-0.5676]],\n              \n                       [[-0.1892]],\n              \n                       [[ 0.0000]],\n              \n                       [[ 0.0000]],\n              \n                       [[-0.0082]],\n              \n                       [[ 0.0082]],\n              \n                       [[ 0.3620]],\n              \n                       [[-0.6828]],\n              \n                       [[ 0.0658]],\n              \n                       [[ 0.0576]],\n              \n                       [[ 0.5018]],\n              \n                       [[ 0.0000]],\n              \n                       [[ 0.0905]],\n              \n                       [[-0.3291]],\n              \n                       [[-0.1152]],\n              \n                       [[ 1.0448]],\n              \n                       [[-0.0329]],\n              \n                       [[-0.0411]],\n              \n                       [[-0.6581]],\n              \n                       [[ 0.1892]]]], size=(16, 32, 1, 1), dtype=torch.qint8,\n                     quantization_scheme=torch.per_channel_affine,\n                     scale=tensor([0.0054, 0.0080, 0.0071, 0.0042, 0.0103, 0.0053, 0.0064, 0.0081, 0.0051,\n                      0.0070, 0.0084, 0.0077, 0.0054, 0.0058, 0.0123, 0.0082],\n                     dtype=torch.float64),\n                     zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n                     axis=0)),\n             ('mnet.features.1.conv.1.bias',\n              Parameter containing:\n              tensor([-1.9232,  0.6055, -1.4134,  1.1110, -0.1196, -0.5535, -0.7871, -2.7955,\n                      -0.9210, -1.8194,  1.6527, -0.5937, -0.8047, -0.2236,  2.5575,  1.9895],\n                     requires_grad=True)),\n             ('mnet.features.1.conv.1.scale', tensor(0.0956)),\n             ('mnet.features.1.conv.1.zero_point', tensor(63)),\n             ('mnet.features.2.conv.0.0.weight',\n              tensor([[[[ 0.1238]],\n              \n                       [[ 0.0211]],\n              \n                       [[ 0.1054]],\n              \n                       ...,\n              \n                       [[-0.1133]],\n              \n                       [[-0.1502]],\n              \n                       [[-0.1844]]],\n              \n              \n                      [[[ 0.0397]],\n              \n                       [[ 0.0568]],\n              \n                       [[ 0.0965]],\n              \n                       ...,\n              \n                       [[ 0.0549]],\n              \n                       [[ 0.0057]],\n              \n                       [[-0.0738]]],\n              \n              \n                      [[[-0.0235]],\n              \n                       [[-0.0178]],\n              \n                       [[-0.0470]],\n              \n                       ...,\n              \n                       [[-0.0437]],\n              \n                       [[-0.0518]],\n              \n                       [[ 0.0300]]],\n              \n              \n                      ...,\n              \n              \n                      [[[ 0.1399]],\n              \n                       [[ 0.0610]],\n              \n                       [[ 0.1112]],\n              \n                       ...,\n              \n                       [[-0.1256]],\n              \n                       [[ 0.0933]],\n              \n                       [[-0.0969]]],\n              \n              \n                      [[[ 0.1495]],\n              \n                       [[ 0.0740]],\n              \n                       [[-0.0584]],\n              \n                       ...,\n              \n                       [[-0.0883]],\n              \n                       [[-0.0171]],\n              \n                       [[-0.0740]]],\n              \n              \n                      [[[-0.0597]],\n              \n                       [[-0.0059]],\n              \n                       [[-0.0641]],\n              \n                       ...,\n              \n                       [[-0.0066]],\n              \n                       [[-0.0170]],\n              \n                       [[ 0.0096]]]], size=(96, 16, 1, 1), dtype=torch.qint8,\n                     quantization_scheme=torch.per_channel_affine,\n                     scale=tensor([0.0026, 0.0019, 0.0008, 0.0017, 0.0012, 0.0014, 0.0013, 0.0011, 0.0030,\n                      0.0009, 0.0027, 0.0019, 0.0004, 0.0006, 0.0007, 0.0025, 0.0015, 0.0022,\n                      0.0027, 0.0016, 0.0019, 0.0011, 0.0006, 0.0007, 0.0012, 0.0036, 0.0010,\n                      0.0028, 0.0014, 0.0009, 0.0031, 0.0005, 0.0009, 0.0018, 0.0032, 0.0012,\n                      0.0025, 0.0019, 0.0007, 0.0020, 0.0038, 0.0016, 0.0009, 0.0007, 0.0011,\n                      0.0020, 0.0018, 0.0008, 0.0011, 0.0006, 0.0024, 0.0032, 0.0036, 0.0012,\n                      0.0033, 0.0008, 0.0032, 0.0028, 0.0008, 0.0016, 0.0038, 0.0022, 0.0008,\n                      0.0007, 0.0033, 0.0011, 0.0008, 0.0022, 0.0014, 0.0010, 0.0012, 0.0033,\n                      0.0002, 0.0021, 0.0007, 0.0014, 0.0019, 0.0021, 0.0010, 0.0020, 0.0037,\n                      0.0022, 0.0017, 0.0027, 0.0041, 0.0038, 0.0007, 0.0043, 0.0039, 0.0009,\n                      0.0009, 0.0039, 0.0037, 0.0036, 0.0014, 0.0007], dtype=torch.float64),\n                     zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n                     axis=0)),\n             ('mnet.features.2.conv.0.0.bias',\n              Parameter containing:\n              tensor([ 1.5737e-02, -1.2232e-02,  2.4170e-01,  3.6565e-01,  2.7892e-01,\n                       1.6961e-01,  8.5801e-02,  6.8653e-02, -4.7720e-02,  1.4869e-01,\n                      -5.9781e-02,  5.1204e-02,  2.0294e-01,  2.2977e-01,  2.0617e-01,\n                       1.2868e-02, -4.7106e-02,  3.0441e-01,  8.2332e-02,  3.4853e-01,\n                       4.1094e-01,  1.4778e-01,  2.7136e-01,  2.9415e-01,  8.0139e-02,\n                      -1.3988e-03,  9.6997e-02, -3.3425e-01,  2.7630e-01,  2.4107e-01,\n                      -8.9783e-03,  2.3318e-01,  1.6361e-01,  2.1910e-02, -7.5205e-03,\n                       1.7180e-01,  1.1713e-01,  9.4551e-02,  2.7588e-01,  7.8572e-02,\n                       2.7520e-02,  2.1859e-01,  2.7045e-01,  2.3398e-01,  1.6900e-01,\n                       3.5869e-03,  2.2214e-01,  1.7671e-01,  2.2882e-01,  1.9106e-01,\n                      -4.5695e-01, -6.7571e-02,  6.9704e-02,  7.1848e-04, -7.1883e-02,\n                       2.5025e-01, -5.7699e-03,  7.2786e-01,  2.5297e-01, -1.8483e-03,\n                       2.3125e-04, -1.2452e-02,  2.4255e-01,  2.5975e-01, -2.7695e-02,\n                       2.3329e-01,  3.4012e-01, -4.1559e-03, -5.9648e-03,  9.6649e-02,\n                       1.5315e-01,  1.7621e-01,  3.5270e-01,  3.4142e-02,  2.0475e-01,\n                       1.7198e-01,  5.7062e-02,  5.5225e-02,  3.7398e-01,  1.1676e-01,\n                      -4.1456e-02,  1.1347e-01,  3.8942e-02,  2.2272e-02,  7.7099e-02,\n                      -5.5117e-02,  2.1786e-01,  2.7154e-02,  7.1733e-02,  1.5787e-01,\n                       2.4351e-01, -7.4056e-02,  1.2476e-02, -6.7239e-02,  2.9533e-01,\n                       1.3465e-01], requires_grad=True)),\n             ('mnet.features.2.conv.0.0.scale', tensor(0.0871)),\n             ('mnet.features.2.conv.0.0.zero_point', tensor(66)),\n             ('mnet.features.2.conv.1.0.weight',\n              tensor([[[[-1.9851e-01, -3.4660e-01, -1.8590e-01],\n                        [-4.0017e-01, -4.0332e-01, -2.8358e-01],\n                        [-8.1924e-02, -2.5207e-01, -8.5075e-02]]],\n              \n              \n                      [[[-3.0501e-01, -1.5789e-01, -3.5884e-03],\n                        [-4.5931e-01, -1.7942e-01, -1.0765e-02],\n                        [-3.8037e-01, -3.4448e-01, -1.1483e-01]]],\n              \n              \n                      [[[-9.0315e-01, -3.7052e-01,  4.4000e-01],\n                        [ 1.3663e+00,  2.9410e+00,  1.3200e+00],\n                        [-1.6210e-01, -1.0884e+00, -2.8021e+00]]],\n              \n              \n                      [[[ 1.8064e-01,  3.1047e-01, -5.0804e-02],\n                        [ 2.8224e-01,  7.1690e-01,  1.6935e-02],\n                        [ 5.6448e-02,  1.9757e-01, -1.2419e-01]]],\n              \n              \n                      [[[-3.4159e-01, -1.1490e+00, -6.8318e-01],\n                        [-4.5028e-01, -5.4344e-01, -6.2108e-02],\n                        [ 6.5213e-01,  1.9719e+00,  7.9187e-01]]],\n              \n              \n                      [[[ 1.4190e-01,  1.1264e+00,  9.7559e-01],\n                        [-3.1928e-01, -3.9911e-01,  3.1928e-01],\n                        [-4.2571e-01, -8.1595e-01, -5.1440e-01]]],\n              \n              \n                      [[[-7.0926e-03, -8.5111e-02, -1.1348e-01],\n                        [ 5.9105e-02, -2.4824e-01, -3.0025e-01],\n                        [ 6.1469e-02, -2.2933e-01, -3.0025e-01]]],\n              \n              \n                      [[[-9.8232e-01,  3.9653e-01, -9.0121e-01],\n                        [-9.2825e-01,  1.0454e+00, -1.0544e+00],\n                        [-1.1536e+00,  7.2998e-01, -8.5615e-01]]],\n              \n              \n                      [[[ 1.0874e-01,  2.0942e-01,  1.5707e-01],\n                        [ 1.9734e-01,  5.1147e-01,  3.9468e-01],\n                        [ 1.0471e-01,  2.8997e-01,  2.3359e-01]]],\n              \n              \n                      [[[-1.4893e+00,  3.0549e+00, -9.9286e-01],\n                        [-4.9643e-01,  1.7184e+00, -9.9286e-01],\n                        [ 1.9857e+00, -4.8879e+00,  1.8712e+00]]],\n              \n              \n                      [[[ 8.1625e-02,  6.3881e-02,  4.2587e-02],\n                        [ 2.7682e-01,  3.9748e-01,  2.5197e-01],\n                        [ 2.4488e-01,  4.5071e-01,  2.3423e-01]]],\n              \n              \n                      [[[-1.0936e-01, -4.0681e-01, -4.6805e-01],\n                        [-6.1240e-02, -5.5553e-01, -5.2054e-01],\n                        [ 2.1871e-02, -2.6246e-02, -1.0498e-01]]],\n              \n              \n                      [[[-2.8579e+00,  3.0835e+00, -6.7687e-01],\n                        [ 6.0918e+00, -9.6266e+00,  3.2339e+00],\n                        [-3.2339e+00,  6.6935e+00, -3.0083e+00]]],\n              \n              \n                      [[[-2.1891e+00, -2.6696e-01,  1.6552e+00],\n                        [-5.1791e+00,  6.7809e+00,  7.4750e-01],\n                        [ 1.2814e+00,  4.6985e+00, -2.4561e+00]]],\n              \n              \n                      [[[-7.4197e-01, -1.5303e+00, -1.1130e+00],\n                        [ 1.5767e+00,  5.8894e+00,  1.3448e+00],\n                        [-6.0285e-01, -4.2200e+00, -1.3912e-01]]],\n              \n              \n                      [[[-1.2557e-01, -1.7938e-01, -4.4846e-02],\n                        [-3.2887e-01, -3.8268e-01, -1.7340e-01],\n                        [-1.7340e-01, -3.4980e-01, -1.6742e-01]]],\n              \n              \n                      [[[-7.6890e-01, -7.6105e-01, -2.5107e-01],\n                        [-1.4123e-01,  3.8445e-01,  7.5321e-01],\n                        [ 1.0200e-01,  5.1783e-01,  9.9643e-01]]],\n              \n              \n                      [[[ 2.2206e-01,  4.4064e-01,  1.5960e-01],\n                        [ 2.9839e-01,  2.8451e-01,  2.7757e-02],\n                        [ 1.3185e-01,  2.1859e-01,  1.0062e-01]]],\n              \n              \n                      [[[-1.9636e-02, -1.8982e-01,  3.2727e-02],\n                        [ 3.2727e-02, -8.3781e-01, -1.6364e-01],\n                        [ 3.2727e-02, -3.5345e-01,  1.2436e-01]]],\n              \n              \n                      [[[-3.8842e-02, -2.1086e-01, -1.6092e-01],\n                        [-1.9976e-01, -7.1026e-01, -5.0495e-01],\n                        [ 4.4391e-02, -4.9385e-01, -3.1074e-01]]],\n              \n              \n                      [[[ 2.1472e-01,  2.5051e-01,  2.0449e-02],\n                        [ 3.2719e-01,  6.4927e-01,  3.0674e-02],\n                        [ 2.0449e-01,  2.0961e-01, -7.6685e-02]]],\n              \n              \n                      [[[-7.8621e-01, -2.0811e-01,  1.2718e+00],\n                        [-8.7870e-01, -1.8961e+00,  2.9367e+00],\n                        [-1.6187e-01, -1.7343e+00,  1.5030e+00]]],\n              \n              \n                      [[[ 1.9491e+00, -5.0817e+00,  2.7845e+00],\n                        [-5.4993e+00,  8.8407e+00, -2.9933e+00],\n                        [ 3.2718e+00, -3.4110e+00,  6.2651e-01]]],\n              \n              \n                      [[[-8.2509e-01,  1.2001e+00, -4.5005e-01],\n                        [ 1.8002e+00, -9.5260e+00,  7.6508e+00],\n                        [-9.7510e-01,  8.2509e+00, -6.9757e+00]]],\n              \n              \n                      [[[-4.2450e-01, -7.1315e-01,  8.1503e-01],\n                        [ 4.2450e-01, -1.1207e+00, -2.1734e+00],\n                        [ 3.9054e-01,  1.8678e+00,  4.5845e-01]]],\n              \n              \n                      [[[ 3.9813e-02,  1.7120e-01,  8.3608e-02],\n                        [ 1.5129e-01,  5.0563e-01,  2.3490e-01],\n                        [ 2.2295e-01,  4.0211e-01,  1.8314e-01]]],\n              \n              \n                      [[[ 2.7970e-02, -2.2376e-01, -8.3911e-02],\n                        [ 6.4332e-01,  1.1841e+00,  6.5264e-01],\n                        [-1.1188e-01, -2.4241e-01, -2.7970e-02]]],\n              \n              \n                      [[[ 1.4704e-01,  3.6759e-01,  3.9210e-01],\n                        [ 1.3070e-01,  4.0843e-01,  5.1871e-01],\n                        [-4.4928e-02,  2.4098e-01,  2.9407e-01]]],\n              \n              \n                      [[[ 2.7817e-01,  5.1287e-01,  2.2601e-01],\n                        [ 3.3032e-01,  1.1040e+00,  4.0856e-01],\n                        [-1.8255e-01,  9.5620e-02, -4.0856e-01]]],\n              \n              \n                      [[[-1.3529e+00,  3.4093e+00, -1.3529e+00],\n                        [ 3.1387e+00, -2.6517e+00, -8.1173e-01],\n                        [ 2.3270e+00, -6.9268e+00,  4.1669e+00]]],\n              \n              \n                      [[[-1.8422e-01, -4.5841e-01, -2.8276e-01],\n                        [-4.2842e-01, -5.4838e-01, -4.1129e-01],\n                        [-1.9708e-01, -3.5559e-01, -9.4253e-02]]],\n              \n              \n                      [[[ 3.1563e+00, -1.5377e+00, -1.5377e+00],\n                        [-1.2949e+00,  1.0278e+01, -3.1563e+00],\n                        [-7.0409e+00,  1.9423e+00,  4.5321e+00]]],\n              \n              \n                      [[[-2.1855e-01,  5.1905e-01, -2.3221e-01],\n                        [-6.1467e-01,  1.7347e+00, -3.5514e-01],\n                        [-2.1855e-01,  9.2883e-01, -2.3221e-01]]],\n              \n              \n                      [[[-2.5965e-02, -3.3177e-01, -3.2312e-01],\n                        [-8.3665e-02, -3.6928e-01, -2.3080e-01],\n                        [ 0.0000e+00, -4.6160e-02, -2.0195e-02]]],\n              \n              \n                      [[[-1.5585e-01, -2.3850e-01, -1.0154e-01],\n                        [-1.7710e-01, -3.0225e-01, -1.1571e-01],\n                        [-8.2647e-02, -1.0626e-01, -8.9731e-02]]],\n              \n              \n                      [[[-1.3886e-01,  3.9133e-01, -2.2723e-01],\n                        [-4.6707e-01,  1.6032e+00, -8.0791e-01],\n                        [-4.1658e-01,  1.3760e+00, -8.0791e-01]]],\n              \n              \n                      [[[-1.2919e-01, -1.9071e-01, -9.2280e-02],\n                        [-1.1227e-01, -1.7687e-01, -2.3070e-02],\n                        [-1.8302e-01, -1.9686e-01, -8.7666e-02]]],\n              \n              \n                      [[[-6.2055e-01, -6.2055e-01,  3.3096e-01],\n                        [-1.3135e+00, -1.0342e+00, -2.2753e-01],\n                        [-9.8254e-01, -1.0342e+00, -2.0685e-02]]],\n              \n              \n                      [[[-1.5318e+00,  3.1770e+00, -1.4750e+00],\n                        [ 3.2338e+00, -7.2618e+00,  3.5174e+00],\n                        [-1.3616e+00,  2.8934e+00, -1.5885e+00]]],\n              \n              \n                      [[[ 2.0873e-02,  1.5075e-01,  3.9427e-02],\n                        [ 3.2469e-02,  2.3656e-01,  1.5539e-01],\n                        [ 7.6534e-02,  2.9454e-01,  2.3424e-01]]],\n              \n              \n                      [[[-7.6769e-02, -1.6830e-01, -2.9527e-03],\n                        [-3.3070e-01, -3.6908e-01, -1.5059e-01],\n                        [-2.2440e-01, -3.7794e-01, -9.1532e-02]]],\n              \n              \n                      [[[ 3.7934e-01,  3.1160e-01,  1.2645e-01],\n                        [ 2.8450e-01,  5.7352e-01,  7.6771e-02],\n                        [ 2.0322e-01,  3.7934e-01, -1.3999e-01]]],\n              \n              \n                      [[[-2.0689e-01, -1.2413e+00, -9.0145e-01],\n                        [ 9.9012e-01,  1.4778e-01, -1.6699e+00],\n                        [-2.2167e-01,  1.8768e+00,  5.6156e-01]]],\n              \n              \n                      [[[-2.2803e-01, -8.2440e-01,  1.0524e+00],\n                        [-2.4557e-01, -1.2980e+00,  2.2276e+00],\n                        [ 8.7702e-02, -1.1226e+00,  1.3682e+00]]],\n              \n              \n                      [[[ 7.5377e-01, -5.2079e-01, -1.5075e-01],\n                        [ 1.7405e+00, -9.1823e-01, -2.4669e-01],\n                        [ 8.2229e-01, -4.9338e-01, -2.7410e-01]]],\n              \n              \n                      [[[ 7.1319e-02,  1.4060e-01,  4.0754e-02],\n                        [ 1.2837e-01,  2.5879e-01,  2.4452e-01],\n                        [ 6.1131e-02,  1.3856e-01,  1.5690e-01]]],\n              \n              \n                      [[[-1.1822e-01, -3.1524e-01,  1.4186e-01],\n                        [-5.2803e-01, -1.0088e+00,  1.5762e-01],\n                        [-3.0736e-01, -7.8023e-01, -1.1822e-01]]],\n              \n              \n                      [[[ 6.3522e-01, -6.2614e+00,  4.2650e+00],\n                        [ 8.1671e+00, -2.3594e+00, -7.0781e+00],\n                        [-7.7134e+00,  1.1525e+01, -1.0889e+00]]],\n              \n              \n                      [[[ 1.2489e+00, -3.8555e+00, -1.6291e-01],\n                        [-5.3216e+00, -3.4210e+00,  4.6700e+00],\n                        [ 1.6291e+00,  6.8964e+00, -1.3576e+00]]],\n              \n              \n                      [[[ 4.0566e-01, -5.7951e-01, -4.0566e-01],\n                        [ 3.8248e+00, -6.5485e+00,  2.4339e+00],\n                        [ 3.8248e+00, -7.4177e+00,  3.2453e+00]]],\n              \n              \n                      [[[ 5.5375e-01,  6.2087e-01,  2.6009e-01],\n                        [ 5.9570e-01,  1.0655e+00,  7.4672e-01],\n                        [ 3.1882e-01,  6.1248e-01,  4.6985e-01]]],\n              \n              \n                      [[[-8.2328e-02, -1.9129e-01, -2.1066e-01],\n                        [-1.9129e-01, -3.0994e-01, -2.4456e-01],\n                        [-1.4286e-01, -1.8161e-01, -5.0850e-02]]],\n              \n              \n                      [[[-9.5754e-02, -1.4363e-01, -3.0641e-01],\n                        [-2.6811e-01, -2.4896e-01, -1.2257e+00],\n                        [-1.1491e-01, -1.7236e-01, -2.5854e-01]]],\n              \n              \n                      [[[ 5.4512e-01,  8.5238e-01,  1.1894e-01],\n                        [-7.2353e-01,  1.2587e+00,  1.0010e+00],\n                        [-2.6761e-01,  2.9734e-02,  5.5504e-01]]],\n              \n              \n                      [[[-1.4113e-01, -1.0585e-01,  5.5445e-02],\n                        [-3.2259e-01, -2.7218e-01,  9.5768e-02],\n                        [-1.8650e-01, -1.8650e-01,  3.0242e-02]]],\n              \n              \n                      [[[ 7.7871e-01,  3.6859e+00,  3.1148e-01],\n                        [-1.0383e+00, -6.5931e+00, -1.0383e+00],\n                        [ 4.1531e-01,  2.9591e+00,  6.7488e-01]]],\n              \n              \n                      [[[ 2.6432e-02,  1.5272e-01,  1.5566e-01],\n                        [ 2.0558e-01,  3.7299e-01,  2.7901e-01],\n                        [ 1.5859e-01,  3.3481e-01,  1.0279e-01]]],\n              \n              \n                      [[[-1.0803e-01, -2.7656e-01, -9.9390e-02],\n                        [-2.1174e-01, -5.4880e-01, -1.3828e-01],\n                        [-5.6177e-02, -1.0803e-01,  0.0000e+00]]],\n              \n              \n                      [[[-1.0245e-01, -8.4518e-01,  7.9396e-01],\n                        [ 1.1525e-01, -1.6007e+00,  1.6263e+00],\n                        [-2.5611e-02, -5.3784e-01,  7.5554e-01]]],\n              \n              \n                      [[[ 1.3769e-01,  4.3929e-01,  9.8349e-02],\n                        [ 1.4424e-01, -2.0325e-01, -3.0816e-01],\n                        [-2.0325e-01, -6.1632e-01, -8.3924e-01]]],\n              \n              \n                      [[[ 1.8699e-02,  8.0139e-02,  1.0952e-01],\n                        [ 5.6098e-02,  3.3926e-01,  2.9117e-01],\n                        [ 5.3426e-02,  1.7096e-01,  2.2706e-01]]],\n              \n              \n                      [[[ 7.0292e-02,  1.0778e-01,  8.2007e-02],\n                        [ 9.3723e-02,  2.9757e-01,  2.0619e-01],\n                        [ 6.3263e-02,  1.4058e-01,  1.2653e-01]]],\n              \n              \n                      [[[-8.9201e+00,  1.0101e+01, -1.7053e+00],\n                        [ 1.2462e+01, -1.6660e+01,  4.0665e+00],\n                        [-2.8859e+00,  5.1160e+00, -1.8365e+00]]],\n              \n              \n                      [[[ 0.0000e+00, -1.8993e-01, -7.8972e-01],\n                        [ 2.6990e-01, -2.8990e-01, -1.2795e+00],\n                        [ 4.0985e-01, -5.1981e-01, -5.7979e-01]]],\n              \n              \n                      [[[-2.3402e-01, -3.2609e-01, -1.3811e-01],\n                        [-4.0666e-01, -4.9106e-01, -2.7622e-01],\n                        [-2.1484e-01, -2.9924e-01, -8.0565e-02]]],\n              \n              \n                      [[[-1.4253e+00, -1.6768e-01,  2.9344e-01],\n                        [ 5.3238e+00,  2.4313e+00,  4.1920e-02],\n                        [-4.1920e+00, -2.3894e+00, -3.3536e-01]]],\n              \n              \n                      [[[-1.1370e+00, -2.4970e+00,  0.0000e+00],\n                        [ 5.3507e-01,  2.8314e+00,  2.8983e-01],\n                        [ 1.5606e-01, -4.0130e-01, -2.2295e-01]]],\n              \n              \n                      [[[-8.4015e-02, -2.7605e-01, -1.4402e-01],\n                        [-2.5204e-01, -4.5608e-01, -4.7208e-01],\n                        [-3.1605e-01, -4.0807e-01, -5.1209e-01]]],\n              \n              \n                      [[[ 1.9440e-01,  2.9971e-01, -4.2121e-01],\n                        [ 4.1311e-01,  3.7261e-01, -1.0368e+00],\n                        [ 8.9102e-02,  4.4551e-01, -7.3711e-01]]],\n              \n              \n                      [[[ 5.8363e-01, -4.7555e-01, -3.0262e-01],\n                        [ 1.3726e+00, -1.0592e+00, -5.6201e-01],\n                        [ 8.1059e-01, -3.3505e-01, -2.2697e-01]]],\n              \n              \n                      [[[ 2.4222e-01,  4.2140e-01,  2.3890e-01],\n                        [ 3.8822e-01,  3.8490e-01,  7.9635e-02],\n                        [ 1.0286e-01,  1.3272e-01,  0.0000e+00]]],\n              \n              \n                      [[[-7.6467e-02, -1.8161e-01, -3.8234e-02],\n                        [-2.8197e-01, -6.1174e-01, -1.3860e-01],\n                        [-2.1506e-01, -3.9189e-01, -1.5771e-01]]],\n              \n              \n                      [[[ 8.3191e-01,  2.7730e-01,  1.0399e+00],\n                        [ 3.9516e+00,  6.6553e+00,  4.1596e+00],\n                        [-4.7835e+00, -8.8737e+00, -3.9516e+00]]],\n              \n              \n                      [[[-2.6051e-01, -2.2725e-01,  1.1086e-02],\n                        [-7.0948e-01, -6.7068e-01, -7.2056e-02],\n                        [-1.4411e-01, -4.6005e-01,  1.2194e-01]]],\n              \n              \n                      [[[ 9.5040e-01, -1.2263e+00, -7.6645e-01],\n                        [-1.9468e+00, -9.5040e-01,  1.4409e+00],\n                        [ 4.9053e-01,  1.8242e+00,  4.5987e-01]]],\n              \n              \n                      [[[ 8.4829e-02, -1.0957e-01, -1.1664e-01],\n                        [-1.0957e-01, -4.5242e-01, -2.1914e-01],\n                        [-2.4035e-01, -3.3578e-01,  7.0691e-02]]],\n              \n              \n                      [[[-2.6629e-01, -5.6809e-01, -2.6629e-01],\n                        [-3.9944e-02, -5.2371e-01, -2.5298e-01],\n                        [ 2.8404e-01,  3.8169e-01,  2.1303e-01]]],\n              \n              \n                      [[[-8.9725e-02, -9.2620e-02, -1.0999e-01],\n                        [-3.1838e-02, -6.8018e-02, -1.8524e-01],\n                        [-1.0999e-01, -1.5630e-01, -1.5485e-01]]],\n              \n              \n                      [[[ 7.3363e-01,  1.1287e+00,  7.1482e-01],\n                        [-9.9698e-01, -2.4078e+00, -9.4055e-01],\n                        [ 3.1979e-01,  9.4055e-01,  3.3860e-01]]],\n              \n              \n                      [[[ 5.0078e-01,  4.8857e-02, -6.5957e-01],\n                        [ 1.5512e+00, -1.2214e-01, -1.2581e+00],\n                        [ 1.0504e+00, -3.6643e-02, -6.9621e-01]]],\n              \n              \n                      [[[-1.2738e-01, -3.2694e-01, -1.7408e-01],\n                        [-4.7130e-01, -5.4348e-01, -4.2035e-01],\n                        [-2.5476e-01, -4.0336e-01, -2.3353e-01]]],\n              \n              \n                      [[[ 1.9237e-01,  1.9956e-01,  3.4159e-02],\n                        [ 1.9417e-01,  2.2833e-01,  5.9329e-02],\n                        [ 1.4203e-01,  1.9956e-01, -1.1506e-01]]],\n              \n              \n                      [[[-4.2474e-01,  6.3394e-02,  9.5091e-02],\n                        [-8.1144e-01, -1.1411e-01,  2.4724e-01],\n                        [-7.4805e-01, -7.6073e-02,  2.2822e-01]]],\n              \n              \n                      [[[ 9.3350e-02,  1.2269e-01,  9.3350e-02],\n                        [ 1.5736e-01,  3.3873e-01,  2.3737e-01],\n                        [ 1.0135e-01,  2.1337e-01,  1.5469e-01]]],\n              \n              \n                      [[[-7.5617e-02, -2.2265e-01, -1.5544e-01],\n                        [-1.6384e-01, -5.3772e-01, -2.2685e-01],\n                        [-1.0502e-01, -2.8146e-01, -1.9324e-01]]],\n              \n              \n                      [[[ 7.8437e-02,  1.7928e-01,  4.8556e-02],\n                        [ 1.4567e-01,  4.7436e-01,  2.8013e-01],\n                        [ 1.6061e-01,  3.3989e-01,  2.2784e-01]]],\n              \n              \n                      [[[-3.1397e+00,  1.2660e+00,  1.2660e+00],\n                        [-3.0384e-01,  6.4312e+00, -2.6839e+00],\n                        [ 3.6967e+00, -2.5320e+00, -3.0384e+00]]],\n              \n              \n                      [[[ 3.6105e-02,  1.3720e-01,  8.9059e-02],\n                        [ 1.1072e-01,  3.0569e-01,  2.0219e-01],\n                        [ 6.0175e-02,  2.0219e-01,  1.2276e-01]]],\n              \n              \n                      [[[-4.2993e-02, -8.5987e-02, -2.1497e-02],\n                        [-1.3704e-01, -1.6122e-01, -1.1151e-01],\n                        [-1.2495e-01, -1.4913e-01, -1.7197e-01]]],\n              \n              \n                      [[[ 5.9339e-01, -1.8060e-01, -2.5800e+00],\n                        [ 1.0320e-01,  3.2765e+00,  1.8060e-01],\n                        [-1.7802e+00, -2.8380e-01,  1.0320e+00]]],\n              \n              \n                      [[[ 1.0926e+00, -1.1919e+00, -1.9865e-01],\n                        [-5.0655e+00,  1.2614e+01, -7.3499e+00],\n                        [ 3.9729e+00, -1.1025e+01,  6.9526e+00]]],\n              \n              \n                      [[[-2.5399e-01, -2.9569e-01, -1.2889e-01],\n                        [-3.4119e-01, -4.8145e-01, -2.8053e-01],\n                        [-1.0236e-01, -3.2223e-01, -1.8955e-01]]],\n              \n              \n                      [[[-6.1422e-03, -9.8276e-02, -9.8276e-02],\n                        [-2.4569e-02, -1.9246e-01, -2.6207e-01],\n                        [-2.0474e-02, -2.1293e-01, -2.1703e-01]]],\n              \n              \n                      [[[ 1.2808e-01,  1.7688e-01, -1.2199e-02],\n                        [ 2.2364e-01,  2.5820e-01, -2.0331e-03],\n                        [ 1.4028e-01,  1.3012e-01, -2.4397e-02]]],\n              \n              \n                      [[[-2.9497e-01, -3.4792e-01, -1.4370e-01],\n                        [-9.6812e-01, -8.1685e-01, -9.0761e-02],\n                        [-2.2690e-01, -1.2858e-01,  1.7396e-01]]],\n              \n              \n                      [[[ 4.9027e-01,  1.8169e+00,  1.2833e+00],\n                        [-4.4701e-01, -1.8457e+00, -1.1392e+00],\n                        [-2.4513e-01, -3.6049e-01, -2.8839e-02]]]], size=(96, 1, 3, 3),\n                     dtype=torch.qint8, quantization_scheme=torch.per_channel_affine,\n                     scale=tensor([0.0032, 0.0036, 0.0232, 0.0056, 0.0155, 0.0089, 0.0024, 0.0090, 0.0040,\n                      0.0382, 0.0035, 0.0044, 0.0752, 0.0534, 0.0464, 0.0030, 0.0078, 0.0035,\n                      0.0065, 0.0055, 0.0051, 0.0231, 0.0696, 0.0750, 0.0170, 0.0040, 0.0093,\n                      0.0041, 0.0087, 0.0541, 0.0043, 0.0809, 0.0137, 0.0029, 0.0024, 0.0126,\n                      0.0015, 0.0103, 0.0567, 0.0023, 0.0030, 0.0045, 0.0148, 0.0175, 0.0137,\n                      0.0020, 0.0079, 0.0907, 0.0543, 0.0580, 0.0084, 0.0024, 0.0096, 0.0099,\n                      0.0025, 0.0519, 0.0029, 0.0043, 0.0128, 0.0066, 0.0027, 0.0023, 0.1312,\n                      0.0100, 0.0038, 0.0419, 0.0223, 0.0040, 0.0081, 0.0108, 0.0033, 0.0048,\n                      0.0693, 0.0055, 0.0153, 0.0035, 0.0044, 0.0014, 0.0188, 0.0122, 0.0042,\n                      0.0018, 0.0063, 0.0027, 0.0042, 0.0037, 0.0506, 0.0024, 0.0013, 0.0258,\n                      0.0993, 0.0038, 0.0020, 0.0020, 0.0076, 0.0144], dtype=torch.float64),\n                     zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n                     axis=0)),\n             ('mnet.features.2.conv.1.0.bias',\n              Parameter containing:\n              tensor([ 0.2594,  0.3376, -0.1830, -0.0042,  0.0562,  0.0855,  0.8965, -0.2307,\n                       0.0019,  0.0295, -0.0207,  0.4330,  0.0448, -1.1508, -0.1162,  0.3196,\n                       0.0029, -0.5428,  0.2048,  0.9405, -0.0791,  0.0189, -0.1451, -0.0630,\n                       0.0517, -0.0911, -0.1600,  0.0059, -0.5645,  0.0199,  0.4349, -1.2214,\n                      -0.2061,  0.7867,  1.2475, -0.0804,  0.5199,  0.5921,  0.0969,  0.0312,\n                       0.4035, -0.4373,  0.1751, -0.2291, -0.1220,  0.1134,  0.7165, -0.0567,\n                      -0.0873,  0.2118,  0.0244,  0.9216,  0.3973, -0.0173,  0.7346, -0.0356,\n                       0.0105,  1.4550, -0.0276,  0.2470,  0.0340,  0.0783,  0.0939,  0.8702,\n                       0.2791,  0.0852,  0.1273,  0.4223,  0.1748,  0.0291, -0.2029,  0.2775,\n                       0.2067,  0.5841, -0.0504,  0.6949,  0.1849,  0.4340,  0.0748,  0.0976,\n                       0.3001,  0.0563,  0.4925,  0.0570,  0.3203,  0.1078, -0.2449,  0.0559,\n                       0.2724, -0.0654,  0.0205,  0.3613,  1.5488,  0.0572,  0.9473,  0.0562],\n                     requires_grad=True)),\n             ('mnet.features.2.conv.1.0.scale', tensor(0.0811)),\n             ('mnet.features.2.conv.1.0.zero_point', tensor(76)),\n             ('mnet.features.2.conv.2.weight',\n              tensor([[[[ 0.2929]],\n              \n                       [[ 0.3037]],\n              \n                       [[-0.0325]],\n              \n                       ...,\n              \n                       [[-0.1193]],\n              \n                       [[ 0.0217]],\n              \n                       [[-0.1573]]],\n              \n              \n                      [[[ 0.0283]],\n              \n                       [[ 0.2929]],\n              \n                       [[ 0.2551]],\n              \n                       ...,\n              \n                       [[-0.0331]],\n              \n                       [[ 0.1134]],\n              \n                       [[ 0.2929]]],\n              \n              \n                      [[[ 0.1944]],\n              \n                       [[ 0.0156]],\n              \n                       [[-0.2216]],\n              \n                       ...,\n              \n                       [[ 0.0855]],\n              \n                       [[ 0.1166]],\n              \n                       [[-0.3693]]],\n              \n              \n                      ...,\n              \n              \n                      [[[ 0.0479]],\n              \n                       [[-0.1224]],\n              \n                       [[-0.1224]],\n              \n                       ...,\n              \n                       [[-0.0106]],\n              \n                       [[-0.0426]],\n              \n                       [[ 0.0426]]],\n              \n              \n                      [[[ 0.2485]],\n              \n                       [[-0.1368]],\n              \n                       [[ 0.0900]],\n              \n                       ...,\n              \n                       [[-0.4033]],\n              \n                       [[ 0.0900]],\n              \n                       [[ 0.2557]]],\n              \n              \n                      [[[-0.1146]],\n              \n                       [[-0.1782]],\n              \n                       [[ 0.2737]],\n              \n                       ...,\n              \n                       [[ 0.3373]],\n              \n                       [[ 0.4328]],\n              \n                       [[ 0.0955]]]], size=(24, 96, 1, 1), dtype=torch.qint8,\n                     quantization_scheme=torch.per_channel_affine,\n                     scale=tensor([0.0054, 0.0047, 0.0039, 0.0038, 0.0033, 0.0048, 0.0045, 0.0070, 0.0052,\n                      0.0057, 0.0039, 0.0075, 0.0069, 0.0071, 0.0075, 0.0050, 0.0059, 0.0051,\n                      0.0042, 0.0044, 0.0053, 0.0053, 0.0036, 0.0064], dtype=torch.float64),\n                     zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n                     axis=0)),\n             ('mnet.features.2.conv.2.bias',\n              Parameter containing:\n              tensor([-1.4148,  0.6109, -0.7236, -0.2045, -0.0924, -0.9681, -0.4613,  0.1535,\n                      -0.1750,  0.0272,  0.0190,  0.4813,  0.1982,  2.3636,  1.8413,  0.7080,\n                       0.2639, -0.2855, -0.5491,  0.2626,  0.4092,  0.3153,  0.1861,  0.1199],\n                     requires_grad=True)),\n             ('mnet.features.2.conv.2.scale', tensor(0.0753)),\n             ('mnet.features.2.conv.2.zero_point', tensor(54)),\n             ('mnet.features.3.conv.0.0.weight',\n              tensor([[[[-0.0097]],\n              \n                       [[-0.0032]],\n              \n                       [[-0.2740]],\n              \n                       ...,\n              \n                       [[ 0.1644]],\n              \n                       [[ 0.0741]],\n              \n                       [[-0.0419]]],\n              \n              \n                      [[[-0.0214]],\n              \n                       [[ 0.1132]],\n              \n                       [[-0.0481]],\n              \n                       ...,\n              \n                       [[ 0.0276]],\n              \n                       [[-0.0544]],\n              \n                       [[-0.0285]]],\n              \n              \n                      [[[ 0.0097]],\n              \n                       [[-0.0613]],\n              \n                       [[ 0.0807]],\n              \n                       ...,\n              \n                       [[ 0.0312]],\n              \n                       [[ 0.0441]],\n              \n                       [[ 0.0032]]],\n              \n              \n                      ...,\n              \n              \n                      [[[ 0.0209]],\n              \n                       [[ 0.0451]],\n              \n                       [[ 0.0275]],\n              \n                       ...,\n              \n                       [[-0.1176]],\n              \n                       [[-0.0857]],\n              \n                       [[-0.0593]]],\n              \n              \n                      [[[-0.0674]],\n              \n                       [[-0.0186]],\n              \n                       [[-0.0027]],\n              \n                       ...,\n              \n                       [[ 0.1091]],\n              \n                       [[ 0.1126]],\n              \n                       [[ 0.0638]]],\n              \n              \n                      [[[ 0.0470]],\n              \n                       [[-0.0143]],\n              \n                       [[ 0.0218]],\n              \n                       ...,\n              \n                       [[-0.0386]],\n              \n                       [[-0.0663]],\n              \n                       [[ 0.0260]]]], size=(144, 24, 1, 1), dtype=torch.qint8,\n                     quantization_scheme=torch.per_channel_affine,\n                     scale=tensor([0.0032, 0.0009, 0.0011, 0.0012, 0.0008, 0.0013, 0.0016, 0.0013, 0.0015,\n                      0.0015, 0.0013, 0.0015, 0.0011, 0.0013, 0.0016, 0.0016, 0.0012, 0.0012,\n                      0.0012, 0.0010, 0.0009, 0.0010, 0.0011, 0.0015, 0.0014, 0.0013, 0.0009,\n                      0.0015, 0.0014, 0.0009, 0.0009, 0.0010, 0.0014, 0.0013, 0.0017, 0.0011,\n                      0.0015, 0.0017, 0.0015, 0.0021, 0.0014, 0.0007, 0.0008, 0.0013, 0.0006,\n                      0.0014, 0.0005, 0.0012, 0.0017, 0.0023, 0.0015, 0.0014, 0.0017, 0.0013,\n                      0.0011, 0.0013, 0.0015, 0.0015, 0.0013, 0.0010, 0.0014, 0.0011, 0.0015,\n                      0.0011, 0.0010, 0.0015, 0.0026, 0.0008, 0.0015, 0.0018, 0.0011, 0.0014,\n                      0.0009, 0.0009, 0.0008, 0.0016, 0.0014, 0.0014, 0.0015, 0.0006, 0.0007,\n                      0.0007, 0.0020, 0.0008, 0.0010, 0.0015, 0.0018, 0.0004, 0.0008, 0.0011,\n                      0.0010, 0.0007, 0.0022, 0.0013, 0.0011, 0.0007, 0.0010, 0.0007, 0.0017,\n                      0.0019, 0.0016, 0.0016, 0.0012, 0.0011, 0.0020, 0.0014, 0.0005, 0.0011,\n                      0.0015, 0.0023, 0.0003, 0.0017, 0.0013, 0.0014, 0.0006, 0.0004, 0.0011,\n                      0.0017, 0.0014, 0.0013, 0.0006, 0.0012, 0.0012, 0.0021, 0.0021, 0.0012,\n                      0.0006, 0.0014, 0.0007, 0.0028, 0.0011, 0.0011, 0.0021, 0.0014, 0.0015,\n                      0.0012, 0.0013, 0.0013, 0.0005, 0.0011, 0.0012, 0.0011, 0.0009, 0.0008],\n                     dtype=torch.float64),\n                     zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n                     axis=0)),\n             ('mnet.features.3.conv.0.0.bias',\n              Parameter containing:\n              tensor([-0.3880,  0.2197, -0.0445,  0.0578, -0.1785,  0.0981,  0.2429,  0.0767,\n                       0.1306,  0.1804,  0.3130,  0.2080,  0.1197,  0.0160, -0.2088, -0.0928,\n                       0.0834,  0.1408,  0.3234,  0.0397,  0.2804,  0.1775,  0.0707,  0.0539,\n                      -0.1614,  0.4890, -0.0516,  0.2098,  0.0275,  0.2899,  0.1541,  0.0873,\n                      -0.0982,  0.0151, -0.0037,  0.1404,  0.0482,  0.0747, -0.1445, -0.0409,\n                       0.2054,  0.1852,  0.2639,  0.2303,  0.2158,  0.2072,  0.1518,  0.2073,\n                       0.1519,  0.0805, -0.0914,  0.2297, -0.0175,  0.2279,  0.1847, -0.0574,\n                      -0.0579,  0.1620, -0.0052,  0.0712,  0.0646, -0.0354,  0.0130,  0.1940,\n                       0.2035, -0.0181,  0.0940,  0.2885,  0.0601,  0.2748,  0.3072, -0.0869,\n                       0.2891,  0.1634,  0.0412, -0.1297,  0.0507, -0.0586,  0.0273,  0.1624,\n                       0.0736,  0.2095, -0.1159, -0.0532,  0.2114,  0.2133, -0.0969,  0.2796,\n                       0.1834,  0.1114,  0.1064,  0.0786, -0.0718,  0.2239, -0.0938,  0.2025,\n                       0.1859,  0.2761, -0.0133, -0.0322,  0.1702,  0.0397,  0.1823,  0.1871,\n                      -0.1414,  0.0539,  0.2195,  0.1725,  0.0784, -0.0092,  1.0123,  0.0020,\n                       0.2146,  0.0606,  0.1485,  0.1917,  0.0268,  0.2381,  0.2405, -0.0226,\n                       0.2639,  0.1643,  0.1963,  0.2099,  0.1916,  0.0970,  0.1654,  0.0921,\n                       0.1682, -0.1917,  0.2707, -0.1034, -0.0988,  0.1343,  0.0189,  0.1614,\n                       0.1955,  0.1775,  0.3193,  0.0966,  0.2807,  0.0498,  0.1058,  0.1156],\n                     requires_grad=True)),\n             ('mnet.features.3.conv.0.0.scale', tensor(0.0436)),\n             ('mnet.features.3.conv.0.0.zero_point', tensor(83)),\n             ('mnet.features.3.conv.1.0.weight',\n              tensor([[[[ 0.2308, -0.4052,  0.3152],\n                        [-0.2702, -0.0338, -0.6810],\n                        [ 0.3490, -0.7204,  0.4221]]],\n              \n              \n                      [[[ 0.1648, -1.0713,  0.4395],\n                        [ 1.7306, -3.4887,  2.2525],\n                        [ 0.1374, -1.2362,  0.8241]]],\n              \n              \n                      [[[-0.2410, -0.9479, -0.4177],\n                        [ 0.4016,  0.7872, -0.3052],\n                        [-0.3052,  2.0403,  0.1285]]],\n              \n              \n                      ...,\n              \n              \n                      [[[ 0.0580,  0.7365, -0.1102],\n                        [ 0.5103,  0.6031, -0.6379],\n                        [-0.1856, -0.6205, -0.5045]]],\n              \n              \n                      [[[ 0.7475, -0.0356, -0.2848],\n                        [-1.3526,  4.5204, -2.5983],\n                        [ 0.2136,  0.4627, -0.6763]]],\n              \n              \n                      [[[-0.1611, -0.2215, -0.1208],\n                        [ 0.2819, -1.0873,  2.5572],\n                        [-0.2416, -0.3624,  0.1208]]]], size=(144, 1, 3, 3),\n                     dtype=torch.qint8, quantization_scheme=torch.per_channel_affine,\n                     scale=tensor([0.0056, 0.0275, 0.0161, 0.0158, 0.0116, 0.0243, 0.0085, 0.0175, 0.0246,\n                      0.0250, 0.0219, 0.0133, 0.0108, 0.0138, 0.0061, 0.0126, 0.0122, 0.0275,\n                      0.0222, 0.0258, 0.0125, 0.0383, 0.0154, 0.0205, 0.0132, 0.0232, 0.0203,\n                      0.0090, 0.0123, 0.0329, 0.0376, 0.0122, 0.0230, 0.0116, 0.0373, 0.0275,\n                      0.0191, 0.0136, 0.0290, 0.0196, 0.0131, 0.0273, 0.0298, 0.0160, 0.0343,\n                      0.0258, 0.0343, 0.0161, 0.0089, 0.0131, 0.0168, 0.0196, 0.0168, 0.0187,\n                      0.0147, 0.0190, 0.0146, 0.0086, 0.0037, 0.0143, 0.0172, 0.0164, 0.0032,\n                      0.0112, 0.0303, 0.0154, 0.0132, 0.0400, 0.0175, 0.0202, 0.0179, 0.0241,\n                      0.0114, 0.0174, 0.0093, 0.0070, 0.0178, 0.0210, 0.0103, 0.0382, 0.0114,\n                      0.0256, 0.0076, 0.0199, 0.0325, 0.0176, 0.0223, 0.0421, 0.0211, 0.0062,\n                      0.0355, 0.0109, 0.0208, 0.0287, 0.0178, 0.0205, 0.0132, 0.0377, 0.0175,\n                      0.0063, 0.0224, 0.0024, 0.0126, 0.0095, 0.0265, 0.0235, 0.0183, 0.0119,\n                      0.0112, 0.0210, 0.0102, 0.0090, 0.0106, 0.0165, 0.0245, 0.0512, 0.0066,\n                      0.0259, 0.0212, 0.0269, 0.0248, 0.0147, 0.0189, 0.0233, 0.0148, 0.0213,\n                      0.0161, 0.0140, 0.0227, 0.0143, 0.0152, 0.0135, 0.0221, 0.0168, 0.0237,\n                      0.0183, 0.0368, 0.0120, 0.0262, 0.0144, 0.0216, 0.0058, 0.0356, 0.0201],\n                     dtype=torch.float64),\n                     zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n                     axis=0)),\n             ('mnet.features.3.conv.1.0.bias',\n              Parameter containing:\n              tensor([ 4.1573e-01,  7.6502e-02,  4.7019e-02,  4.1476e-02, -5.2507e-02,\n                      -1.2714e-02,  3.3375e-01,  2.4142e-01,  1.2843e-02,  5.0268e-04,\n                       4.4015e-01,  2.6401e-01,  4.0027e-02,  4.5574e-01,  1.1250e-02,\n                      -8.5568e-03, -5.2289e-02, -3.1640e-02,  2.4454e-01, -4.9439e-03,\n                       2.7419e-01, -3.4087e-01, -3.4892e-02, -1.3557e-01,  3.7028e-01,\n                       6.5079e-01,  6.2040e-02,  1.9111e-02, -1.9498e-02,  3.9671e-01,\n                      -4.2949e-01,  4.3898e-02, -5.3406e-02,  1.5295e-01, -1.1877e-02,\n                       1.3038e-02, -1.3136e-02, -4.7394e-02,  9.7670e-02,  1.0680e-01,\n                       6.0482e-01, -1.1286e-01, -2.3433e-01,  2.6800e-01,  1.6644e-01,\n                      -6.2067e-03,  2.0155e-02,  2.9101e-01,  8.1647e-01,  1.6772e-02,\n                       2.2113e-01,  6.1431e-01,  6.3729e-02, -1.1940e-02, -1.2206e-02,\n                       4.3688e-01,  1.1220e-01,  8.1480e-01,  2.3039e-01,  1.3805e-02,\n                       9.3890e-02,  6.9402e-02,  2.9776e-01, -1.5599e-02,  2.6043e-03,\n                       4.5578e-02,  7.9013e-01, -3.7931e-01, -5.2900e-02, -7.6506e-02,\n                      -5.1973e-02,  6.8924e-02,  2.9430e-01, -1.6266e-01,  8.6918e-02,\n                      -7.0445e-03, -1.4775e-02, -8.6049e-03, -2.8698e-02,  5.1694e-02,\n                       2.1648e-01,  6.5272e-02,  1.9334e-01, -1.8208e-01, -1.9612e-01,\n                       7.2159e-01, -1.4321e-01, -1.7142e-01, -5.2382e-02, -5.6490e-05,\n                       5.9120e-02,  2.9169e-02,  2.3862e-02, -2.5594e-01,  4.5849e-02,\n                      -1.9407e-01,  7.9074e-02, -5.1611e-01, -1.0801e-01,  2.8039e-01,\n                       2.1276e-02,  6.6784e-02,  4.4217e-02, -2.9381e-02,  1.1161e-01,\n                      -5.0151e-02,  2.2570e-01,  2.1459e-02,  2.1950e-01,  8.6789e-02,\n                       3.5660e+00,  3.6305e-01, -6.5333e-03,  7.0834e-03, -5.7297e-01,\n                       3.8836e-03,  1.8551e-01,  2.8395e-02,  6.9055e-02,  5.0417e-02,\n                       3.4466e-01,  1.4083e-02,  1.3828e-01,  1.4051e-01,  5.6906e-02,\n                      -2.4921e-01, -6.6423e-01,  3.7535e-02, -6.5327e-02,  2.3082e-01,\n                      -2.6350e-02, -2.2380e-02, -1.3803e-01, -1.2437e-01, -2.0099e-01,\n                       1.0964e-01,  6.4731e-02,  3.8031e-01, -2.0002e-01, -3.4325e-01,\n                      -5.8912e-01,  1.3888e-01, -3.9053e-01,  8.8426e-02],\n                     requires_grad=True)),\n             ('mnet.features.3.conv.1.0.scale', tensor(0.0401)),\n             ('mnet.features.3.conv.1.0.zero_point', tensor(64)),\n             ('mnet.features.3.conv.2.weight',\n              tensor([[[[-0.0224]],\n              \n                       [[-0.0504]],\n              \n                       [[-0.2185]],\n              \n                       ...,\n              \n                       [[-0.1681]],\n              \n                       [[ 0.2017]],\n              \n                       [[ 0.0168]]],\n              \n              \n                      [[[ 0.2613]],\n              \n                       [[-0.3439]],\n              \n                       [[ 0.2957]],\n              \n                       ...,\n              \n                       [[-0.1582]],\n              \n                       [[ 0.0963]],\n              \n                       [[ 0.3439]]],\n              \n              \n                      [[[ 0.4366]],\n              \n                       [[-0.2629]],\n              \n                       [[ 0.0892]],\n              \n                       ...,\n              \n                       [[-0.0845]],\n              \n                       [[-0.1408]],\n              \n                       [[ 0.0047]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-0.3354]],\n              \n                       [[-0.1372]],\n              \n                       [[-0.3354]],\n              \n                       ...,\n              \n                       [[ 0.1905]],\n              \n                       [[-0.1143]],\n              \n                       [[-0.0610]]],\n              \n              \n                      [[[ 0.3010]],\n              \n                       [[ 0.2189]],\n              \n                       [[-0.1733]],\n              \n                       ...,\n              \n                       [[ 0.1459]],\n              \n                       [[-0.1186]],\n              \n                       [[-0.0821]]],\n              \n              \n                      [[[ 0.1574]],\n              \n                       [[-0.2249]],\n              \n                       [[-0.0150]],\n              \n                       ...,\n              \n                       [[ 0.4498]],\n              \n                       [[-0.1874]],\n              \n                       [[-0.2849]]]], size=(24, 144, 1, 1), dtype=torch.qint8,\n                     quantization_scheme=torch.per_channel_affine,\n                     scale=tensor([0.0056, 0.0069, 0.0047, 0.0059, 0.0073, 0.0051, 0.0070, 0.0047, 0.0058,\n                      0.0040, 0.0063, 0.0044, 0.0055, 0.0060, 0.0065, 0.0058, 0.0037, 0.0056,\n                      0.0041, 0.0046, 0.0067, 0.0076, 0.0091, 0.0075], dtype=torch.float64),\n                     zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n                     axis=0)),\n             ('mnet.features.3.conv.2.bias',\n              Parameter containing:\n              tensor([-1.5046, -1.1612,  0.0896, -0.2018, -1.2815,  1.1035,  0.7118,  0.0781,\n                       0.1119, -0.0736, -2.5197, -0.6958,  0.6191, -1.1680,  0.1282, -0.3233,\n                       0.3662, -0.5515, -0.5325,  0.3691,  1.0300,  0.9252, -1.9331,  1.3962],\n                     requires_grad=True)),\n             ('mnet.features.3.conv.2.scale', tensor(0.0650)),\n             ('mnet.features.3.conv.2.zero_point', tensor(60)),\n             ('mnet.features.4.conv.0.0.weight',\n              tensor([[[[-0.0431]],\n              \n                       [[-0.0841]],\n              \n                       [[ 0.0984]],\n              \n                       ...,\n              \n                       [[ 0.1005]],\n              \n                       [[-0.0041]],\n              \n                       [[-0.0800]]],\n              \n              \n                      [[[-0.0365]],\n              \n                       [[ 0.0176]],\n              \n                       [[-0.0095]],\n              \n                       ...,\n              \n                       [[-0.0013]],\n              \n                       [[ 0.0060]],\n              \n                       [[ 0.0546]]],\n              \n              \n                      [[[ 0.0713]],\n              \n                       [[ 0.0487]],\n              \n                       [[-0.0252]],\n              \n                       ...,\n              \n                       [[-0.1015]],\n              \n                       [[-0.0604]],\n              \n                       [[ 0.0344]]],\n              \n              \n                      ...,\n              \n              \n                      [[[ 0.0833]],\n              \n                       [[ 0.0075]],\n              \n                       [[ 0.0423]],\n              \n                       ...,\n              \n                       [[ 0.0932]],\n              \n                       [[-0.0410]],\n              \n                       [[ 0.1504]]],\n              \n              \n                      [[[ 0.0652]],\n              \n                       [[-0.0066]],\n              \n                       [[-0.1057]],\n              \n                       ...,\n              \n                       [[-0.0578]],\n              \n                       [[ 0.0941]],\n              \n                       [[ 0.0132]]],\n              \n              \n                      [[[-0.0503]],\n              \n                       [[-0.0990]],\n              \n                       [[-0.0642]],\n              \n                       ...,\n              \n                       [[ 0.0556]],\n              \n                       [[ 0.0868]],\n              \n                       [[-0.0208]]]], size=(144, 24, 1, 1), dtype=torch.qint8,\n                     quantization_scheme=torch.per_channel_affine,\n                     scale=tensor([0.0021, 0.0004, 0.0008, 0.0006, 0.0012, 0.0005, 0.0010, 0.0020, 0.0008,\n                      0.0010, 0.0011, 0.0013, 0.0009, 0.0012, 0.0008, 0.0009, 0.0010, 0.0015,\n                      0.0014, 0.0016, 0.0015, 0.0008, 0.0013, 0.0009, 0.0019, 0.0010, 0.0004,\n                      0.0012, 0.0010, 0.0009, 0.0008, 0.0012, 0.0008, 0.0014, 0.0014, 0.0019,\n                      0.0013, 0.0011, 0.0012, 0.0013, 0.0009, 0.0022, 0.0016, 0.0007, 0.0016,\n                      0.0010, 0.0006, 0.0015, 0.0010, 0.0011, 0.0016, 0.0011, 0.0012, 0.0023,\n                      0.0014, 0.0009, 0.0011, 0.0009, 0.0014, 0.0010, 0.0013, 0.0011, 0.0002,\n                      0.0002, 0.0013, 0.0008, 0.0011, 0.0012, 0.0017, 0.0011, 0.0004, 0.0014,\n                      0.0018, 0.0008, 0.0010, 0.0020, 0.0008, 0.0010, 0.0008, 0.0005, 0.0010,\n                      0.0004, 0.0003, 0.0011, 0.0010, 0.0007, 0.0010, 0.0009, 0.0014, 0.0026,\n                      0.0013, 0.0015, 0.0008, 0.0013, 0.0017, 0.0004, 0.0012, 0.0005, 0.0006,\n                      0.0010, 0.0008, 0.0003, 0.0012, 0.0009, 0.0012, 0.0013, 0.0004, 0.0002,\n                      0.0008, 0.0018, 0.0002, 0.0009, 0.0012, 0.0016, 0.0010, 0.0002, 0.0009,\n                      0.0009, 0.0006, 0.0012, 0.0005, 0.0018, 0.0005, 0.0012, 0.0009, 0.0013,\n                      0.0015, 0.0006, 0.0010, 0.0011, 0.0017, 0.0008, 0.0021, 0.0009, 0.0011,\n                      0.0004, 0.0009, 0.0014, 0.0021, 0.0008, 0.0008, 0.0012, 0.0008, 0.0017],\n                     dtype=torch.float64),\n                     zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n                     axis=0)),\n             ('mnet.features.4.conv.0.0.bias',\n              Parameter containing:\n              tensor([ 0.1127,  0.2102,  0.0360,  0.2503, -0.0319, -0.0610,  0.0999,  0.0360,\n                       0.2314,  0.0578, -0.1672, -0.0876,  0.0879, -0.1482, -0.0217, -0.0980,\n                       0.1089,  0.1484, -0.0227, -0.1688, -0.0079, -0.0147, -0.0601, -0.0414,\n                       0.0776, -0.0242,  0.2402,  0.0805, -0.1742, -0.2006,  0.0965,  0.0005,\n                       0.2607, -0.0817, -0.0303, -0.1464,  0.1868,  0.1946,  0.0556, -0.0644,\n                      -0.0405, -0.0078, -0.2807,  0.1331,  0.0385, -0.0464,  0.3319,  0.1690,\n                       0.0940,  0.2142,  0.0889,  0.0581, -0.0092, -0.3311,  0.2396,  0.0254,\n                       0.0277,  0.1690,  0.2122,  0.1327, -0.0061,  0.0638,  0.1977,  0.2483,\n                       0.0350, -0.1856, -0.0763, -0.0116, -0.0564, -0.0770,  0.1502,  0.1567,\n                      -0.0351, -0.0606, -0.2887,  0.0148,  0.2714, -0.0030, -0.1983,  0.2049,\n                      -0.0541,  0.1680,  0.1675, -0.2019,  0.0565, -0.0946,  0.2280, -0.1263,\n                       0.0313, -0.3290, -0.1665,  0.0245,  0.2908, -0.0622, -0.2586,  0.1386,\n                      -0.0091, -0.1716,  0.1685,  0.0384,  0.0497,  0.2727, -0.0939, -0.0290,\n                      -0.2389, -0.0063,  0.1663,  0.1354,  0.0712,  0.1807,  0.2166,  0.0134,\n                      -0.0846,  0.0210,  0.0689,  0.2030,  0.2404,  0.0819,  0.1614,  0.2111,\n                       0.1414, -0.0040,  0.1556,  0.1688,  0.2197, -0.0237, -0.0466,  0.0367,\n                       0.0553,  0.0086,  0.1361,  0.2082,  0.1587,  0.0109,  0.1882,  0.2486,\n                       0.1298, -0.0542,  0.1464, -0.1654,  0.0459, -0.0148,  0.1161, -0.0309],\n                     requires_grad=True)),\n             ('mnet.features.4.conv.0.0.scale', tensor(0.0382)),\n             ('mnet.features.4.conv.0.0.zero_point', tensor(71)),\n             ('mnet.features.4.conv.1.0.weight',\n              tensor([[[[-0.1871, -0.3462, -0.2105],\n                        [-0.3602, -0.5941, -0.4678],\n                        [-0.2526, -0.5006, -0.3462]]],\n              \n              \n                      [[[-0.1647,  1.5237, -0.5491],\n                        [-1.7570,  1.4001,  0.6040],\n                        [-1.5374, -0.3432,  1.1393]]],\n              \n              \n                      [[[-0.3843, -0.7911, -0.6178],\n                        [-0.7384, -0.9644, -0.8288],\n                        [-0.5726, -0.8589, -0.5801]]],\n              \n              \n                      ...,\n              \n              \n                      [[[ 0.0691,  0.2616,  0.1629],\n                        [ 0.3011,  0.6270,  0.4295],\n                        [ 0.2271,  0.4443,  0.3258]]],\n              \n              \n                      [[[ 0.1844,  0.2878,  0.1844],\n                        [ 0.2248,  0.5710,  0.4002],\n                        [ 0.0270,  0.2923,  0.3507]]],\n              \n              \n                      [[[ 0.1634,  0.3172,  0.2018],\n                        [ 0.2787,  0.6103,  0.4181],\n                        [ 0.1538,  0.3700,  0.2499]]]], size=(144, 1, 3, 3),\n                     dtype=torch.qint8, quantization_scheme=torch.per_channel_affine,\n                     scale=tensor([0.0047, 0.0137, 0.0075, 0.0104, 0.0055, 0.0129, 0.0030, 0.0033, 0.0065,\n                      0.0069, 0.0051, 0.0073, 0.0054, 0.0121, 0.0071, 0.0070, 0.0046, 0.0026,\n                      0.0044, 0.0096, 0.0061, 0.0059, 0.0049, 0.0063, 0.0029, 0.0117, 0.0151,\n                      0.0032, 0.0106, 0.0060, 0.0051, 0.0063, 0.0078, 0.0043, 0.0083, 0.0056,\n                      0.0035, 0.0045, 0.0065, 0.0065, 0.0089, 0.0099, 0.0125, 0.0037, 0.0051,\n                      0.0047, 0.0052, 0.0059, 0.0101, 0.0034, 0.0037, 0.0072, 0.0085, 0.0081,\n                      0.0035, 0.0100, 0.0051, 0.0064, 0.0028, 0.0168, 0.0053, 0.0050, 0.0443,\n                      0.0310, 0.0041, 0.0080, 0.0065, 0.0041, 0.0047, 0.0065, 0.0104, 0.0039,\n                      0.0030, 0.0062, 0.0176, 0.0051, 0.0056, 0.0079, 0.0199, 0.0128, 0.0083,\n                      0.0145, 0.0145, 0.0070, 0.0070, 0.0057, 0.0047, 0.0050, 0.0055, 0.0085,\n                      0.0050, 0.0037, 0.0093, 0.0065, 0.0079, 0.0209, 0.0042, 0.0118, 0.0090,\n                      0.0039, 0.0097, 0.0160, 0.0049, 0.0052, 0.0114, 0.0041, 0.0088, 0.0258,\n                      0.0090, 0.0036, 0.0153, 0.0063, 0.0046, 0.0035, 0.0130, 0.0338, 0.0063,\n                      0.0033, 0.0101, 0.0052, 0.0161, 0.0050, 0.0131, 0.0033, 0.0053, 0.0062,\n                      0.0051, 0.0074, 0.0059, 0.0073, 0.0026, 0.0115, 0.0023, 0.0096, 0.0050,\n                      0.0148, 0.0040, 0.0060, 0.0023, 0.0097, 0.0099, 0.0049, 0.0045, 0.0048],\n                     dtype=torch.float64),\n                     zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n                     axis=0)),\n             ('mnet.features.4.conv.1.0.bias',\n              Parameter containing:\n              tensor([ 9.8480e-01, -7.9054e-02,  6.2118e-01,  1.2929e+00,  2.8171e-01,\n                      -6.6627e-03,  6.6965e-01,  4.9553e-03,  1.0275e+00,  4.4703e-01,\n                       1.8852e-02, -4.4444e-02,  5.4980e-01,  1.0400e-02,  4.4247e-01,\n                      -3.5100e-02, -3.3881e-02,  1.0362e-01,  3.5507e-01, -9.5655e-02,\n                       4.1857e-01,  3.5706e-01,  9.7871e-01, -2.0967e-02,  1.4240e-02,\n                      -5.3672e-03, -4.3688e-02,  5.3420e-01, -1.4697e-01,  2.4836e-02,\n                       6.5622e-01,  4.5371e-01,  4.5488e-02,  2.0444e-02, -4.1924e-02,\n                      -5.8788e-02, -5.6768e-02,  9.8915e-01,  9.6218e-01, -1.8652e-02,\n                       4.2685e-01,  3.1817e-01, -3.0251e-02,  9.1946e-01,  2.3583e-02,\n                       7.1432e-01,  1.5488e+00,  9.0853e-01,  5.0673e-03,  3.6934e-02,\n                       2.4067e-02,  4.1145e-01,  3.5774e-01, -6.9850e-02, -5.6707e-02,\n                       1.6272e-03,  1.0625e-02,  1.3735e+00,  1.3318e-01, -9.9350e-03,\n                       5.3729e-01, -6.4240e-02, -2.8642e-01, -1.0619e-01, -1.0096e-02,\n                       5.9168e-01, -1.1579e-01,  5.6474e-01,  3.6793e-02, -1.1941e-02,\n                       9.1960e-01,  6.4650e-04,  9.8104e-02,  5.3331e-04, -3.8116e-02,\n                      -2.8849e-03,  8.1312e-01,  6.0816e-01,  1.3072e-02,  5.4471e-01,\n                       5.4157e-01, -3.6967e-02,  1.5775e-02,  1.7428e-02,  8.3862e-01,\n                      -3.9151e-02, -2.5726e-01,  4.2834e-02, -1.2874e-01, -5.5132e-02,\n                       1.6601e-02,  4.4617e-01,  1.4598e+00,  1.3274e-02, -3.5884e-02,\n                       8.7004e-02,  2.5430e-01, -7.2968e-03, -7.9015e-02,  4.8587e-01,\n                       1.5786e-02,  9.3265e-02,  3.6719e-01,  4.3296e-02, -1.2205e-02,\n                       4.6628e-02, -6.7004e-03,  6.4527e-01,  3.7852e-02,  2.3526e-02,\n                      -4.6695e-02,  5.5629e-01,  2.6035e-02,  4.1581e-01, -3.0651e-01,\n                      -4.6881e-02, -2.8830e-01,  5.9533e-01, -4.0888e-02,  1.0733e+00,\n                       5.3046e-02,  8.0627e-01, -3.8431e-02,  9.7206e-03,  1.0182e+00,\n                      -2.8335e-02,  3.5014e-01,  6.1735e-01, -4.1619e-01,  4.9222e-01,\n                       7.0202e-02, -8.6104e-02,  2.2201e-02, -2.8189e-03,  7.5279e-01,\n                       6.7543e-02,  6.3937e-01, -2.8323e-02,  7.0884e-02, -5.8698e-02,\n                      -9.9608e-04,  9.8073e-03, -2.2216e-02,  6.1220e-02],\n                     requires_grad=True)),\n             ('mnet.features.4.conv.1.0.scale', tensor(0.0449)),\n             ('mnet.features.4.conv.1.0.zero_point', tensor(69)),\n             ('mnet.features.4.conv.2.weight',\n              tensor([[[[-0.2115]],\n              \n                       [[ 0.1844]],\n              \n                       [[-0.0813]],\n              \n                       ...,\n              \n                       [[-0.0976]],\n              \n                       [[-0.1410]],\n              \n                       [[-0.3742]]],\n              \n              \n                      [[[ 0.1722]],\n              \n                       [[ 0.0205]],\n              \n                       [[-0.2419]],\n              \n                       ...,\n              \n                       [[-0.1558]],\n              \n                       [[ 0.0533]],\n              \n                       [[ 0.1886]]],\n              \n              \n                      [[[ 0.2913]],\n              \n                       [[ 0.0127]],\n              \n                       [[ 0.1182]],\n              \n                       ...,\n              \n                       [[ 0.0802]],\n              \n                       [[-0.1689]],\n              \n                       [[-0.0464]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-0.1232]],\n              \n                       [[-0.1232]],\n              \n                       [[ 0.3514]],\n              \n                       ...,\n              \n                       [[ 0.0411]],\n              \n                       [[ 0.0274]],\n              \n                       [[ 0.0091]]],\n              \n              \n                      [[[-0.1005]],\n              \n                       [[-0.0391]],\n              \n                       [[ 0.0503]],\n              \n                       ...,\n              \n                       [[ 0.0782]],\n              \n                       [[ 0.1396]],\n              \n                       [[-0.0726]]],\n              \n              \n                      [[[ 0.1157]],\n              \n                       [[ 0.1777]],\n              \n                       [[ 0.0124]],\n              \n                       ...,\n              \n                       [[ 0.3679]],\n              \n                       [[ 0.1116]],\n              \n                       [[-0.1075]]]], size=(32, 144, 1, 1), dtype=torch.qint8,\n                     quantization_scheme=torch.per_channel_affine,\n                     scale=tensor([0.0054, 0.0041, 0.0042, 0.0042, 0.0040, 0.0067, 0.0053, 0.0054, 0.0041,\n                      0.0050, 0.0038, 0.0054, 0.0049, 0.0047, 0.0043, 0.0044, 0.0038, 0.0048,\n                      0.0039, 0.0050, 0.0040, 0.0043, 0.0045, 0.0054, 0.0046, 0.0043, 0.0045,\n                      0.0044, 0.0043, 0.0046, 0.0056, 0.0041], dtype=torch.float64),\n                     zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0]),\n                     axis=0)),\n             ('mnet.features.4.conv.2.bias',\n              Parameter containing:\n              tensor([ 0.8318,  0.5356, -0.6600,  0.7257,  0.1258, -1.4927, -0.2815, -0.1848,\n                       0.0502, -1.6181,  0.7936,  1.6673,  0.7044,  0.8292,  0.0846, -0.5648,\n                       0.6487, -0.0954,  0.4989, -0.2936, -0.1627,  0.3953, -0.4530,  0.4187,\n                      -0.7965,  0.9821, -0.1626, -0.9804,  0.2880, -1.0740, -0.6616, -0.1832],\n                     requires_grad=True)),\n             ('mnet.features.4.conv.2.scale', tensor(0.0514)),\n             ('mnet.features.4.conv.2.zero_point', tensor(67)),\n             ('mnet.features.5.conv.0.0.weight',\n              tensor([[[[ 0.0071]],\n              \n                       [[-0.0177]],\n              \n                       [[ 0.0451]],\n              \n                       ...,\n              \n                       [[ 0.0468]],\n              \n                       [[ 0.0000]],\n              \n                       [[ 0.0742]]],\n              \n              \n                      [[[ 0.0040]],\n              \n                       [[-0.0229]],\n              \n                       [[-0.0007]],\n              \n                       ...,\n              \n                       [[-0.0243]],\n              \n                       [[ 0.0653]],\n              \n                       [[ 0.0094]]],\n              \n              \n                      [[[ 0.0016]],\n              \n                       [[ 0.0044]],\n              \n                       [[ 0.0032]],\n              \n                       ...,\n              \n                       [[ 0.0107]],\n              \n                       [[ 0.0190]],\n              \n                       [[ 0.0186]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-0.0401]],\n              \n                       [[-0.0459]],\n              \n                       [[-0.1013]],\n              \n                       ...,\n              \n                       [[ 0.0392]],\n              \n                       [[ 0.0115]],\n              \n                       [[ 0.0554]]],\n              \n              \n                      [[[ 0.0055]],\n              \n                       [[ 0.0089]],\n              \n                       [[ 0.0545]],\n              \n                       ...,\n              \n                       [[ 0.0477]],\n              \n                       [[ 0.0463]],\n              \n                       [[ 0.0484]]],\n              \n              \n                      [[[ 0.0671]],\n              \n                       [[-0.0172]],\n              \n                       [[-0.0590]],\n              \n                       ...,\n              \n                       [[ 0.1152]],\n              \n                       [[-0.0708]],\n              \n                       [[-0.0690]]]], size=(192, 32, 1, 1), dtype=torch.qint8,\n                     quantization_scheme=torch.per_channel_affine,\n                     scale=tensor([0.0009, 0.0007, 0.0004, 0.0007, 0.0012, 0.0003, 0.0007, 0.0006, 0.0009,\n                      0.0011, 0.0004, 0.0020, 0.0004, 0.0005, 0.0011, 0.0010, 0.0009, 0.0006,\n                      0.0005, 0.0008, 0.0004, 0.0013, 0.0007, 0.0007, 0.0006, 0.0003, 0.0007,\n                      0.0005, 0.0008, 0.0008, 0.0006, 0.0006, 0.0009, 0.0008, 0.0007, 0.0004,\n                      0.0004, 0.0006, 0.0003, 0.0005, 0.0009, 0.0007, 0.0003, 0.0005, 0.0006,\n                      0.0006, 0.0003, 0.0004, 0.0002, 0.0007, 0.0006, 0.0005, 0.0008, 0.0007,\n                      0.0003, 0.0003, 0.0005, 0.0009, 0.0004, 0.0008, 0.0004, 0.0010, 0.0006,\n                      0.0004, 0.0013, 0.0004, 0.0005, 0.0004, 0.0011, 0.0011, 0.0005, 0.0008,\n                      0.0012, 0.0010, 0.0011, 0.0005, 0.0004, 0.0007, 0.0005, 0.0006, 0.0011,\n                      0.0009, 0.0007, 0.0013, 0.0012, 0.0005, 0.0005, 0.0009, 0.0010, 0.0006,\n                      0.0008, 0.0004, 0.0006, 0.0008, 0.0012, 0.0007, 0.0009, 0.0004, 0.0005,\n                      0.0005, 0.0009, 0.0005, 0.0005, 0.0008, 0.0006, 0.0008, 0.0007, 0.0009,\n                      0.0006, 0.0005, 0.0005, 0.0004, 0.0012, 0.0004, 0.0004, 0.0006, 0.0010,\n                      0.0004, 0.0005, 0.0008, 0.0008, 0.0006, 0.0007, 0.0008, 0.0010, 0.0005,\n                      0.0007, 0.0006, 0.0005, 0.0007, 0.0007, 0.0007, 0.0007, 0.0003, 0.0004,\n                      0.0008, 0.0004, 0.0006, 0.0010, 0.0008, 0.0009, 0.0008, 0.0004, 0.0010,\n                      0.0005, 0.0011, 0.0006, 0.0004, 0.0005, 0.0004, 0.0005, 0.0006, 0.0010,\n                      0.0007, 0.0005, 0.0012, 0.0008, 0.0007, 0.0011, 0.0006, 0.0006, 0.0005,\n                      0.0007, 0.0009, 0.0004, 0.0007, 0.0009, 0.0010, 0.0009, 0.0007, 0.0009,\n                      0.0005, 0.0007, 0.0005, 0.0007, 0.0007, 0.0005, 0.0007, 0.0009, 0.0006,\n                      0.0007, 0.0007, 0.0005, 0.0006, 0.0006, 0.0008, 0.0003, 0.0003, 0.0003,\n                      0.0010, 0.0007, 0.0009], dtype=torch.float64),\n                     zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n                     axis=0)),\n             ('mnet.features.5.conv.0.0.bias',\n              Parameter containing:\n              tensor([ 0.0911, -0.0188,  0.1998,  0.0710, -0.1243,  0.2123,  0.2216,  0.2113,\n                      -0.0657, -0.0121,  0.1808,  0.1482,  0.1534,  0.1599, -0.0541, -0.0775,\n                       0.1759,  0.2084,  0.0831,  0.2100,  0.2800,  0.1421,  0.2401,  0.2898,\n                       0.1724,  0.1998, -0.0462,  0.0507,  0.1358, -0.1014,  0.0013,  0.1166,\n                      -0.1178, -0.1406, -0.0490,  0.1585,  0.2130,  0.2063,  0.1985,  0.1658,\n                       0.1923,  0.2100,  0.1527,  0.1523,  0.1334,  0.1809,  0.2030,  0.1705,\n                       0.1492,  0.0541,  0.1687,  0.1742,  0.1854, -0.0386,  0.2170,  0.2659,\n                       0.2204,  0.1312,  0.2127,  0.0749,  0.1526, -0.0238,  0.1748,  0.1753,\n                      -0.0826,  0.2334,  0.1749,  0.2306, -0.0396,  0.1226,  0.1809, -0.1277,\n                       0.0616,  0.0168,  0.0185,  0.2117,  0.1397,  0.1972,  0.2201, -0.0061,\n                       0.0201,  0.2380,  0.0197,  0.1251,  0.0023,  0.1163,  0.1158,  0.0637,\n                      -0.0633,  0.2472,  0.0258, -0.1169,  0.0976,  0.1235, -0.0457,  0.0866,\n                       0.1455,  0.2107,  0.1940,  0.2695,  0.0600,  0.1479,  0.1295,  0.1513,\n                       0.0073,  0.0397,  0.2475,  0.2065,  0.1896,  0.1282,  0.0527,  0.2052,\n                       0.0850,  0.1636,  0.1807,  0.1308,  0.0830,  0.2045,  0.2734,  0.1590,\n                       0.1256,  0.1765, -0.0288,  0.0687, -0.1476,  0.1999, -0.0685,  0.1345,\n                       0.1843,  0.0673,  0.1460,  0.1352,  0.1756,  0.1724,  0.2492, -0.1540,\n                       0.1501,  0.0871,  0.0803,  0.0489,  0.0030,  0.0434,  0.1440,  0.2199,\n                       0.1839, -0.0616,  0.0706,  0.1249,  0.1879,  0.2005,  0.2065,  0.3704,\n                       0.1669,  0.1084,  0.2870,  0.0937,  0.1528,  0.3713,  0.2793,  0.1892,\n                       0.1547,  0.1607,  0.2028, -0.0582,  0.2074,  0.0533,  0.0104,  0.1762,\n                       0.0958,  0.1377, -0.0733,  0.1149,  0.1395,  0.2200,  0.0682,  0.1378,\n                       0.4049, -0.0186, -0.0702,  0.0040, -0.0500,  0.1110,  0.1137,  0.1943,\n                       0.1604,  0.2964,  0.1245,  0.1989,  0.2277,  0.2384,  0.2868,  0.0282],\n                     requires_grad=True)),\n             ('mnet.features.5.conv.0.0.scale', tensor(0.0158)),\n             ('mnet.features.5.conv.0.0.zero_point', tensor(60)),\n             ('mnet.features.5.conv.1.0.weight',\n              tensor([[[[ 0.7358, -0.0968, -0.2130],\n                        [ 0.2323, -2.4784,  1.2005],\n                        [-0.2130,  0.6002,  1.0456]]],\n              \n              \n                      [[[ 0.0358,  1.2994,  0.4172],\n                        [ 0.2980,  1.2636,  0.1192],\n                        [-0.5007, -1.5259, -0.9775]]],\n              \n              \n                      [[[ 1.6189,  1.0837,  0.4014],\n                        [ 0.6288, -0.8028, -1.4048],\n                        [ 0.8295, -1.6992, -0.0937]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-0.1406, -0.5424, -0.1406],\n                        [-0.4721,  1.2757, -0.4721],\n                        [-0.1507, -0.3716, -0.2612]]],\n              \n              \n                      [[[ 0.2960, -0.1837,  0.0714],\n                        [-0.1735,  0.5001, -0.4592],\n                        [-0.1123, -1.3063, -0.3062]]],\n              \n              \n                      [[[-0.5519,  0.2293,  0.1274],\n                        [-1.0020,  0.2802,  1.0784],\n                        [-0.2038, -0.1698,  0.2547]]]], size=(192, 1, 3, 3),\n                     dtype=torch.qint8, quantization_scheme=torch.per_channel_affine,\n                     scale=tensor([0.0194, 0.0119, 0.0134, 0.0094, 0.0147, 0.0239, 0.0091, 0.0131, 0.0041,\n                      0.0115, 0.0241, 0.0206, 0.0277, 0.0108, 0.0156, 0.0093, 0.0214, 0.0139,\n                      0.0151, 0.0126, 0.0272, 0.0202, 0.0136, 0.0134, 0.0134, 0.0279, 0.0187,\n                      0.0106, 0.0134, 0.0095, 0.0038, 0.0113, 0.0228, 0.0295, 0.0104, 0.0234,\n                      0.0169, 0.0246, 0.0176, 0.0118, 0.0072, 0.0153, 0.0323, 0.0111, 0.0137,\n                      0.0141, 0.0334, 0.0121, 0.0314, 0.0129, 0.0133, 0.0154, 0.0115, 0.0145,\n                      0.0320, 0.0194, 0.0158, 0.0200, 0.0144, 0.0094, 0.0314, 0.0184, 0.0129,\n                      0.0222, 0.0032, 0.0211, 0.0161, 0.0128, 0.0143, 0.0147, 0.0219, 0.0328,\n                      0.0041, 0.0039, 0.0061, 0.0233, 0.0193, 0.0131, 0.0096, 0.0243, 0.0084,\n                      0.0051, 0.0117, 0.0182, 0.0125, 0.0053, 0.0097, 0.0098, 0.0135, 0.0131,\n                      0.0092, 0.0432, 0.0103, 0.0227, 0.0057, 0.0109, 0.0147, 0.0327, 0.0120,\n                      0.0146, 0.0141, 0.0225, 0.0050, 0.0124, 0.0174, 0.0107, 0.0086, 0.0102,\n                      0.0083, 0.0211, 0.0078, 0.0132, 0.0079, 0.0211, 0.0101, 0.0133, 0.0117,\n                      0.0107, 0.0321, 0.0129, 0.0131, 0.0177, 0.0131, 0.0143, 0.0205, 0.0118,\n                      0.0129, 0.0142, 0.0100, 0.0184, 0.0160, 0.0116, 0.0152, 0.0227, 0.0141,\n                      0.0372, 0.0351, 0.0168, 0.0130, 0.0092, 0.0083, 0.0076, 0.0133, 0.0104,\n                      0.0151, 0.0178, 0.0133, 0.0256, 0.0132, 0.0152, 0.0163, 0.0151, 0.0111,\n                      0.0134, 0.0158, 0.0204, 0.0141, 0.0150, 0.0117, 0.0122, 0.0247, 0.0311,\n                      0.0219, 0.0209, 0.0135, 0.0155, 0.0082, 0.0128, 0.0130, 0.0183, 0.0067,\n                      0.0159, 0.0226, 0.0309, 0.0034, 0.0087, 0.0196, 0.0182, 0.0171, 0.0164,\n                      0.0195, 0.0137, 0.0139, 0.0111, 0.0103, 0.0242, 0.0430, 0.0247, 0.0202,\n                      0.0100, 0.0102, 0.0085], dtype=torch.float64),\n                     zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n                     axis=0)),\n             ('mnet.features.5.conv.1.0.bias',\n              Parameter containing:\n              tensor([-1.8502e-01, -5.5495e-02, -1.5819e-01, -3.5175e-01, -4.1763e-02,\n                       2.7078e-01,  5.5840e-02, -3.0596e-02,  2.0285e-01,  1.3358e-01,\n                       3.3218e-03, -1.2535e-01, -1.0739e-01, -7.1829e-02,  7.6603e-03,\n                       1.4688e-01, -5.0508e-03, -4.0031e-03, -8.3377e-02,  2.1384e-02,\n                       2.0584e-02,  1.6631e-02,  3.3286e-01,  4.2758e-01, -3.4441e-02,\n                       2.9091e-01, -1.1688e-01, -7.4797e-02, -1.2022e-01,  4.9155e-01,\n                       1.2232e-01, -1.2081e-01,  3.6548e-01, -9.7571e-03, -5.9449e-02,\n                       7.5760e-02, -2.0260e-02, -2.0958e-02,  7.6306e-03, -1.0981e-02,\n                       2.9450e-01,  1.9161e-01, -2.4590e-02, -4.3731e-02,  2.0167e-01,\n                      -2.6821e-03,  8.8127e-02, -6.1692e-02,  1.8276e-02,  1.6433e-02,\n                      -4.4113e-02,  7.1366e-02, -9.8520e-03, -4.4807e-02,  4.0321e-01,\n                      -2.7548e-02,  4.9080e-01, -1.2765e-02,  3.8585e-01,  4.7963e-03,\n                       9.7388e-03, -2.3570e-03,  4.6750e-03,  1.0702e-02,  1.4091e-01,\n                       9.6669e-01, -1.0706e-02,  3.4769e-01,  4.1701e-01,  4.7771e-01,\n                       2.9951e-02, -1.8526e-01,  1.9248e-01,  2.1503e-01,  2.7342e-01,\n                      -4.5577e-02,  8.8300e-02, -2.7706e-02,  3.0632e-01,  1.0966e-03,\n                       2.0926e-03,  3.7744e-01, -1.8540e-01,  1.9337e-02, -2.4676e-01,\n                       2.6418e-01, -7.4546e-01,  5.6664e-02,  1.9828e-01, -1.3573e-02,\n                      -1.4293e-01, -2.7846e-01, -6.1851e-03, -5.1822e-02,  3.3534e-01,\n                       4.0697e-01,  4.1127e-02, -5.4226e-01, -9.2497e-03, -1.9319e-02,\n                      -3.8224e-02, -1.1630e-01,  3.1038e-01,  2.5770e-04, -1.7521e-01,\n                      -3.7975e-01,  3.7690e-01,  3.5603e-04,  2.5340e-01, -7.4998e-03,\n                      -3.2603e-01, -3.8403e-02, -4.0883e-03,  6.5609e-01,  3.5211e-01,\n                       3.1372e-01,  3.8790e-01, -2.6984e-02, -1.3583e-02, -2.5747e-02,\n                      -2.1427e-02,  2.4071e-02, -1.5059e-01,  4.2889e-03, -1.5699e-02,\n                      -2.4387e-02, -6.6077e-02,  4.2826e-02, -2.1795e-02, -3.1198e-01,\n                      -5.6988e-03, -7.3474e-02, -7.1515e-02,  3.0200e-02,  6.6751e-01,\n                      -2.9401e-02,  8.1136e-01, -9.2589e-02, -2.0908e-03,  1.0396e-02,\n                       3.9027e-01, -9.2394e-03,  3.8280e-01,  1.4974e-01, -1.8854e-02,\n                      -3.4324e-01, -2.3467e-01,  2.2789e-02, -7.4100e-02,  2.0332e-02,\n                      -2.9922e-03,  4.8248e-01, -5.3633e-05, -3.2468e-02,  1.5367e-01,\n                       7.1269e-02, -1.0037e-01,  6.5519e-01, -1.4743e-02,  1.8060e-02,\n                       4.1907e-03,  1.6694e-01,  1.0511e-01, -5.1513e-03, -2.5130e-02,\n                      -1.8139e-01, -2.7731e-04, -5.5858e-01, -1.6078e-02, -1.0184e-01,\n                       4.8779e-01, -1.7327e-03, -1.8205e-01,  5.7724e-02,  2.2173e-01,\n                      -8.8697e-02,  7.4637e-01, -2.6371e-01, -1.0311e-01,  6.7515e-02,\n                      -3.9745e-02, -3.7339e-02, -5.9461e-02, -9.7746e-03,  2.4723e-03,\n                       1.3686e-01,  8.4838e-01, -1.2913e-02, -9.5301e-02,  3.2520e-01,\n                       4.8191e-01,  7.7255e-03], requires_grad=True)),\n             ('mnet.features.5.conv.1.0.scale', tensor(0.0277)),\n             ('mnet.features.5.conv.1.0.zero_point', tensor(72)),\n             ('mnet.features.5.conv.2.weight',\n              tensor([[[[-0.1586]],\n              \n                       [[ 0.2270]],\n              \n                       [[-0.1009]],\n              \n                       ...,\n              \n                       [[-0.1117]],\n              \n                       [[ 0.2018]],\n              \n                       [[-0.0577]]],\n              \n              \n                      [[[ 0.1143]],\n              \n                       [[ 0.0286]],\n              \n                       [[ 0.0357]],\n              \n                       ...,\n              \n                       [[ 0.0607]],\n              \n                       [[ 0.0036]],\n              \n                       [[-0.0929]]],\n              \n              \n                      [[[ 0.0548]],\n              \n                       [[ 0.0591]],\n              \n                       [[ 0.0717]],\n              \n                       ...,\n              \n                       [[-0.0675]],\n              \n                       [[-0.1645]],\n              \n                       [[-0.0211]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-0.2360]],\n              \n                       [[ 0.2596]],\n              \n                       [[ 0.0354]],\n              \n                       ...,\n              \n                       [[-0.1888]],\n              \n                       [[ 0.3422]],\n              \n                       [[-0.0531]]],\n              \n              \n                      [[[ 0.0576]],\n              \n                       [[ 0.0672]],\n              \n                       [[-0.0640]],\n              \n                       ...,\n              \n                       [[-0.1793]],\n              \n                       [[-0.1409]],\n              \n                       [[ 0.3234]]],\n              \n              \n                      [[[-0.3046]],\n              \n                       [[-0.1047]],\n              \n                       [[-0.3664]],\n              \n                       ...,\n              \n                       [[ 0.0428]],\n              \n                       [[ 0.1142]],\n              \n                       [[ 0.3236]]]], size=(32, 192, 1, 1), dtype=torch.qint8,\n                     quantization_scheme=torch.per_channel_affine,\n                     scale=tensor([0.0036, 0.0036, 0.0042, 0.0033, 0.0051, 0.0031, 0.0062, 0.0041, 0.0034,\n                      0.0043, 0.0052, 0.0032, 0.0031, 0.0039, 0.0037, 0.0037, 0.0061, 0.0038,\n                      0.0032, 0.0069, 0.0043, 0.0027, 0.0036, 0.0066, 0.0039, 0.0035, 0.0043,\n                      0.0055, 0.0040, 0.0059, 0.0032, 0.0048], dtype=torch.float64),\n                     zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0]),\n                     axis=0)),\n             ('mnet.features.5.conv.2.bias',\n              Parameter containing:\n              tensor([ 0.0496,  0.2240, -0.3860,  0.6543, -0.2904, -0.0449,  0.2077, -0.3671,\n                      -0.5336,  0.1111,  0.1336, -0.2376, -0.4741, -0.5614,  0.0211, -0.3570,\n                       0.4153, -0.1789,  0.1563,  0.4878,  0.7725, -0.3785,  0.2525, -0.5551,\n                       0.1030, -0.1601, -0.4488,  0.5104,  0.5576, -0.1477, -0.2362,  0.1624],\n                     requires_grad=True)),\n             ('mnet.features.5.conv.2.scale', tensor(0.0437)),\n             ('mnet.features.5.conv.2.zero_point', tensor(62)),\n             ('mnet.features.6.conv.0.0.weight',\n              tensor([[[[-0.0528]],\n              \n                       [[-0.0138]],\n              \n                       [[ 0.0146]],\n              \n                       ...,\n              \n                       [[ 0.0000]],\n              \n                       [[ 0.0395]],\n              \n                       [[ 0.0173]]],\n              \n              \n                      [[[-0.0030]],\n              \n                       [[-0.0166]],\n              \n                       [[-0.0092]],\n              \n                       ...,\n              \n                       [[-0.0009]],\n              \n                       [[ 0.0211]],\n              \n                       [[-0.0044]]],\n              \n              \n                      [[[ 0.0083]],\n              \n                       [[-0.0147]],\n              \n                       [[ 0.0230]],\n              \n                       ...,\n              \n                       [[-0.0102]],\n              \n                       [[-0.0246]],\n              \n                       [[-0.0155]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-0.0464]],\n              \n                       [[ 0.0121]],\n              \n                       [[-0.0348]],\n              \n                       ...,\n              \n                       [[-0.0685]],\n              \n                       [[-0.0397]],\n              \n                       [[ 0.0337]]],\n              \n              \n                      [[[-0.0887]],\n              \n                       [[ 0.0628]],\n              \n                       [[-0.0121]],\n              \n                       ...,\n              \n                       [[ 0.0430]],\n              \n                       [[-0.0121]],\n              \n                       [[ 0.0207]]],\n              \n              \n                      [[[-0.0012]],\n              \n                       [[ 0.0460]],\n              \n                       [[-0.0577]],\n              \n                       ...,\n              \n                       [[-0.0183]],\n              \n                       [[ 0.0359]],\n              \n                       [[-0.0513]]]], size=(192, 32, 1, 1), dtype=torch.qint8,\n                     quantization_scheme=torch.per_channel_affine,\n                     scale=tensor([4.4385e-04, 2.9657e-04, 2.6744e-04, 1.4911e-04, 3.4220e-04, 8.6609e-04,\n                      2.2631e-05, 8.5281e-04, 8.2960e-04, 4.1147e-04, 6.3019e-04, 9.3072e-04,\n                      5.8214e-04, 4.6142e-04, 5.8261e-04, 5.6282e-04, 3.7138e-04, 4.0531e-04,\n                      9.0671e-04, 6.2014e-04, 3.7578e-04, 9.3200e-04, 7.4039e-04, 4.6329e-04,\n                      4.5594e-04, 5.0444e-04, 6.4648e-04, 2.3231e-04, 3.4866e-04, 4.1062e-04,\n                      7.6021e-04, 3.4003e-04, 5.4916e-04, 6.5872e-04, 7.0833e-04, 8.0045e-04,\n                      5.5836e-04, 5.2724e-04, 8.1346e-04, 5.4227e-04, 6.0609e-04, 4.8602e-04,\n                      3.0311e-04, 2.9055e-04, 7.1767e-04, 9.1375e-04, 4.0500e-04, 6.7476e-04,\n                      5.1708e-04, 6.3814e-04, 6.1194e-04, 8.5497e-04, 1.0866e-03, 5.7647e-04,\n                      4.4456e-04, 7.3910e-04, 3.3840e-04, 3.5749e-04, 7.1696e-04, 8.3134e-04,\n                      3.8516e-04, 2.2274e-04, 1.9671e-04, 4.8888e-04, 6.1639e-04, 6.1518e-04,\n                      4.6696e-04, 6.0564e-04, 1.0691e-03, 5.7040e-04, 6.6241e-04, 4.7775e-04,\n                      6.5693e-04, 7.9537e-04, 7.2757e-04, 3.3670e-04, 2.4507e-04, 6.7521e-04,\n                      5.4473e-04, 6.0607e-04, 6.4157e-04, 6.0443e-04, 5.0659e-04, 7.3903e-04,\n                      6.6997e-04, 7.2268e-04, 6.7368e-04, 4.4471e-04, 4.3176e-04, 7.0799e-04,\n                      6.0928e-04, 2.8637e-04, 7.9677e-04, 5.6812e-04, 4.6183e-04, 6.8671e-04,\n                      7.9138e-04, 5.1222e-04, 5.1402e-04, 3.1754e-04, 5.6228e-04, 3.7202e-04,\n                      5.8157e-04, 9.7251e-04, 4.0484e-04, 4.8383e-04, 8.5267e-04, 7.1887e-04,\n                      3.8369e-04, 7.5952e-04, 6.4210e-04, 7.0666e-04, 7.3004e-04, 6.2926e-04,\n                      6.3648e-04, 5.2904e-04, 3.5199e-04, 2.2108e-04, 5.1201e-04, 9.2066e-04,\n                      4.0036e-04, 8.6020e-04, 6.2216e-04, 4.3538e-04, 7.1655e-04, 5.4186e-04,\n                      6.4554e-04, 5.7961e-04, 7.4339e-04, 6.2491e-04, 6.2483e-04, 2.9332e-04,\n                      8.6287e-04, 5.9352e-04, 4.6685e-04, 4.6026e-04, 5.4064e-04, 6.3054e-04,\n                      3.3890e-04, 7.3059e-04, 6.5980e-04, 7.4522e-04, 5.9024e-04, 6.3309e-04,\n                      7.1681e-04, 5.1474e-04, 6.6537e-04, 6.9031e-04, 4.4761e-04, 7.2658e-04,\n                      4.8764e-04, 6.3418e-04, 6.2993e-04, 5.3659e-04, 7.1823e-04, 6.8370e-04,\n                      5.6783e-04, 4.5911e-04, 4.3068e-04, 4.9576e-04, 8.2896e-04, 6.0559e-04,\n                      4.8688e-04, 7.1541e-04, 8.3440e-04, 4.6505e-04, 3.0364e-04, 5.3881e-04,\n                      4.6959e-04, 4.1620e-04, 4.9684e-04, 3.8739e-04, 6.8497e-04, 8.9711e-04,\n                      6.5238e-04, 6.1172e-04, 5.1232e-04, 6.4180e-04, 3.4845e-04, 9.2618e-04,\n                      4.5076e-04, 8.3388e-04, 4.7783e-04, 8.2812e-04, 6.8233e-04, 6.1511e-04,\n                      5.5305e-04, 5.0183e-04, 4.9328e-04, 5.5208e-04, 8.6081e-04, 5.8924e-04],\n                     dtype=torch.float64),\n                     zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n                     axis=0)),\n             ('mnet.features.6.conv.0.0.bias',\n              Parameter containing:\n              tensor([ 0.0888,  0.1474,  0.1527,  0.3375,  0.2018,  0.1652, -0.0230,  0.0451,\n                       0.1216,  0.0992, -0.1444, -0.0475,  0.1001,  0.1545,  0.0838,  0.0650,\n                       0.1417,  0.1465,  0.0122,  0.0094,  0.1777, -0.0094,  0.0489,  0.1380,\n                       0.0532,  0.0796,  0.0045,  0.1328,  0.0940,  0.3247,  0.0875,  0.1563,\n                       0.1411,  0.0022,  0.1397, -0.0663,  0.0641,  0.0459,  0.0666,  0.1407,\n                      -0.1460,  0.0873,  0.1310,  0.1855, -0.0701, -0.0450,  0.1691,  0.0680,\n                       0.2455, -0.0093,  0.0542, -0.0163, -0.0374,  0.0605,  0.1476,  0.0347,\n                       0.2166,  0.1468,  0.0344,  0.0298,  0.1637,  0.2203,  0.1510,  0.0256,\n                       0.1290,  0.2019,  0.0954,  0.1720,  0.0294,  0.1565, -0.0105,  0.0978,\n                       0.1063,  0.0914, -0.0032,  0.1706,  0.1365,  0.1206,  0.1686,  0.1256,\n                       0.1517,  0.1114,  0.1562, -0.0471,  0.1211, -0.0519, -0.0605,  0.1910,\n                       0.1529,  0.1181,  0.1524,  0.1872, -0.0187, -0.0249,  0.0233, -0.1522,\n                       0.0343,  0.0929,  0.1287,  0.1317,  0.0169,  0.1163,  0.0564, -0.0871,\n                       0.1377,  0.1406,  0.0853, -0.0058,  0.1763,  0.1984, -0.1051,  0.0810,\n                      -0.0889,  0.0850,  0.0254,  0.1671,  0.1700,  0.1490, -0.0701,  0.2291,\n                       0.1611,  0.0158, -0.0625,  0.2139,  0.1330,  0.1071,  0.0417,  0.1685,\n                       0.1472,  0.1524, -0.0207,  0.1151,  0.0702,  0.1067,  0.0212,  0.1609,\n                       0.1045,  0.0935,  0.2216,  0.0044,  0.1567, -0.0339,  0.1088,  0.1881,\n                       0.0675,  0.0640,  0.1095,  0.1776,  0.1598,  0.1522,  0.1077,  0.1421,\n                       0.0293,  0.0434,  0.0712, -0.1521,  0.1822,  0.1229,  0.1316,  0.1591,\n                       0.0647,  0.0965,  0.0617, -0.0614,  0.0186,  0.1178,  0.1162,  0.1169,\n                       0.0259,  0.1379, -0.1249,  0.1989,  0.0815, -0.1174,  0.0339,  0.1282,\n                       0.0617, -0.1824,  0.1189, -0.0812,  0.1107,  0.0819,  0.0803, -0.0215,\n                       0.0591,  0.0033,  0.0199,  0.1498, -0.0685,  0.0808,  0.0259,  0.1028],\n                     requires_grad=True)),\n             ('mnet.features.6.conv.0.0.scale', tensor(0.0152)),\n             ('mnet.features.6.conv.0.0.zero_point', tensor(59)),\n             ('mnet.features.6.conv.1.0.weight',\n              tensor([[[[-0.3925, -0.7344, -0.3039],\n                        [ 0.0507, -1.1143, -0.2279],\n                        [ 0.2659,  1.6082,  0.5318]]],\n              \n              \n                      [[[ 1.0064,  2.9049,  1.0522],\n                        [-1.8527, -1.3724, -1.1665],\n                        [ 0.2059, -1.0293, -0.0915]]],\n              \n              \n                      [[[-0.3000,  1.4087,  0.2087],\n                        [ 1.1869,  1.0565,  1.6565],\n                        [-0.2348,  1.4217, -0.2087]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-0.1952,  0.7707, -0.1233],\n                        [-0.9659,  0.2877,  1.3050],\n                        [-0.3494, -1.2022, -0.0411]]],\n              \n              \n                      [[[-0.0107,  0.2246,  0.1498],\n                        [ 0.0749,  1.3585,  0.3530],\n                        [ 0.2139,  0.5455,  0.1818]]],\n              \n              \n                      [[[-0.3195, -0.8987,  0.4693],\n                        [-1.2782,  0.0399,  0.9986],\n                        [ 0.1198,  1.1384, -0.6890]]]], size=(192, 1, 3, 3),\n                     dtype=torch.qint8, quantization_scheme=torch.per_channel_affine,\n                     scale=tensor([0.0127, 0.0229, 0.0130, 0.0391, 0.0074, 0.0071, 0.0003, 0.0062, 0.0102,\n                      0.0131, 0.0084, 0.0087, 0.0107, 0.0130, 0.0126, 0.0169, 0.0250, 0.0183,\n                      0.0079, 0.0093, 0.0167, 0.0171, 0.0089, 0.0195, 0.0147, 0.0140, 0.0051,\n                      0.0182, 0.0172, 0.0074, 0.0084, 0.0179, 0.0123, 0.0042, 0.0109, 0.0146,\n                      0.0162, 0.0106, 0.0046, 0.0128, 0.0244, 0.0108, 0.0158, 0.0166, 0.0112,\n                      0.0070, 0.0308, 0.0147, 0.0099, 0.0089, 0.0077, 0.0127, 0.0055, 0.0082,\n                      0.0194, 0.0097, 0.0171, 0.0179, 0.0086, 0.0108, 0.0167, 0.0390, 0.0392,\n                      0.0106, 0.0050, 0.0232, 0.0096, 0.0069, 0.0061, 0.0124, 0.0167, 0.0157,\n                      0.0077, 0.0124, 0.0126, 0.0315, 0.0272, 0.0055, 0.0100, 0.0137, 0.0215,\n                      0.0093, 0.0118, 0.0161, 0.0103, 0.0029, 0.0114, 0.0122, 0.0097, 0.0119,\n                      0.0088, 0.0247, 0.0102, 0.0132, 0.0112, 0.0254, 0.0118, 0.0095, 0.0261,\n                      0.0282, 0.0111, 0.0134, 0.0164, 0.0214, 0.0202, 0.0066, 0.0098, 0.0103,\n                      0.0107, 0.0271, 0.0151, 0.0068, 0.0159, 0.0131, 0.0127, 0.0110, 0.0257,\n                      0.0209, 0.0228, 0.0135, 0.0169, 0.0170, 0.0101, 0.0179, 0.0136, 0.0168,\n                      0.0047, 0.0141, 0.0078, 0.0195, 0.0064, 0.0133, 0.0179, 0.0144, 0.0086,\n                      0.0132, 0.0175, 0.0103, 0.0291, 0.0103, 0.0207, 0.0071, 0.0148, 0.0090,\n                      0.0088, 0.0108, 0.0129, 0.0055, 0.0192, 0.0143, 0.0087, 0.0189, 0.0081,\n                      0.0102, 0.0071, 0.0121, 0.0065, 0.0111, 0.0217, 0.0284, 0.0086, 0.0111,\n                      0.0113, 0.0142, 0.0068, 0.0147, 0.0144, 0.0102, 0.0111, 0.0143, 0.0341,\n                      0.0191, 0.0043, 0.0226, 0.0154, 0.0132, 0.0116, 0.0339, 0.0159, 0.0032,\n                      0.0208, 0.0067, 0.0122, 0.0145, 0.0054, 0.0079, 0.0071, 0.0112, 0.0166,\n                      0.0103, 0.0107, 0.0100], dtype=torch.float64),\n                     zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n                     axis=0)),\n             ('mnet.features.6.conv.1.0.bias',\n              Parameter containing:\n              tensor([-1.7444e-02,  2.4885e-06, -1.1689e+00,  4.1745e-01,  8.5951e-01,\n                       3.4106e-01, -1.3507e-03,  2.5296e-01, -4.3041e-02, -1.8658e-01,\n                      -6.4516e-02, -5.2229e-02,  5.3666e-01, -5.6290e-02, -1.8363e-02,\n                      -1.1997e-01, -3.0845e-01, -2.8527e-01,  3.5136e-01,  4.6346e-01,\n                      -5.1987e-02, -2.2644e-01, -6.1410e-02, -9.4109e-03, -2.3370e-01,\n                      -8.5444e-02,  2.3444e-01,  3.4069e-02, -5.6131e-01,  7.7445e-01,\n                       5.1879e-01, -6.5234e-02, -2.1310e-02,  1.7629e-01,  7.2697e-03,\n                      -8.9725e-02,  1.2625e-01, -2.4273e-01,  2.2747e-01,  1.3072e-02,\n                      -9.2494e-02, -3.7316e-02, -2.5044e-01,  1.9305e-02, -8.1593e-02,\n                       1.4761e-01, -2.4024e-01, -8.7573e-02,  2.9752e-01,  3.4686e-01,\n                       2.6463e-01, -2.3738e-01,  4.0346e-01,  1.3286e-02, -8.9223e-02,\n                      -3.3882e-01,  1.1485e+00,  3.9342e-03,  2.0013e-02,  1.5714e-01,\n                      -1.7491e-01,  1.2231e+00,  9.3559e-01,  1.7649e-01,  3.4457e-01,\n                      -1.0243e-01,  3.9192e-03,  4.5550e-01,  2.5043e-01, -3.9568e-02,\n                      -3.3325e-03, -4.5965e-02,  1.4575e-01, -3.2716e-03, -2.2578e-01,\n                      -2.9465e-01,  1.0405e-01,  1.8234e-01, -2.7844e-02, -7.2485e-04,\n                      -1.5962e-01, -8.9381e-03, -3.4492e-02, -8.2184e-05, -5.6518e-03,\n                       2.8025e-01, -2.4588e-01, -5.1546e-02, -1.0353e-02, -1.6575e-02,\n                       1.3686e-02,  3.5374e-02, -2.7509e-01, -9.8762e-02,  3.6078e-01,\n                      -1.9773e-01,  4.6030e-01,  2.6209e-01, -4.4940e-02, -6.1829e-02,\n                       1.1026e-01, -1.1612e-02, -2.3426e-01, -2.5476e-01, -1.1952e-01,\n                       1.0213e-01,  2.1291e-03,  3.1313e-01, -4.6741e-02, -9.8661e-02,\n                      -9.6482e-02,  3.5563e-01, -1.4971e-02,  4.4182e-02,  5.4221e-02,\n                      -4.6044e-02, -2.0608e-02, -4.4409e-01, -1.0541e-01,  7.7070e-01,\n                       1.0664e-02, -7.7643e-02, -3.9320e-02, -2.3566e-01, -2.2279e-02,\n                      -2.4368e-01,  2.2214e-01,  2.8368e-02,  4.4318e-01, -2.5951e-01,\n                       4.9929e-01, -5.0739e-03, -4.4716e-01, -7.8004e-02, -1.0231e-01,\n                      -5.5298e-02, -3.4741e-01, -8.5875e-04, -8.0438e-01,  3.9392e-01,\n                       1.2254e-02, -1.1415e-01,  3.8469e-02,  6.2418e-01, -1.0833e-02,\n                       7.5625e-04,  2.5550e-02,  4.0513e-01,  1.6570e-02, -6.3296e-02,\n                      -3.2525e-02, -8.2391e-02,  2.8918e-01,  5.3337e-03,  4.2225e-01,\n                      -1.4472e-01,  3.7689e-01,  1.3856e-02, -2.2130e-01,  7.7541e-03,\n                      -3.9831e-02, -3.4038e-02, -9.1990e-02, -2.1898e-01,  2.2601e-01,\n                       5.2631e-03, -4.1725e-02, -4.1862e-03, -3.4181e-01,  1.0437e-01,\n                      -7.3987e-02, -4.4949e-02,  2.5438e-01, -2.7567e-01,  7.1171e-02,\n                       5.7826e-02, -2.0438e-03, -2.2490e-01, -3.1060e-01,  3.1673e-01,\n                      -4.3089e-01,  3.0315e-01, -1.2013e-02, -1.0253e-01,  3.0053e-01,\n                       5.3351e-03,  1.7344e-01, -1.2977e-02, -9.7626e-02, -6.4480e-03,\n                      -2.8068e-01, -1.4770e-02], requires_grad=True)),\n             ('mnet.features.6.conv.1.0.scale', tensor(0.0262)),\n             ('mnet.features.6.conv.1.0.zero_point', tensor(70)),\n             ('mnet.features.6.conv.2.weight',\n              tensor([[[[-0.1127]],\n              \n                       [[ 0.1408]],\n              \n                       [[-0.2066]],\n              \n                       ...,\n              \n                       [[-0.0047]],\n              \n                       [[-0.0282]],\n              \n                       [[ 0.0282]]],\n              \n              \n                      [[[-0.2377]],\n              \n                       [[-0.0047]],\n              \n                       [[ 0.2517]],\n              \n                       ...,\n              \n                       [[ 0.0140]],\n              \n                       [[ 0.0280]],\n              \n                       [[ 0.0932]]],\n              \n              \n                      [[[-0.0309]],\n              \n                       [[-0.1502]],\n              \n                       [[-0.3622]],\n              \n                       ...,\n              \n                       [[-0.0133]],\n              \n                       [[ 0.1988]],\n              \n                       [[-0.1546]]],\n              \n              \n                      ...,\n              \n              \n                      [[[ 0.0699]],\n              \n                       [[ 0.2389]],\n              \n                       [[-0.2447]],\n              \n                       ...,\n              \n                       [[-0.0291]],\n              \n                       [[-0.0291]],\n              \n                       [[-0.1049]]],\n              \n              \n                      [[[ 0.1183]],\n              \n                       [[-0.4259]],\n              \n                       [[ 0.1301]],\n              \n                       ...,\n              \n                       [[-0.5738]],\n              \n                       [[-0.2958]],\n              \n                       [[ 0.0651]]],\n              \n              \n                      [[[ 0.1763]],\n              \n                       [[-0.2874]],\n              \n                       [[-0.1110]],\n              \n                       ...,\n              \n                       [[ 0.0522]],\n              \n                       [[-0.0718]],\n              \n                       [[-0.2351]]]], size=(32, 192, 1, 1), dtype=torch.qint8,\n                     quantization_scheme=torch.per_channel_affine,\n                     scale=tensor([0.0047, 0.0047, 0.0044, 0.0036, 0.0047, 0.0057, 0.0049, 0.0047, 0.0039,\n                      0.0057, 0.0073, 0.0029, 0.0032, 0.0030, 0.0029, 0.0049, 0.0071, 0.0051,\n                      0.0033, 0.0067, 0.0047, 0.0053, 0.0040, 0.0049, 0.0052, 0.0067, 0.0057,\n                      0.0045, 0.0047, 0.0058, 0.0059, 0.0065], dtype=torch.float64),\n                     zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0]),\n                     axis=0)),\n             ('mnet.features.6.conv.2.bias',\n              Parameter containing:\n              tensor([-0.4031,  0.0629, -0.2610,  0.2254,  0.2488,  0.3302, -0.3730,  0.2361,\n                      -0.0132,  0.6261,  0.2445,  0.1554,  0.1755,  0.2180,  0.2707, -0.5602,\n                      -0.6095, -0.5360,  0.0985,  0.7963,  0.2822,  0.1766, -0.0605,  0.2343,\n                      -0.0994, -0.6745, -0.3791,  0.4018, -0.3114, -0.4477,  0.5344, -0.3280],\n                     requires_grad=True)),\n             ('mnet.features.6.conv.2.scale', tensor(0.0360)),\n             ('mnet.features.6.conv.2.zero_point', tensor(64)),\n             ('mnet.features.7.conv.0.0.weight',\n              tensor([[[[ 0.0306]],\n              \n                       [[ 0.1387]],\n              \n                       [[ 0.0612]],\n              \n                       ...,\n              \n                       [[ 0.0415]],\n              \n                       [[-0.0109]],\n              \n                       [[ 0.0120]]],\n              \n              \n                      [[[ 0.0076]],\n              \n                       [[ 0.0131]],\n              \n                       [[ 0.0200]],\n              \n                       ...,\n              \n                       [[ 0.0007]],\n              \n                       [[ 0.0173]],\n              \n                       [[-0.0090]]],\n              \n              \n                      [[[-0.0023]],\n              \n                       [[-0.0162]],\n              \n                       [[ 0.0416]],\n              \n                       ...,\n              \n                       [[-0.0145]],\n              \n                       [[ 0.0075]],\n              \n                       [[ 0.0509]]],\n              \n              \n                      ...,\n              \n              \n                      [[[ 0.0342]],\n              \n                       [[ 0.0192]],\n              \n                       [[-0.0134]],\n              \n                       ...,\n              \n                       [[-0.0484]],\n              \n                       [[-0.0058]],\n              \n                       [[-0.0159]]],\n              \n              \n                      [[[-0.0395]],\n              \n                       [[ 0.0523]],\n              \n                       [[-0.0483]],\n              \n                       ...,\n              \n                       [[ 0.0710]],\n              \n                       [[-0.0434]],\n              \n                       [[-0.0414]]],\n              \n              \n                      [[[ 0.0604]],\n              \n                       [[-0.0040]],\n              \n                       [[-0.0277]],\n              \n                       ...,\n              \n                       [[ 0.0704]],\n              \n                       [[-0.0377]],\n              \n                       [[ 0.0634]]]], size=(192, 32, 1, 1), dtype=torch.qint8,\n                     quantization_scheme=torch.per_channel_affine,\n                     scale=tensor([0.0011, 0.0007, 0.0006, 0.0010, 0.0005, 0.0005, 0.0006, 0.0008, 0.0013,\n                      0.0011, 0.0007, 0.0009, 0.0006, 0.0004, 0.0004, 0.0008, 0.0006, 0.0009,\n                      0.0005, 0.0013, 0.0009, 0.0006, 0.0002, 0.0013, 0.0009, 0.0004, 0.0008,\n                      0.0009, 0.0009, 0.0010, 0.0008, 0.0009, 0.0007, 0.0011, 0.0008, 0.0006,\n                      0.0009, 0.0008, 0.0012, 0.0007, 0.0006, 0.0005, 0.0008, 0.0012, 0.0009,\n                      0.0006, 0.0010, 0.0010, 0.0007, 0.0006, 0.0010, 0.0009, 0.0006, 0.0006,\n                      0.0009, 0.0005, 0.0006, 0.0015, 0.0007, 0.0007, 0.0006, 0.0003, 0.0010,\n                      0.0009, 0.0006, 0.0008, 0.0006, 0.0006, 0.0009, 0.0007, 0.0013, 0.0012,\n                      0.0010, 0.0006, 0.0011, 0.0003, 0.0002, 0.0008, 0.0004, 0.0011, 0.0007,\n                      0.0010, 0.0011, 0.0004, 0.0008, 0.0003, 0.0006, 0.0008, 0.0010, 0.0010,\n                      0.0007, 0.0006, 0.0009, 0.0011, 0.0005, 0.0004, 0.0011, 0.0010, 0.0009,\n                      0.0013, 0.0014, 0.0004, 0.0010, 0.0007, 0.0007, 0.0012, 0.0006, 0.0004,\n                      0.0009, 0.0005, 0.0004, 0.0006, 0.0007, 0.0006, 0.0008, 0.0008, 0.0008,\n                      0.0008, 0.0003, 0.0010, 0.0007, 0.0011, 0.0009, 0.0006, 0.0007, 0.0011,\n                      0.0004, 0.0007, 0.0008, 0.0008, 0.0011, 0.0007, 0.0008, 0.0006, 0.0010,\n                      0.0006, 0.0009, 0.0007, 0.0010, 0.0005, 0.0006, 0.0005, 0.0004, 0.0012,\n                      0.0010, 0.0007, 0.0012, 0.0008, 0.0008, 0.0004, 0.0013, 0.0011, 0.0008,\n                      0.0008, 0.0006, 0.0006, 0.0005, 0.0010, 0.0003, 0.0006, 0.0010, 0.0010,\n                      0.0006, 0.0007, 0.0006, 0.0012, 0.0010, 0.0008, 0.0007, 0.0007, 0.0012,\n                      0.0004, 0.0005, 0.0012, 0.0012, 0.0010, 0.0014, 0.0009, 0.0011, 0.0009,\n                      0.0005, 0.0019, 0.0007, 0.0008, 0.0005, 0.0005, 0.0007, 0.0005, 0.0008,\n                      0.0008, 0.0010, 0.0010], dtype=torch.float64),\n                     zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n                     axis=0)),\n             ('mnet.features.7.conv.0.0.bias',\n              Parameter containing:\n              tensor([-0.0149,  0.0806,  0.2500, -0.0645,  0.2613,  0.2027, -0.0281,  0.0518,\n                       0.0300,  0.1968,  0.0853,  0.0081,  0.2239,  0.1624,  0.2203, -0.0762,\n                       0.0969, -0.1517,  0.2729,  0.1872, -0.0811,  0.1071,  0.2676, -0.1207,\n                       0.0700,  0.1284,  0.2065,  0.0164, -0.0326, -0.0202,  0.0417,  0.1158,\n                      -0.0603, -0.0764, -0.0835,  0.2284, -0.0900, -0.0555, -0.0193,  0.3115,\n                       0.2202,  0.1221, -0.2652,  0.1133, -0.0174,  0.1141, -0.0754, -0.0380,\n                       0.1107,  0.2191,  0.0065,  0.0634, -0.0236,  0.0370,  0.1004,  0.1192,\n                       0.1362, -0.1095,  0.0632,  0.0841,  0.1281,  0.1666, -0.0622, -0.1717,\n                       0.1249, -0.0201,  0.0872, -0.1375, -0.0795, -0.0004, -0.0342,  0.0451,\n                      -0.0696,  0.1207, -0.0638,  0.2224,  0.1428,  0.1344,  0.2634, -0.0191,\n                       0.1497, -0.0150, -0.0621,  0.1709,  0.1007,  0.1333,  0.0856, -0.0085,\n                      -0.0212, -0.0899,  0.0045,  0.0067,  0.0128, -0.0624,  0.1203,  0.1446,\n                      -0.0960,  0.1572, -0.1622,  0.0747, -0.0616,  0.2528, -0.1209,  0.0886,\n                      -0.0928, -0.0353,  0.0476,  0.2101, -0.1419,  0.0256,  0.2115,  0.2370,\n                       0.0702,  0.2298,  0.0576,  0.0872,  0.0688, -0.1189,  0.1489, -0.1388,\n                       0.0072, -0.0954,  0.1381,  0.0768,  0.0506,  0.0663,  0.2663,  0.1316,\n                       0.1874, -0.0374, -0.0494, -0.1436, -0.0763,  0.1119, -0.0365,  0.0942,\n                       0.1171,  0.0066,  0.0896,  0.1332,  0.1694,  0.1105,  0.1119, -0.0979,\n                      -0.0848,  0.1953,  0.0942, -0.0293, -0.0991,  0.2023, -0.1255,  0.0597,\n                      -0.0015, -0.1447, -0.0872, -0.0021,  0.1903, -0.0040,  0.2054,  0.0514,\n                      -0.0066, -0.0078, -0.1270, -0.1315, -0.0188, -0.1042, -0.2325,  0.2594,\n                       0.0613,  0.1177,  0.1826,  0.2416, -0.1337,  0.1038,  0.2333, -0.0672,\n                      -0.1056,  0.0398, -0.0360,  0.1877,  0.0819, -0.1436,  0.0991, -0.0786,\n                       0.2226,  0.2251, -0.0465,  0.1316,  0.1290,  0.0442,  0.0067, -0.1975],\n                     requires_grad=True)),\n             ('mnet.features.7.conv.0.0.scale', tensor(0.0245)),\n             ('mnet.features.7.conv.0.0.zero_point', tensor(63)),\n             ('mnet.features.7.conv.1.0.weight',\n              tensor([[[[ 0.0691,  0.1204,  0.1070],\n                        [ 0.1315,  0.2831,  0.2430],\n                        [ 0.1070,  0.1984,  0.1828]]],\n              \n              \n                      [[[-0.1319, -0.2361, -0.1875],\n                        [-0.2361, -0.4444, -0.3541],\n                        [-0.1736, -0.3611, -0.2708]]],\n              \n              \n                      [[[-0.2730, -0.4753, -0.3034],\n                        [-0.6674, -1.2944, -0.6068],\n                        [-0.4349, -0.8293, -0.2832]]],\n              \n              \n                      ...,\n              \n              \n                      [[[ 0.1692,  0.3275,  0.2402],\n                        [ 0.3329,  0.6932,  0.4858],\n                        [ 0.2674,  0.4749,  0.2893]]],\n              \n              \n                      [[[-0.1167, -0.3666, -0.2611],\n                        [-0.2277, -0.7110, -0.5721],\n                        [-0.1444, -0.5055, -0.3944]]],\n              \n              \n                      [[[ 0.2925,  0.6115,  0.4874],\n                        [ 0.5140,  1.1255,  0.9394],\n                        [ 0.3456,  0.7799,  0.6381]]]], size=(192, 1, 3, 3),\n                     dtype=torch.qint8, quantization_scheme=torch.per_channel_affine,\n                     scale=tensor([0.0022, 0.0035, 0.0101, 0.0055, 0.0043, 0.0065, 0.0066, 0.0071, 0.0030,\n                      0.0069, 0.0061, 0.0057, 0.0048, 0.0075, 0.0069, 0.0051, 0.0058, 0.0063,\n                      0.0086, 0.0037, 0.0063, 0.0033, 0.0173, 0.0069, 0.0052, 0.0124, 0.0059,\n                      0.0034, 0.0044, 0.0047, 0.0042, 0.0049, 0.0060, 0.0057, 0.0041, 0.0065,\n                      0.0045, 0.0060, 0.0060, 0.0048, 0.0032, 0.0098, 0.0109, 0.0040, 0.0055,\n                      0.0120, 0.0077, 0.0057, 0.0065, 0.0053, 0.0029, 0.0055, 0.0074, 0.0061,\n                      0.0029, 0.0053, 0.0060, 0.0054, 0.0048, 0.0046, 0.0049, 0.0174, 0.0033,\n                      0.0079, 0.0044, 0.0047, 0.0055, 0.0078, 0.0059, 0.0027, 0.0057, 0.0039,\n                      0.0051, 0.0047, 0.0065, 0.0070, 0.0190, 0.0069, 0.0088, 0.0068, 0.0062,\n                      0.0060, 0.0051, 0.0123, 0.0046, 0.0136, 0.0066, 0.0047, 0.0053, 0.0047,\n                      0.0040, 0.0053, 0.0028, 0.0068, 0.0066, 0.0150, 0.0053, 0.0029, 0.0088,\n                      0.0044, 0.0029, 0.0066, 0.0092, 0.0047, 0.0071, 0.0036, 0.0055, 0.0061,\n                      0.0071, 0.0079, 0.0072, 0.0053, 0.0045, 0.0076, 0.0047, 0.0043, 0.0026,\n                      0.0067, 0.0121, 0.0058, 0.0047, 0.0060, 0.0053, 0.0039, 0.0055, 0.0035,\n                      0.0040, 0.0044, 0.0033, 0.0056, 0.0062, 0.0118, 0.0044, 0.0052, 0.0066,\n                      0.0059, 0.0034, 0.0035, 0.0047, 0.0148, 0.0043, 0.0143, 0.0124, 0.0054,\n                      0.0068, 0.0053, 0.0031, 0.0038, 0.0075, 0.0117, 0.0067, 0.0038, 0.0058,\n                      0.0067, 0.0083, 0.0070, 0.0083, 0.0063, 0.0057, 0.0032, 0.0061, 0.0060,\n                      0.0073, 0.0046, 0.0055, 0.0060, 0.0098, 0.0054, 0.0030, 0.0048, 0.0049,\n                      0.0104, 0.0132, 0.0038, 0.0029, 0.0064, 0.0039, 0.0055, 0.0046, 0.0042,\n                      0.0047, 0.0037, 0.0046, 0.0063, 0.0036, 0.0033, 0.0054, 0.0155, 0.0062,\n                      0.0055, 0.0056, 0.0089], dtype=torch.float64),\n                     zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n                     axis=0)),\n             ('mnet.features.7.conv.1.0.bias',\n              Parameter containing:\n              tensor([ 7.1332e-02,  7.7587e-01,  1.5572e+00, -4.0734e-02,  1.2873e+00,\n                       7.4262e-01,  7.9395e-01,  5.6659e-01,  7.2450e-01,  8.7699e-01,\n                       6.8992e-01,  6.7198e-01,  8.8649e-01,  7.6893e-01,  1.1075e+00,\n                       6.8576e-03,  4.0551e-01, -6.7805e-03,  1.3255e+00, -3.1290e-02,\n                      -3.9475e-02,  4.1713e-01,  1.1410e+00, -6.0263e-02,  5.1470e-01,\n                       4.0008e-02,  1.0154e+00,  8.1080e-01,  8.7874e-01,  1.3537e-02,\n                       4.9523e-01, -6.5611e-02, -1.6112e-02, -2.6312e-03,  3.3739e-02,\n                       5.9047e-01,  8.0069e-01, -4.3328e-02,  7.6329e-01,  1.4193e+00,\n                       5.0640e-01, -1.5448e-01, -2.2789e-02, -2.3656e-02, -5.0484e-02,\n                      -3.0679e-02, -3.9353e-03, -6.3546e-02,  2.0156e-02,  1.1507e+00,\n                      -3.9147e-03,  7.1517e-01,  2.4837e-01,  6.2483e-01,  3.4183e-01,\n                       5.1923e-01,  7.4337e-01,  3.5759e-03,  8.8224e-01,  5.7450e-01,\n                      -3.0280e-02, -7.6450e-02,  4.3542e-01, -3.8982e-02,  7.4230e-01,\n                      -2.5163e-02,  6.2562e-01, -9.1459e-03, -4.1406e-02,  4.2302e-01,\n                      -5.2366e-02, -7.0959e-02, -1.5812e-02,  8.9721e-01, -3.4768e-02,\n                       9.6738e-01, -2.9930e-03,  5.6773e-01,  5.7511e-01, -5.3089e-02,\n                       9.3298e-01, -5.8306e-02, -2.3915e-02,  2.9147e-02, -8.8919e-02,\n                      -1.5538e-03,  6.4698e-01,  4.1971e-02,  7.8221e-01,  2.1210e-02,\n                       5.7891e-01,  6.3545e-01, -9.3974e-03, -6.1778e-02,  5.3713e-01,\n                      -1.8855e-02,  1.6263e-02, -2.1550e-02, -2.9392e-02, -5.8392e-02,\n                       7.2227e-02, -3.8804e-01, -3.8807e-02,  6.3783e-01, -2.5247e-02,\n                       3.0281e-02,  4.8874e-01,  1.2167e+00, -7.8912e-02,  5.9082e-01,\n                       9.1068e-01,  1.2938e+00,  9.6197e-01,  1.0137e+00, -5.3707e-02,\n                       5.3660e-01,  4.3704e-01, -1.1913e-02,  9.0915e-01, -3.0784e-03,\n                       5.1486e-01, -1.9854e-02,  6.0238e-01,  5.4827e-01,  4.8485e-01,\n                       7.7386e-01,  9.5375e-01,  6.6790e-01,  5.0959e-01, -2.7497e-02,\n                      -4.2165e-02, -1.6545e-02,  2.9441e-02,  6.1245e-01, -6.7195e-02,\n                       2.1305e-01, -3.3741e-02,  4.3249e-01,  5.6536e-03, -4.3455e-02,\n                       5.7551e-01, -1.7057e-02, -1.8770e-02,  6.5119e-03, -5.3306e-02,\n                       9.3490e-01,  1.5363e-02,  4.7190e-02, -2.8023e-02, -1.5636e-01,\n                       5.2408e-03, -2.6228e-03,  4.6763e-01, -3.3288e-02,  6.2886e-01,\n                       5.7138e-01,  4.7789e-01, -1.0122e-01,  8.4435e-01,  8.3081e-01,\n                       6.4009e-01, -1.6549e-01, -8.6008e-03,  3.2595e-02, -3.2285e-02,\n                       1.2092e-02, -4.6222e-02,  8.9309e-01,  5.7335e-01,  6.1015e-01,\n                       6.5360e-01,  1.4003e+00,  8.9197e-03,  5.2578e-01,  5.1510e-01,\n                      -5.0978e-02,  3.1711e-02, -1.3827e-02, -1.9634e-02,  1.0397e+00,\n                       5.3051e-01,  1.6134e-02,  4.5822e-01, -3.4118e-02,  5.5746e-01,\n                       5.7507e-01,  8.9619e-03,  2.5011e-02,  6.3139e-01, -2.7220e-02,\n                       1.0269e+00, -3.1868e-02], requires_grad=True)),\n             ('mnet.features.7.conv.1.0.scale', tensor(0.0267)),\n             ('mnet.features.7.conv.1.0.zero_point', tensor(55)),\n             ('mnet.features.7.conv.2.weight',\n              tensor([[[[ 0.1463]],\n              \n                       [[-0.1525]],\n              \n                       [[ 0.2147]],\n              \n                       ...,\n              \n                       [[ 0.0467]],\n              \n                       [[-0.0156]],\n              \n                       [[ 0.2894]]],\n              \n              \n                      [[[-0.0864]],\n              \n                       [[ 0.0518]],\n              \n                       [[-0.0432]],\n              \n                       ...,\n              \n                       [[ 0.0907]],\n              \n                       [[ 0.1469]],\n              \n                       [[ 0.0864]]],\n              \n              \n                      [[[ 0.1614]],\n              \n                       [[ 0.0862]],\n              \n                       [[-0.0696]],\n              \n                       ...,\n              \n                       [[ 0.3033]],\n              \n                       [[-0.0334]],\n              \n                       [[ 0.1725]]],\n              \n              \n                      ...,\n              \n              \n                      [[[ 0.0310]],\n              \n                       [[ 0.3033]],\n              \n                       [[ 0.2930]],\n              \n                       ...,\n              \n                       [[ 0.0069]],\n              \n                       [[-0.0448]],\n              \n                       [[-0.1551]]],\n              \n              \n                      [[[-0.3454]],\n              \n                       [[ 0.1001]],\n              \n                       [[-0.1130]],\n              \n                       ...,\n              \n                       [[ 0.1162]],\n              \n                       [[ 0.0904]],\n              \n                       [[-0.0904]]],\n              \n              \n                      [[[-0.0459]],\n              \n                       [[ 0.0337]],\n              \n                       [[-0.1194]],\n              \n                       ...,\n              \n                       [[-0.0184]],\n              \n                       [[ 0.3889]],\n              \n                       [[-0.1164]]]], size=(64, 192, 1, 1), dtype=torch.qint8,\n                     quantization_scheme=torch.per_channel_affine,\n                     scale=tensor([0.0031, 0.0043, 0.0028, 0.0037, 0.0034, 0.0042, 0.0027, 0.0038, 0.0031,\n                      0.0036, 0.0028, 0.0035, 0.0039, 0.0038, 0.0033, 0.0031, 0.0035, 0.0030,\n                      0.0036, 0.0042, 0.0046, 0.0053, 0.0033, 0.0033, 0.0031, 0.0030, 0.0026,\n                      0.0039, 0.0031, 0.0026, 0.0028, 0.0032, 0.0031, 0.0030, 0.0029, 0.0041,\n                      0.0035, 0.0038, 0.0030, 0.0028, 0.0037, 0.0047, 0.0032, 0.0038, 0.0032,\n                      0.0035, 0.0034, 0.0047, 0.0033, 0.0027, 0.0033, 0.0027, 0.0039, 0.0037,\n                      0.0033, 0.0028, 0.0025, 0.0026, 0.0033, 0.0031, 0.0035, 0.0034, 0.0032,\n                      0.0031], dtype=torch.float64),\n                     zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n                     axis=0)),\n             ('mnet.features.7.conv.2.bias',\n              Parameter containing:\n              tensor([-0.4976, -0.1440, -0.7594, -0.0722, -0.5986, -0.0992,  0.1153, -0.8721,\n                      -0.0578,  1.8691, -1.3159,  1.1798,  0.0801, -0.4784,  0.4480,  0.1196,\n                      -0.2310, -0.3213, -0.1607, -0.3104, -0.0503, -0.9118,  0.7164,  0.3580,\n                       0.1254, -0.1515,  0.6041, -0.7956,  0.1655, -0.9316,  0.1159,  0.2582,\n                       1.0710,  0.5034,  0.2782, -0.1022, -0.1333, -0.0306, -1.0268, -0.1422,\n                       1.2094,  0.8521, -0.7866, -0.7901, -0.6849, -0.0222,  0.2029, -0.3936,\n                       0.8338,  0.3265,  0.5582,  0.1807,  1.1612,  0.5268,  0.8648, -0.0116,\n                       0.7818, -1.0110,  0.2141, -0.2658, -0.2521,  0.6579,  1.0101, -0.1479],\n                     requires_grad=True)),\n             ('mnet.features.7.conv.2.scale', tensor(0.0449)),\n             ('mnet.features.7.conv.2.zero_point', tensor(66)),\n             ('mnet.features.8.conv.0.0.weight',\n              tensor([[[[ 0.0091]],\n              \n                       [[ 0.0323]],\n              \n                       [[ 0.0168]],\n              \n                       ...,\n              \n                       [[-0.0192]],\n              \n                       [[-0.0175]],\n              \n                       [[ 0.0340]]],\n              \n              \n                      [[[-0.0076]],\n              \n                       [[-0.0139]],\n              \n                       [[ 0.0413]],\n              \n                       ...,\n              \n                       [[-0.0170]],\n              \n                       [[-0.0170]],\n              \n                       [[ 0.0408]]],\n              \n              \n                      [[[ 0.0348]],\n              \n                       [[-0.0097]],\n              \n                       [[-0.0129]],\n              \n                       ...,\n              \n                       [[ 0.0348]],\n              \n                       [[ 0.0116]],\n              \n                       [[ 0.0026]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-0.0319]],\n              \n                       [[-0.0061]],\n              \n                       [[ 0.0030]],\n              \n                       ...,\n              \n                       [[ 0.0046]],\n              \n                       [[ 0.0349]],\n              \n                       [[ 0.0177]]],\n              \n              \n                      [[[-0.0040]],\n              \n                       [[ 0.0017]],\n              \n                       [[-0.0288]],\n              \n                       ...,\n              \n                       [[ 0.0456]],\n              \n                       [[-0.0179]],\n              \n                       [[ 0.0058]]],\n              \n              \n                      [[[ 0.0336]],\n              \n                       [[ 0.0244]],\n              \n                       [[-0.0076]],\n              \n                       ...,\n              \n                       [[-0.0193]],\n              \n                       [[-0.0163]],\n              \n                       [[-0.0493]]]], size=(384, 64, 1, 1), dtype=torch.qint8,\n                     quantization_scheme=torch.per_channel_affine,\n                     scale=tensor([0.0003, 0.0004, 0.0006, 0.0005, 0.0008, 0.0005, 0.0005, 0.0008, 0.0004,\n                      0.0007, 0.0006, 0.0003, 0.0004, 0.0007, 0.0008, 0.0005, 0.0005, 0.0006,\n                      0.0004, 0.0005, 0.0004, 0.0007, 0.0005, 0.0006, 0.0004, 0.0004, 0.0006,\n                      0.0008, 0.0005, 0.0004, 0.0004, 0.0004, 0.0005, 0.0006, 0.0007, 0.0010,\n                      0.0006, 0.0003, 0.0004, 0.0008, 0.0008, 0.0003, 0.0004, 0.0003, 0.0005,\n                      0.0005, 0.0006, 0.0004, 0.0006, 0.0006, 0.0006, 0.0003, 0.0004, 0.0009,\n                      0.0004, 0.0005, 0.0006, 0.0003, 0.0003, 0.0004, 0.0004, 0.0004, 0.0006,\n                      0.0005, 0.0005, 0.0009, 0.0004, 0.0005, 0.0006, 0.0005, 0.0005, 0.0005,\n                      0.0011, 0.0004, 0.0004, 0.0002, 0.0004, 0.0005, 0.0005, 0.0004, 0.0003,\n                      0.0006, 0.0004, 0.0004, 0.0003, 0.0006, 0.0006, 0.0010, 0.0005, 0.0003,\n                      0.0004, 0.0004, 0.0007, 0.0004, 0.0003, 0.0007, 0.0005, 0.0004, 0.0006,\n                      0.0008, 0.0005, 0.0005, 0.0004, 0.0004, 0.0011, 0.0005, 0.0005, 0.0005,\n                      0.0008, 0.0009, 0.0003, 0.0005, 0.0008, 0.0003, 0.0004, 0.0005, 0.0003,\n                      0.0006, 0.0004, 0.0006, 0.0005, 0.0004, 0.0005, 0.0007, 0.0005, 0.0007,\n                      0.0005, 0.0008, 0.0004, 0.0004, 0.0004, 0.0002, 0.0002, 0.0005, 0.0006,\n                      0.0004, 0.0003, 0.0005, 0.0006, 0.0008, 0.0004, 0.0007, 0.0005, 0.0003,\n                      0.0005, 0.0005, 0.0004, 0.0004, 0.0007, 0.0004, 0.0005, 0.0005, 0.0006,\n                      0.0007, 0.0005, 0.0005, 0.0005, 0.0006, 0.0006, 0.0004, 0.0007, 0.0006,\n                      0.0003, 0.0004, 0.0003, 0.0005, 0.0005, 0.0003, 0.0004, 0.0006, 0.0005,\n                      0.0006, 0.0008, 0.0008, 0.0002, 0.0007, 0.0005, 0.0006, 0.0006, 0.0005,\n                      0.0004, 0.0009, 0.0004, 0.0003, 0.0005, 0.0004, 0.0003, 0.0004, 0.0004,\n                      0.0004, 0.0003, 0.0005, 0.0004, 0.0002, 0.0005, 0.0003, 0.0004, 0.0005,\n                      0.0006, 0.0006, 0.0004, 0.0005, 0.0004, 0.0004, 0.0005, 0.0003, 0.0003,\n                      0.0003, 0.0005, 0.0004, 0.0003, 0.0004, 0.0008, 0.0007, 0.0004, 0.0004,\n                      0.0008, 0.0005, 0.0008, 0.0003, 0.0008, 0.0004, 0.0004, 0.0009, 0.0005,\n                      0.0005, 0.0003, 0.0008, 0.0005, 0.0006, 0.0010, 0.0006, 0.0005, 0.0005,\n                      0.0007, 0.0007, 0.0005, 0.0004, 0.0007, 0.0005, 0.0005, 0.0006, 0.0005,\n                      0.0005, 0.0003, 0.0008, 0.0006, 0.0004, 0.0003, 0.0005, 0.0004, 0.0006,\n                      0.0004, 0.0006, 0.0007, 0.0006, 0.0006, 0.0007, 0.0004, 0.0008, 0.0005,\n                      0.0004, 0.0008, 0.0004, 0.0006, 0.0003, 0.0004, 0.0004, 0.0007, 0.0004,\n                      0.0007, 0.0003, 0.0005, 0.0006, 0.0003, 0.0005, 0.0003, 0.0003, 0.0003,\n                      0.0004, 0.0005, 0.0005, 0.0003, 0.0003, 0.0006, 0.0007, 0.0003, 0.0005,\n                      0.0004, 0.0003, 0.0009, 0.0004, 0.0004, 0.0004, 0.0003, 0.0005, 0.0004,\n                      0.0003, 0.0007, 0.0004, 0.0004, 0.0004, 0.0004, 0.0006, 0.0007, 0.0003,\n                      0.0006, 0.0003, 0.0004, 0.0005, 0.0004, 0.0007, 0.0003, 0.0003, 0.0009,\n                      0.0006, 0.0010, 0.0005, 0.0005, 0.0004, 0.0006, 0.0005, 0.0004, 0.0004,\n                      0.0008, 0.0004, 0.0006, 0.0003, 0.0004, 0.0005, 0.0003, 0.0007, 0.0006,\n                      0.0005, 0.0004, 0.0005, 0.0004, 0.0005, 0.0005, 0.0005, 0.0004, 0.0003,\n                      0.0006, 0.0007, 0.0006, 0.0006, 0.0006, 0.0011, 0.0004, 0.0004, 0.0003,\n                      0.0006, 0.0006, 0.0007, 0.0004, 0.0006, 0.0004, 0.0004, 0.0008, 0.0009,\n                      0.0009, 0.0007, 0.0003, 0.0005, 0.0003, 0.0004, 0.0003, 0.0005, 0.0006,\n                      0.0002, 0.0004, 0.0008, 0.0006, 0.0004, 0.0007, 0.0003, 0.0006, 0.0007,\n                      0.0006, 0.0006, 0.0006, 0.0005, 0.0006, 0.0005], dtype=torch.float64),\n                     zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n                     axis=0)),\n             ('mnet.features.8.conv.0.0.bias',\n              Parameter containing:\n              tensor([ 0.1564,  0.1161,  0.1162,  0.1456,  0.1691,  0.0610,  0.1781,  0.0255,\n                       0.1363, -0.0663,  0.1114,  0.1465,  0.1575,  0.0518, -0.0179,  0.0976,\n                       0.1466, -0.0916,  0.1761,  0.1009,  0.1245,  0.0436,  0.1443,  0.0808,\n                       0.0081,  0.0055, -0.0480,  0.1512,  0.1372,  0.1165,  0.1930,  0.1869,\n                       0.1514,  0.0139,  0.1280,  0.0029,  0.1141,  0.1458,  0.1726, -0.1177,\n                       0.0805,  0.1643,  0.1378,  0.2230,  0.0857,  0.1367,  0.1265,  0.1085,\n                      -0.0493,  0.1262,  0.1363,  0.1536,  0.1607, -0.1057,  0.1583,  0.1621,\n                       0.0384,  0.1261,  0.1318,  0.1753,  0.2288,  0.1118,  0.0913,  0.2177,\n                       0.1590,  0.0635,  0.1821,  0.1263,  0.0132,  0.1718,  0.1562,  0.1196,\n                      -0.0349,  0.1202,  0.0233,  0.2023,  0.1131,  0.2404, -0.0165,  0.0406,\n                      -0.0793, -0.0305,  0.0042,  0.1582,  0.1957,  0.0731,  0.0465, -0.0215,\n                      -0.0297,  0.1103,  0.0850,  0.2074, -0.0072,  0.0686, -0.0295,  0.0864,\n                       0.1224,  0.2307,  0.0334,  0.0185,  0.0316,  0.1394,  0.1201,  0.1489,\n                       0.2051,  0.2142,  0.1690,  0.1262, -0.0740, -0.0030,  0.1363,  0.1713,\n                       0.0304,  0.1277,  0.0824,  0.1553,  0.1674,  0.1253,  0.1988, -0.0329,\n                       0.0162,  0.1907,  0.0789, -0.0036,  0.0014,  0.0229,  0.1697, -0.0408,\n                       0.1114,  0.0029,  0.0937,  0.0459,  0.1178,  0.0200,  0.1035,  0.1131,\n                       0.1269,  0.0367,  0.1310,  0.0578,  0.2015,  0.1319,  0.1315,  0.1412,\n                       0.1134,  0.0744,  0.1137,  0.0168, -0.0011,  0.1554,  0.1755, -0.0173,\n                       0.1239,  0.0351,  0.1465,  0.1311,  0.1562,  0.0151,  0.1352,  0.1217,\n                       0.1047,  0.0574,  0.1280,  0.1180,  0.1618,  0.1445,  0.1721,  0.1406,\n                       0.1666,  0.1333,  0.0766,  0.0513,  0.1733,  0.1657,  0.1288,  0.0834,\n                       0.2116,  0.0733,  0.1636,  0.0605,  0.1229,  0.2076,  0.1258,  0.1967,\n                       0.1890, -0.0545,  0.1509,  0.1527,  0.1291,  0.4497,  0.1050,  0.0608,\n                       0.2174,  0.1677,  0.1263,  0.1349,  0.1157,  0.0706,  0.0429,  0.1969,\n                       0.1756,  0.1572,  0.0743,  0.1001,  0.1231,  0.1101,  0.1262,  0.1005,\n                      -0.0295,  0.1753,  0.1214,  0.0767, -0.0059,  0.0977,  0.0318,  0.1353,\n                       0.1089, -0.0291, -0.0185,  0.1681, -0.0113,  0.0661,  0.2088,  0.0963,\n                       0.0113,  0.1251,  0.2042, -0.0199,  0.0134,  0.1279, -0.0628,  0.0197,\n                       0.0550,  0.1019,  0.0825,  0.1426,  0.0895,  0.1374,  0.1283,  0.0584,\n                       0.1444,  0.0768,  0.0612,  0.0385,  0.1676, -0.0543,  0.1428,  0.0715,\n                       0.1754,  0.1507,  0.1286,  0.1801,  0.1922,  0.0488, -0.0733,  0.0387,\n                       0.0330,  0.0119,  0.1186,  0.0432,  0.1229,  0.1099,  0.1501,  0.1437,\n                       0.0435,  0.1191,  0.1748,  0.1236,  0.1014,  0.1763, -0.0265,  0.1457,\n                      -0.0286, -0.0027,  0.1982,  0.0110,  0.1874,  0.1241,  0.1327,  0.1534,\n                      -0.0772,  0.1435,  0.1535,  0.1547, -0.0554,  0.0480,  0.1400,  0.1752,\n                      -0.0303,  0.1687,  0.1205,  0.0909, -0.0345,  0.1300,  0.1312,  0.0895,\n                       0.1530,  0.1431,  0.1935,  0.1442,  0.1173,  0.1310,  0.1217,  0.2068,\n                      -0.1543,  0.1523,  0.1577,  0.1644,  0.1201,  0.0723,  0.1364, -0.0425,\n                       0.0968,  0.1511,  0.0368,  0.0791,  0.2600,  0.0979,  0.1813,  0.1331,\n                       0.1274,  0.0226,  0.1750,  0.0797, -0.2453,  0.1488,  0.0073,  0.2050,\n                       0.1378,  0.0952,  0.1805,  0.1479,  0.0029,  0.1054,  0.1186,  0.1609,\n                       0.1531,  0.1652,  0.1696,  0.0985,  0.1315, -0.0326,  0.1796,  0.0200,\n                      -0.0721,  0.0509,  0.0465, -0.2009, -0.0738,  0.0496,  0.0582,  0.0687,\n                       0.1683, -0.1080,  0.1580,  0.1167, -0.0350,  0.0817, -0.0467,  0.0336,\n                       0.0575,  0.1852,  0.1992,  0.1082,  0.1119,  0.1276,  0.1417,  0.0326,\n                       0.1258,  0.1746,  0.1350,  0.0220,  0.1418,  0.0596, -0.0184,  0.1806,\n                      -0.0522, -0.0689,  0.1866,  0.1760,  0.1307,  0.0912,  0.0311,  0.0654],\n                     requires_grad=True)),\n             ('mnet.features.8.conv.0.0.scale', tensor(0.0123)),\n             ('mnet.features.8.conv.0.0.zero_point', tensor(60)),\n             ('mnet.features.8.conv.1.0.weight',\n              tensor([[[[-0.6512, -1.2784, -0.6271],\n                        [ 0.5548,  3.0633,  0.3618],\n                        [ 0.0241, -1.1819, -0.6271]]],\n              \n              \n                      [[[-0.8179,  0.6652,  0.0109],\n                        [-0.8506,  1.3850,  0.3926],\n                        [-0.9379,  0.9488, -0.2290]]],\n              \n              \n                      [[[ 0.1144,  0.0858,  0.1144],\n                        [ 1.8168, -0.6438, -1.3733],\n                        [ 0.4435, -0.0572, -0.2718]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-0.0917, -0.7755, -0.8672],\n                        [ 0.5336,  0.0083, -0.6837],\n                        [ 0.4252,  1.0590,  0.2418]]],\n              \n              \n                      [[[-0.2933, -0.3722, -0.2369],\n                        [-0.3497,  1.4325, -0.3610],\n                        [-0.3610, -0.3046, -0.2707]]],\n              \n              \n                      [[[-0.0572, -0.3457, -0.1524],\n                        [-0.1688, -0.3294, -0.1851],\n                        [-0.1606, -0.2722, -0.1198]]]], size=(384, 1, 3, 3),\n                     dtype=torch.qint8, quantization_scheme=torch.per_channel_affine,\n                     scale=tensor([0.0241, 0.0109, 0.0143, 0.0220, 0.0119, 0.0125, 0.0133, 0.0045, 0.0114,\n                      0.0255, 0.0113, 0.0242, 0.0145, 0.0136, 0.0150, 0.0090, 0.0091, 0.0126,\n                      0.0108, 0.0129, 0.0065, 0.0041, 0.0122, 0.0050, 0.0183, 0.0145, 0.0119,\n                      0.0101, 0.0107, 0.0099, 0.0184, 0.0135, 0.0105, 0.0047, 0.0112, 0.0101,\n                      0.0039, 0.0117, 0.0141, 0.0044, 0.0112, 0.0215, 0.0288, 0.0148, 0.0114,\n                      0.0110, 0.0143, 0.0231, 0.0197, 0.0125, 0.0308, 0.0180, 0.0146, 0.0164,\n                      0.0112, 0.0099, 0.0029, 0.0250, 0.0093, 0.0114, 0.0128, 0.0242, 0.0140,\n                      0.0152, 0.0098, 0.0043, 0.0121, 0.0056, 0.0135, 0.0138, 0.0174, 0.0189,\n                      0.0023, 0.0105, 0.0200, 0.0218, 0.0116, 0.0104, 0.0209, 0.0121, 0.0312,\n                      0.0205, 0.0182, 0.0143, 0.0125, 0.0033, 0.0071, 0.0128, 0.0067, 0.0202,\n                      0.0151, 0.0181, 0.0049, 0.0045, 0.0100, 0.0044, 0.0136, 0.0097, 0.0045,\n                      0.0147, 0.0145, 0.0141, 0.0151, 0.0178, 0.0126, 0.0136, 0.0181, 0.0077,\n                      0.0135, 0.0216, 0.0116, 0.0134, 0.0042, 0.0180, 0.0135, 0.0095, 0.0245,\n                      0.0183, 0.0150, 0.0222, 0.0110, 0.0144, 0.0031, 0.0231, 0.0139, 0.0050,\n                      0.0120, 0.0035, 0.0155, 0.0146, 0.0138, 0.0133, 0.0260, 0.0211, 0.0202,\n                      0.0147, 0.0256, 0.0103, 0.0141, 0.0032, 0.0132, 0.0092, 0.0111, 0.0155,\n                      0.0089, 0.0209, 0.0179, 0.0108, 0.0104, 0.0418, 0.0080, 0.0113, 0.0120,\n                      0.0033, 0.0107, 0.0105, 0.0112, 0.0161, 0.0057, 0.0050, 0.0043, 0.0122,\n                      0.0208, 0.0112, 0.0108, 0.0162, 0.0153, 0.0265, 0.0175, 0.0309, 0.0045,\n                      0.0074, 0.0111, 0.0128, 0.0102, 0.0062, 0.0136, 0.0148, 0.0067, 0.0075,\n                      0.0146, 0.0117, 0.0154, 0.0119, 0.0142, 0.0175, 0.0140, 0.0261, 0.0112,\n                      0.0099, 0.0083, 0.0097, 0.0200, 0.0168, 0.0202, 0.0176, 0.0193, 0.0144,\n                      0.0039, 0.0060, 0.0172, 0.0126, 0.0112, 0.0156, 0.0155, 0.0124, 0.0187,\n                      0.0146, 0.0158, 0.0126, 0.0142, 0.0111, 0.0042, 0.0042, 0.0152, 0.0155,\n                      0.0127, 0.0065, 0.0096, 0.0158, 0.0204, 0.0145, 0.0120, 0.0111, 0.0127,\n                      0.0133, 0.0079, 0.0200, 0.0113, 0.0152, 0.0197, 0.0192, 0.0291, 0.0107,\n                      0.0191, 0.0111, 0.0094, 0.0161, 0.0208, 0.0098, 0.0107, 0.0157, 0.0136,\n                      0.0060, 0.0147, 0.0135, 0.0161, 0.0104, 0.0185, 0.0187, 0.0109, 0.0105,\n                      0.0117, 0.0089, 0.0198, 0.0038, 0.0088, 0.0037, 0.0109, 0.0050, 0.0168,\n                      0.0095, 0.0128, 0.0058, 0.0141, 0.0193, 0.0233, 0.0162, 0.0174, 0.0175,\n                      0.0201, 0.0122, 0.0198, 0.0119, 0.0150, 0.0124, 0.0096, 0.0137, 0.0171,\n                      0.0101, 0.0246, 0.0144, 0.0126, 0.0237, 0.0104, 0.0021, 0.0092, 0.0133,\n                      0.0172, 0.0176, 0.0031, 0.0140, 0.0258, 0.0127, 0.0152, 0.0147, 0.0216,\n                      0.0239, 0.0053, 0.0099, 0.0095, 0.0048, 0.0080, 0.0129, 0.0239, 0.0270,\n                      0.0120, 0.0211, 0.0209, 0.0136, 0.0088, 0.0027, 0.0107, 0.0114, 0.0040,\n                      0.0152, 0.0056, 0.0121, 0.0109, 0.0136, 0.0113, 0.0069, 0.0038, 0.0093,\n                      0.0255, 0.0104, 0.0099, 0.0218, 0.0143, 0.0080, 0.0117, 0.0189, 0.0220,\n                      0.0101, 0.0091, 0.0153, 0.0086, 0.0106, 0.0142, 0.0159, 0.0103, 0.0213,\n                      0.0109, 0.0036, 0.0105, 0.0185, 0.0102, 0.0186, 0.0275, 0.0162, 0.0244,\n                      0.0190, 0.0102, 0.0206, 0.0180, 0.0146, 0.0263, 0.0044, 0.0188, 0.0106,\n                      0.0045, 0.0193, 0.0075, 0.0064, 0.0072, 0.0110, 0.0195, 0.0030, 0.0097,\n                      0.0188, 0.0154, 0.0029, 0.0223, 0.0066, 0.0111, 0.0195, 0.0134, 0.0155,\n                      0.0118, 0.0128, 0.0116, 0.0083, 0.0113, 0.0027], dtype=torch.float64),\n                     zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n                     axis=0)),\n             ('mnet.features.8.conv.1.0.bias',\n              Parameter containing:\n              tensor([-2.9261e-01, -2.0471e-01, -7.0345e-02, -2.3422e-03, -1.4511e-02,\n                      -9.0726e-02,  3.7177e-02,  1.9513e-01, -8.7970e-03,  6.6721e-05,\n                      -3.1283e-02,  2.5630e-01, -1.5683e-02, -2.6205e-01, -1.7603e-01,\n                       4.6517e-02, -3.5633e-02, -1.0218e-01,  2.4861e-02, -6.8063e-02,\n                       3.6848e-01,  2.3135e-01, -2.4798e-02,  3.1574e-01, -2.7918e-01,\n                      -2.1107e-01, -9.7424e-03, -1.2117e-02, -2.7742e-02, -3.5870e-02,\n                       1.7546e-01,  1.9937e-02,  3.1221e-02,  2.1407e-01,  2.7083e-02,\n                       5.0496e-02,  2.5970e-01, -1.2452e-02,  2.2440e-03,  2.7253e-01,\n                      -3.6879e-02,  3.1702e-03, -1.4514e-01,  9.5171e-02, -9.7355e-03,\n                      -1.3158e-02,  1.9320e-01, -1.8536e-01,  1.7747e-01, -7.9983e-03,\n                      -3.0455e-02, -3.5326e-01,  2.6257e-01,  1.6890e-02, -1.6954e-02,\n                       6.9714e-04,  1.9972e-01, -1.8773e-02,  3.1752e-03,  9.9285e-03,\n                       7.9895e-01, -7.9333e-02, -1.7430e-02,  5.3437e-02, -1.1646e-02,\n                       2.3349e-01,  1.9058e-02,  2.8494e-01, -7.1104e-02, -4.8782e-03,\n                      -4.8940e-03, -3.2832e-02,  2.2511e-01, -6.4964e-03, -2.9827e-01,\n                       1.3448e-01, -5.4513e-03,  8.2713e-03, -5.1384e-02,  4.2502e-02,\n                      -1.2230e-01, -2.8887e-02, -1.0542e-01, -2.0353e-02,  4.5251e-02,\n                       1.4704e-01,  2.2348e-01, -2.8750e-01,  2.6795e-01, -3.4923e-01,\n                       4.6531e-02,  3.1536e-01,  2.3506e-01,  2.0968e-01, -8.9813e-02,\n                       2.5719e-01, -1.3108e-02,  4.7429e-01,  2.2682e-01,  1.8533e-02,\n                      -2.9786e-02,  2.0626e-01, -1.7601e-01, -1.1725e-02, -3.2312e-03,\n                       4.4714e-03,  2.2437e-01,  5.8918e-03, -1.2464e-01, -3.3191e-01,\n                       2.2636e-01, -4.8571e-02,  2.1682e-01, -1.9942e-02, -4.0861e-01,\n                       8.0963e-02,  1.5264e-01, -9.1508e-02,  1.1363e-02, -6.2733e-02,\n                       9.3737e-02, -1.3628e-02,  1.7906e-01, -7.4909e-02, -2.5940e-01,\n                       2.3280e-01,  2.9148e-02,  1.6617e-01, -5.0190e-03, -1.4723e-01,\n                      -3.4030e-02, -6.0243e-01, -6.8120e-01, -1.1991e-01, -5.8935e-02,\n                      -3.3028e-02,  2.2075e-02,  1.8531e-01, -5.3562e-03,  2.2895e-01,\n                       8.8390e-03,  4.7033e-03, -1.0297e-02,  3.8330e-05, -5.3651e-03,\n                      -3.2279e-02, -9.3395e-02, -9.5939e-02,  6.4117e-02, -9.7590e-01,\n                       5.0456e-01, -9.2413e-02, -4.5668e-02,  1.7105e-01, -2.8034e-03,\n                      -1.9910e-03, -1.4181e-02, -1.5424e-01,  2.6171e-01,  2.8779e-01,\n                       2.4349e-01, -2.3346e-02,  1.3321e-01, -1.2781e-02, -2.1843e-03,\n                      -2.3424e-01,  5.4969e-03,  1.2910e-01, -1.8467e-01, -2.2156e-01,\n                       2.6545e-01, -4.4739e-02, -5.8448e-03, -3.1480e-02,  3.5045e-02,\n                       2.1389e-01, -1.7683e-02,  2.5149e-01,  3.8623e-01, -2.7054e-02,\n                      -8.3327e-02,  2.5000e-01,  9.1796e-03,  8.5127e-01,  2.1830e-02,\n                      -4.1851e-02, -5.2515e-02,  2.8867e-02, -1.3334e-02,  9.3808e-01,\n                      -2.4370e-02, -2.2877e-01,  1.3620e-01,  7.3216e-01,  4.3807e-02,\n                      -7.3415e-02, -2.4245e-02, -6.7917e-02,  1.7628e-01,  5.8459e-01,\n                       1.8373e-02, -1.6542e-02,  1.1073e-02,  1.2645e-02, -7.4040e-02,\n                      -2.3574e-02,  5.6913e-02, -4.3343e-03, -8.0495e-02,  3.0391e-02,\n                      -2.9718e-02, -1.8284e-02,  2.2439e-01,  2.4160e-01,  1.4974e-01,\n                       9.6131e-03, -1.8687e-02,  2.1188e-01, -3.4977e-02,  1.0682e-01,\n                       2.7841e-01, -6.0350e-02,  1.5590e-01, -3.3025e-02, -2.3420e-03,\n                      -1.9279e-03,  6.6097e-01, -2.5276e-01, -2.0346e-01, -2.9600e-02,\n                       2.0390e-01, -2.6612e-01, -8.7466e-02, -1.2206e-01, -1.9146e-01,\n                       7.7354e-03,  2.2802e-01,  4.9675e-03,  1.7589e-02,  2.0456e-01,\n                       1.8935e-01, -4.6974e-02, -1.4875e-01,  2.6393e-01,  7.0074e-02,\n                       5.2126e-02, -9.4212e-02, -6.8657e-03,  2.4064e-02, -1.1339e-01,\n                      -3.0792e-02,  3.5552e-01, -5.6588e-02,  5.0740e-02, -5.4938e-04,\n                      -2.0291e-01,  2.1428e-03,  2.2570e-01,  1.0395e-02,  1.8200e-01,\n                       1.0021e-01, -1.6462e-02, -4.7522e-03,  4.7359e-01, -7.2877e-03,\n                      -6.3078e-02, -1.3557e-01, -2.5887e-02,  2.4449e-01,  6.4046e-02,\n                      -2.8876e-01, -1.0877e-02, -3.4177e-01, -2.5403e-01,  5.5703e-02,\n                       6.5154e-03,  1.6569e-02,  4.1522e-02,  2.1617e-01, -1.3889e-03,\n                       1.4774e-03, -7.7933e-03,  8.2102e-03,  1.5844e-01, -5.2582e-04,\n                       2.5507e-01, -4.0536e-03, -3.3401e-03, -1.6566e-01, -2.1102e-02,\n                       1.7154e-01,  9.5450e-02, -1.6107e-01,  1.0973e-01, -1.0983e-01,\n                      -1.7600e-01,  7.9367e-03,  1.5505e-01,  3.8703e-01, -5.1297e-03,\n                      -2.3754e-02,  3.9284e-01,  1.6346e-01, -3.4169e-03,  9.2897e-02,\n                       2.8515e-03,  3.6520e-01,  5.7583e-02, -5.7695e-02, -2.9042e-02,\n                       1.5016e-01,  1.1671e-01,  4.3104e-02, -6.4046e-03,  1.8942e-01,\n                       2.4841e-01,  4.2295e-01,  2.1256e-01,  4.9993e-03, -1.9742e-02,\n                       1.5134e-02,  2.3598e-01,  3.8296e-01, -5.7730e-02, -1.5576e-01,\n                       2.2467e-01, -1.3093e-01,  1.2199e-01,  4.8641e-02, -2.9843e-02,\n                      -4.3759e-02,  3.7329e-02, -1.8221e-01, -1.7341e-02, -1.0561e-02,\n                       2.6310e-02, -2.9350e-03,  6.1247e-04,  2.0692e-02,  7.1764e-03,\n                      -6.2773e-03, -1.5293e-01, -6.0433e-04,  1.6787e-01, -2.7128e-02,\n                      -6.1281e-03,  2.7991e-02, -1.8111e-01, -1.5947e-01,  1.9304e-01,\n                      -4.1522e-01, -1.5999e-01,  8.9266e-03,  1.6469e-01,  1.7813e-02,\n                      -2.8026e-02, -1.5736e-01,  2.3803e-01, -1.8574e-01,  3.7102e-01,\n                       2.7128e-01, -2.5288e-02,  2.8196e-01,  3.0300e-01,  5.1565e-03,\n                       2.9633e-03,  1.7794e-01,  1.9642e-01, -2.4108e-02, -1.0486e-01,\n                      -9.7832e-03,  2.7792e-01, -1.5966e-01,  2.1428e-01, -8.5239e-02,\n                       6.5659e-02, -1.1236e-01, -2.1131e-01,  2.6116e-01, -5.7166e-03,\n                      -4.5294e-03, -1.8536e-02,  3.3964e-01,  2.6113e-01],\n                     requires_grad=True)),\n             ('mnet.features.8.conv.1.0.scale', tensor(0.0240)),\n             ('mnet.features.8.conv.1.0.zero_point', tensor(75)),\n             ('mnet.features.8.conv.2.weight',\n              tensor([[[[-0.0212]],\n              \n                       [[ 0.0061]],\n              \n                       [[-0.0726]],\n              \n                       ...,\n              \n                       [[ 0.3026]],\n              \n                       [[-0.0757]],\n              \n                       [[-0.2633]]],\n              \n              \n                      [[[ 0.0147]],\n              \n                       [[-0.0783]],\n              \n                       [[ 0.0220]],\n              \n                       ...,\n              \n                       [[ 0.0440]],\n              \n                       [[ 0.0342]],\n              \n                       [[ 0.0660]]],\n              \n              \n                      [[[ 0.1805]],\n              \n                       [[ 0.1010]],\n              \n                       [[-0.0031]],\n              \n                       ...,\n              \n                       [[ 0.0000]],\n              \n                       [[-0.0673]],\n              \n                       [[-0.0673]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-0.0033]],\n              \n                       [[-0.0657]],\n              \n                       [[-0.0625]],\n              \n                       ...,\n              \n                       [[ 0.0033]],\n              \n                       [[ 0.0920]],\n              \n                       [[ 0.2400]]],\n              \n              \n                      [[[-0.0968]],\n              \n                       [[ 0.0346]],\n              \n                       [[-0.0323]],\n              \n                       ...,\n              \n                       [[-0.0438]],\n              \n                       [[ 0.0783]],\n              \n                       [[ 0.1152]]],\n              \n              \n                      [[[ 0.1896]],\n              \n                       [[ 0.0443]],\n              \n                       [[ 0.0837]],\n              \n                       ...,\n              \n                       [[ 0.1330]],\n              \n                       [[ 0.1133]],\n              \n                       [[-0.0690]]]], size=(64, 384, 1, 1), dtype=torch.qint8,\n                     quantization_scheme=torch.per_channel_affine,\n                     scale=tensor([0.0030, 0.0024, 0.0031, 0.0031, 0.0025, 0.0029, 0.0031, 0.0045, 0.0032,\n                      0.0025, 0.0022, 0.0031, 0.0026, 0.0030, 0.0030, 0.0024, 0.0031, 0.0019,\n                      0.0033, 0.0025, 0.0023, 0.0027, 0.0033, 0.0045, 0.0032, 0.0033, 0.0030,\n                      0.0022, 0.0041, 0.0033, 0.0038, 0.0024, 0.0021, 0.0021, 0.0041, 0.0020,\n                      0.0042, 0.0030, 0.0023, 0.0018, 0.0028, 0.0025, 0.0027, 0.0028, 0.0035,\n                      0.0024, 0.0033, 0.0039, 0.0033, 0.0027, 0.0031, 0.0057, 0.0047, 0.0030,\n                      0.0030, 0.0030, 0.0039, 0.0025, 0.0029, 0.0046, 0.0036, 0.0033, 0.0023,\n                      0.0025], dtype=torch.float64),\n                     zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n                     axis=0)),\n             ('mnet.features.8.conv.2.bias',\n              Parameter containing:\n              tensor([-0.3388, -0.1594,  0.0849,  0.2442, -0.1424,  0.0135,  0.0246, -0.5359,\n                      -0.3311,  0.0220, -0.2424,  0.1431,  0.0893, -0.0830,  0.0657,  0.2821,\n                       0.2807,  0.0797,  0.3264, -0.2616, -0.1792, -0.4920,  0.0158,  0.5124,\n                       0.1436, -0.2772,  1.0004, -0.1480,  0.1979,  0.1055,  0.3801,  0.6643,\n                      -0.1136,  0.1505, -1.0896, -0.4375,  0.6192, -0.0600,  0.1388, -0.2622,\n                      -0.1172, -0.3420,  0.3066, -0.0998,  0.5285,  0.1353,  0.0165,  0.7019,\n                       0.1845,  0.1802, -0.0811,  0.3170,  0.1423, -0.2443, -0.3586,  0.4486,\n                       0.4074, -0.0794,  0.2878,  0.8192,  0.8321, -0.5838,  0.0021, -0.2808],\n                     requires_grad=True)),\n             ('mnet.features.8.conv.2.scale', tensor(0.0325)),\n             ('mnet.features.8.conv.2.zero_point', tensor(62)),\n             ('mnet.features.9.conv.0.0.weight',\n              tensor([[[[-0.0007]],\n              \n                       [[-0.0241]],\n              \n                       [[-0.0345]],\n              \n                       ...,\n              \n                       [[-0.0130]],\n              \n                       [[ 0.0091]],\n              \n                       [[-0.0260]]],\n              \n              \n                      [[[ 0.0066]],\n              \n                       [[-0.0230]],\n              \n                       [[ 0.0003]],\n              \n                       ...,\n              \n                       [[-0.0009]],\n              \n                       [[ 0.0130]],\n              \n                       [[-0.0169]]],\n              \n              \n                      [[[ 0.0147]],\n              \n                       [[-0.0120]],\n              \n                       [[ 0.0316]],\n              \n                       ...,\n              \n                       [[ 0.0049]],\n              \n                       [[ 0.0365]],\n              \n                       [[-0.0033]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-0.0243]],\n              \n                       [[ 0.0019]],\n              \n                       [[-0.0670]],\n              \n                       ...,\n              \n                       [[ 0.0441]],\n              \n                       [[ 0.0370]],\n              \n                       [[-0.0683]]],\n              \n              \n                      [[[-0.0347]],\n              \n                       [[-0.0049]],\n              \n                       [[-0.0100]],\n              \n                       ...,\n              \n                       [[-0.0071]],\n              \n                       [[-0.0120]],\n              \n                       [[ 0.0136]]],\n              \n              \n                      [[[-0.0370]],\n              \n                       [[-0.0199]],\n              \n                       [[ 0.0262]],\n              \n                       ...,\n              \n                       [[-0.0028]],\n              \n                       [[ 0.0051]],\n              \n                       [[-0.0336]]]], size=(384, 64, 1, 1), dtype=torch.qint8,\n                     quantization_scheme=torch.per_channel_affine,\n                     scale=tensor([6.5002e-04, 3.0199e-04, 5.4469e-04, 5.4087e-04, 6.4894e-04, 4.1313e-04,\n                      4.4597e-04, 5.4360e-04, 3.9524e-04, 5.1389e-04, 5.7222e-04, 3.0619e-04,\n                      3.3664e-04, 4.2500e-04, 3.4067e-04, 3.3634e-04, 3.9959e-04, 9.0999e-04,\n                      4.9995e-04, 5.1488e-04, 3.3044e-04, 3.4444e-04, 4.0743e-04, 3.6943e-04,\n                      7.7336e-04, 6.6659e-04, 2.4198e-04, 5.7377e-04, 7.6804e-04, 5.4452e-04,\n                      4.6587e-04, 4.4628e-04, 2.7835e-04, 4.3395e-04, 4.4432e-04, 2.7157e-04,\n                      7.6400e-04, 4.2183e-04, 6.4272e-04, 3.5724e-04, 4.3324e-04, 5.5095e-04,\n                      4.2114e-04, 6.2483e-04, 3.7236e-04, 4.8230e-04, 5.3650e-04, 3.5980e-04,\n                      3.2357e-04, 5.5774e-04, 5.0019e-04, 5.4871e-04, 5.5667e-04, 3.4858e-04,\n                      2.5448e-04, 3.2674e-04, 3.2640e-04, 3.9325e-04, 3.7766e-04, 5.4444e-04,\n                      4.4204e-04, 3.8634e-04, 3.3038e-04, 3.8991e-04, 4.3825e-04, 3.5042e-04,\n                      5.3192e-04, 3.4703e-04, 5.3853e-04, 2.4289e-04, 4.1264e-04, 2.8098e-04,\n                      4.0935e-04, 3.4845e-04, 6.6463e-04, 6.0665e-04, 3.1344e-04, 3.8939e-04,\n                      4.3545e-04, 3.7255e-04, 1.1289e-04, 5.2880e-04, 4.4509e-04, 7.4642e-04,\n                      4.1940e-04, 5.8455e-04, 4.0889e-04, 2.7220e-04, 5.1177e-04, 3.8817e-04,\n                      4.7715e-04, 3.6122e-04, 1.8335e-05, 3.0555e-04, 5.1633e-04, 3.3348e-04,\n                      5.9360e-04, 1.3851e-04, 3.8498e-04, 5.3932e-04, 5.0154e-04, 3.2446e-04,\n                      6.9712e-04, 5.1184e-04, 2.5862e-04, 8.0427e-04, 5.4349e-04, 5.3256e-04,\n                      5.5144e-04, 4.7632e-04, 2.5700e-04, 2.1883e-04, 5.4154e-04, 4.5821e-04,\n                      9.3853e-05, 8.7262e-04, 4.4412e-04, 6.9948e-04, 1.7392e-04, 5.4811e-04,\n                      4.5768e-04, 4.8538e-04, 3.0189e-04, 5.6390e-04, 6.4451e-04, 3.4945e-04,\n                      3.3490e-04, 3.1998e-04, 4.2438e-04, 4.7579e-04, 3.8727e-04, 3.1615e-04,\n                      4.7321e-04, 5.2048e-04, 4.9165e-04, 7.5993e-04, 5.8534e-04, 4.7596e-04,\n                      3.9463e-04, 3.4482e-04, 6.3303e-04, 5.6979e-04, 5.3706e-04, 5.5475e-04,\n                      5.1055e-04, 5.3558e-04, 3.7231e-04, 4.9544e-04, 1.9297e-04, 3.9285e-04,\n                      7.3694e-04, 3.4423e-04, 3.9258e-04, 5.4742e-04, 4.1526e-04, 5.1964e-04,\n                      5.0797e-04, 2.9650e-04, 3.3863e-04, 4.2655e-04, 3.3770e-04, 5.0802e-04,\n                      5.6874e-04, 4.1342e-04, 4.3990e-04, 4.2784e-04, 3.1792e-04, 6.6132e-04,\n                      3.0203e-04, 2.0811e-04, 7.7013e-04, 4.8645e-04, 5.5925e-04, 6.9542e-04,\n                      6.7368e-04, 9.1032e-04, 4.3318e-04, 3.2199e-04, 3.2429e-04, 4.7664e-04,\n                      4.3409e-04, 4.7733e-04, 3.3151e-04, 5.2535e-04, 6.2657e-04, 3.6362e-04,\n                      3.8550e-04, 5.5515e-04, 4.5176e-04, 6.0057e-04, 3.0362e-04, 3.8698e-04,\n                      5.8358e-04, 4.0120e-04, 2.7565e-04, 4.8181e-04, 4.6171e-04, 2.1076e-04,\n                      7.0656e-04, 5.9831e-04, 5.3499e-04, 3.0013e-04, 2.8209e-04, 3.8497e-04,\n                      6.1522e-04, 2.3199e-04, 4.2249e-04, 5.2317e-04, 5.6788e-04, 4.7306e-04,\n                      5.1273e-04, 4.1756e-04, 3.1468e-04, 7.4887e-04, 2.8834e-04, 3.7387e-04,\n                      2.2071e-04, 6.2026e-04, 8.1423e-05, 5.0367e-04, 3.8815e-04, 3.2769e-04,\n                      3.9503e-04, 5.3596e-04, 5.1177e-04, 4.2134e-04, 4.9521e-04, 5.0413e-04,\n                      4.3111e-04, 5.2740e-04, 3.9364e-04, 7.8212e-04, 4.4342e-04, 4.4106e-04,\n                      7.0376e-04, 2.7288e-04, 3.9720e-04, 4.7940e-04, 2.9747e-04, 3.5449e-04,\n                      3.2186e-04, 3.7483e-04, 6.2149e-04, 4.1961e-04, 5.0238e-04, 4.1799e-04,\n                      3.6073e-04, 4.3671e-04, 6.2655e-04, 3.5486e-04, 3.5720e-04, 6.0959e-04,\n                      6.2285e-04, 2.8381e-04, 3.3442e-04, 3.8795e-04, 7.0475e-04, 4.0055e-04,\n                      5.0699e-04, 5.2270e-04, 5.2365e-04, 4.4691e-04, 4.1789e-04, 3.5353e-04,\n                      5.8249e-04, 5.2252e-04, 8.5688e-06, 4.5380e-04, 4.5289e-04, 4.3540e-04,\n                      6.3114e-04, 3.7517e-04, 3.5014e-04, 3.3214e-04, 4.1676e-04, 4.6397e-04,\n                      4.5344e-04, 4.6294e-04, 5.1558e-04, 6.2858e-04, 4.3256e-04, 2.1744e-04,\n                      5.6159e-04, 5.0587e-04, 3.3178e-04, 7.3117e-04, 2.4489e-04, 4.7325e-04,\n                      2.1996e-04, 3.8925e-04, 4.2052e-04, 5.6679e-04, 3.5869e-04, 4.3507e-04,\n                      3.5085e-04, 3.8080e-04, 3.5580e-04, 2.6326e-04, 2.9682e-04, 5.5082e-04,\n                      5.3990e-04, 2.7954e-04, 5.7003e-04, 4.5785e-04, 4.1373e-04, 5.9249e-04,\n                      4.8157e-04, 5.0366e-04, 5.3636e-04, 6.6208e-04, 6.1823e-04, 2.9860e-04,\n                      4.0003e-04, 4.5755e-04, 5.0297e-04, 3.2855e-04, 1.8315e-05, 5.3007e-04,\n                      8.0246e-04, 4.1441e-04, 4.1499e-04, 6.4315e-04, 3.1977e-04, 3.3032e-04,\n                      4.8001e-04, 5.4774e-04, 5.9371e-04, 2.8054e-04, 7.1438e-04, 3.6286e-04,\n                      5.3962e-04, 3.3619e-04, 4.7233e-04, 3.3368e-04, 2.7432e-04, 3.6816e-04,\n                      2.4895e-04, 4.1535e-04, 6.5911e-04, 3.5651e-04, 3.5025e-04, 4.8703e-04,\n                      4.8102e-04, 3.7744e-04, 2.2475e-04, 7.6760e-04, 2.9008e-04, 5.9762e-04,\n                      3.0766e-04, 3.9032e-04, 4.5781e-04, 8.4641e-04, 4.9862e-04, 2.1695e-04,\n                      3.1283e-04, 5.3896e-04, 3.8056e-04, 2.9219e-04, 5.3549e-04, 5.7504e-04,\n                      4.3016e-04, 6.8914e-04, 6.2679e-04, 4.6695e-04, 4.7999e-04, 2.4610e-04,\n                      4.9665e-04, 4.7717e-04, 4.8981e-04, 4.0928e-04, 5.8372e-04, 5.0128e-04,\n                      7.2016e-04, 5.4599e-04, 5.3415e-04, 4.6028e-04, 4.0313e-04, 4.6121e-04,\n                      6.0935e-04, 5.3441e-04, 4.6010e-04, 6.3853e-04, 3.2386e-04, 5.6898e-04],\n                     dtype=torch.float64),\n                     zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n                     axis=0)),\n             ('mnet.features.9.conv.0.0.bias',\n              Parameter containing:\n              tensor([ 0.0529,  0.1304,  0.0768, -0.0701, -0.0047,  0.1181,  0.0789, -0.0429,\n                       0.0265,  0.0212,  0.0773,  0.1381,  0.0025, -0.0340,  0.1466,  0.1276,\n                       0.0695,  0.0781,  0.1706, -0.0109, -0.0738,  0.0953,  0.0254,  0.1105,\n                      -0.0273, -0.0603,  0.1506,  0.0370,  0.0253,  0.0696,  0.0017,  0.1301,\n                       0.1372,  0.0926,  0.0090,  0.1458, -0.0591,  0.0364, -0.0560,  0.1066,\n                       0.0857,  0.0579,  0.0644, -0.0852,  0.1060,  0.0712,  0.0028,  0.1134,\n                       0.1145,  0.0355,  0.1136, -0.0103,  0.0076,  0.0377,  0.0822,  0.0415,\n                       0.1164, -0.0032,  0.0696, -0.1065, -0.0114,  0.0586,  0.1521,  0.0788,\n                       0.0434,  0.0628, -0.0473,  0.1210, -0.0657,  0.0386,  0.0493,  0.1221,\n                       0.1387,  0.1337,  0.0219, -0.0854,  0.1402,  0.1326,  0.0917,  0.1215,\n                       0.1021,  0.0231,  0.0089,  0.0695,  0.0672,  0.0376,  0.0185,  0.1056,\n                       0.0606,  0.1895, -0.0562,  0.2015, -0.0380,  0.1356, -0.1121,  0.0724,\n                       0.0464,  0.0016,  0.0543, -0.0349, -0.0019,  0.2037,  0.1156,  0.0547,\n                      -0.0171, -0.0190,  0.0637, -0.0537,  0.0297,  0.0427,  0.0629,  0.1639,\n                       0.0785,  0.0275,  0.1655,  0.0249,  0.0516, -0.0597,  0.1285,  0.2335,\n                       0.0855, -0.0183,  0.1232, -0.0202,  0.0304,  0.1221,  0.0580,  0.0897,\n                       0.0179,  0.0302,  0.1155,  0.0799, -0.0082,  0.0413,  0.0554,  0.0944,\n                       0.0224,  0.0863, -0.0069,  0.0937,  0.0214,  0.0857, -0.0134,  0.0558,\n                       0.1381, -0.0112,  0.0600, -0.0663,  0.1546,  0.0306, -0.0396,  0.0688,\n                       0.0787,  0.1410,  0.0527,  0.1523, -0.0300,  0.2475,  0.1047,  0.0222,\n                       0.1090,  0.0368, -0.1108, -0.0311,  0.1448,  0.0668,  0.1465, -0.0786,\n                      -0.0030,  0.1920,  0.0722,  0.1138, -0.0416,  0.0494,  0.0573, -0.0076,\n                       0.0347, -0.0121, -0.0185,  0.1423,  0.0525,  0.1399, -0.0164,  0.0070,\n                      -0.0304,  0.1351,  0.1287,  0.0010,  0.0781,  0.0318,  0.1069,  0.0112,\n                       0.0249,  0.0366,  0.1070,  0.0918,  0.1014,  0.1375, -0.0258, -0.0056,\n                      -0.0097,  0.1936,  0.1995, -0.0404,  0.0806,  0.1433,  0.0157,  0.0176,\n                      -0.0234,  0.0128, -0.0759, -0.0404,  0.0844, -0.0218,  0.1375,  0.0401,\n                       0.1390,  0.0746,  0.2155,  0.0859,  0.1595,  0.1111, -0.0414, -0.0571,\n                      -0.0080,  0.0570,  0.0414,  0.0122,  0.0601,  0.0249, -0.0386, -0.0335,\n                       0.0477,  0.0340,  0.1826,  0.0198,  0.0983,  0.0057,  0.0751,  0.1189,\n                       0.0750,  0.0507,  0.0640,  0.0555,  0.0751,  0.0176,  0.1312,  0.0051,\n                      -0.0635,  0.1106,  0.1010, -0.0135,  0.0594,  0.1151,  0.0757,  0.0308,\n                       0.0049,  0.0233,  0.0522,  0.0625,  0.0632,  0.0670,  0.0935,  0.1524,\n                      -0.0032,  0.0147, -0.0140,  0.0442,  0.0643,  0.0828,  0.0222,  0.0280,\n                       0.0722,  0.2076,  0.1669,  0.0414,  0.1103,  0.0906, -0.0056,  0.0028,\n                       0.0537,  0.1271,  0.0684,  0.1006, -0.0421,  0.1122,  0.1395,  0.0379,\n                       0.1479,  0.1075,  0.0941, -0.0620,  0.0173,  0.0046,  0.0545,  0.0834,\n                       0.0907, -0.0129,  0.0149,  0.0085,  0.0205,  0.0733,  0.0185,  0.1751,\n                      -0.0642,  0.1205,  0.0255,  0.0007,  0.0840, -0.0491,  0.0177,  0.1159,\n                       0.1530,  0.0106,  0.0171,  0.1550, -0.0255,  0.0849, -0.0494,  0.0504,\n                       0.0043,  0.0824,  0.0851,  0.0214,  0.0180, -0.0081,  0.0407,  0.1401,\n                       0.0776,  0.1438, -0.0005,  0.0522, -0.1016, -0.0593,  0.1447,  0.0764,\n                       0.1996, -0.0171,  0.0889,  0.1330, -0.0511, -0.0796,  0.0945,  0.0170,\n                       0.1366,  0.0202,  0.1562, -0.0531,  0.0962,  0.0905, -0.0569, -0.0775,\n                      -0.0582,  0.1140,  0.1877,  0.1090,  0.0987,  0.1964,  0.1315, -0.0420,\n                       0.0540, -0.0429, -0.1049,  0.0922, -0.0064,  0.1952, -0.0127,  0.0161,\n                       0.0054,  0.1423,  0.0336,  0.0247,  0.1400,  0.0478,  0.1397,  0.0036,\n                       0.0997,  0.1188,  0.1006, -0.0007, -0.0744, -0.0192,  0.0661, -0.0799],\n                     requires_grad=True)),\n             ('mnet.features.9.conv.0.0.scale', tensor(0.0107)),\n             ('mnet.features.9.conv.0.0.zero_point', tensor(65)),\n             ('mnet.features.9.conv.1.0.weight',\n              tensor([[[[ 0.0000, -0.0886, -0.1771],\n                        [-0.1107,  1.1292, -0.0664],\n                        [-0.2546, -1.4171, -0.1329]]],\n              \n              \n                      [[[-0.1879, -0.1504,  0.3947],\n                        [-0.9585, -1.8042,  2.3868],\n                        [-0.2067,  0.0000,  0.2631]]],\n              \n              \n                      [[[ 1.4615,  1.1260,  0.2396],\n                        [ 0.2156, -1.5333, -0.5990],\n                        [ 0.0240, -0.6589, -0.2276]]],\n              \n              \n                      ...,\n              \n              \n                      [[[ 0.2526,  0.2261,  0.3058],\n                        [ 0.2526,  1.6887,  0.2128],\n                        [ 0.1729,  0.3191,  0.2128]]],\n              \n              \n                      [[[ 0.2502, -0.0726,  0.4277],\n                        [ 1.0248, -0.4196,  0.9845],\n                        [ 0.2824, -0.1372,  0.2098]]],\n              \n              \n                      [[[ 0.5776,  2.2227,  0.6826],\n                        [-0.1575, -0.8926, -0.1400],\n                        [ 0.1575, -0.5075,  0.0875]]]], size=(384, 1, 3, 3),\n                     dtype=torch.qint8, quantization_scheme=torch.per_channel_affine,\n                     scale=tensor([0.0111, 0.0188, 0.0120, 0.0131, 0.0233, 0.0144, 0.0196, 0.0139, 0.0116,\n                      0.0122, 0.0116, 0.0300, 0.0145, 0.0107, 0.0153, 0.0125, 0.0166, 0.0045,\n                      0.0142, 0.0116, 0.0245, 0.0133, 0.0114, 0.0196, 0.0107, 0.0162, 0.0364,\n                      0.0045, 0.0139, 0.0043, 0.0163, 0.0095, 0.0193, 0.0114, 0.0111, 0.0152,\n                      0.0156, 0.0059, 0.0144, 0.0170, 0.0226, 0.0048, 0.0094, 0.0186, 0.0134,\n                      0.0115, 0.0146, 0.0102, 0.0140, 0.0050, 0.0105, 0.0124, 0.0039, 0.0107,\n                      0.0109, 0.0107, 0.0177, 0.0141, 0.0187, 0.0236, 0.0168, 0.0080, 0.0145,\n                      0.0108, 0.0055, 0.0094, 0.0029, 0.0113, 0.0223, 0.0238, 0.0192, 0.0127,\n                      0.0131, 0.0098, 0.0062, 0.0218, 0.0125, 0.0084, 0.0113, 0.0122, 0.0479,\n                      0.0071, 0.0168, 0.0217, 0.0119, 0.0126, 0.0047, 0.0259, 0.0079, 0.0128,\n                      0.0235, 0.0135, 0.0059, 0.0255, 0.0255, 0.0149, 0.0035, 0.0297, 0.0151,\n                      0.0145, 0.0050, 0.0144, 0.0100, 0.0041, 0.0293, 0.0114, 0.0058, 0.0159,\n                      0.0044, 0.0130, 0.0104, 0.0223, 0.0096, 0.0130, 0.0867, 0.0069, 0.0149,\n                      0.0167, 0.0280, 0.0128, 0.0045, 0.0139, 0.0220, 0.0029, 0.0075, 0.0104,\n                      0.0131, 0.0147, 0.0137, 0.0098, 0.0107, 0.0201, 0.0108, 0.0037, 0.0116,\n                      0.0057, 0.0044, 0.0052, 0.0106, 0.0094, 0.0067, 0.0037, 0.0112, 0.0035,\n                      0.0073, 0.0178, 0.0077, 0.0115, 0.0224, 0.0117, 0.0055, 0.0150, 0.0095,\n                      0.0065, 0.0116, 0.0125, 0.0075, 0.0207, 0.0148, 0.0092, 0.0115, 0.0135,\n                      0.0120, 0.0077, 0.0131, 0.0147, 0.0174, 0.0139, 0.0089, 0.0119, 0.0067,\n                      0.0080, 0.0121, 0.0042, 0.0046, 0.0128, 0.0043, 0.0059, 0.0191, 0.0121,\n                      0.0136, 0.0031, 0.0101, 0.0050, 0.0134, 0.0094, 0.0095, 0.0158, 0.0134,\n                      0.0114, 0.0102, 0.0187, 0.0179, 0.0110, 0.0105, 0.0138, 0.0108, 0.0187,\n                      0.0150, 0.0138, 0.0106, 0.0183, 0.0219, 0.0076, 0.0077, 0.0182, 0.0096,\n                      0.0166, 0.0058, 0.0125, 0.0267, 0.0104, 0.0042, 0.0193, 0.0111, 0.0034,\n                      0.0171, 0.0054, 0.0661, 0.0183, 0.0116, 0.0042, 0.0150, 0.0146, 0.0076,\n                      0.0154, 0.0100, 0.0046, 0.0089, 0.0101, 0.0077, 0.0175, 0.0025, 0.0108,\n                      0.0044, 0.0144, 0.0113, 0.0204, 0.0148, 0.0091, 0.0064, 0.0140, 0.0071,\n                      0.0440, 0.0104, 0.0117, 0.0174, 0.0189, 0.0184, 0.0127, 0.0053, 0.0121,\n                      0.0091, 0.0137, 0.0172, 0.0131, 0.0073, 0.0049, 0.0114, 0.0057, 0.0089,\n                      0.0101, 0.0099, 0.0057, 0.0098, 0.0149, 0.0005, 0.0214, 0.0124, 0.0089,\n                      0.0130, 0.0062, 0.0166, 0.0211, 0.0063, 0.0032, 0.0045, 0.0108, 0.0170,\n                      0.0132, 0.0103, 0.0160, 0.0134, 0.0094, 0.0123, 0.0095, 0.0213, 0.0112,\n                      0.0217, 0.0114, 0.0235, 0.0077, 0.0150, 0.0112, 0.0202, 0.0111, 0.0243,\n                      0.0400, 0.0124, 0.0157, 0.0128, 0.0124, 0.0105, 0.0190, 0.0184, 0.0058,\n                      0.0063, 0.0116, 0.0162, 0.0217, 0.0058, 0.0134, 0.0113, 0.0070, 0.0179,\n                      0.0248, 0.0020, 0.0126, 0.0147, 0.0112, 0.0222, 0.0038, 0.0093, 0.0158,\n                      0.0098, 0.0150, 0.0103, 0.0161, 0.0080, 0.0116, 0.0114, 0.0136, 0.0185,\n                      0.0312, 0.0205, 0.0149, 0.0218, 0.0123, 0.0172, 0.0098, 0.0148, 0.0201,\n                      0.0161, 0.0196, 0.0298, 0.0053, 0.0058, 0.0111, 0.0149, 0.0041, 0.0168,\n                      0.0124, 0.0268, 0.0337, 0.0103, 0.0048, 0.0193, 0.0133, 0.0084, 0.0105,\n                      0.0123, 0.0228, 0.0210, 0.0123, 0.0086, 0.0199, 0.0190, 0.0119, 0.0106,\n                      0.0190, 0.0121, 0.0038, 0.0105, 0.0036, 0.0078, 0.0111, 0.0206, 0.0134,\n                      0.0145, 0.0036, 0.0228, 0.0133, 0.0081, 0.0175], dtype=torch.float64),\n                     zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n                     axis=0)),\n             ('mnet.features.9.conv.1.0.bias',\n              Parameter containing:\n              tensor([ 2.2964e-01,  3.9884e-03, -2.5968e-02,  1.7443e-01, -1.8810e-02,\n                       2.1978e-01, -3.1333e-01, -1.2686e-01,  4.7001e-02, -1.3941e-01,\n                       3.1564e-01, -1.3837e-01, -1.4433e-01, -4.4995e-02,  8.7318e-02,\n                      -3.5764e-02, -3.0285e-02,  2.8289e-01,  1.0411e-01, -1.2883e-01,\n                      -2.2282e-01, -1.3030e-01, -9.5827e-02, -2.8083e-02,  2.9889e-01,\n                       3.9526e-02,  1.5690e-01,  2.0751e-01,  2.9144e-01,  2.5547e-01,\n                      -2.2892e-01,  2.7683e-01,  8.7591e-02,  6.4136e-02, -1.1146e-03,\n                      -3.3598e-02, -1.6369e-01,  2.2852e-01, -6.9480e-02, -1.0212e-01,\n                      -2.5726e-01,  2.1895e-01,  2.0956e-02,  2.8046e-01, -3.6475e-03,\n                       1.0580e-01,  1.6166e-01, -1.0863e-02, -4.7759e-02,  2.5199e-01,\n                      -3.3069e-02,  6.2174e-02,  2.5717e-01, -1.4957e-01, -4.5709e-01,\n                      -2.1888e-01, -4.0953e-02, -3.0686e-02, -1.7885e-01,  5.0644e-02,\n                      -9.6039e-02, -4.6623e-02,  2.8658e-01, -4.7643e-02,  2.0239e-01,\n                      -7.8601e-02,  2.0838e-01,  7.8315e-02,  4.3052e-02, -2.1763e-01,\n                      -1.8669e-01, -1.1677e-01, -2.1864e-02,  4.5183e-01,  1.2889e-02,\n                      -1.6076e-01, -3.5749e-02, -6.9100e-02, -2.5143e-02, -9.1921e-02,\n                      -8.5562e-01,  2.6774e-01, -7.7697e-03, -3.6656e-02, -7.3880e-02,\n                       9.4371e-02,  2.1017e-01, -2.4476e-01,  7.2620e-02,  4.8758e-01,\n                      -1.1594e-02,  6.1228e-01, -2.7178e-02, -4.4374e-02,  2.9835e-03,\n                      -2.2124e-02,  2.0240e-01, -4.6426e-01, -8.1636e-02, -6.8238e-02,\n                       1.8928e-01,  1.1625e-01, -3.3123e-02,  2.1231e-01, -2.7911e-01,\n                       1.2903e-01,  2.4839e-01, -1.2019e-01,  2.0351e-01, -8.0678e-02,\n                      -2.1718e-01,  1.8944e-01, -4.4164e-02, -3.0800e-02,  6.1084e-01,\n                       2.7472e-01, -1.6620e-01, -2.7474e-01, -3.8447e-02,  6.0221e-02,\n                       3.1489e-01, -1.3758e-01, -1.2393e-01,  1.9573e-01,  1.4456e-02,\n                       4.3706e-01, -9.7317e-02,  1.4401e-01, -1.1719e-01, -7.7850e-03,\n                       1.6323e-01, -9.7624e-02, -1.0678e-02,  2.1213e-01, -9.6724e-03,\n                       2.2903e-01,  2.0243e-01,  2.1084e-01, -9.2447e-02,  2.6425e-01,\n                       1.9700e-01,  2.1667e-01, -1.1324e-01,  2.4764e-01, -1.7532e-02,\n                       1.5517e-01,  2.6410e-01, -6.2276e-02,  1.2269e-01, -1.5437e-02,\n                       2.5026e-01, -1.2708e-01, -2.5762e-02,  3.1768e-01,  1.1076e-01,\n                       3.0832e-01,  1.7183e-01,  5.7632e-01, -5.0719e-02,  1.2730e-01,\n                       7.8989e-02,  2.0103e-01, -1.7049e-01,  1.0037e-01, -2.1492e-02,\n                      -3.3198e-02,  2.3670e-01, -5.8151e-03, -1.6016e-01, -3.7307e-02,\n                       2.9235e-01,  2.5847e-01,  3.3477e-01,  2.3099e-01,  1.7841e-01,\n                      -1.4412e-01,  2.7826e-01, -1.0158e-01, -1.5807e-01, -4.7967e-02,\n                       6.9874e-02,  2.9754e-01, -5.2545e-02,  2.1021e-01,  1.1294e-01,\n                       4.3143e-01,  3.9432e-01, -2.7727e-01, -3.1472e-02, -9.5712e-02,\n                      -4.0433e-02,  2.8832e-01,  7.5421e-02, -9.0758e-02,  2.2473e-01,\n                      -3.8678e-02, -1.5295e-02,  1.9403e-02, -5.4416e-02, -1.9948e-01,\n                      -6.3423e-03,  2.2840e-02,  2.6717e-01, -1.1605e-02,  2.9374e-01,\n                       4.2164e-02,  1.3715e-02, -7.2773e-02,  1.5053e-01, -1.7468e-01,\n                      -1.7875e-02, -1.2412e-01,  2.3199e-01, -1.8840e-01, -2.9137e-01,\n                       1.7972e-01,  5.4841e-02,  2.2856e-01, -1.6984e+00, -9.0041e-02,\n                       1.1901e-02,  2.6509e-01, -1.9264e-02, -1.3649e-01,  3.4723e-01,\n                       2.4578e-01, -1.0250e-01,  2.2269e-01,  2.4239e-01, -8.0029e-02,\n                      -8.8089e-02, -3.8807e-01,  1.9823e-01,  9.3909e-03,  3.6555e-01,\n                      -3.4559e-01, -3.1581e-04, -1.7898e-01, -9.5106e-02,  2.7377e-01,\n                       2.4490e-01, -2.8250e-02,  2.9336e-01, -4.4609e-01,  2.4169e-02,\n                      -3.5556e-02,  1.5477e-02, -1.2645e-01, -2.0062e-01,  4.0781e-03,\n                       2.9175e-01, -1.3171e-01,  2.0964e-01,  1.2951e-01, -4.8618e-01,\n                      -9.3768e-02,  2.4096e-01,  1.9051e-01, -8.4509e-02,  3.0297e-01,\n                      -1.4883e-01,  3.0108e-01,  2.1861e-01,  5.1463e-01,  1.9739e-01,\n                      -2.2885e-01, -7.3480e-03, -1.8004e-01, -1.0229e-01, -4.9292e-04,\n                      -1.4705e-02,  1.9198e-01, -1.4414e-01,  3.4894e-01,  3.3497e-01,\n                       1.9559e-01,  2.0132e-01, -4.1114e-02, -7.1695e-02, -3.9646e-02,\n                      -6.2354e-02, -1.7104e-02,  1.8824e-02, -5.1635e-02, -7.9057e-02,\n                       6.2693e-02, -2.4589e-03, -2.2881e-02,  1.5451e-01, -1.2850e-01,\n                      -9.5671e-02, -7.3359e-03, -1.0605e-01, -1.1325e-01, -4.6626e-01,\n                      -8.2738e-03, -2.0018e-02, -3.6475e-01, -5.5333e-02,  3.0603e-03,\n                      -4.2784e-01, -3.6721e-02,  4.5320e-02, -6.9578e-03, -1.2767e-02,\n                       3.2378e-01,  2.1408e-01,  9.1641e-04,  2.3446e-01, -1.1039e-01,\n                       1.6234e-01, -1.3233e-02, -1.7635e-02,  2.3390e-01, -1.7946e-01,\n                       7.3636e-02, -1.7406e-02,  2.8637e-01, -1.3490e-02,  4.8029e-02,\n                      -2.5978e-02,  2.5750e-01, -4.1609e-03, -1.7678e-01, -2.4098e-02,\n                      -1.5579e-01,  3.1419e-01,  1.0133e-01,  8.8490e-02,  3.0219e-01,\n                       3.0916e-01, -6.0730e-02, -5.4715e-03, -2.5576e-01,  2.4758e-02,\n                      -1.5198e-01,  4.1391e-01,  1.9179e-01,  3.7257e-01,  1.3313e-01,\n                      -9.1119e-04, -7.8778e-02, -9.8824e-02, -2.5273e-01,  1.7791e-01,\n                       1.8691e-01,  4.3296e-01, -8.8879e-02, -4.8733e-02,  2.5628e-01,\n                      -1.8245e-01, -1.4393e-01, -2.0594e-01, -2.8165e-02,  4.6416e-01,\n                      -5.3265e-01, -5.5835e-02,  5.2592e-01,  3.3851e-01, -4.5144e-03,\n                      -5.4658e-02, -1.9948e-01,  3.6710e-02, -5.0447e-03, -6.7500e-02,\n                       3.9490e-01, -1.5204e-01, -8.9826e-02, -1.2257e-01, -5.0662e-02,\n                      -1.6168e-01,  1.8434e-01,  2.7902e-01,  1.9307e-01,  2.8139e-01,\n                      -1.1837e-02, -1.6185e-02,  4.7850e-04,  3.1978e-01,  1.7505e-01,\n                      -1.6522e-01, -1.6725e-01, -1.7109e-01,  7.5372e-02],\n                     requires_grad=True)),\n             ('mnet.features.9.conv.1.0.scale', tensor(0.0214)),\n             ('mnet.features.9.conv.1.0.zero_point', tensor(65)),\n             ('mnet.features.9.conv.2.weight',\n              tensor([[[[ 0.3104]],\n              \n                       [[ 0.1451]],\n              \n                       [[-0.1282]],\n              \n                       ...,\n              \n                       [[-0.1721]],\n              \n                       [[-0.0945]],\n              \n                       [[ 0.0439]]],\n              \n              \n                      [[[-0.0219]],\n              \n                       [[-0.0849]],\n              \n                       [[-0.0575]],\n              \n                       ...,\n              \n                       [[-0.0383]],\n              \n                       [[-0.1123]],\n              \n                       [[-0.0493]]],\n              \n              \n                      [[[ 0.1192]],\n              \n                       [[-0.0024]],\n              \n                       [[ 0.0048]],\n              \n                       ...,\n              \n                       [[-0.0453]],\n              \n                       [[-0.0167]],\n              \n                       [[ 0.1502]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-0.0313]],\n              \n                       [[-0.1992]],\n              \n                       [[ 0.0057]],\n              \n                       ...,\n              \n                       [[ 0.0085]],\n              \n                       [[-0.3614]],\n              \n                       [[ 0.0398]]],\n              \n              \n                      [[[ 0.1514]],\n              \n                       [[-0.0093]],\n              \n                       [[-0.0336]],\n              \n                       ...,\n              \n                       [[-0.0448]],\n              \n                       [[-0.0355]],\n              \n                       [[-0.0598]]],\n              \n              \n                      [[[ 0.0849]],\n              \n                       [[-0.0364]],\n              \n                       [[-0.0049]],\n              \n                       ...,\n              \n                       [[-0.0873]],\n              \n                       [[ 0.1334]],\n              \n                       [[ 0.1601]]]], size=(64, 384, 1, 1), dtype=torch.qint8,\n                     quantization_scheme=torch.per_channel_affine,\n                     scale=tensor([0.0034, 0.0027, 0.0024, 0.0038, 0.0025, 0.0029, 0.0033, 0.0027, 0.0042,\n                      0.0030, 0.0030, 0.0029, 0.0032, 0.0024, 0.0030, 0.0022, 0.0030, 0.0020,\n                      0.0043, 0.0025, 0.0020, 0.0026, 0.0027, 0.0039, 0.0028, 0.0025, 0.0034,\n                      0.0033, 0.0034, 0.0036, 0.0054, 0.0034, 0.0026, 0.0019, 0.0050, 0.0021,\n                      0.0031, 0.0030, 0.0027, 0.0020, 0.0029, 0.0033, 0.0022, 0.0039, 0.0039,\n                      0.0027, 0.0034, 0.0033, 0.0024, 0.0019, 0.0037, 0.0053, 0.0036, 0.0029,\n                      0.0029, 0.0028, 0.0040, 0.0031, 0.0026, 0.0045, 0.0035, 0.0028, 0.0019,\n                      0.0024], dtype=torch.float64),\n                     zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n                     axis=0)),\n             ('mnet.features.9.conv.2.bias',\n              Parameter containing:\n              tensor([ 3.0932e-01,  1.3830e-01, -3.1259e-01, -1.4913e-01, -2.7602e-01,\n                      -2.8834e-01,  2.6770e-01,  7.6291e-02, -4.5367e-01,  8.0133e-02,\n                       1.7386e-01, -2.7064e-01, -1.0904e-01, -3.0194e-01,  7.5536e-02,\n                       2.8048e-01,  1.6171e-01, -3.0962e-01,  4.2133e-01,  5.6776e-02,\n                       2.8550e-01,  4.3641e-01,  2.8841e-01, -4.1554e-02,  2.1531e-01,\n                      -1.1007e-01,  1.7108e-01, -1.9869e-02,  3.9978e-02,  2.5709e-03,\n                       3.6874e-04, -8.5361e-02, -2.8764e-02, -4.0084e-01,  1.7551e-01,\n                      -2.2281e-01, -2.0247e-01, -4.3038e-01,  1.1981e-01,  3.3572e-01,\n                      -1.7507e-01, -2.3526e-01,  1.8479e-01,  1.9140e-01,  1.7791e-01,\n                       2.1551e-01, -8.4437e-02,  3.3765e-01, -2.1490e-01,  2.6769e-04,\n                       4.3865e-02, -3.8176e-01, -2.9349e-01, -8.8650e-02, -3.8126e-02,\n                       1.6051e-02,  8.1657e-03,  4.6626e-02, -1.9906e-01, -2.3908e-01,\n                       9.6506e-01,  4.3008e-01,  2.1812e-01, -1.7546e-01],\n                     requires_grad=True)),\n             ('mnet.features.9.conv.2.scale', tensor(0.0227)),\n             ('mnet.features.9.conv.2.zero_point', tensor(66)),\n             ('mnet.features.10.conv.0.0.weight',\n              tensor([[[[-0.0012]],\n              \n                       [[ 0.0006]],\n              \n                       [[-0.0009]],\n              \n                       ...,\n              \n                       [[ 0.0005]],\n              \n                       [[-0.0003]],\n              \n                       [[ 0.0008]]],\n              \n              \n                      [[[-0.0461]],\n              \n                       [[ 0.0333]],\n              \n                       [[ 0.0053]],\n              \n                       ...,\n              \n                       [[ 0.0045]],\n              \n                       [[ 0.0068]],\n              \n                       [[ 0.0242]]],\n              \n              \n                      [[[ 0.0216]],\n              \n                       [[ 0.0111]],\n              \n                       [[ 0.0197]],\n              \n                       ...,\n              \n                       [[ 0.0242]],\n              \n                       [[ 0.0233]],\n              \n                       [[-0.0239]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-0.0131]],\n              \n                       [[ 0.0012]],\n              \n                       [[ 0.0073]],\n              \n                       ...,\n              \n                       [[ 0.0010]],\n              \n                       [[ 0.0020]],\n              \n                       [[ 0.0083]]],\n              \n              \n                      [[[-0.0043]],\n              \n                       [[ 0.0126]],\n              \n                       [[-0.0261]],\n              \n                       ...,\n              \n                       [[ 0.0104]],\n              \n                       [[ 0.0139]],\n              \n                       [[-0.0200]]],\n              \n              \n                      [[[ 0.0142]],\n              \n                       [[-0.0174]],\n              \n                       [[ 0.0238]],\n              \n                       ...,\n              \n                       [[ 0.0567]],\n              \n                       [[-0.0232]],\n              \n                       [[ 0.0097]]]], size=(384, 64, 1, 1), dtype=torch.qint8,\n                     quantization_scheme=torch.per_channel_affine,\n                     scale=tensor([1.1639e-05, 7.5581e-04, 3.2756e-04, 3.0443e-04, 3.3435e-04, 3.6472e-04,\n                      5.7015e-04, 5.5392e-04, 4.8273e-04, 4.0753e-04, 4.0415e-04, 4.9864e-04,\n                      4.8783e-04, 4.3706e-04, 7.8321e-04, 5.1170e-04, 6.1597e-04, 5.0884e-04,\n                      4.5178e-04, 4.3906e-04, 3.9268e-04, 4.0997e-04, 4.6570e-04, 2.3816e-04,\n                      7.5368e-04, 6.3781e-04, 3.9615e-04, 3.7726e-04, 3.7927e-04, 7.4346e-04,\n                      6.1458e-04, 3.7450e-04, 5.5748e-04, 2.6595e-04, 6.7525e-04, 4.6853e-04,\n                      3.7754e-04, 5.7870e-04, 5.5752e-04, 5.4095e-04, 6.9521e-04, 4.4778e-04,\n                      4.6822e-04, 5.4042e-04, 4.6845e-04, 4.3643e-04, 5.0601e-04, 6.2501e-04,\n                      5.7687e-04, 5.0111e-04, 5.0972e-04, 2.7385e-04, 2.5413e-04, 8.8893e-04,\n                      5.6845e-04, 6.2076e-04, 4.3743e-04, 4.8221e-04, 6.5502e-04, 4.6478e-04,\n                      4.6751e-04, 4.6978e-04, 4.3012e-04, 3.2277e-04, 5.6538e-04, 5.5175e-04,\n                      3.0759e-04, 5.4937e-04, 5.0224e-04, 6.2302e-04, 3.6509e-04, 5.7797e-04,\n                      7.4584e-04, 4.8172e-04, 4.2208e-04, 4.1612e-04, 5.3321e-04, 4.9359e-04,\n                      5.4034e-04, 3.8941e-04, 6.2451e-04, 4.8249e-04, 4.1862e-04, 8.3126e-04,\n                      2.6479e-04, 3.3535e-04, 4.8366e-04, 3.0828e-04, 4.2455e-04, 4.7570e-04,\n                      3.7311e-04, 2.1134e-04, 4.5479e-04, 4.1559e-04, 3.9227e-04, 3.7642e-04,\n                      2.2595e-04, 4.4524e-04, 3.0586e-04, 4.5743e-04, 5.0595e-04, 6.1918e-04,\n                      3.6923e-04, 3.7456e-04, 4.9074e-04, 4.5013e-04, 4.4591e-04, 3.9243e-04,\n                      7.3746e-04, 4.8511e-04, 3.3733e-04, 3.7791e-04, 5.9017e-04, 3.7476e-04,\n                      4.3777e-04, 4.2143e-04, 4.4613e-04, 3.6113e-04, 6.9944e-04, 7.5347e-04,\n                      5.0244e-04, 4.4726e-04, 3.4603e-04, 2.7447e-04, 5.5324e-04, 3.4331e-04,\n                      7.6769e-04, 4.5133e-04, 6.4296e-04, 3.3688e-04, 6.4565e-04, 4.6617e-04,\n                      4.3363e-04, 5.3325e-04, 6.9662e-04, 5.2312e-04, 4.3781e-04, 3.4004e-04,\n                      3.5713e-04, 4.3925e-04, 3.6403e-04, 5.0585e-04, 5.2637e-04, 3.8220e-04,\n                      3.2875e-04, 7.8891e-04, 3.2111e-04, 4.1735e-04, 3.5981e-04, 4.5723e-06,\n                      5.6597e-04, 4.2804e-04, 4.2796e-04, 4.5023e-04, 4.6771e-04, 5.4457e-04,\n                      4.0440e-05, 5.4414e-04, 4.6593e-04, 4.2057e-04, 4.3454e-04, 5.9068e-04,\n                      3.1883e-04, 3.9009e-04, 4.4966e-04, 3.9048e-04, 4.7738e-04, 5.5886e-04,\n                      3.8334e-04, 4.7964e-04, 3.7904e-04, 6.2496e-04, 3.5370e-04, 4.0641e-04,\n                      8.2848e-04, 4.7595e-04, 3.8302e-04, 1.9123e-04, 2.8335e-04, 4.7361e-04,\n                      3.7995e-04, 5.3772e-04, 3.7878e-05, 3.8766e-04, 4.6958e-04, 5.4606e-04,\n                      4.8231e-04, 7.1333e-04, 4.2761e-04, 5.5684e-04, 6.9540e-04, 7.4178e-04,\n                      5.9279e-04, 2.9394e-04, 4.7558e-04, 4.2251e-04, 5.0211e-04, 5.1276e-04,\n                      5.8517e-04, 5.2248e-04, 1.7561e-04, 4.0423e-04, 6.9505e-04, 2.4422e-04,\n                      5.8521e-04, 5.7294e-04, 3.2248e-04, 3.6760e-04, 3.1957e-04, 3.9465e-04,\n                      5.3811e-04, 5.5783e-04, 4.8246e-04, 4.3393e-04, 4.5283e-04, 6.2982e-04,\n                      4.0064e-04, 4.5010e-04, 4.5833e-04, 4.0242e-04, 5.4747e-04, 4.0416e-04,\n                      4.5544e-04, 6.4392e-04, 6.6267e-04, 6.6431e-04, 3.6113e-04, 9.9372e-04,\n                      3.9447e-04, 4.6794e-04, 3.6051e-04, 3.2531e-04, 5.0939e-04, 4.3722e-04,\n                      3.8599e-04, 3.6500e-04, 4.7465e-04, 4.4795e-04, 7.3327e-04, 2.8147e-04,\n                      3.3507e-04, 3.8748e-04, 5.0973e-04, 4.3272e-04, 3.4703e-04, 4.1970e-04,\n                      3.7635e-04, 5.5311e-04, 4.2028e-04, 6.0308e-04, 4.6358e-04, 4.6868e-04,\n                      6.8282e-04, 5.8799e-04, 3.9293e-04, 5.2051e-04, 3.0996e-04, 6.0778e-04,\n                      3.8364e-04, 5.5102e-04, 3.5825e-04, 2.0576e-04, 5.2405e-04, 4.5008e-04,\n                      4.2356e-04, 6.2137e-04, 7.4306e-04, 1.6276e-04, 2.7891e-04, 5.4709e-04,\n                      3.8070e-04, 2.2571e-05, 3.1581e-04, 5.0478e-04, 5.8125e-04, 3.7977e-04,\n                      4.4631e-04, 4.9706e-04, 5.3997e-04, 4.9637e-04, 5.0420e-04, 4.8655e-04,\n                      5.1407e-04, 2.8662e-04, 3.6094e-04, 3.9050e-04, 3.0093e-04, 3.8194e-04,\n                      4.9470e-04, 3.9656e-04, 4.1939e-04, 4.8360e-04, 4.6813e-04, 3.1372e-04,\n                      5.6490e-04, 6.6076e-04, 3.6150e-04, 4.9986e-04, 4.5925e-04, 4.9028e-04,\n                      4.7388e-04, 3.1005e-04, 4.3699e-04, 4.6321e-04, 3.5791e-04, 3.8744e-04,\n                      5.2725e-04, 5.1789e-04, 4.3380e-04, 3.7887e-04, 5.0006e-04, 6.2525e-04,\n                      4.2980e-04, 5.0084e-04, 4.4170e-04, 5.7516e-04, 5.7285e-04, 5.0935e-04,\n                      4.0597e-04, 2.7748e-04, 6.1230e-04, 4.7076e-04, 5.5879e-04, 4.8414e-04,\n                      3.8594e-04, 4.5879e-04, 5.4557e-04, 4.7451e-04, 3.9128e-04, 3.6692e-04,\n                      4.8575e-04, 4.5423e-04, 6.0846e-04, 4.2742e-04, 3.4726e-04, 4.0765e-04,\n                      3.8061e-04, 3.7156e-04, 4.9763e-04, 3.2516e-04, 5.0164e-04, 4.1052e-04,\n                      5.1384e-04, 5.0647e-04, 2.0668e-04, 4.2298e-04, 6.5293e-04, 5.4322e-04,\n                      3.3298e-04, 5.8428e-04, 4.9964e-04, 4.1633e-04, 2.9881e-04, 2.4795e-04,\n                      6.3372e-04, 4.6056e-04, 3.7000e-04, 5.1286e-04, 3.3175e-04, 4.5238e-04,\n                      3.9438e-04, 3.2335e-04, 4.8201e-04, 3.4841e-04, 5.7032e-04, 6.1605e-04,\n                      7.1574e-04, 1.6641e-04, 4.5520e-04, 3.5677e-04, 5.1150e-04, 7.4703e-04,\n                      4.4567e-04, 6.6382e-04, 5.4010e-04, 4.2787e-04, 2.6518e-04, 3.9629e-04,\n                      4.4347e-04, 3.5378e-04, 3.4473e-04, 2.0143e-04, 4.3485e-04, 6.4441e-04],\n                     dtype=torch.float64),\n                     zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n                     axis=0)),\n             ('mnet.features.10.conv.0.0.bias',\n              Parameter containing:\n              tensor([-2.8315e-02, -7.2319e-02,  1.8193e-02,  7.2902e-02,  1.1690e-01,\n                       1.0364e-01, -3.6840e-02, -2.3292e-02,  6.1753e-02,  3.3685e-02,\n                       1.3528e-01,  2.3764e-02, -1.1561e-02,  6.2228e-02,  1.7849e-01,\n                      -7.4358e-02, -1.0519e-02,  2.1123e-02,  1.1093e-02,  6.2901e-02,\n                       2.1830e-02, -9.2421e-03, -4.6171e-02, -1.1130e-02, -4.5301e-02,\n                       5.2909e-02, -2.4598e-02,  1.2150e-01,  7.1767e-02,  1.1975e-01,\n                       3.0708e-02,  3.3553e-02, -1.2331e-01, -2.0241e-02,  1.1703e-01,\n                       8.3062e-02, -4.9738e-02,  5.6784e-02,  3.0191e-02,  3.5116e-02,\n                       3.3673e-02,  1.7835e-02,  1.7939e-02,  8.8635e-02,  2.1896e-02,\n                       4.6266e-03,  5.2235e-02,  2.1340e-02, -5.3263e-02, -2.3263e-02,\n                      -8.7251e-02,  1.8616e-02,  1.6079e-01, -5.3549e-02, -8.1461e-02,\n                       1.3691e-01,  3.3561e-02,  7.9045e-02, -7.7261e-02,  1.0237e-01,\n                       6.2272e-02,  7.2686e-02,  9.7683e-02,  4.5309e-02, -1.1178e-02,\n                       4.1363e-02,  8.6830e-02,  3.9514e-02, -2.1566e-02, -4.6154e-02,\n                       9.7173e-02, -1.6226e-02, -5.2249e-02, -2.0741e-02,  2.1246e-02,\n                       2.5155e-02,  2.1593e-02, -1.1157e-02, -7.5198e-02, -6.8852e-03,\n                       9.6218e-03, -4.2104e-02, -3.5975e-02,  1.2115e-02,  1.1253e-02,\n                       1.8682e-02, -2.7881e-02,  1.8451e-01,  9.7305e-03, -4.1818e-02,\n                       5.4850e-02,  1.3568e-01, -9.8102e-03,  1.3036e-01, -5.7111e-02,\n                      -3.6062e-02, -5.2473e-03, -5.5090e-02, -1.2350e-02,  2.8976e-02,\n                       8.0259e-02,  1.2361e-01, -9.6943e-02,  1.3003e-01,  9.0903e-02,\n                      -7.1788e-03,  8.9974e-02,  6.5673e-02, -1.0843e-01,  3.8543e-02,\n                       2.9344e-02, -4.4887e-02,  2.7775e-02,  1.0747e-01,  7.8663e-03,\n                       3.5794e-03,  1.9910e-02,  2.8129e-02, -1.2320e-02, -1.0734e-01,\n                      -5.8420e-02,  1.4672e-01, -8.2024e-03,  1.1119e-01, -6.9809e-03,\n                       1.1423e-01, -2.9093e-02,  2.9563e-02,  2.8250e-02,  1.2830e-01,\n                       2.1844e-02,  1.9648e-02, -5.9178e-02, -3.3750e-02,  4.9633e-02,\n                       3.1460e-02, -4.8520e-02,  1.0505e-01,  5.9106e-02,  1.4531e-01,\n                       2.2702e-01, -3.7452e-02,  7.0343e-02,  1.7724e-01,  1.1061e-01,\n                      -9.6510e-02, -5.9325e-04,  1.6456e-01,  1.4855e-01, -1.4189e-02,\n                      -1.1647e-01, -7.7711e-03,  1.1563e-01,  1.5025e-01,  9.8951e-02,\n                       1.1078e-01, -5.8143e-02, -1.1634e-02,  4.2906e-02,  8.5176e-02,\n                      -4.4350e-02,  1.0144e-02,  7.8704e-02,  6.6459e-02, -1.9646e-02,\n                      -7.1351e-02,  6.1251e-02, -9.4523e-03,  2.0618e-03,  3.3250e-02,\n                       1.3479e-01, -9.9400e-02,  1.8643e-01, -2.2297e-02,  3.3144e-02,\n                       3.3832e-02,  3.2489e-02,  1.5400e-01, -1.2270e-02,  2.1493e-02,\n                       8.4602e-03, -8.1469e-02, -5.3899e-02, -2.0336e-02,  7.9024e-02,\n                      -7.4429e-02, -4.4470e-02, -1.0082e-01,  1.8453e-01, -7.6754e-02,\n                       7.2888e-02,  5.0115e-02,  1.4738e-03, -2.7494e-02,  4.8034e-02,\n                       1.1976e-01, -2.6026e-03,  4.2479e-02, -4.6713e-03,  8.0322e-03,\n                       1.0699e-01,  4.7211e-02, -6.2924e-02, -1.3286e-02,  7.1622e-02,\n                      -9.9226e-02,  2.0863e-01,  4.4908e-02, -6.4401e-02, -4.7506e-03,\n                      -1.3567e-02, -1.4217e-01,  3.8460e-02, -5.0835e-02, -9.4467e-02,\n                      -3.0136e-02,  2.7528e-02,  2.2219e-03,  5.0745e-03,  5.0317e-02,\n                       6.1105e-02,  2.1141e-01,  3.9582e-02,  6.7582e-02,  6.0355e-03,\n                       4.3233e-02,  8.9661e-02,  3.3803e-02,  4.9213e-02,  6.1661e-05,\n                      -2.0653e-02, -3.1748e-02,  6.9779e-02,  1.4681e-02, -4.5012e-02,\n                       1.5902e-01, -6.0825e-02,  8.2689e-02, -5.1980e-02,  1.2423e-01,\n                       9.4152e-02,  4.0508e-02, -2.1981e-02,  3.0227e-02,  6.9596e-02,\n                       2.9494e-03, -6.0273e-02,  1.9701e-01, -5.1875e-02, -1.7529e-02,\n                      -1.1264e-02,  1.3506e-02,  7.3286e-03,  4.5340e-03,  2.0417e-01,\n                       1.1854e-01,  1.2665e-01, -1.0429e-01, -1.5308e-02, -5.7549e-03,\n                       1.2509e-01,  7.1503e-02, -4.2997e-03,  7.7619e-02, -1.8317e-02,\n                      -2.3873e-02,  1.9593e-02,  1.6115e-01,  1.2073e-01,  1.2630e-02,\n                       3.6421e-03, -4.8989e-02,  2.0777e-01,  5.8984e-02, -2.4730e-02,\n                       6.1961e-02, -6.5034e-02, -7.4051e-02,  3.7537e-02, -3.7861e-02,\n                       2.3475e-02, -1.2976e-01, -5.4221e-02,  7.8000e-02, -3.6010e-02,\n                      -4.6516e-02, -4.6879e-02,  7.4224e-02,  3.0830e-02,  1.0716e-01,\n                      -2.5889e-02,  1.3103e-01,  4.7129e-02,  8.5810e-02,  2.3739e-02,\n                      -1.0134e-01, -5.1542e-02, -8.5154e-02, -3.7313e-02, -2.1434e-02,\n                       5.4205e-02,  4.6114e-02,  1.9020e-02, -1.4043e-02,  1.4041e-01,\n                      -8.0630e-03,  4.2130e-02, -7.6532e-02, -1.8222e-02, -6.0697e-03,\n                       7.1989e-02, -6.2373e-02,  2.4269e-03, -4.7134e-02,  4.2457e-02,\n                      -2.8201e-02, -3.0596e-02, -1.9618e-02, -4.1205e-02,  1.2728e-01,\n                       7.9179e-02,  2.8312e-02, -2.0696e-02, -7.2872e-02,  8.5908e-03,\n                      -8.9917e-02, -6.6696e-02,  2.4518e-02, -1.6009e-03,  2.4410e-02,\n                       1.6417e-02,  1.4954e-01, -3.2089e-02,  5.3470e-02,  1.0247e-01,\n                       3.2227e-02, -9.2358e-04,  2.2281e-01,  1.4422e-02, -1.1962e-02,\n                      -5.3405e-02,  1.9293e-01, -4.4585e-02,  5.2534e-03,  1.8394e-01,\n                      -3.6779e-04,  8.6078e-02, -1.7449e-02,  1.0158e-01,  1.4565e-01,\n                       2.0413e-02, -2.7606e-02,  1.3293e-01,  9.6729e-02, -4.2203e-02,\n                      -1.2114e-01,  1.2780e-01,  7.3132e-02,  8.4825e-02,  9.1811e-03,\n                       1.3036e-01,  1.2555e-01,  5.3967e-02,  1.7977e-03, -4.2659e-02,\n                       1.3356e-02, -3.9826e-02,  1.5615e-01, -1.0821e-03,  1.1972e-01,\n                      -1.9778e-02, -3.7995e-01,  6.9468e-02, -7.0854e-02,  4.7595e-02,\n                      -6.9360e-02,  1.2735e-01, -2.0921e-02, -1.2147e-02,  9.0328e-02,\n                       2.8905e-02,  1.2805e-01,  1.9733e-02,  9.0542e-02],\n                     requires_grad=True)),\n             ('mnet.features.10.conv.0.0.scale', tensor(0.0121)),\n             ('mnet.features.10.conv.0.0.zero_point', tensor(68)),\n             ('mnet.features.10.conv.1.0.weight',\n              tensor([[[[-0.0861,  0.0412,  0.0917],\n                        [-0.0487, -0.1442,  0.2378],\n                        [ 0.0094,  0.0768,  0.1011]]],\n              \n              \n                      [[[ 0.1072,  0.0153,  0.0919],\n                        [ 0.5511,  1.9443,  0.3674],\n                        [ 0.5358, -0.3062,  0.5511]]],\n              \n              \n                      [[[ 0.3727, -0.1514, -1.3392],\n                        [ 1.4790,  0.5473, -0.5240],\n                        [ 0.6405,  0.4076, -0.2446]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-0.2460, -0.6151, -0.2706],\n                        [-0.9104, -2.1160, -0.7381],\n                        [ 0.2706,  3.1247,  0.0984]]],\n              \n              \n                      [[[ 0.3443, -0.2367, -0.1291],\n                        [ 0.7425, -0.7425, -0.3121],\n                        [ 1.3666,  0.4519, -0.1829]]],\n              \n              \n                      [[[-0.0293,  0.2344, -0.1758],\n                        [-0.4394,  1.2402,  0.3613],\n                        [-0.7812, -0.8984, -0.1367]]]], size=(384, 1, 3, 3),\n                     dtype=torch.qint8, quantization_scheme=torch.per_channel_affine,\n                     scale=tensor([0.0019, 0.0153, 0.0116, 0.0072, 0.0108, 0.0124, 0.0196, 0.0133, 0.0129,\n                      0.0130, 0.0107, 0.0139, 0.0181, 0.0054, 0.0026, 0.0139, 0.0109, 0.0165,\n                      0.0175, 0.0149, 0.0107, 0.0120, 0.0086, 0.0319, 0.0114, 0.0050, 0.0150,\n                      0.0119, 0.0117, 0.0055, 0.0029, 0.0181, 0.0211, 0.0342, 0.0085, 0.0048,\n                      0.0181, 0.0056, 0.0155, 0.0054, 0.0051, 0.0108, 0.0135, 0.0087, 0.0079,\n                      0.0109, 0.0118, 0.0065, 0.0146, 0.0147, 0.0137, 0.0168, 0.0187, 0.0048,\n                      0.0146, 0.0074, 0.0126, 0.0050, 0.0090, 0.0104, 0.0121, 0.0133, 0.0169,\n                      0.0096, 0.0134, 0.0188, 0.0129, 0.0052, 0.0196, 0.0110, 0.0145, 0.0115,\n                      0.0120, 0.0123, 0.0104, 0.0095, 0.0120, 0.0158, 0.0117, 0.0194, 0.0096,\n                      0.0126, 0.0141, 0.0112, 0.0105, 0.0145, 0.0122, 0.0112, 0.0176, 0.0091,\n                      0.0112, 0.0248, 0.0117, 0.0154, 0.0155, 0.0135, 0.0144, 0.0175, 0.0105,\n                      0.0045, 0.0074, 0.0112, 0.0248, 0.0142, 0.0061, 0.0192, 0.0051, 0.0270,\n                      0.0208, 0.0236, 0.0091, 0.0133, 0.0107, 0.0182, 0.0147, 0.0138, 0.0080,\n                      0.0134, 0.0081, 0.0160, 0.0153, 0.0073, 0.0113, 0.0189, 0.0100, 0.0178,\n                      0.0083, 0.0124, 0.0124, 0.0091, 0.0184, 0.0045, 0.0166, 0.0197, 0.0098,\n                      0.0052, 0.0093, 0.0178, 0.0127, 0.0133, 0.0035, 0.0168, 0.0114, 0.0065,\n                      0.0060, 0.0099, 0.0191, 0.0088, 0.0165, 0.0004, 0.0310, 0.0166, 0.0059,\n                      0.0061, 0.0120, 0.0107, 0.0085, 0.0183, 0.0120, 0.0098, 0.0131, 0.0199,\n                      0.0208, 0.0092, 0.0137, 0.0260, 0.0104, 0.0061, 0.0129, 0.0096, 0.0087,\n                      0.0159, 0.0089, 0.0160, 0.0070, 0.0145, 0.0114, 0.0182, 0.0209, 0.0122,\n                      0.0113, 0.0197, 0.0059, 0.0214, 0.0177, 0.0074, 0.0156, 0.0165, 0.0105,\n                      0.0109, 0.0082, 0.0051, 0.0039, 0.0162, 0.0094, 0.0043, 0.0134, 0.0149,\n                      0.0141, 0.0049, 0.0235, 0.0129, 0.0121, 0.0236, 0.0082, 0.0185, 0.0097,\n                      0.0141, 0.0148, 0.0107, 0.0139, 0.0358, 0.0044, 0.0176, 0.0092, 0.0106,\n                      0.0156, 0.0195, 0.0158, 0.0053, 0.0133, 0.0081, 0.0133, 0.0141, 0.0119,\n                      0.0055, 0.0070, 0.0030, 0.0054, 0.0094, 0.0160, 0.0375, 0.0157, 0.0095,\n                      0.0256, 0.0235, 0.0151, 0.0264, 0.0169, 0.0059, 0.0048, 0.0155, 0.0120,\n                      0.0205, 0.0154, 0.0104, 0.0233, 0.0200, 0.0122, 0.0116, 0.0061, 0.0125,\n                      0.0106, 0.0060, 0.0060, 0.0070, 0.0227, 0.0101, 0.0162, 0.0204, 0.0100,\n                      0.0211, 0.0119, 0.0044, 0.0113, 0.0039, 0.0098, 0.0272, 0.0136, 0.0123,\n                      0.0077, 0.0064, 0.0133, 0.0093, 0.0132, 0.0128, 0.0119, 0.0132, 0.0098,\n                      0.0139, 0.0093, 0.0075, 0.0085, 0.0156, 0.0158, 0.0138, 0.0211, 0.0140,\n                      0.0067, 0.0092, 0.0163, 0.0116, 0.0156, 0.0274, 0.0088, 0.0236, 0.0267,\n                      0.0154, 0.0189, 0.0109, 0.0138, 0.0160, 0.0116, 0.0201, 0.0176, 0.0152,\n                      0.0034, 0.0168, 0.0159, 0.0166, 0.0054, 0.0136, 0.0198, 0.0145, 0.0118,\n                      0.0105, 0.0037, 0.0281, 0.0135, 0.0254, 0.0085, 0.0072, 0.0048, 0.0201,\n                      0.0152, 0.0137, 0.0097, 0.0075, 0.0108, 0.0159, 0.0083, 0.0091, 0.0108,\n                      0.0078, 0.0051, 0.0087, 0.0174, 0.0078, 0.0122, 0.0092, 0.0100, 0.0067,\n                      0.0198, 0.0143, 0.0226, 0.0045, 0.0089, 0.0160, 0.0222, 0.0051, 0.0112,\n                      0.0141, 0.0243, 0.0165, 0.0122, 0.0199, 0.0132, 0.0054, 0.0060, 0.0088,\n                      0.0126, 0.0207, 0.0052, 0.0160, 0.0094, 0.0157, 0.0048, 0.0267, 0.0132,\n                      0.0071, 0.0168, 0.0514, 0.0064, 0.0094, 0.0115, 0.0363, 0.0222, 0.0074,\n                      0.0122, 0.0087, 0.0121, 0.0246, 0.0108, 0.0098], dtype=torch.float64),\n                     zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n                     axis=0)),\n             ('mnet.features.10.conv.1.0.bias',\n              Parameter containing:\n              tensor([-1.6964e-02, -1.2062e-01, -1.2163e-01,  3.2957e-01, -3.8326e-02,\n                       2.2896e-02, -1.8647e-01, -3.9491e-02,  1.1569e-01,  2.7959e-02,\n                       4.3306e-01, -1.8933e-01, -1.8219e-01,  2.8661e-01,  3.0592e-01,\n                      -2.4952e-02, -2.9061e-02, -2.4572e-01, -1.3726e-01, -6.9890e-02,\n                      -2.1724e-01,  4.7837e-02, -1.1883e-01, -3.1497e-01,  3.0563e-01,\n                       2.3859e-01, -1.2158e-01,  2.6828e-01, -1.8838e-01,  3.5473e-01,\n                       3.3591e-01, -2.0927e-01, -1.2273e-01, -2.1957e-01,  2.1907e-01,\n                       2.3785e-01,  1.2948e-02,  3.0963e-01, -6.9344e-03,  2.8516e-01,\n                       2.6976e-01,  1.9665e-01, -1.7258e-02,  1.6558e-01,  2.2937e-01,\n                      -1.7644e-01,  2.9928e-01,  2.6211e-01, -7.4383e-02, -6.4296e-02,\n                      -4.7487e-03, -2.6942e-01,  6.8691e-01,  2.8541e-02, -1.1259e-01,\n                       4.3432e-01, -9.7558e-02,  2.3327e-01, -4.6301e-02,  2.6538e-01,\n                      -1.5387e-02,  1.9289e-01,  3.3657e-01,  1.1386e-03, -1.6712e-01,\n                      -2.4078e-02, -1.0029e-02,  2.4901e-01, -2.1624e-01, -4.2408e-02,\n                       2.3570e-01, -1.7264e-01,  3.2290e-02,  2.5150e-01,  1.3775e-01,\n                       8.3038e-02, -1.8431e-02, -2.0998e-01,  1.1552e-01, -3.0310e-01,\n                       1.2490e-01, -6.0364e-02, -5.2126e-02,  2.0048e-01, -2.7794e-01,\n                      -2.6135e-01,  6.3774e-02,  2.4091e-01, -1.6653e-01,  1.2129e-01,\n                      -1.1499e-01, -6.3887e-02, -1.0165e-01,  4.3193e-01, -4.9402e-02,\n                      -6.6494e-02, -2.6489e-01, -1.6085e-01, -1.2900e-01,  2.2548e-01,\n                       1.9627e-01,  3.0231e-02, -6.8491e-02,  3.1212e-01,  3.5163e-01,\n                      -2.1088e-01,  2.8148e-01, -1.7057e-01,  6.2015e-02, -4.0143e-02,\n                       8.9626e-02,  5.4718e-02,  2.1868e-01, -1.3100e-01, -2.5479e-01,\n                      -1.0830e-01, -8.1953e-02, -1.2944e-01, -1.2925e-01, -2.3152e-01,\n                      -1.3360e-01,  4.4057e-01, -3.6254e-01, -3.9840e-01,  2.6671e-01,\n                       2.2438e-01, -1.5381e-01,  1.4750e-01, -1.0559e-02,  3.3218e-01,\n                       6.9525e-03,  2.0356e-01, -6.6933e-02, -1.7110e-01,  4.1289e-01,\n                       2.3550e-01, -3.6585e-02, -2.8527e-01, -4.9199e-02, -1.8774e-01,\n                       5.5474e-01,  5.3417e-03, -4.5630e-02,  3.9440e-01,  3.7709e-01,\n                      -5.0806e-03, -2.8939e-01,  4.5763e-01,  2.7505e-01,  1.4521e-03,\n                      -7.0557e-02, -1.6255e-01,  2.6760e-01,  3.2244e-01, -7.5473e-02,\n                       3.3569e-01, -3.6276e-02, -5.6712e-02, -1.2060e-01,  4.9556e-02,\n                      -9.0593e-02, -1.4265e-01, -2.2499e-01,  1.6768e-01, -4.2535e-02,\n                      -1.5757e-01, -1.2999e-01,  2.5650e-01, -9.7104e-02,  2.2236e-01,\n                       3.5429e-01, -6.5945e-02,  4.8865e-01, -1.7096e-01,  2.5166e-01,\n                      -1.0614e-01, -5.8198e-02, -1.6991e-01, -2.8917e-01, -1.5859e-01,\n                      -1.1403e-01, -1.3846e-01, -2.0192e-02, -2.1938e-01, -1.1419e-02,\n                      -7.5051e-02, -3.8386e-02, -5.0825e-02,  5.2894e-01, -4.0316e-02,\n                       2.9439e-01,  2.4970e-01,  1.5210e-01, -1.1128e-01,  2.2887e-01,\n                       3.8687e-01, -2.0856e-01, -3.9258e-02, -4.4429e-02,  2.2421e-01,\n                      -1.4975e-02, -2.3036e-02, -3.6338e-02, -1.9119e-01,  6.1095e-02,\n                      -1.9142e-01,  5.3839e-01, -4.8118e-01, -1.1038e-01, -5.3091e-03,\n                       2.2077e-01, -1.9831e-01,  2.2981e-01, -1.5573e-01,  1.5636e-02,\n                      -3.2875e-02,  3.1262e-01, -7.5545e-02, -1.1491e-01,  2.9937e-01,\n                       2.3305e-01,  5.6450e-01, -2.2747e-02, -7.2002e-03, -7.5243e-03,\n                       2.3408e-01,  3.0411e-01,  2.8768e-01,  2.0297e-01, -6.7475e-02,\n                      -1.0600e-01, -1.9870e-01,  2.9323e-01,  2.7139e-01, -1.8990e-01,\n                       2.4355e-01, -2.0969e-02, -9.7136e-02, -1.1241e-01,  3.3441e-01,\n                       2.7015e-01, -2.1052e-01, -1.2030e-01, -9.2178e-02, -1.6302e-02,\n                      -2.6890e-02, -3.4321e-01,  3.1131e-01, -3.0820e-03,  2.8847e-01,\n                       2.4233e-01,  1.2963e-01,  1.2400e-01,  3.9943e-01,  7.5788e-01,\n                       3.1843e-01, -1.2894e-01, -1.9212e-01, -6.5179e-02, -5.0155e-01,\n                       4.4506e-01, -2.0348e-01,  1.5186e-01,  3.0923e-01, -3.1081e-02,\n                       2.2602e-01,  1.4090e-01, -6.9517e-01, -2.7047e-01, -6.3565e-02,\n                      -9.5891e-02, -3.7589e-02,  4.6169e-01,  3.8060e-01, -1.6708e-01,\n                       1.5463e-01, -1.7949e-02, -1.7731e-01,  2.4790e-01, -4.2016e-02,\n                       2.1495e-01, -5.7220e-03, -1.9436e-01, -7.0736e-02, -1.1005e-01,\n                      -1.4036e-01, -2.3495e-01, -1.3950e-01,  1.8461e-01,  2.0077e-01,\n                      -6.8314e-02,  1.8637e-01, -9.0713e-03, -1.3330e-01, -6.6856e-02,\n                      -9.1108e-02, -1.4590e-01, -4.3232e-03, -2.0760e-01,  9.3723e-02,\n                       1.9725e-01, -1.6700e-02, -4.1565e-01, -3.2259e-01,  1.0948e-01,\n                      -2.7133e-01,  3.0035e-01, -9.9517e-02, -2.4595e-01, -3.8200e-01,\n                       2.0824e-01, -3.6789e-02, -2.2134e-01, -1.5456e-02, -2.9654e-01,\n                       2.7247e-01,  1.9877e-01, -9.2335e-03, -1.8060e-01, -3.6436e-01,\n                       1.4897e-01,  2.0987e-01,  1.7235e-01, -1.9880e-01, -1.4712e-01,\n                       1.0789e-01, -7.3381e-05,  2.2026e-01, -1.4987e-01, -1.9805e-01,\n                       1.9064e-01,  4.0060e-01, -3.5761e-02,  2.4032e-01,  3.3986e-01,\n                      -2.0561e-01, -1.5753e-01,  8.2662e-01,  2.3152e-01, -2.3628e-01,\n                      -1.2530e-01,  6.1467e-01, -1.6932e-01, -9.0999e-03,  1.5115e-01,\n                       2.3469e-01,  1.1934e-01, -2.0766e-01, -3.0347e-01,  3.9374e-01,\n                      -6.7034e-02,  8.8283e-04, -5.5406e-02, -3.5605e-02, -5.5544e-02,\n                      -2.3839e-03,  3.4033e-01,  2.7690e-01,  2.7345e-01, -6.6333e-02,\n                       2.6839e-01, -1.1476e+00,  3.1289e-01, -2.4924e-01, -1.6448e-01,\n                      -1.0992e-01,  1.4401e-02,  3.1633e-01, -2.2577e-01,  3.6618e-01,\n                      -1.7909e-02, -5.0951e-01,  3.3606e-01, -7.2123e-02,  6.0538e-02,\n                      -4.8504e-02, -1.9136e-01,  1.7624e-01,  3.9503e-03,  3.9571e-01,\n                      -1.9247e-01,  1.5973e-01,  6.5029e-02,  1.0466e-01],\n                     requires_grad=True)),\n             ('mnet.features.10.conv.1.0.scale', tensor(0.0391)),\n             ('mnet.features.10.conv.1.0.zero_point', tensor(51)),\n             ('mnet.features.10.conv.2.weight',\n              tensor([[[[ 0.0233]],\n              \n                       [[-0.1086]],\n              \n                       [[-0.0724]],\n              \n                       ...,\n              \n                       [[-0.1060]],\n              \n                       [[-0.1706]],\n              \n                       [[ 0.1499]]],\n              \n              \n                      [[[-0.0163]],\n              \n                       [[ 0.0897]],\n              \n                       [[ 0.0897]],\n              \n                       ...,\n              \n                       [[-0.0272]],\n              \n                       [[ 0.1930]],\n              \n                       [[ 0.0218]]],\n              \n              \n                      [[[-0.0064]],\n              \n                       [[-0.1277]],\n              \n                       [[ 0.0064]],\n              \n                       ...,\n              \n                       [[ 0.1852]],\n              \n                       [[ 0.0575]],\n              \n                       [[-0.0064]]],\n              \n              \n                      ...,\n              \n              \n                      [[[ 0.0231]],\n              \n                       [[ 0.0595]],\n              \n                       [[ 0.0297]],\n              \n                       ...,\n              \n                       [[ 0.0132]],\n              \n                       [[ 0.0859]],\n              \n                       [[-0.1619]]],\n              \n              \n                      [[[ 0.0109]],\n              \n                       [[-0.1137]],\n              \n                       [[ 0.0197]],\n              \n                       ...,\n              \n                       [[ 0.0262]],\n              \n                       [[ 0.0240]],\n              \n                       [[ 0.0459]]],\n              \n              \n                      [[[-0.0274]],\n              \n                       [[ 0.1020]],\n              \n                       [[ 0.1368]],\n              \n                       ...,\n              \n                       [[ 0.0970]],\n              \n                       [[ 0.0547]],\n              \n                       [[-0.0224]]]], size=(64, 384, 1, 1), dtype=torch.qint8,\n                     quantization_scheme=torch.per_channel_affine,\n                     scale=tensor([0.0026, 0.0027, 0.0064, 0.0028, 0.0028, 0.0027, 0.0030, 0.0034, 0.0032,\n                      0.0040, 0.0029, 0.0025, 0.0031, 0.0026, 0.0019, 0.0024, 0.0023, 0.0020,\n                      0.0036, 0.0023, 0.0028, 0.0024, 0.0026, 0.0047, 0.0037, 0.0034, 0.0040,\n                      0.0027, 0.0035, 0.0034, 0.0041, 0.0036, 0.0047, 0.0022, 0.0031, 0.0039,\n                      0.0048, 0.0025, 0.0027, 0.0047, 0.0026, 0.0032, 0.0028, 0.0030, 0.0031,\n                      0.0025, 0.0031, 0.0034, 0.0033, 0.0020, 0.0034, 0.0033, 0.0032, 0.0030,\n                      0.0025, 0.0028, 0.0038, 0.0050, 0.0029, 0.0034, 0.0065, 0.0033, 0.0022,\n                      0.0025], dtype=torch.float64),\n                     zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n                     axis=0)),\n             ('mnet.features.10.conv.2.bias',\n              Parameter containing:\n              tensor([-0.2670, -0.1612,  0.1301, -0.5240,  0.5795, -0.1257, -0.1615,  0.2403,\n                      -0.7027,  0.5774, -0.0623, -0.4758, -0.4918, -0.4742, -0.3948, -0.2573,\n                       0.2135, -0.1659,  0.2868,  0.1639,  0.0856,  0.1809, -0.5679,  0.1501,\n                      -0.0268, -0.0926,  0.2904,  0.1077, -0.1736, -0.2280, -0.0254, -0.6072,\n                      -0.6875,  0.2264,  0.2788,  0.0572, -0.0596, -0.1481, -0.1190,  0.1878,\n                       0.4454,  0.0142,  0.0031,  0.0917, -0.0808, -0.1506, -0.2452, -0.3784,\n                       0.2165, -0.3201, -0.0295, -0.0329, -0.3271,  0.2763,  0.0303, -0.0688,\n                       0.2107, -0.3069, -0.8245, -0.3281,  0.0623, -0.3074, -0.0442,  0.1819],\n                     requires_grad=True)),\n             ('mnet.features.10.conv.2.scale', tensor(0.0554)),\n             ('mnet.features.10.conv.2.zero_point', tensor(75)),\n             ('mnet.features.11.conv.0.0.weight',\n              tensor([[[[ 0.0384]],\n              \n                       [[ 0.0136]],\n              \n                       [[ 0.0216]],\n              \n                       ...,\n              \n                       [[-0.0333]],\n              \n                       [[ 0.0047]],\n              \n                       [[ 0.0136]]],\n              \n              \n                      [[[ 0.0237]],\n              \n                       [[-0.0359]],\n              \n                       [[-0.0013]],\n              \n                       ...,\n              \n                       [[-0.0190]],\n              \n                       [[-0.0178]],\n              \n                       [[-0.0275]]],\n              \n              \n                      [[[ 0.0191]],\n              \n                       [[ 0.0116]],\n              \n                       [[ 0.0075]],\n              \n                       ...,\n              \n                       [[-0.0280]],\n              \n                       [[-0.0511]],\n              \n                       [[ 0.0116]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-0.0722]],\n              \n                       [[ 0.0471]],\n              \n                       [[ 0.0208]],\n              \n                       ...,\n              \n                       [[ 0.0006]],\n              \n                       [[-0.0312]],\n              \n                       [[ 0.0257]]],\n              \n              \n                      [[[ 0.0062]],\n              \n                       [[ 0.0187]],\n              \n                       [[-0.0071]],\n              \n                       ...,\n              \n                       [[-0.0356]],\n              \n                       [[-0.0116]],\n              \n                       [[-0.0249]]],\n              \n              \n                      [[[ 0.0187]],\n              \n                       [[ 0.0150]],\n              \n                       [[-0.0194]],\n              \n                       ...,\n              \n                       [[-0.0135]],\n              \n                       [[-0.0449]],\n              \n                       [[-0.0508]]]], size=(384, 64, 1, 1), dtype=torch.qint8,\n                     quantization_scheme=torch.per_channel_affine,\n                     scale=tensor([0.0005, 0.0004, 0.0007, 0.0004, 0.0005, 0.0006, 0.0005, 0.0008, 0.0006,\n                      0.0003, 0.0002, 0.0005, 0.0006, 0.0008, 0.0009, 0.0005, 0.0006, 0.0005,\n                      0.0005, 0.0006, 0.0007, 0.0005, 0.0005, 0.0005, 0.0004, 0.0004, 0.0004,\n                      0.0005, 0.0006, 0.0005, 0.0004, 0.0004, 0.0006, 0.0003, 0.0007, 0.0003,\n                      0.0006, 0.0005, 0.0006, 0.0007, 0.0005, 0.0006, 0.0003, 0.0005, 0.0007,\n                      0.0007, 0.0005, 0.0005, 0.0006, 0.0006, 0.0003, 0.0004, 0.0004, 0.0005,\n                      0.0004, 0.0004, 0.0005, 0.0009, 0.0005, 0.0003, 0.0005, 0.0004, 0.0004,\n                      0.0007, 0.0008, 0.0004, 0.0004, 0.0005, 0.0005, 0.0003, 0.0005, 0.0006,\n                      0.0002, 0.0007, 0.0006, 0.0003, 0.0006, 0.0006, 0.0005, 0.0005, 0.0007,\n                      0.0006, 0.0007, 0.0008, 0.0007, 0.0007, 0.0009, 0.0007, 0.0007, 0.0005,\n                      0.0005, 0.0005, 0.0007, 0.0006, 0.0007, 0.0006, 0.0006, 0.0005, 0.0005,\n                      0.0007, 0.0004, 0.0004, 0.0005, 0.0006, 0.0004, 0.0007, 0.0006, 0.0008,\n                      0.0006, 0.0010, 0.0005, 0.0006, 0.0003, 0.0005, 0.0005, 0.0006, 0.0005,\n                      0.0005, 0.0003, 0.0007, 0.0008, 0.0004, 0.0004, 0.0006, 0.0004, 0.0004,\n                      0.0005, 0.0007, 0.0005, 0.0006, 0.0005, 0.0005, 0.0004, 0.0006, 0.0004,\n                      0.0005, 0.0006, 0.0005, 0.0007, 0.0006, 0.0005, 0.0006, 0.0007, 0.0007,\n                      0.0006, 0.0007, 0.0005, 0.0005, 0.0008, 0.0007, 0.0005, 0.0006, 0.0003,\n                      0.0005, 0.0004, 0.0006, 0.0006, 0.0006, 0.0005, 0.0002, 0.0005, 0.0006,\n                      0.0005, 0.0003, 0.0006, 0.0003, 0.0007, 0.0005, 0.0006, 0.0007, 0.0004,\n                      0.0004, 0.0004, 0.0004, 0.0010, 0.0006, 0.0004, 0.0004, 0.0007, 0.0006,\n                      0.0007, 0.0003, 0.0006, 0.0004, 0.0008, 0.0007, 0.0006, 0.0009, 0.0012,\n                      0.0006, 0.0005, 0.0006, 0.0006, 0.0005, 0.0006, 0.0007, 0.0007, 0.0005,\n                      0.0008, 0.0003, 0.0006, 0.0005, 0.0006, 0.0004, 0.0010, 0.0005, 0.0006,\n                      0.0005, 0.0003, 0.0006, 0.0006, 0.0008, 0.0006, 0.0009, 0.0006, 0.0006,\n                      0.0010, 0.0004, 0.0005, 0.0006, 0.0006, 0.0006, 0.0004, 0.0006, 0.0003,\n                      0.0006, 0.0007, 0.0008, 0.0007, 0.0006, 0.0005, 0.0006, 0.0003, 0.0008,\n                      0.0006, 0.0005, 0.0005, 0.0005, 0.0004, 0.0005, 0.0006, 0.0004, 0.0007,\n                      0.0008, 0.0007, 0.0007, 0.0005, 0.0007, 0.0008, 0.0009, 0.0004, 0.0006,\n                      0.0006, 0.0004, 0.0003, 0.0005, 0.0005, 0.0006, 0.0005, 0.0002, 0.0006,\n                      0.0004, 0.0004, 0.0010, 0.0006, 0.0005, 0.0007, 0.0008, 0.0003, 0.0009,\n                      0.0005, 0.0006, 0.0006, 0.0003, 0.0006, 0.0006, 0.0004, 0.0005, 0.0006,\n                      0.0006, 0.0006, 0.0008, 0.0004, 0.0005, 0.0005, 0.0006, 0.0006, 0.0005,\n                      0.0007, 0.0003, 0.0006, 0.0008, 0.0007, 0.0006, 0.0006, 0.0005, 0.0004,\n                      0.0005, 0.0007, 0.0010, 0.0010, 0.0004, 0.0010, 0.0004, 0.0006, 0.0009,\n                      0.0007, 0.0004, 0.0007, 0.0009, 0.0007, 0.0006, 0.0004, 0.0003, 0.0007,\n                      0.0006, 0.0005, 0.0003, 0.0003, 0.0007, 0.0006, 0.0006, 0.0007, 0.0006,\n                      0.0006, 0.0006, 0.0004, 0.0003, 0.0006, 0.0007, 0.0007, 0.0007, 0.0006,\n                      0.0005, 0.0004, 0.0004, 0.0003, 0.0007, 0.0003, 0.0006, 0.0006, 0.0008,\n                      0.0002, 0.0003, 0.0006, 0.0005, 0.0006, 0.0006, 0.0006, 0.0005, 0.0004,\n                      0.0007, 0.0006, 0.0004, 0.0007, 0.0006, 0.0005, 0.0007, 0.0005, 0.0005,\n                      0.0007, 0.0006, 0.0005, 0.0005, 0.0005, 0.0006, 0.0007, 0.0006, 0.0005,\n                      0.0006, 0.0007, 0.0006, 0.0007, 0.0008, 0.0007, 0.0005, 0.0004, 0.0006,\n                      0.0005, 0.0004, 0.0007, 0.0006, 0.0004, 0.0007], dtype=torch.float64),\n                     zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n                     axis=0)),\n             ('mnet.features.11.conv.0.0.bias',\n              Parameter containing:\n              tensor([ 1.7306e-01,  1.3249e-01, -7.7511e-03,  1.4535e-01,  3.1507e-02,\n                       8.2396e-02,  1.3795e-01,  1.8907e-01,  7.4903e-02,  2.5131e-01,\n                       1.9539e-01,  5.2288e-02,  3.9698e-02, -1.2004e-01, -2.7982e-03,\n                      -5.4593e-02, -1.6173e-02,  1.2726e-01,  1.2005e-01, -4.3134e-02,\n                      -2.3436e-02, -2.8625e-02, -2.6326e-02, -8.3807e-04,  6.1571e-02,\n                       2.0138e-01,  3.1067e-02, -4.0296e-02,  5.6617e-02, -4.4484e-02,\n                       9.1762e-02,  2.0969e-01, -1.2350e-02,  1.8311e-01, -2.4891e-02,\n                       2.2558e-01,  3.8294e-02,  2.1753e-01,  1.8446e-03, -4.8333e-02,\n                       1.7807e-02,  7.8003e-02,  1.9483e-01,  1.1450e-02,  1.4631e-01,\n                       1.3005e-01, -4.2092e-03,  1.9012e-01,  1.0236e-01, -1.1220e-01,\n                       1.3830e-01,  6.8041e-02, -1.5156e-02,  6.4984e-02, -2.4171e-02,\n                       1.7494e-01,  5.6008e-02,  4.8285e-02,  1.5347e-01,  1.8466e-01,\n                       4.1400e-02,  6.7241e-02,  1.4973e-01,  1.2649e-01,  1.0405e-02,\n                       1.7540e-01,  2.0256e-01,  1.3852e-01, -1.9205e-02,  1.9137e-01,\n                      -2.8151e-02, -3.8539e-02,  2.1529e-01,  1.4794e-01,  9.9187e-02,\n                       3.9609e-02, -6.8919e-02, -1.3675e-02,  4.2359e-02, -1.6868e-01,\n                       1.5444e-02,  9.0622e-03, -5.0978e-02, -9.7931e-02, -8.2890e-03,\n                      -7.9384e-02,  9.4872e-03,  6.2167e-02,  3.6646e-02,  3.8789e-02,\n                       4.0697e-04,  2.2421e-02, -1.1198e-03,  2.7558e-02, -7.8918e-03,\n                       3.0779e-01, -1.0799e-02, -2.0523e-02,  4.2592e-02, -2.2235e-02,\n                       1.7910e-02,  8.9199e-02,  7.7223e-02,  1.3077e-01, -2.9051e-02,\n                       2.0370e-01,  1.2294e-01,  7.4118e-02,  6.9163e-02, -1.8459e-02,\n                       1.1670e-01,  3.4302e-02,  2.2024e-01,  3.6545e-02,  2.2523e-01,\n                       1.6953e-01,  5.6514e-02,  4.7386e-02,  2.3931e-01,  1.2980e-02,\n                      -3.2438e-02,  2.0212e-01,  2.0329e-01,  2.4668e-02,  2.1341e-01,\n                       9.6370e-02,  4.2250e-02, -4.5143e-02, -1.6273e-02,  9.9131e-02,\n                       9.8157e-02,  1.5891e-02, -6.1698e-02,  1.0995e-01,  9.8056e-02,\n                       1.9554e-01, -5.1092e-03,  1.4807e-01,  2.3510e-02, -3.1086e-02,\n                       6.7097e-04,  3.0230e-02,  1.7426e-02,  9.7628e-03,  6.4489e-02,\n                       4.2347e-02,  1.0738e-01, -4.7193e-02,  6.1543e-02,  1.5402e-02,\n                       1.1902e-01, -4.2956e-02,  1.2962e-01,  2.6383e-02,  5.2808e-02,\n                       1.7656e-02,  1.5382e-01,  2.4651e-01,  4.5683e-02,  1.6068e-01,\n                       1.3142e-01,  1.8047e-01,  3.5372e-02,  2.2691e-01,  1.7911e-01,\n                       1.2394e-01,  3.4171e-02,  2.2722e-01,  5.1960e-02,  1.3413e-01,\n                       8.9310e-02,  1.0203e-01,  6.8323e-02,  1.7275e-01, -5.2054e-03,\n                       5.3410e-03,  1.5563e-01,  1.4396e-01, -3.8546e-02, -8.4485e-02,\n                      -9.1431e-03,  1.4672e-01,  7.0940e-02,  2.7762e-01,  1.8537e-02,\n                       3.6927e-02, -4.5904e-02,  2.1768e-02, -2.7838e-03, -3.1625e-02,\n                       7.1595e-02,  1.2376e-01,  2.6972e-02,  2.1439e-02,  8.2836e-02,\n                       3.3192e-02, -8.8286e-03,  1.2770e-01, -6.9287e-03,  1.9633e-01,\n                       1.0494e-01,  4.2374e-02, -2.0260e-02,  2.5003e-02, -8.8331e-02,\n                      -3.6387e-02,  1.0245e-01,  1.5739e-01,  1.2488e-01, -7.5689e-02,\n                      -3.3841e-02,  7.0603e-02, -4.7225e-03, -1.4433e-02,  7.9623e-02,\n                       1.5401e-01,  2.3722e-02, -2.2812e-02,  1.6444e-01,  2.5010e-02,\n                       1.7714e-01,  4.4599e-02,  1.0230e-01, -7.5904e-05,  1.5224e-01,\n                       3.8523e-02, -7.7402e-02, -8.1506e-03, -6.9676e-02, -3.0956e-02,\n                      -4.8132e-02,  2.7831e-02,  1.6940e-01, -3.7623e-02,  1.8583e-01,\n                       1.8947e-01, -1.1636e-02,  1.7550e-01,  2.4594e-01, -5.4371e-02,\n                       2.7706e-02,  6.2654e-02, -3.6127e-02, -2.1125e-02, -5.1089e-02,\n                       1.2211e-01,  6.4994e-02,  4.7080e-02,  7.9164e-02,  3.9923e-02,\n                       1.2707e-01,  6.1818e-02,  1.3419e-01,  1.0500e-01,  1.1835e-01,\n                       9.0553e-02, -3.1363e-02, -8.1010e-03,  1.3732e-02,  1.0896e-01,\n                       8.4712e-03,  7.6556e-02,  1.8369e-01,  1.8783e-02, -1.9988e-02,\n                      -3.8730e-02,  2.0775e-02,  8.4135e-02,  2.0833e-01, -6.7206e-02,\n                       9.2489e-02,  5.3931e-02, -5.7305e-02,  1.3855e-01,  1.0356e-01,\n                       7.8735e-02,  5.1112e-02,  2.2677e-01,  2.9314e-02, -1.3793e-02,\n                       1.5121e-02,  2.3880e-02,  1.0350e-01,  2.2453e-01,  1.8619e-02,\n                       5.8984e-02, -4.5450e-02,  6.6662e-02, -5.3684e-03,  8.2803e-02,\n                       1.0899e-01,  3.8582e-02, -2.9869e-03, -4.5834e-03,  3.6671e-02,\n                       6.7906e-02,  8.8569e-02,  1.1402e-01, -1.1425e-02,  1.6310e-02,\n                       8.6806e-02,  6.5888e-02,  2.6037e-02,  4.9623e-02,  6.3661e-02,\n                       3.0603e-02, -1.3081e-02,  3.9899e-02,  1.7893e-02,  3.4330e-02,\n                      -5.7743e-02,  2.2919e-01,  7.8566e-02,  2.6009e-01,  8.8593e-02,\n                       1.1376e-01, -9.7998e-02,  1.3977e-01,  2.1965e-01, -4.5718e-02,\n                       1.7891e-03,  1.0750e-02,  4.4400e-02,  1.1452e-02,  7.0647e-02,\n                      -2.6096e-02,  1.5453e-01,  9.9461e-02,  1.7439e-01,  9.2486e-02,\n                      -4.3206e-02,  4.3559e-02,  2.9693e-02,  5.1772e-02,  5.7613e-02,\n                       2.1090e-01,  1.6778e-01,  9.8198e-02,  1.4673e-01,  7.3218e-02,\n                       1.2798e-01,  7.2298e-02,  2.5775e-01,  1.7156e-01, -8.2990e-02,\n                      -4.8266e-02,  1.4921e-01,  9.8577e-02,  1.1613e-01,  1.4159e-01,\n                       2.1152e-01,  5.1128e-02,  1.4499e-01,  1.5167e-01,  1.3739e-01,\n                       2.6157e-02,  3.1996e-02,  4.5598e-02,  6.1054e-02,  5.1562e-02,\n                       1.2024e-01,  9.8937e-04,  1.4287e-01, -1.8085e-02,  1.5191e-01,\n                      -2.7078e-02,  8.0132e-02, -9.3476e-02,  1.9231e-01,  5.7774e-02,\n                       2.5798e-02, -5.5455e-02,  5.9482e-02,  1.5055e-02,  1.1746e-01,\n                       6.2504e-02,  1.1584e-01,  1.3277e-01,  3.9083e-02,  8.9329e-02,\n                      -2.1839e-02, -2.6844e-02,  2.9011e-01, -6.9142e-02],\n                     requires_grad=True)),\n             ('mnet.features.11.conv.0.0.scale', tensor(0.0186)),\n             ('mnet.features.11.conv.0.0.zero_point', tensor(59)),\n             ('mnet.features.11.conv.1.0.weight',\n              tensor([[[[ 0.0359,  0.0179, -0.2332],\n                        [ 0.6908,  0.5652, -1.1393],\n                        [-0.0449, -0.0449, -0.5113]]],\n              \n              \n                      [[[ 0.2968,  1.9838,  0.7185],\n                        [ 0.1093, -1.5776, -0.0937],\n                        [-0.0937, -0.3436, -0.2499]]],\n              \n              \n                      [[[-0.1214,  0.0221, -0.1214],\n                        [ 0.2207,  1.4017,  0.5850],\n                        [-0.1324,  0.4967,  0.0441]]],\n              \n              \n                      ...,\n              \n              \n                      [[[ 0.1395,  0.0837, -0.2093],\n                        [ 0.1116,  1.7720,  0.0000],\n                        [ 0.0698,  0.2093,  0.3209]]],\n              \n              \n                      [[[-0.3991, -0.1746, -0.4311],\n                        [-0.0036, -0.3848, -0.1853],\n                        [-0.4383, -0.1639, -0.4561]]],\n              \n              \n                      [[[-0.0143, -0.2152,  0.9613],\n                        [-1.2196,  0.2583,  1.8222],\n                        [-0.1004, -0.6887, -0.0574]]]], size=(384, 1, 3, 3),\n                     dtype=torch.qint8, quantization_scheme=torch.per_channel_affine,\n                     scale=tensor([0.0090, 0.0156, 0.0110, 0.0068, 0.0160, 0.0048, 0.0131, 0.0083, 0.0098,\n                      0.0100, 0.0353, 0.0046, 0.0043, 0.0165, 0.0094, 0.0189, 0.0165, 0.0147,\n                      0.0096, 0.0108, 0.0166, 0.0144, 0.0155, 0.0053, 0.0163, 0.0228, 0.0188,\n                      0.0180, 0.0067, 0.0172, 0.0145, 0.0187, 0.0166, 0.0226, 0.0157, 0.0136,\n                      0.0135, 0.0083, 0.0137, 0.0146, 0.0164, 0.0080, 0.0293, 0.0063, 0.0152,\n                      0.0089, 0.0150, 0.0094, 0.0058, 0.0127, 0.0280, 0.0117, 0.0151, 0.0173,\n                      0.0241, 0.0148, 0.0209, 0.0188, 0.0177, 0.0221, 0.0187, 0.0047, 0.0129,\n                      0.0080, 0.0173, 0.0094, 0.0116, 0.0192, 0.0154, 0.0171, 0.0115, 0.0143,\n                      0.0343, 0.0058, 0.0076, 0.0165, 0.0115, 0.0152, 0.0067, 0.0330, 0.0137,\n                      0.0164, 0.0153, 0.0070, 0.0148, 0.0187, 0.0046, 0.0058, 0.0119, 0.0185,\n                      0.0194, 0.0153, 0.0167, 0.0178, 0.0145, 0.0028, 0.0131, 0.0127, 0.0137,\n                      0.0157, 0.0218, 0.0215, 0.0052, 0.0077, 0.0149, 0.0092, 0.0048, 0.0162,\n                      0.0133, 0.0100, 0.0098, 0.0132, 0.0405, 0.0102, 0.0233, 0.0162, 0.0146,\n                      0.0043, 0.0119, 0.0087, 0.0165, 0.0072, 0.0127, 0.0181, 0.0151, 0.0136,\n                      0.0094, 0.0123, 0.0115, 0.0102, 0.0102, 0.0119, 0.0225, 0.0082, 0.0054,\n                      0.0110, 0.0099, 0.0091, 0.0146, 0.0171, 0.0063, 0.0047, 0.0140, 0.0203,\n                      0.0040, 0.0100, 0.0187, 0.0097, 0.0124, 0.0156, 0.0055, 0.0084, 0.0053,\n                      0.0168, 0.0123, 0.0185, 0.0176, 0.0130, 0.0040, 0.0196, 0.0139, 0.0166,\n                      0.0113, 0.0134, 0.0113, 0.0403, 0.0143, 0.0121, 0.0196, 0.0054, 0.0181,\n                      0.0237, 0.0211, 0.0067, 0.0044, 0.0139, 0.0132, 0.0148, 0.0149, 0.0114,\n                      0.0154, 0.0087, 0.0061, 0.0157, 0.0170, 0.0178, 0.0151, 0.0150, 0.0102,\n                      0.0184, 0.0154, 0.0183, 0.0062, 0.0090, 0.0069, 0.0147, 0.0081, 0.0052,\n                      0.0157, 0.0155, 0.0060, 0.0117, 0.0033, 0.0172, 0.0147, 0.0094, 0.0093,\n                      0.0184, 0.0301, 0.0143, 0.0168, 0.0147, 0.0100, 0.0164, 0.0090, 0.0044,\n                      0.0125, 0.0118, 0.0098, 0.0150, 0.0165, 0.0154, 0.0185, 0.0134, 0.0339,\n                      0.0064, 0.0130, 0.0052, 0.0191, 0.0175, 0.0158, 0.0183, 0.0160, 0.0104,\n                      0.0063, 0.0238, 0.0107, 0.0200, 0.0160, 0.0163, 0.0155, 0.0187, 0.0160,\n                      0.0081, 0.0071, 0.0179, 0.0092, 0.0186, 0.0047, 0.0046, 0.0269, 0.0202,\n                      0.0114, 0.0078, 0.0074, 0.0148, 0.0140, 0.0045, 0.0194, 0.0377, 0.0155,\n                      0.0161, 0.0232, 0.0120, 0.0084, 0.0092, 0.0050, 0.0096, 0.0199, 0.0064,\n                      0.0060, 0.0044, 0.0186, 0.0280, 0.0038, 0.0081, 0.0159, 0.0139, 0.0066,\n                      0.0142, 0.0149, 0.0148, 0.0161, 0.0196, 0.0192, 0.0167, 0.0203, 0.0057,\n                      0.0159, 0.0198, 0.0064, 0.0090, 0.0144, 0.0105, 0.0070, 0.0123, 0.0210,\n                      0.0151, 0.0113, 0.0167, 0.0167, 0.0141, 0.0163, 0.0168, 0.0047, 0.0061,\n                      0.0104, 0.0193, 0.0096, 0.0129, 0.0134, 0.0158, 0.0239, 0.0312, 0.0177,\n                      0.0138, 0.0135, 0.0302, 0.0206, 0.0144, 0.0194, 0.0180, 0.0046, 0.0213,\n                      0.0055, 0.0202, 0.0193, 0.0219, 0.0102, 0.0124, 0.0135, 0.0107, 0.0129,\n                      0.0039, 0.0044, 0.0150, 0.0214, 0.0114, 0.0315, 0.0138, 0.0083, 0.0077,\n                      0.0498, 0.0284, 0.0193, 0.0205, 0.0071, 0.0181, 0.0058, 0.0160, 0.0163,\n                      0.0140, 0.0092, 0.0269, 0.0141, 0.0133, 0.0126, 0.0051, 0.0166, 0.0084,\n                      0.0093, 0.0145, 0.0147, 0.0149, 0.0175, 0.0173, 0.0127, 0.0263, 0.0202,\n                      0.0188, 0.0117, 0.0151, 0.0180, 0.0162, 0.0130, 0.0080, 0.0350, 0.0084,\n                      0.0062, 0.0129, 0.0179, 0.0140, 0.0036, 0.0143], dtype=torch.float64),\n                     zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n                     axis=0)),\n             ('mnet.features.11.conv.1.0.bias',\n              Parameter containing:\n              tensor([ 5.1624e-01,  4.5061e-02, -8.0155e-02,  4.7909e-01, -3.4173e-01,\n                       3.6954e-01,  2.9911e-02,  5.8907e-01,  5.8811e-01,  6.0680e-01,\n                       9.1300e-01,  3.4969e-01,  2.9766e-01,  9.4373e-03,  1.1104e-01,\n                      -1.6549e-02, -5.9668e-02,  3.3978e-02,  4.7643e-01,  1.2646e-01,\n                      -1.8817e-01, -3.5662e-02, -1.6562e-01,  3.0236e-01, -1.6500e-01,\n                      -1.1019e+00, -2.4823e-02, -1.0343e-01,  3.5620e-01, -3.1355e-03,\n                       1.5046e-01,  6.8598e-02, -5.2268e-02,  3.5992e-01, -9.5366e-02,\n                       9.1678e-01, -4.3779e-02,  5.9484e-01, -2.0846e-01, -1.7376e-01,\n                      -8.7487e-02,  3.0455e-01, -3.2902e-02,  5.2995e-01,  1.9743e-01,\n                       4.5564e-01, -1.1169e-01,  6.2695e-01,  4.5006e-01, -6.9592e-03,\n                      -4.6784e-02, -3.8299e-02, -3.3963e-02, -3.2950e-01, -2.9487e-01,\n                       6.1451e-01, -4.7230e-02, -3.5156e-01, -1.3140e-01,  6.7060e-01,\n                      -3.5205e-01,  3.8869e-01,  7.5245e-01,  1.3290e-01, -3.2150e-01,\n                       4.2651e-02, -2.8565e-03,  1.4098e-01, -1.5454e-01,  8.6227e-01,\n                      -9.2086e-03, -2.6036e-02,  1.2518e+00,  5.7942e-01,  3.8046e-01,\n                      -5.9735e-02, -1.7011e-01, -1.0069e-01,  6.0041e-01, -1.8847e-01,\n                      -1.8025e-01, -3.3400e-01, -1.3225e-01,  1.6379e-02, -3.9045e-02,\n                      -1.3660e-02,  3.2080e-01,  3.5427e-01, -2.5082e-01, -2.1341e-01,\n                      -3.6339e-01, -7.7669e-02, -1.4925e-02, -1.2396e-01, -1.0113e-01,\n                       7.5633e-01,  3.9611e-01,  5.3735e-02, -5.1965e-02, -2.2463e-02,\n                      -2.2629e-01, -2.5303e-02,  4.0199e-01,  5.1083e-01, -9.5414e-03,\n                       7.4719e-01,  3.6037e-01, -2.5494e-01, -1.3942e-01,  2.5273e-01,\n                       4.5512e-01,  9.4897e-04,  1.4626e+00,  4.8646e-01, -1.3133e+00,\n                      -3.5472e-02, -7.8950e-02,  4.8416e-01,  6.7405e-01,  3.9177e-01,\n                      -1.5680e-01,  6.8042e-01,  7.5758e-01, -1.2182e-01,  4.3211e-01,\n                       1.9890e-01,  4.3838e-01,  3.0195e-01, -2.5137e-02,  4.7625e-01,\n                       3.5686e-02, -8.1689e-02, -1.9895e-01,  3.2780e-01,  4.5450e-01,\n                       2.7113e-01,  3.6007e-01,  5.6259e-01, -5.9240e-02, -9.3128e-02,\n                       3.5793e-01,  3.5376e-01, -8.4697e-03, -5.1872e-01,  3.4505e-01,\n                       3.9906e-01, -8.1293e-02, -5.6888e-03, -1.6119e-01, -7.3293e-02,\n                       4.1802e-01,  4.0842e-02,  4.5055e-01, -2.2209e-01, -2.2681e-01,\n                      -1.0146e-01,  1.1665e-01,  8.9906e-01,  3.4869e-01,  7.0255e-01,\n                       1.5314e-02,  6.1642e-01, -2.6310e-03,  8.3691e-01,  2.0121e-01,\n                      -6.0589e-01, -4.1159e-02,  2.9418e-01, -3.8828e-01,  5.0010e-01,\n                      -1.8301e-01, -1.9179e-01, -1.4854e-01,  6.1039e-01,  2.8555e-01,\n                      -7.6810e-02,  6.5325e-01, -8.9802e-02, -1.3557e-02, -1.1220e-01,\n                      -9.4982e-02,  2.8593e-01,  4.2436e-01,  1.0123e+00, -4.3236e-01,\n                      -2.7014e-02, -2.5535e-02, -1.5170e-01,  5.6803e-02, -6.8385e-02,\n                      -7.4345e-02, -2.0122e-01,  2.9348e-01, -2.2928e-01,  3.5244e-01,\n                      -9.1102e-03,  4.0245e-01,  4.5584e-01, -2.5824e-01,  8.2068e-01,\n                       3.8068e-01,  1.8726e-01,  4.4123e-01, -4.7748e-02, -2.3168e-02,\n                      -9.9878e-03,  3.7232e-01,  2.2644e-01, -3.5633e-01, -1.2879e-01,\n                      -4.5828e-02, -3.2407e-01, -2.6770e-02, -2.3749e-02,  3.7103e-01,\n                       4.6056e-01,  4.1733e-01, -8.5082e-03,  2.3925e-02, -1.1034e-01,\n                       3.2746e-01, -5.7875e-02, -3.3704e-01,  2.5177e-01, -4.6933e-01,\n                       3.1973e-01,  1.1652e-02,  3.7660e-01, -4.7057e-02, -1.0509e-01,\n                      -5.3013e-02, -1.0932e-01,  7.6637e-01, -1.2996e-02,  5.6296e-01,\n                       4.6832e-01, -9.4747e-02, -1.0869e+00,  9.5268e-01, -1.2404e-01,\n                      -9.9236e-02, -1.9720e-01, -5.5439e-02,  3.8075e-01,  4.5326e-01,\n                      -1.6883e-01,  4.4580e-01, -9.7030e-02,  4.8531e-01,  2.9453e-01,\n                      -4.2838e-01, -6.4948e-01,  1.3697e-01,  4.2965e-01,  4.0964e-01,\n                      -6.4926e-03, -3.6519e-03,  3.4215e-01, -1.9935e-01, -8.1223e-01,\n                      -3.2117e-02, -4.9759e-03,  4.8258e-02, -7.8911e-02,  4.1930e-01,\n                      -5.4211e-03,  3.9966e-01,  3.1743e-01,  9.8652e-01,  3.7184e-01,\n                       5.2966e-01,  4.2372e-01, -1.2365e-01, -2.2271e-01,  3.5336e-01,\n                       1.8339e-01, -3.0220e-01, -3.9142e-02,  3.0279e-01, -6.8265e-02,\n                      -1.5292e-02, -5.2582e-01,  1.6727e-03, -1.1382e+00, -1.5806e-01,\n                      -2.4602e-01, -4.0199e-03,  5.1174e-01, -3.9166e-02, -3.2025e-01,\n                       4.5821e-01,  5.6585e-01,  7.2198e-03,  3.8154e-01,  7.2120e-02,\n                      -5.5038e-02, -4.4458e-01, -3.4344e-02, -2.2071e-02, -1.4043e-01,\n                      -1.1230e-02, -5.6247e-03, -1.5808e-01, -2.7481e-01,  3.8545e-01,\n                       3.8871e-01, -1.1479e-02, -2.7087e-01, -4.2181e-01, -1.2603e-01,\n                      -2.6843e-02,  2.1426e-03, -5.9855e-01,  1.4129e+00, -2.0696e-01,\n                       2.7978e-03, -5.2121e-02, -2.6804e-01,  1.1790e+00, -1.4617e-01,\n                      -2.5495e-01, -1.4476e-01,  3.5766e-01, -3.0912e-01,  4.0837e-01,\n                      -1.7411e-01,  5.5393e-02, -6.4063e-01,  7.1787e-01, -1.8334e-01,\n                       2.0479e-01,  3.6672e-01, -2.9644e-01,  3.6859e-01,  2.9782e-01,\n                       8.8912e-01, -3.7493e-01,  4.7158e-02, -3.0722e-01,  1.7216e-01,\n                       5.8623e-01,  4.2940e-01,  6.8727e-01,  6.3077e-02, -4.8777e-02,\n                      -1.4472e-01,  4.6780e-01,  1.9820e-02,  4.1519e-01, -1.8760e-01,\n                       9.5377e-02, -3.8566e-01,  3.9661e-01, -3.2838e-01,  2.3210e-01,\n                      -3.3782e-01, -4.9571e-02,  2.8988e-01, -7.5272e-02,  4.3120e-01,\n                       4.8188e-01, -1.0832e-01,  2.1539e-02, -3.5621e-02,  4.7892e-01,\n                      -2.3688e-01,  7.0433e-02, -2.2063e-01,  1.4694e-01, -4.3294e-01,\n                      -1.1349e-02, -4.1250e-02, -1.0291e-01, -9.7976e-02,  9.7653e-02,\n                       3.5006e-01, -3.6799e-01,  5.1935e-01,  3.6998e-01, -5.9392e-02,\n                      -1.3347e-01, -7.3228e-02,  7.7383e-01,  1.8141e-01],\n                     requires_grad=True)),\n             ('mnet.features.11.conv.1.0.scale', tensor(0.0289)),\n             ('mnet.features.11.conv.1.0.zero_point', tensor(54)),\n             ('mnet.features.11.conv.2.weight',\n              tensor([[[[-0.0797]],\n              \n                       [[ 0.0025]],\n              \n                       [[ 0.0548]],\n              \n                       ...,\n              \n                       [[-0.0698]],\n              \n                       [[-0.0623]],\n              \n                       [[-0.0274]]],\n              \n              \n                      [[[ 0.0206]],\n              \n                       [[ 0.0206]],\n              \n                       [[-0.0124]],\n              \n                       ...,\n              \n                       [[ 0.0206]],\n              \n                       [[ 0.1401]],\n              \n                       [[ 0.0062]]],\n              \n              \n                      [[[ 0.0594]],\n              \n                       [[-0.0880]],\n              \n                       [[ 0.0880]],\n              \n                       ...,\n              \n                       [[ 0.2068]],\n              \n                       [[-0.0635]],\n              \n                       [[-0.0798]]],\n              \n              \n                      ...,\n              \n              \n                      [[[ 0.0485]],\n              \n                       [[ 0.0633]],\n              \n                       [[-0.2173]],\n              \n                       ...,\n              \n                       [[-0.0570]],\n              \n                       [[-0.0042]],\n              \n                       [[-0.1287]]],\n              \n              \n                      [[[-0.0147]],\n              \n                       [[-0.0660]],\n              \n                       [[-0.0196]],\n              \n                       ...,\n              \n                       [[-0.0416]],\n              \n                       [[-0.1638]],\n              \n                       [[-0.0758]]],\n              \n              \n                      [[[-0.0236]],\n              \n                       [[ 0.0839]],\n              \n                       [[-0.0419]],\n              \n                       ...,\n              \n                       [[ 0.0131]],\n              \n                       [[-0.0865]],\n              \n                       [[ 0.1232]]]], size=(96, 384, 1, 1), dtype=torch.qint8,\n                     quantization_scheme=torch.per_channel_affine,\n                     scale=tensor([0.0025, 0.0021, 0.0020, 0.0023, 0.0022, 0.0021, 0.0021, 0.0020, 0.0021,\n                      0.0020, 0.0022, 0.0023, 0.0022, 0.0027, 0.0025, 0.0029, 0.0023, 0.0016,\n                      0.0021, 0.0031, 0.0026, 0.0020, 0.0029, 0.0027, 0.0019, 0.0020, 0.0023,\n                      0.0017, 0.0023, 0.0027, 0.0023, 0.0017, 0.0027, 0.0030, 0.0023, 0.0020,\n                      0.0025, 0.0025, 0.0022, 0.0029, 0.0027, 0.0026, 0.0025, 0.0022, 0.0025,\n                      0.0020, 0.0024, 0.0020, 0.0024, 0.0026, 0.0018, 0.0021, 0.0021, 0.0020,\n                      0.0027, 0.0020, 0.0028, 0.0026, 0.0018, 0.0023, 0.0025, 0.0027, 0.0028,\n                      0.0031, 0.0029, 0.0023, 0.0024, 0.0032, 0.0018, 0.0020, 0.0028, 0.0020,\n                      0.0020, 0.0021, 0.0024, 0.0028, 0.0023, 0.0033, 0.0021, 0.0023, 0.0021,\n                      0.0026, 0.0021, 0.0023, 0.0030, 0.0024, 0.0050, 0.0031, 0.0039, 0.0031,\n                      0.0029, 0.0021, 0.0023, 0.0021, 0.0024, 0.0026], dtype=torch.float64),\n                     zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n                     axis=0)),\n             ('mnet.features.11.conv.2.bias',\n              Parameter containing:\n              tensor([ 0.1082,  0.8117,  0.2695,  0.0417, -0.2583, -0.0658, -0.1253,  0.1259,\n                      -0.6712, -0.0243,  0.1557,  0.0033,  0.1645, -0.0152, -0.3156, -0.4182,\n                      -0.3922, -0.0835, -0.4762, -0.3577,  0.8521,  0.0158,  0.1894, -0.2920,\n                       0.2535, -0.1928,  0.0803, -0.1422, -0.0542,  0.2505, -0.3106, -0.2813,\n                       0.2542, -0.2486,  0.1132,  0.1639, -0.2832,  0.1218,  0.3503, -0.3025,\n                      -0.5350,  0.0417, -0.1251, -0.7251,  0.4104,  0.2742, -0.4671, -0.1473,\n                       0.6217, -0.0118,  0.3215,  0.2127,  0.4627, -0.1970,  0.0732,  0.6188,\n                      -0.4945,  0.3591, -0.1202,  0.0274,  0.1895, -0.7644,  0.2448, -0.3842,\n                       0.3631,  0.1289, -0.5089,  0.1134, -0.2071, -0.1536, -0.1507,  0.4231,\n                      -0.6299, -0.0339, -0.0968,  0.1281, -0.0955,  0.3610, -0.3682, -0.0444,\n                      -0.0630, -0.6347, -0.3037, -0.4797, -0.0315, -0.1786, -0.5025, -0.1376,\n                       0.1006, -0.2857,  0.4505, -0.2916, -0.6532, -0.1855, -0.4171,  0.2811],\n                     requires_grad=True)),\n             ('mnet.features.11.conv.2.scale', tensor(0.0381)),\n             ('mnet.features.11.conv.2.zero_point', tensor(63)),\n             ('mnet.features.12.conv.0.0.weight',\n              tensor([[[[-0.0223]],\n              \n                       [[-0.0508]],\n              \n                       [[-0.0049]],\n              \n                       ...,\n              \n                       [[-0.0188]],\n              \n                       [[-0.0508]],\n              \n                       [[-0.0474]]],\n              \n              \n                      [[[-0.0456]],\n              \n                       [[ 0.0075]],\n              \n                       [[ 0.0282]],\n              \n                       ...,\n              \n                       [[ 0.0166]],\n              \n                       [[-0.0017]],\n              \n                       [[-0.0315]]],\n              \n              \n                      [[[-0.0330]],\n              \n                       [[-0.0165]],\n              \n                       [[-0.0157]],\n              \n                       ...,\n              \n                       [[ 0.0180]],\n              \n                       [[ 0.0157]],\n              \n                       [[-0.0090]]],\n              \n              \n                      ...,\n              \n              \n                      [[[ 0.0104]],\n              \n                       [[-0.0192]],\n              \n                       [[-0.0425]],\n              \n                       ...,\n              \n                       [[-0.0040]],\n              \n                       [[ 0.0425]],\n              \n                       [[ 0.0088]]],\n              \n              \n                      [[[ 0.0283]],\n              \n                       [[ 0.0305]],\n              \n                       [[ 0.0123]],\n              \n                       ...,\n              \n                       [[-0.0523]],\n              \n                       [[-0.0407]],\n              \n                       [[ 0.0276]]],\n              \n              \n                      [[[-0.0267]],\n              \n                       [[-0.0243]],\n              \n                       [[-0.0079]],\n              \n                       ...,\n              \n                       [[ 0.0273]],\n              \n                       [[ 0.0243]],\n              \n                       [[-0.0243]]]], size=(576, 96, 1, 1), dtype=torch.qint8,\n                     quantization_scheme=torch.per_channel_affine,\n                     scale=tensor([6.9639e-04, 8.2907e-04, 7.4969e-04, 6.3398e-04, 4.9708e-04, 5.9226e-04,\n                      6.1853e-04, 6.7205e-04, 7.2770e-04, 7.0374e-04, 6.2629e-04, 3.6484e-04,\n                      4.6452e-04, 6.6260e-04, 6.6423e-04, 5.9605e-04, 5.3837e-04, 5.7830e-04,\n                      8.3855e-04, 9.7944e-04, 5.8505e-04, 7.1357e-04, 6.3141e-04, 6.4678e-04,\n                      5.2501e-04, 6.8800e-04, 6.6675e-04, 5.5690e-04, 8.3886e-04, 5.4198e-04,\n                      5.0085e-04, 6.2826e-04, 5.0016e-04, 3.4157e-04, 4.3575e-04, 5.6763e-04,\n                      4.6684e-04, 5.8089e-04, 7.6179e-04, 6.6176e-04, 7.0939e-04, 5.8660e-04,\n                      9.2088e-04, 3.9102e-04, 8.1601e-04, 7.9981e-04, 6.7581e-04, 8.3363e-04,\n                      9.8550e-04, 4.3727e-04, 6.0371e-04, 5.9220e-04, 6.4033e-04, 4.5416e-04,\n                      5.2245e-04, 1.0226e-03, 5.9511e-04, 5.8877e-04, 5.1227e-04, 6.6204e-04,\n                      5.2157e-04, 1.0424e-03, 7.1520e-04, 4.5055e-04, 5.5040e-04, 5.6763e-04,\n                      6.6163e-04, 4.9780e-04, 6.5452e-04, 5.8284e-04, 5.6457e-04, 7.8624e-04,\n                      6.2645e-04, 6.3207e-04, 6.7955e-04, 8.1555e-04, 5.8944e-04, 5.7292e-04,\n                      4.1229e-04, 7.6356e-04, 3.7394e-04, 7.2590e-04, 6.8355e-04, 7.1251e-04,\n                      8.3049e-04, 9.0548e-04, 7.4021e-04, 7.2872e-04, 4.2874e-04, 1.2939e-05,\n                      1.2092e-05, 7.6530e-04, 9.2695e-04, 6.2013e-04, 8.8441e-04, 8.3435e-04,\n                      5.2739e-04, 5.5549e-04, 7.1625e-04, 4.5362e-04, 5.7217e-04, 6.2772e-04,\n                      5.3062e-04, 8.3509e-04, 4.8052e-04, 6.0225e-04, 6.5187e-04, 6.7781e-04,\n                      4.3766e-04, 6.0136e-04, 4.6759e-04, 8.8249e-04, 8.2736e-04, 5.4931e-04,\n                      5.3416e-04, 5.7344e-04, 7.4794e-04, 1.7881e-05, 7.0635e-04, 6.2323e-04,\n                      6.2920e-04, 5.7941e-04, 8.3202e-04, 4.8731e-04, 4.8567e-04, 5.0386e-04,\n                      6.6193e-04, 4.8957e-04, 6.8234e-04, 7.0500e-04, 5.5935e-04, 1.2996e-03,\n                      5.3073e-04, 7.4697e-04, 5.6105e-04, 9.5768e-04, 7.5583e-04, 7.5718e-04,\n                      3.1145e-04, 4.8900e-04, 5.5774e-04, 7.8215e-04, 7.9561e-04, 5.5283e-04,\n                      8.6358e-04, 7.2220e-04, 6.2983e-04, 9.0887e-04, 6.6868e-04, 6.2183e-04,\n                      5.8269e-04, 4.6990e-04, 5.1099e-04, 6.4262e-04, 5.3307e-04, 5.5761e-04,\n                      5.7116e-04, 5.4434e-04, 6.5960e-04, 5.0425e-04, 8.2704e-04, 3.9661e-04,\n                      6.3262e-04, 1.4109e-05, 5.1824e-04, 6.2690e-04, 7.0204e-04, 9.4860e-04,\n                      5.5503e-04, 8.6655e-04, 7.7572e-04, 5.3453e-04, 5.4066e-04, 3.2268e-04,\n                      5.9583e-04, 7.7286e-04, 6.8503e-04, 7.9302e-04, 9.9569e-04, 6.0513e-04,\n                      4.3593e-04, 4.7004e-04, 7.3862e-04, 1.1037e-03, 7.5450e-04, 6.3403e-04,\n                      5.9289e-04, 4.6080e-04, 5.5611e-04, 9.5145e-04, 6.2820e-04, 7.4028e-04,\n                      7.5269e-04, 8.9689e-04, 5.9534e-04, 7.0136e-04, 7.7177e-04, 6.5209e-04,\n                      7.3213e-04, 6.4588e-04, 4.9039e-04, 5.3372e-04, 5.8039e-04, 7.3622e-04,\n                      6.3058e-04, 7.4234e-04, 6.4234e-04, 7.7205e-04, 4.4203e-04, 9.0886e-04,\n                      6.3042e-04, 5.4175e-04, 8.3142e-04, 5.4487e-04, 6.3992e-04, 7.5255e-04,\n                      6.4682e-04, 8.2148e-04, 6.2219e-04, 5.6698e-04, 8.6236e-04, 9.8185e-04,\n                      7.0357e-04, 5.7201e-04, 6.5408e-04, 4.7783e-04, 7.2096e-04, 9.5451e-04,\n                      7.5745e-04, 4.9046e-04, 5.7664e-04, 7.5558e-04, 1.3936e-05, 3.3788e-04,\n                      8.9362e-04, 1.0506e-05, 5.3924e-04, 5.5923e-04, 4.6070e-04, 7.3926e-04,\n                      5.3299e-04, 5.9185e-04, 8.4733e-04, 8.7068e-04, 6.3896e-04, 4.6388e-04,\n                      7.7683e-04, 7.6127e-04, 8.4517e-04, 7.1298e-04, 1.4364e-03, 7.6379e-04,\n                      5.5542e-04, 7.2002e-04, 5.4695e-04, 5.6498e-04, 8.2365e-04, 5.2728e-04,\n                      4.6930e-04, 6.6105e-04, 6.6608e-04, 7.4484e-04, 9.3929e-04, 3.8373e-04,\n                      5.3365e-04, 5.6055e-04, 5.6296e-04, 5.6703e-04, 4.7027e-04, 6.4397e-04,\n                      7.3136e-04, 5.9635e-04, 9.3372e-04, 6.0210e-04, 8.8909e-04, 7.2503e-04,\n                      6.8679e-04, 6.3001e-04, 6.4433e-04, 7.1640e-04, 6.4229e-04, 4.8424e-04,\n                      4.2269e-04, 1.1771e-03, 5.5924e-04, 5.8171e-04, 6.7377e-04, 3.8116e-04,\n                      7.8765e-04, 5.9961e-04, 5.3615e-04, 6.4581e-04, 8.4750e-04, 8.2412e-04,\n                      1.0511e-03, 5.1157e-04, 4.6552e-04, 4.9966e-04, 3.5562e-04, 8.1910e-04,\n                      6.6935e-04, 3.4895e-04, 6.3903e-04, 6.9410e-04, 4.8452e-04, 4.5556e-04,\n                      6.1740e-04, 8.5021e-04, 5.7213e-04, 5.6990e-04, 7.3502e-04, 7.5855e-04,\n                      3.5570e-04, 6.2508e-04, 8.9627e-04, 7.0863e-04, 8.6808e-04, 6.1022e-04,\n                      5.5115e-04, 6.0324e-04, 4.8397e-04, 5.8147e-04, 6.8683e-04, 6.7005e-04,\n                      4.1925e-04, 5.8393e-04, 7.8948e-04, 5.8858e-04, 5.2415e-04, 7.4239e-04,\n                      5.2050e-04, 9.0488e-04, 5.6270e-04, 6.5726e-04, 5.6405e-04, 7.0019e-04,\n                      8.1707e-04, 3.8492e-04, 9.7137e-04, 8.2350e-04, 6.1223e-04, 8.7398e-04,\n                      7.7930e-04, 6.0047e-04, 5.3081e-04, 3.4832e-04, 9.6042e-04, 7.7143e-04,\n                      7.0982e-04, 9.9296e-04, 4.6084e-04, 5.4450e-04, 5.4849e-04, 8.9615e-04,\n                      6.3627e-04, 6.2419e-04, 6.3172e-04, 5.5214e-04, 5.4772e-04, 6.4769e-04,\n                      7.4887e-04, 7.2514e-04, 8.2063e-04, 3.5798e-04, 6.8884e-04, 7.6336e-04,\n                      6.1476e-04, 7.9903e-04, 1.1952e-03, 1.9728e-05, 5.8391e-04, 4.1746e-04,\n                      9.9035e-04, 4.9699e-04, 7.6822e-04, 7.9499e-04, 8.1724e-04, 3.3892e-04,\n                      7.7661e-04, 4.5252e-04, 6.0373e-04, 5.0374e-04, 7.8071e-04, 1.0100e-03,\n                      5.1638e-04, 7.1695e-04, 2.9354e-04, 5.9002e-04, 7.5533e-04, 7.4402e-04,\n                      8.9650e-04, 6.3618e-04, 7.9566e-04, 1.0154e-03, 5.4132e-04, 8.0421e-04,\n                      6.8035e-04, 6.4221e-04, 7.2140e-04, 6.1938e-04, 6.4058e-04, 6.7933e-04,\n                      7.6408e-04, 4.1797e-04, 5.7786e-04, 4.9727e-04, 5.7142e-04, 9.1405e-04,\n                      6.0871e-04, 6.5775e-04, 6.0540e-04, 5.5438e-04, 8.8581e-04, 3.6534e-04,\n                      5.0374e-04, 9.9457e-04, 3.7742e-04, 6.0250e-04, 5.0380e-04, 5.5038e-04,\n                      4.2883e-04, 5.0434e-04, 9.0535e-04, 6.8492e-04, 7.0974e-04, 5.6046e-04,\n                      8.0124e-04, 5.0651e-04, 5.0141e-04, 6.0437e-04, 4.3100e-04, 8.5045e-04,\n                      5.4417e-04, 7.4845e-04, 5.9754e-04, 6.6465e-04, 6.1685e-04, 5.9628e-04,\n                      9.5352e-04, 7.8315e-04, 1.1347e-03, 8.1748e-04, 6.0202e-04, 8.0467e-04,\n                      6.6291e-04, 6.1276e-04, 7.7240e-04, 7.8607e-04, 6.1094e-04, 1.1950e-03,\n                      4.2303e-04, 6.3111e-04, 6.4467e-04, 4.3799e-04, 6.2750e-04, 7.2610e-04,\n                      7.9435e-04, 8.0054e-04, 5.9425e-04, 4.7999e-04, 5.8613e-04, 2.8728e-04,\n                      3.7200e-04, 9.6231e-04, 2.3709e-04, 7.1957e-04, 4.1754e-04, 5.6374e-04,\n                      9.1092e-04, 3.9441e-04, 5.7393e-04, 7.5654e-04, 5.3465e-04, 1.0130e-03,\n                      7.8182e-04, 5.9608e-04, 6.2090e-04, 1.1526e-03, 6.2901e-04, 6.1532e-04,\n                      8.4854e-04, 6.8189e-04, 6.2418e-04, 6.4372e-04, 9.5780e-04, 5.9316e-04,\n                      5.8323e-04, 8.6420e-04, 1.1568e-03, 6.3261e-04, 3.9816e-04, 6.9584e-04,\n                      8.4466e-04, 7.0441e-04, 7.1901e-04, 5.5299e-04, 6.7827e-04, 6.1544e-04,\n                      6.7286e-04, 5.4894e-04, 7.5989e-04, 6.3753e-04, 9.8472e-04, 4.4424e-04,\n                      2.7629e-04, 6.0845e-04, 4.1758e-04, 7.1216e-04, 5.7491e-04, 6.5199e-04,\n                      6.6362e-04, 7.2540e-04, 5.7576e-04, 5.8181e-04, 5.9942e-04, 4.6854e-04,\n                      5.3642e-04, 6.5599e-04, 5.3762e-04, 7.2572e-04, 8.2308e-04, 5.9730e-04,\n                      7.4686e-04, 4.8870e-04, 8.3258e-04, 1.0407e-03, 3.9059e-04, 8.9439e-04,\n                      5.6492e-04, 4.4907e-04, 8.2560e-04, 6.5128e-04, 5.0759e-04, 7.9736e-04,\n                      1.0450e-03, 4.2194e-04, 6.9883e-04, 5.2104e-04, 4.6763e-04, 8.3674e-04,\n                      8.4251e-04, 3.4376e-04, 7.8398e-04, 6.8851e-04, 7.9300e-04, 6.8477e-04,\n                      1.0523e-03, 4.2340e-04, 6.0707e-04, 7.6576e-04, 3.9142e-04, 5.1014e-04,\n                      7.3017e-04, 5.6458e-04, 3.7858e-04, 4.6146e-04, 8.2185e-04, 1.0377e-03,\n                      5.8482e-04, 5.8943e-04, 7.0019e-04, 7.8698e-04, 6.2102e-04, 8.0625e-04,\n                      6.6006e-04, 6.1719e-04, 8.9319e-04, 7.3780e-04, 6.4762e-04, 4.8554e-04,\n                      8.7852e-04, 7.2323e-04, 8.5613e-04, 8.0118e-04, 7.2598e-04, 6.0740e-04],\n                     dtype=torch.float64),\n                     zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n                     axis=0)),\n             ('mnet.features.12.conv.0.0.bias',\n              Parameter containing:\n              tensor([ 1.6012e-01, -3.3752e-02,  7.3927e-02,  5.0969e-02,  2.9477e-02,\n                      -7.5692e-02, -7.6626e-02, -4.0753e-02,  1.4009e-01, -1.5122e-02,\n                      -1.5760e-02,  9.7933e-02,  1.0181e-01,  3.7009e-02,  3.5030e-02,\n                      -4.0955e-02,  9.2522e-02, -3.5068e-02,  7.7897e-03, -1.4169e-01,\n                       1.2789e-01, -3.6193e-03,  6.4323e-02, -2.8132e-02, -6.1017e-02,\n                      -1.6588e-01,  3.6505e-03, -1.1619e-01, -2.0204e-02, -3.7609e-02,\n                       6.3429e-02, -2.5834e-02, -5.8903e-02,  1.9019e-01,  1.4742e-01,\n                      -6.0512e-03,  1.8464e-01, -8.4715e-02,  4.7457e-02,  8.2279e-03,\n                       1.3187e-02, -9.6777e-02, -3.0021e-02, -1.4381e-02,  3.8873e-03,\n                      -5.2956e-02, -2.2193e-01, -1.2799e-02, -1.9613e-01,  7.7283e-02,\n                      -9.9685e-03,  4.2693e-02, -1.4581e-01,  6.3051e-02,  4.3762e-02,\n                      -3.5985e-02,  4.1143e-02,  8.3583e-02,  1.3670e-01, -4.4257e-02,\n                       1.1606e-01, -1.1002e-01, -4.2201e-02,  3.3003e-02, -8.0610e-02,\n                      -1.0153e-02,  1.3474e-02,  1.6219e-01,  2.3167e-02, -4.7509e-02,\n                      -1.0219e-01,  6.6037e-02,  2.1209e-01,  8.3668e-02, -1.0297e-02,\n                      -4.9955e-02, -2.9841e-02, -8.6551e-02,  1.0559e-01, -7.1081e-02,\n                       1.1726e-01,  5.3105e-02, -5.2156e-04,  5.5203e-03,  7.1067e-02,\n                      -5.6400e-02,  2.3763e-03, -6.8321e-02,  6.3960e-02, -1.9704e-02,\n                      -3.0285e-02, -1.0102e-01, -6.6461e-02,  1.6091e-01, -7.6690e-03,\n                       6.4844e-02,  6.9257e-02,  1.3586e-01,  6.2045e-03, -1.1934e-01,\n                       1.6498e-02,  1.1153e-02, -5.9473e-02, -1.1383e-01,  9.8728e-02,\n                      -7.0467e-02, -2.5986e-02, -5.8073e-02,  8.7120e-02, -6.0692e-04,\n                       1.6011e-01, -1.2624e-02, -6.7645e-02,  7.1080e-02,  1.3266e-01,\n                      -1.0774e-01, -4.6564e-02, -2.8130e-02, -5.2225e-02,  1.5091e-01,\n                      -6.8661e-03,  2.1766e-01, -6.0972e-03,  5.9136e-02,  8.7824e-02,\n                       1.7858e-01,  1.5290e-01,  9.5272e-02,  6.8842e-02, -3.8008e-02,\n                       1.1547e-01, -6.7948e-01,  1.2668e-01, -2.6617e-01, -8.6280e-02,\n                       3.6103e-03, -1.2586e-01, -1.1768e-01, -7.2241e-02, -3.8473e-03,\n                       3.4160e-02, -7.3973e-02, -1.2605e-02, -4.0948e-02,  2.9368e-02,\n                      -8.7224e-03, -4.4394e-02, -2.7834e-02, -5.0173e-02, -6.8809e-02,\n                       8.2312e-02,  1.0518e-01, -4.5582e-02,  4.4284e-02, -1.3656e-01,\n                      -2.0470e-02,  9.4889e-02, -7.8890e-03, -3.0725e-02, -5.9993e-02,\n                       2.9005e-02,  1.6693e-01,  4.7039e-02, -2.5792e-02, -1.7221e-02,\n                      -1.1559e-01, -3.7813e-02, -6.1598e-02, -6.4040e-03, -1.6718e-02,\n                      -8.9827e-02,  1.5486e-01,  4.3785e-02,  1.3699e-01,  1.0202e-01,\n                      -6.4889e-03, -6.5166e-02,  1.2032e-01, -5.8163e-02,  4.0737e-02,\n                       5.9355e-02,  7.5264e-02,  3.9100e-02,  1.1671e-01, -4.3620e-02,\n                      -1.0383e-01,  1.4448e-02, -1.9494e-02,  1.0101e-01,  2.4455e-02,\n                      -4.0765e-04, -1.0016e-01, -4.6626e-02,  4.9567e-02,  8.6517e-02,\n                       6.9715e-02, -8.5729e-03,  7.8190e-02, -1.6236e-02, -5.6784e-02,\n                       1.0868e-01,  4.1310e-02, -1.7243e-01,  2.2656e-02, -8.6257e-02,\n                      -2.4373e-02,  2.5681e-02, -5.2408e-02, -2.6041e-02,  5.1585e-03,\n                      -1.5479e-02, -2.4806e-02, -7.2429e-02,  5.5928e-02,  3.0868e-02,\n                       5.0844e-02,  3.5093e-02, -4.1469e-02, -5.6961e-02, -2.4751e-02,\n                      -1.4574e-02, -6.2959e-02,  2.2231e-01,  1.8888e-01,  3.7042e-03,\n                       9.4826e-02,  8.5784e-02, -4.8123e-02, -1.9095e-02, -2.6806e-02,\n                      -4.2097e-02,  4.6244e-02, -2.5631e-02,  1.3605e-01, -6.5678e-02,\n                      -2.3281e-02, -6.4792e-03, -2.8311e-02, -7.6619e-02,  4.3730e-02,\n                       1.3032e-02,  7.4582e-02,  2.6035e-02, -1.3683e-01, -2.7083e-03,\n                       7.7213e-03,  5.1793e-03, -2.9078e-02,  1.1882e-03, -3.5331e-02,\n                      -4.4193e-02,  1.4447e-02,  6.6473e-02,  1.2185e-01,  1.2656e-01,\n                       1.2803e-02, -4.6422e-02, -5.8309e-02, -5.1723e-02, -1.3184e-02,\n                       1.5435e-01,  5.2300e-02, -1.0368e-02,  2.8768e-03,  8.6840e-02,\n                       1.3678e-02,  9.1461e-02,  5.4873e-02,  1.1502e-01, -1.3027e-03,\n                      -1.4845e-02, -3.6290e-02, -3.5914e-02, -6.7926e-02, -6.2005e-02,\n                       6.6568e-02, -2.7193e-02, -8.1570e-03,  1.6215e-01, -4.3435e-02,\n                       1.4237e-01, -2.9266e-02,  9.7247e-02, -4.2960e-02,  1.7072e-02,\n                       1.6334e-01,  1.3690e-01,  1.5334e-01, -1.9328e-02, -4.5507e-02,\n                      -2.9101e-02, -6.6050e-02, -5.8204e-02, -5.8806e-02, -7.3590e-02,\n                      -4.2440e-02,  1.2207e-01, -4.0585e-04, -2.7988e-02, -6.7383e-02,\n                       1.7468e-02,  1.6994e-01,  6.1952e-02,  2.0301e-03,  8.9507e-02,\n                      -6.1741e-02,  3.4198e-02,  2.4592e-02,  1.1848e-01, -4.7392e-03,\n                       3.9190e-02, -1.2814e-01,  1.2727e-01,  1.2101e-01, -6.8614e-02,\n                      -6.0921e-02, -1.0966e-01, -9.7277e-03, -5.4068e-02, -3.3886e-02,\n                       7.7542e-02, -7.3226e-02,  1.4465e-02,  7.5254e-02, -2.7840e-02,\n                      -1.9203e-02, -7.3903e-02, -3.3427e-02,  2.8135e-02,  1.4682e-02,\n                      -1.0452e-01,  1.7749e-02,  4.0771e-02,  7.4971e-02,  1.8479e-02,\n                      -3.1129e-02, -6.2762e-02,  1.6291e-01,  5.6326e-02,  1.7164e-02,\n                       8.8285e-02, -6.5044e-02, -4.9192e-02, -2.0146e-02, -4.6336e-02,\n                       9.9487e-02, -4.1733e-02,  3.4822e-02,  3.5404e-02, -1.8254e-02,\n                       1.4417e-01, -4.9665e-02, -5.6762e-03, -1.8281e-02, -1.7829e-02,\n                      -8.7625e-02,  1.2178e-01,  1.0283e-02,  1.7292e-02,  9.4729e-02,\n                      -3.2778e-02, -1.0712e-01, -3.2526e-03,  1.1816e-01, -5.6468e-02,\n                       8.8639e-02,  7.3536e-02,  3.6828e-02, -7.7939e-03, -2.2387e-02,\n                       1.4080e-01,  1.0881e-01,  2.1332e-02, -1.7308e-01, -5.6384e-02,\n                      -8.1865e-02, -3.1790e-02,  1.7500e-01,  4.8796e-02,  1.3229e-01,\n                       1.7329e-02, -6.4213e-02, -2.4018e-02, -2.1576e-02, -6.4478e-02,\n                       6.1314e-02,  1.8464e-01,  5.4248e-02, -5.4844e-02, -2.2233e-02,\n                       4.9604e-03, -8.4521e-02, -8.6005e-02, -1.0662e-02, -5.8976e-02,\n                       1.8311e-01, -7.4207e-02, -1.1078e-01, -8.1431e-02,  7.5390e-02,\n                       4.9858e-02, -5.4989e-02,  1.0158e-02, -9.5497e-02, -6.5929e-02,\n                      -4.1784e-02,  6.7656e-03, -1.5155e-03,  9.5525e-02,  6.5330e-02,\n                       5.4224e-02,  1.6465e-01, -2.0833e-03,  1.5778e-01, -1.0798e-02,\n                       4.9832e-02,  1.4097e-01, -1.6510e-02,  1.0177e-01,  5.7382e-03,\n                      -6.3912e-02,  7.9204e-02,  7.7312e-03, -9.6972e-02,  8.9184e-02,\n                      -6.9150e-02,  3.7228e-02, -7.0729e-02,  2.2207e-01, -2.8243e-02,\n                      -9.1216e-02, -2.3305e-02,  5.0250e-02,  4.3907e-02,  5.8284e-02,\n                      -3.7382e-02, -6.4371e-02, -7.3702e-02,  2.5744e-01, -4.5724e-02,\n                      -3.2581e-02, -6.4414e-02,  2.5885e-03, -8.6720e-02,  3.1690e-02,\n                      -6.6197e-02, -1.3642e-01,  4.6427e-02,  1.2916e-01, -2.5554e-02,\n                       1.0662e-01,  2.0280e-02,  1.1798e-02,  1.8722e-01,  1.2004e-01,\n                       9.5226e-02,  1.4984e-02, -1.2641e-01, -4.2097e-02, -6.4911e-02,\n                      -2.6939e-03, -2.2483e-02,  1.0834e-01, -1.0502e-01,  2.1229e-01,\n                      -5.6017e-02, -6.4519e-02, -7.7332e-02,  1.3001e-01,  6.7168e-02,\n                       3.5217e-02,  3.0106e-02,  8.6334e-03, -7.6900e-02,  1.6716e-02,\n                      -6.5309e-03,  1.5966e-01, -4.8310e-02,  3.3990e-02,  9.3884e-02,\n                      -4.1564e-02,  1.4736e-01,  9.4960e-02, -4.1056e-02,  5.0626e-03,\n                      -7.7016e-02, -1.5571e-02,  6.0383e-02, -7.5481e-02, -1.5669e-02,\n                       1.2579e-01,  9.7201e-02, -4.3347e-02, -1.7116e-01, -3.2212e-02,\n                       1.3494e-01, -9.1791e-02,  3.0455e-02,  2.1820e-03,  2.7922e-02,\n                       1.5921e-02,  4.8345e-03,  5.1693e-02,  3.9296e-02,  2.4513e-01,\n                       5.3453e-02,  1.6208e-01,  1.8102e-03, -7.3596e-02,  1.2118e-01,\n                       4.7541e-02, -4.0825e-02, -8.0493e-02,  7.8146e-02,  7.7743e-02,\n                       5.5962e-02, -2.4605e-02,  4.8331e-02,  8.4194e-03,  4.1091e-03,\n                       8.7259e-02,  2.2547e-02, -4.3287e-02, -9.7953e-02,  1.4816e-01,\n                      -5.7351e-02,  8.0969e-02,  7.3631e-02, -1.5367e-02,  8.6668e-02,\n                      -2.4724e-02,  1.3390e-01, -7.7516e-02, -2.4583e-02, -8.0253e-02,\n                      -4.2521e-02, -7.5265e-02,  1.4050e-01, -2.4030e-02,  1.0631e-01,\n                      -5.7394e-02,  1.6829e-01, -8.4347e-02,  8.1876e-02,  6.2162e-03,\n                      -3.9212e-02,  4.0400e-02,  1.8612e-01,  1.6644e-02,  2.1206e-02,\n                       1.1410e-01,  1.7157e-02,  2.4301e-01, -4.1038e-02,  1.3469e-01,\n                       2.8756e-02, -8.0740e-02, -3.1170e-02, -3.2092e-02, -2.4322e-02,\n                      -1.2752e-02, -1.9900e-01,  5.9815e-02,  2.0635e-02, -5.9063e-02,\n                      -4.8555e-02, -6.9471e-02,  2.3935e-02,  7.0737e-02,  4.5275e-02,\n                      -3.1793e-03, -9.6777e-02,  4.8704e-02, -3.4807e-02, -1.4050e-02,\n                      -9.0542e-02], requires_grad=True)),\n             ('mnet.features.12.conv.0.0.scale', tensor(0.0172)),\n             ('mnet.features.12.conv.0.0.zero_point', tensor(65)),\n             ('mnet.features.12.conv.1.0.weight',\n              tensor([[[[-0.6518, -0.9040, -0.3889],\n                        [-0.6938,  1.3350, -0.4625],\n                        [-0.5571, -0.5151, -0.3364]]],\n              \n              \n                      [[[ 0.3129,  0.4118,  0.2800],\n                        [ 0.4118,  2.0917,  0.4447],\n                        [ 0.2965,  0.2471,  0.2800]]],\n              \n              \n                      [[[-0.8066,  0.1698,  0.9198],\n                        [-1.8113,  0.3113,  1.7264],\n                        [ 0.5802, -0.0991, -0.3679]]],\n              \n              \n                      ...,\n              \n              \n                      [[[ 0.5816, -0.1193,  0.5667],\n                        [ 0.4026,  1.8939,  0.2684],\n                        [ 0.3728,  0.5219,  0.3132]]],\n              \n              \n                      [[[ 0.2579,  0.3457,  0.1537],\n                        [ 0.5268,  0.6970,  0.6311],\n                        [ 0.2524,  0.3732,  0.2744]]],\n              \n              \n                      [[[ 0.4327,  0.1751,  0.4945],\n                        [ 0.4945,  1.3084,  0.4533],\n                        [ 0.3812,  0.7933,  0.5151]]]], size=(576, 1, 3, 3),\n                     dtype=torch.qint8, quantization_scheme=torch.per_channel_affine,\n                     scale=tensor([0.0105, 0.0165, 0.0142, 0.0062, 0.0105, 0.0220, 0.0043, 0.0115, 0.0078,\n                      0.0173, 0.0198, 0.0190, 0.0182, 0.0152, 0.0059, 0.0152, 0.0114, 0.0042,\n                      0.0121, 0.0201, 0.0083, 0.0141, 0.0102, 0.0103, 0.0215, 0.0356, 0.0033,\n                      0.0456, 0.0045, 0.0140, 0.0185, 0.0031, 0.0185, 0.0156, 0.0283, 0.0102,\n                      0.0162, 0.0237, 0.0096, 0.0175, 0.0142, 0.0173, 0.0130, 0.0153, 0.0042,\n                      0.0140, 0.0428, 0.0147, 0.0316, 0.0212, 0.0137, 0.0108, 0.0110, 0.0046,\n                      0.0078, 0.0152, 0.0182, 0.0044, 0.0088, 0.0142, 0.0096, 0.0230, 0.0148,\n                      0.0165, 0.0286, 0.0191, 0.0174, 0.0166, 0.0120, 0.0146, 0.0150, 0.0054,\n                      0.0172, 0.0130, 0.0099, 0.0166, 0.0115, 0.0192, 0.0186, 0.0121, 0.0153,\n                      0.0207, 0.0037, 0.0064, 0.0040, 0.0179, 0.0237, 0.0157, 0.0057, 0.0010,\n                      0.0019, 0.0108, 0.0073, 0.0124, 0.0223, 0.0083, 0.0048, 0.0083, 0.0130,\n                      0.0492, 0.0058, 0.0042, 0.0228, 0.0106, 0.0140, 0.0170, 0.0156, 0.0111,\n                      0.0176, 0.0061, 0.0074, 0.0195, 0.0037, 0.0126, 0.0164, 0.0225, 0.0240,\n                      0.0010, 0.0176, 0.0056, 0.0150, 0.0140, 0.0162, 0.0136, 0.0133, 0.0084,\n                      0.0184, 0.0148, 0.0140, 0.0197, 0.0188, 0.0699, 0.0075, 0.0452, 0.0152,\n                      0.0040, 0.0178, 0.0256, 0.0366, 0.0121, 0.0062, 0.0062, 0.0166, 0.0261,\n                      0.0090, 0.0149, 0.0189, 0.0109, 0.0205, 0.0149, 0.0094, 0.0162, 0.0168,\n                      0.0037, 0.0150, 0.0236, 0.0106, 0.0143, 0.0046, 0.0093, 0.0153, 0.0105,\n                      0.0062, 0.0009, 0.0152, 0.0237, 0.0183, 0.0130, 0.0131, 0.0110, 0.0108,\n                      0.0178, 0.0062, 0.0212, 0.0070, 0.0053, 0.0244, 0.0104, 0.0042, 0.0150,\n                      0.0120, 0.0050, 0.0043, 0.0205, 0.0034, 0.0134, 0.0060, 0.0261, 0.0048,\n                      0.0162, 0.0255, 0.0220, 0.0101, 0.0132, 0.0159, 0.0141, 0.0142, 0.0206,\n                      0.0144, 0.0181, 0.0110, 0.0167, 0.0235, 0.0053, 0.0072, 0.0165, 0.0135,\n                      0.0185, 0.0192, 0.0134, 0.0123, 0.0241, 0.0047, 0.0075, 0.0063, 0.0110,\n                      0.0045, 0.0152, 0.0062, 0.0163, 0.0153, 0.0060, 0.0189, 0.0077, 0.0053,\n                      0.0133, 0.0178, 0.0114, 0.0128, 0.0158, 0.0171, 0.0164, 0.0009, 0.0177,\n                      0.0041, 0.0004, 0.0058, 0.0164, 0.0274, 0.0107, 0.0112, 0.0063, 0.0055,\n                      0.0217, 0.0121, 0.0081, 0.0041, 0.0140, 0.0190, 0.0161, 0.0133, 0.0049,\n                      0.0143, 0.0077, 0.0174, 0.0072, 0.0083, 0.0263, 0.0231, 0.0067, 0.0091,\n                      0.0137, 0.0119, 0.0213, 0.0175, 0.0044, 0.0139, 0.0064, 0.0147, 0.0052,\n                      0.0036, 0.0142, 0.0055, 0.0230, 0.0128, 0.0056, 0.0068, 0.0155, 0.0057,\n                      0.0167, 0.0121, 0.0173, 0.0119, 0.0201, 0.0041, 0.0065, 0.0094, 0.0139,\n                      0.0224, 0.0111, 0.0160, 0.0142, 0.0131, 0.0253, 0.0109, 0.0160, 0.0208,\n                      0.0174, 0.0327, 0.0226, 0.0089, 0.0116, 0.0127, 0.0116, 0.0069, 0.0145,\n                      0.0124, 0.0070, 0.0120, 0.0149, 0.0114, 0.0136, 0.0243, 0.0039, 0.0267,\n                      0.0044, 0.0233, 0.0176, 0.0185, 0.0102, 0.0077, 0.0109, 0.0115, 0.0153,\n                      0.0164, 0.0170, 0.0094, 0.0141, 0.0217, 0.0041, 0.0225, 0.0119, 0.0053,\n                      0.0134, 0.0164, 0.0046, 0.0189, 0.0164, 0.0154, 0.0051, 0.0158, 0.0240,\n                      0.0171, 0.0100, 0.0142, 0.0158, 0.0135, 0.0211, 0.0163, 0.0246, 0.0060,\n                      0.0236, 0.0140, 0.0109, 0.0075, 0.0097, 0.0115, 0.0140, 0.0080, 0.0128,\n                      0.0040, 0.0215, 0.0037, 0.0177, 0.0040, 0.0134, 0.0173, 0.0055, 0.0056,\n                      0.0015, 0.0104, 0.0141, 0.0109, 0.0352, 0.0180, 0.0144, 0.0155, 0.0197,\n                      0.0064, 0.0129, 0.0061, 0.0105, 0.0168, 0.0198, 0.0164, 0.0130, 0.0163,\n                      0.0062, 0.0157, 0.0052, 0.0061, 0.0260, 0.0173, 0.0107, 0.0157, 0.0068,\n                      0.0217, 0.0215, 0.0141, 0.0051, 0.0207, 0.0154, 0.0054, 0.0416, 0.0168,\n                      0.0144, 0.0144, 0.0197, 0.0162, 0.0185, 0.0136, 0.0143, 0.0136, 0.0134,\n                      0.0210, 0.0069, 0.0222, 0.0193, 0.0049, 0.0048, 0.0258, 0.0112, 0.0186,\n                      0.0195, 0.0049, 0.0244, 0.0038, 0.0189, 0.0107, 0.0201, 0.0205, 0.0039,\n                      0.0117, 0.0046, 0.0091, 0.0118, 0.0064, 0.0200, 0.0097, 0.0175, 0.0111,\n                      0.0082, 0.0109, 0.0141, 0.0043, 0.0036, 0.0116, 0.0043, 0.0188, 0.0129,\n                      0.0181, 0.0192, 0.0248, 0.0096, 0.0075, 0.0052, 0.0055, 0.0183, 0.0059,\n                      0.0233, 0.0060, 0.0376, 0.0112, 0.0187, 0.0287, 0.0059, 0.0297, 0.0124,\n                      0.0169, 0.0277, 0.0063, 0.0183, 0.0054, 0.0109, 0.0101, 0.0052, 0.0190,\n                      0.0042, 0.0063, 0.0095, 0.0224, 0.0161, 0.0093, 0.0045, 0.0041, 0.0237,\n                      0.0035, 0.0038, 0.0150, 0.0209, 0.0197, 0.0096, 0.0055, 0.0267, 0.0100,\n                      0.0080, 0.0054, 0.0052, 0.0160, 0.0049, 0.0154, 0.0044, 0.0123, 0.0121,\n                      0.0148, 0.0171, 0.0188, 0.0052, 0.0095, 0.0163, 0.0050, 0.0039, 0.0281,\n                      0.0051, 0.0131, 0.0163, 0.0050, 0.0089, 0.0149, 0.0037, 0.0043, 0.0176,\n                      0.0146, 0.0092, 0.0131, 0.0106, 0.0275, 0.0129, 0.0232, 0.0167, 0.0167,\n                      0.0057, 0.0288, 0.0051, 0.0103, 0.0265, 0.0128, 0.0159, 0.0183, 0.0119,\n                      0.0147, 0.0172, 0.0144, 0.0046, 0.0197, 0.0217, 0.0091, 0.0211, 0.0298,\n                      0.0106, 0.0265, 0.0113, 0.0070, 0.0225, 0.0250, 0.0178, 0.0142, 0.0032,\n                      0.0195, 0.0212, 0.0067, 0.0225, 0.0050, 0.0108, 0.0130, 0.0211, 0.0178,\n                      0.0113, 0.0049, 0.0158, 0.0113, 0.0084, 0.0054, 0.0149, 0.0055, 0.0103],\n                     dtype=torch.float64),\n                     zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n                     axis=0)),\n             ('mnet.features.12.conv.1.0.bias',\n              Parameter containing:\n              tensor([ 4.2381e-01, -3.4919e-01, -7.9241e-02,  3.0758e-01, -2.7888e-01,\n                      -1.0290e-01,  2.1074e-01, -1.1743e-03, -1.3258e-01, -1.3267e-01,\n                      -8.5453e-02, -1.8020e-01, -5.8145e-01, -3.7047e-01,  2.5827e-01,\n                      -4.8588e-02,  2.2302e-02,  3.1430e-01,  4.6439e-02,  1.0037e-01,\n                       4.1914e-01, -4.5280e-01, -1.5997e-01,  1.5492e-01, -2.0679e-01,\n                      -2.6542e-01,  2.2776e-01, -1.7517e-01,  2.6943e-01, -2.0554e-01,\n                      -1.8051e-01,  2.7314e-01, -1.6243e-01,  8.6116e-01,  2.2032e-01,\n                      -1.9097e-01, -7.9674e-02, -1.3388e-01,  6.8364e-02,  1.5630e-01,\n                      -1.9486e-01, -4.4722e-02, -6.8464e-02, -1.0906e-01,  2.3486e-01,\n                      -3.8475e-02, -1.4069e-01, -3.9831e-01, -5.4458e-01, -2.5839e-01,\n                      -2.4536e-01,  5.1006e-02, -2.9873e-02,  3.2325e-01,  3.2981e-01,\n                      -2.2127e-01, -5.4332e-02,  3.0075e-01,  4.0514e-02, -1.2493e-01,\n                      -3.4143e-02, -2.6528e-02, -2.5519e-01, -1.3092e-01, -1.0434e-01,\n                      -1.6026e-01, -2.8296e-01,  2.1505e-01, -3.3703e-01, -1.1130e-01,\n                       9.9542e-03,  3.2135e-01,  2.6777e-01,  1.5688e-01,  3.8354e-01,\n                       1.2890e-01, -1.4733e-02, -2.0585e-01, -2.2133e-02,  1.2813e-01,\n                       1.0754e-01, -1.1035e-01,  2.5248e-01,  2.5397e-01,  3.9096e-01,\n                      -2.8821e-01, -1.3541e-01, -9.8995e-02,  3.1138e-01, -1.6893e-02,\n                      -6.3017e-03, -1.2071e-02,  2.2700e-01, -1.1628e-01, -1.5695e-01,\n                      -4.5291e-02,  2.7657e-01,  9.9943e-03, -1.3480e-02, -2.3884e-01,\n                       2.5754e-01,  2.3156e-01, -1.0588e-01, -2.7052e-02,  1.5287e-02,\n                      -1.5249e-01, -7.7801e-02, -8.1817e-02, -3.9005e-02,  2.1792e-01,\n                       5.0523e-01, -4.2652e-01,  2.4729e-01, -1.5419e-01, -7.6182e-02,\n                      -3.4841e-02, -1.7041e-01, -1.2501e-02, -5.9711e-03,  4.7282e-01,\n                      -1.0111e-01,  9.4431e-01,  9.5542e-02, -3.7580e-03, -1.4105e-01,\n                       4.4026e-01,  1.9305e-01, -9.4417e-02,  1.1673e-01, -3.0291e-01,\n                       2.0722e-01, -2.0995e-01,  5.2659e-01, -6.6127e-02, -3.8233e-02,\n                       2.7055e-01, -9.6467e-02, -1.0756e-01, -2.6507e-01, -8.6947e-02,\n                       3.1831e-01,  2.1918e-01,  1.8190e-02, -3.6829e-01,  3.0168e-01,\n                      -1.8223e-01, -7.5761e-02, -4.9659e-01, -2.4409e-02, -8.8435e-02,\n                       3.1065e-01, -5.9494e-01, -1.6017e-01,  2.5448e-01, -1.1015e-02,\n                      -4.1156e-01,  1.9824e-02, -3.0773e-01, -1.2473e-01, -2.0311e-01,\n                       3.4830e-02,  4.9097e-01,  2.8184e-01, -6.6650e-03, -3.4188e-01,\n                      -7.2264e-03, -2.1435e-01,  7.0895e-02, -1.2617e-01,  1.6406e-01,\n                      -4.8639e-02,  1.8221e-01,  2.9985e-01,  4.7821e-02,  4.2269e-01,\n                       2.4627e-01, -2.3314e-02, -1.6465e-01,  2.4251e-01, -5.7168e-02,\n                      -1.9843e-02,  3.2333e-01,  2.5695e-01, -5.8340e-01,  3.1483e-01,\n                       9.7634e-03,  2.4364e-01, -2.4070e-01,  3.8886e-01, -2.6074e-01,\n                      -3.4495e-01, -3.5538e-01, -1.8650e-01, -1.4477e-02, -1.8225e-02,\n                       2.2499e-03, -1.0439e-01,  4.3655e-01,  9.0723e-02,  2.1723e-01,\n                      -6.5976e-02, -5.8374e-02, -1.3699e-01,  2.6565e-01,  2.2266e-01,\n                      -4.3138e-02, -2.5857e-01, -1.2002e-01, -2.9070e-01,  1.3306e-01,\n                      -7.9006e-02,  1.2130e-02,  3.1100e-01,  2.5684e-01,  2.5565e-01,\n                      -4.9014e-02,  2.8548e-01, -4.5787e-02,  2.2957e-01, -2.8380e-01,\n                      -8.2887e-02,  2.0606e-01,  4.7540e-01,  7.5635e-01,  2.7385e-01,\n                      -5.0652e-02, -4.1243e-02,  1.1296e-01, -1.6528e-01, -1.4819e-01,\n                      -1.0028e-01, -1.0107e-01, -1.1117e-02,  4.2048e-02,  3.2583e-01,\n                      -1.0144e-02,  2.2961e-01, -2.6015e-01, -1.4816e-01,  2.7893e-02,\n                      -3.9522e-03,  3.2852e-01,  3.0778e-01, -8.3137e-02, -5.3899e-02,\n                      -3.7928e-02,  2.7509e-01, -1.3056e-01, -4.7820e-02, -1.1801e-01,\n                      -1.5227e-01,  2.8726e-01, -3.8328e-01,  4.2420e-01,  1.1024e-01,\n                       2.6585e-01,  3.2551e-01, -3.5404e-01, -1.0287e-01, -1.0180e-01,\n                       5.7651e-01,  4.5929e-02, -1.9838e-02, -3.1935e-01, -7.6257e-02,\n                       2.8290e-01,  2.1124e-01,  2.2824e-01, -2.1313e-02,  2.6883e-01,\n                       2.1922e-01, -2.1989e-01,  2.0621e-01, -8.1860e-02,  2.5914e-01,\n                       3.0697e-01, -1.5043e-01, -2.4985e-01,  4.7821e-01, -2.2645e-01,\n                       1.9180e-02, -2.2220e-01, -1.0580e-01, -2.6558e-01,  2.3130e-01,\n                       5.1312e-01,  1.9377e-02, -9.1537e-02, -3.9636e-02, -6.7860e-03,\n                      -3.4984e-01,  8.3454e-02,  1.3147e-01, -1.2022e-01,  3.5795e-01,\n                      -2.2272e-01, -1.9774e-01, -2.4612e-01, -3.1004e-01, -4.0059e-02,\n                       2.4788e-01,  6.3851e-01, -4.0545e-02, -4.6810e-02,  4.0210e-01,\n                      -1.4996e-02, -4.0932e-01,  2.4255e-01,  7.4851e-02, -3.8690e-02,\n                      -2.7537e-02, -1.0091e-01,  5.1650e-03,  3.5952e-01, -2.5192e-01,\n                       2.1562e-01, -1.0884e-01, -2.0363e-02, -3.6388e-02, -1.1624e-02,\n                      -9.6442e-02, -5.7922e-02,  9.8842e-03, -4.0003e-01, -2.0006e-01,\n                      -3.2738e-01, -9.1157e-02, -1.9730e-01, -4.4597e-01,  2.4023e-01,\n                      -1.2668e-01, -1.2454e-01,  2.8418e-01,  2.7128e-01, -2.3121e-02,\n                       2.4839e-01, -1.0038e-01, -4.5256e-02,  4.3334e-01,  2.2197e-01,\n                       9.5093e-03, -2.0653e-01,  2.1062e-01,  6.6096e-04, -1.4007e-02,\n                      -7.4150e-02, -2.4583e-01,  3.1425e-01, -1.6376e-01, -3.5426e-01,\n                       4.2808e-01, -2.6433e-01, -3.6985e-01,  1.8159e-01,  2.7640e-01,\n                      -1.5363e-01,  6.0709e-02, -2.6397e-01,  2.4006e-01, -7.6269e-02,\n                       2.4742e-01, -3.5194e-01,  2.4766e-01,  2.7641e-02,  2.5397e-01,\n                      -1.8670e-02, -5.5422e-02,  2.6592e-01,  2.6378e-01, -1.6914e-02,\n                      -4.9339e-02, -9.4128e-02,  2.2777e-01, -2.2164e-01, -1.1454e-02,\n                       2.1242e-01, -1.3537e-01, -1.2617e-01,  2.7323e-01, -4.7924e-02,\n                       2.4718e-01, -2.8255e-02, -3.1066e-01, -2.1313e-02, -4.3384e-03,\n                       1.2548e-01, -1.3533e+00,  2.8890e-01,  1.1970e-01,  2.2729e-01,\n                       2.4409e-01, -9.7071e-02,  4.5396e-01, -3.4461e-02, -2.5517e-01,\n                       5.7998e-01, -6.8130e-02, -2.6287e-01, -7.9794e-02,  3.9343e-01,\n                       3.1540e-03, -2.9513e-01,  3.2597e-01, -1.4605e-01, -1.5588e-01,\n                      -2.3180e-01, -2.9029e-01, -6.6531e-02, -4.4996e-02, -3.9297e-01,\n                      -2.1294e-01,  1.4690e-01, -3.5055e-01, -1.9942e-02, -1.7082e-01,\n                       2.9890e-01,  3.1085e-01,  3.5648e-02,  3.5507e-01,  2.9756e-01,\n                      -1.7050e-01, -1.6530e-02, -4.2326e-01, -1.6215e-01,  2.8887e-01,\n                      -2.4818e-01,  3.5094e-01, -1.5469e-01,  6.7043e-01, -1.0220e-01,\n                      -1.7787e-01,  2.3904e-01, -1.1742e-01,  3.1761e-01,  2.5708e-01,\n                      -1.1730e-01,  2.6098e-01, -1.7430e-01, -6.8014e-01, -7.1102e-02,\n                       3.2449e-03,  2.3590e-01,  8.8519e-02, -1.7917e-01,  2.8614e-01,\n                       2.6090e-01,  2.4706e-01,  2.2018e-01,  1.9099e-01,  2.8760e-01,\n                      -6.6470e-02, -1.2766e-02, -2.6424e-01,  6.5974e-01, -5.4937e-01,\n                       3.6412e-01,  2.8642e-01, -1.7532e-01,  2.2369e-01, -7.4316e-02,\n                       2.6308e-01, -2.9656e-01, -4.8274e-03, -2.4960e-01,  1.2110e+00,\n                       2.2636e-01, -2.4988e-01, -1.5825e-01,  3.3032e-01, -2.1764e-01,\n                       2.8811e-01, -2.2152e-02,  2.7022e-01,  4.2986e-01,  1.2154e-01,\n                       2.5018e-01, -6.4022e-01,  2.9407e-01,  2.5065e-01,  2.8307e-01,\n                      -1.9951e-01, -1.0950e-01,  8.9376e-03,  2.4055e-01,  2.8600e-01,\n                      -1.6878e-01,  2.8709e-01,  2.7223e-01, -1.7662e-01, -3.2368e-01,\n                      -1.4515e-01,  9.9708e-02,  1.8807e-01, -1.8530e-01, -4.4366e-02,\n                       4.3400e-01, -6.6493e-02,  2.4771e-01, -1.5375e-01,  2.1082e-01,\n                       1.2297e-02,  2.5867e-01,  1.2302e-01, -5.3601e-02,  1.2790e+00,\n                       8.7934e-02, -7.9714e-02,  2.3956e-01, -4.2186e-02, -2.8709e-01,\n                       3.1747e-01,  2.1441e-01, -4.6571e-02,  2.9966e-01, -3.5129e-02,\n                      -4.8663e-01,  1.7451e-01, -8.6151e-03, -1.5432e-02,  2.4795e-01,\n                       3.1297e-01, -2.6178e-02, -4.3374e-01, -2.2830e-01,  1.0708e-01,\n                      -9.0133e-02, -3.2880e-01, -1.1772e-02, -1.0901e-02, -5.0098e-02,\n                       1.1407e-02,  3.2589e-01, -2.2045e-01,  3.3117e-01, -3.6013e-01,\n                      -2.0048e-01, -2.4390e-01, -2.1726e-03, -1.9849e-01,  1.8336e-01,\n                      -1.0252e-01,  7.0988e-01,  7.9283e-02,  3.3110e-01, -2.0660e-02,\n                       3.7648e-01,  3.2796e-01,  9.6545e-01, -7.5334e-02, -1.7209e-02,\n                      -1.8869e-01, -7.2264e-02,  6.0780e-01, -2.0920e-01, -1.4291e-01,\n                      -5.5441e-02, -1.1185e-01,  2.6334e-01, -2.5542e-01, -3.1685e-02,\n                       1.5690e-02, -1.6484e-01,  2.6252e-01, -5.5558e-02, -3.1904e-01,\n                       3.6390e-02, -1.2877e-02, -2.3456e-01,  3.5176e-01, -5.1516e-01,\n                       2.4140e-01, -6.4969e-02,  3.2192e-01, -2.5810e-01, -1.0105e-01,\n                      -2.3208e-02], requires_grad=True)),\n             ('mnet.features.12.conv.1.0.scale', tensor(0.0367)),\n             ('mnet.features.12.conv.1.0.zero_point', tensor(50)),\n             ('mnet.features.12.conv.2.weight',\n              tensor([[[[-0.0203]],\n              \n                       [[-0.2060]],\n              \n                       [[ 0.0712]],\n              \n                       ...,\n              \n                       [[-0.1043]],\n              \n                       [[-0.0229]],\n              \n                       [[-0.1297]]],\n              \n              \n                      [[[ 0.2341]],\n              \n                       [[-0.1536]],\n              \n                       [[-0.0037]],\n              \n                       ...,\n              \n                       [[ 0.0146]],\n              \n                       [[-0.2341]],\n              \n                       [[-0.1610]]],\n              \n              \n                      [[[ 0.0807]],\n              \n                       [[ 0.1323]],\n              \n                       [[-0.0202]],\n              \n                       ...,\n              \n                       [[ 0.1121]],\n              \n                       [[-0.0695]],\n              \n                       [[ 0.0134]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-0.1374]],\n              \n                       [[-0.0935]],\n              \n                       [[ 0.0058]],\n              \n                       ...,\n              \n                       [[ 0.2163]],\n              \n                       [[ 0.0468]],\n              \n                       [[-0.0175]]],\n              \n              \n                      [[[-0.0093]],\n              \n                       [[-0.0806]],\n              \n                       [[-0.0744]],\n              \n                       ...,\n              \n                       [[-0.0031]],\n              \n                       [[ 0.0744]],\n              \n                       [[-0.0031]]],\n              \n              \n                      [[[ 0.0918]],\n              \n                       [[-0.2296]],\n              \n                       [[ 0.0459]],\n              \n                       ...,\n              \n                       [[-0.1530]],\n              \n                       [[-0.1416]],\n              \n                       [[-0.1263]]]], size=(96, 576, 1, 1), dtype=torch.qint8,\n                     quantization_scheme=torch.per_channel_affine,\n                     scale=tensor([0.0025, 0.0037, 0.0022, 0.0015, 0.0028, 0.0026, 0.0024, 0.0025, 0.0025,\n                      0.0040, 0.0019, 0.0027, 0.0025, 0.0020, 0.0025, 0.0036, 0.0036, 0.0024,\n                      0.0028, 0.0026, 0.0022, 0.0037, 0.0021, 0.0031, 0.0027, 0.0026, 0.0025,\n                      0.0026, 0.0019, 0.0027, 0.0026, 0.0024, 0.0021, 0.0023, 0.0022, 0.0025,\n                      0.0024, 0.0038, 0.0025, 0.0031, 0.0017, 0.0030, 0.0029, 0.0019, 0.0034,\n                      0.0023, 0.0019, 0.0027, 0.0038, 0.0029, 0.0023, 0.0022, 0.0018, 0.0024,\n                      0.0030, 0.0028, 0.0024, 0.0016, 0.0032, 0.0027, 0.0025, 0.0034, 0.0019,\n                      0.0025, 0.0023, 0.0021, 0.0030, 0.0032, 0.0018, 0.0032, 0.0027, 0.0034,\n                      0.0025, 0.0026, 0.0030, 0.0020, 0.0030, 0.0031, 0.0020, 0.0025, 0.0029,\n                      0.0032, 0.0033, 0.0026, 0.0037, 0.0030, 0.0029, 0.0033, 0.0030, 0.0020,\n                      0.0034, 0.0025, 0.0023, 0.0029, 0.0031, 0.0038], dtype=torch.float64),\n                     zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n                     axis=0)),\n             ('mnet.features.12.conv.2.bias',\n              Parameter containing:\n              tensor([ 2.4758e-01, -4.0357e-02, -3.5150e-01, -1.4170e-01,  3.1718e-01,\n                       5.5075e-02,  1.1002e-01,  5.3870e-01,  1.3093e-01,  1.8354e-01,\n                       1.6671e-01,  7.5608e-01, -9.0086e-02,  1.9745e-01,  3.7986e-01,\n                      -2.9682e-01,  4.4229e-01,  2.3430e-02,  3.1788e-02, -5.1775e-02,\n                       6.2716e-01, -1.0140e-01, -3.9200e-02, -5.8816e-01,  2.8728e-03,\n                      -5.3521e-01,  1.4551e-01,  1.8847e-01, -1.6438e-01,  1.9091e-01,\n                       6.5380e-01, -3.1306e-02,  8.9889e-02, -1.9458e-01,  4.3543e-01,\n                      -1.0094e-01,  4.9207e-01,  4.1865e-01,  1.5755e-01, -9.7035e-02,\n                       1.9053e-02, -2.3265e-02,  1.9323e-02, -5.4984e-01, -1.7664e-01,\n                      -2.3479e-02, -2.6802e-01,  4.7186e-02,  1.5816e-01,  4.3451e-01,\n                       2.0749e-01, -7.3251e-02, -6.7301e-01, -3.5287e-04, -9.9035e-02,\n                       7.9437e-01,  4.9267e-01, -9.0015e-02,  1.0637e-01,  1.4711e-01,\n                       5.6613e-01, -4.4892e-01, -2.0474e-01,  3.4691e-01, -9.5668e-02,\n                       2.1107e-01,  1.3712e-01,  1.1259e-01, -1.3028e-01,  2.1578e-01,\n                      -1.9000e-01,  2.1594e-01,  1.2053e-01, -6.3766e-01, -7.7542e-02,\n                      -5.9577e-02,  3.7075e-01, -2.1363e-01,  9.1248e-03,  2.5764e-01,\n                      -1.9325e-01,  2.2709e-01, -3.2906e-01, -1.4532e-01,  5.3973e-01,\n                      -1.5193e-02, -1.3050e-01,  1.7582e-01, -2.9850e-03, -4.7008e-01,\n                      -3.3191e-01,  6.1219e-02, -3.6747e-01,  4.9303e-02,  6.7871e-01,\n                       6.1821e-01], requires_grad=True)),\n             ('mnet.features.12.conv.2.scale', tensor(0.0582)),\n             ('mnet.features.12.conv.2.zero_point', tensor(56)),\n             ('mnet.features.13.conv.0.0.weight',\n              tensor([[[[-0.0174]],\n              \n                       [[ 0.0413]],\n              \n                       [[-0.0233]],\n              \n                       ...,\n              \n                       [[ 0.0103]],\n              \n                       [[ 0.0013]],\n              \n                       [[-0.0271]]],\n              \n              \n                      [[[-0.0169]],\n              \n                       [[ 0.0281]],\n              \n                       [[-0.0120]],\n              \n                       ...,\n              \n                       [[-0.0070]],\n              \n                       [[-0.0281]],\n              \n                       [[ 0.0070]]],\n              \n              \n                      [[[ 0.0116]],\n              \n                       [[ 0.0014]],\n              \n                       [[ 0.0087]],\n              \n                       ...,\n              \n                       [[-0.0022]],\n              \n                       [[ 0.0144]],\n              \n                       [[-0.0094]]],\n              \n              \n                      ...,\n              \n              \n                      [[[ 0.0151]],\n              \n                       [[ 0.0174]],\n              \n                       [[-0.0283]],\n              \n                       ...,\n              \n                       [[ 0.0118]],\n              \n                       [[-0.0095]],\n              \n                       [[-0.0187]]],\n              \n              \n                      [[[ 0.0283]],\n              \n                       [[ 0.0078]],\n              \n                       [[ 0.0397]],\n              \n                       ...,\n              \n                       [[ 0.0156]],\n              \n                       [[-0.0127]],\n              \n                       [[-0.0014]]],\n              \n              \n                      [[[ 0.0148]],\n              \n                       [[-0.0137]],\n              \n                       [[ 0.0324]],\n              \n                       ...,\n              \n                       [[ 0.0077]],\n              \n                       [[ 0.0280]],\n              \n                       [[-0.0319]]]], size=(576, 96, 1, 1), dtype=torch.qint8,\n                     quantization_scheme=torch.per_channel_affine,\n                     scale=tensor([6.4608e-04, 7.0354e-04, 7.2222e-04, 4.3558e-04, 6.1418e-04, 5.3227e-04,\n                      2.9850e-04, 5.5302e-04, 3.5371e-04, 5.9645e-04, 5.7073e-04, 8.2440e-04,\n                      9.3370e-04, 3.1049e-04, 4.7738e-04, 4.5476e-04, 8.4635e-04, 5.1428e-04,\n                      5.8085e-04, 8.1326e-04, 6.8709e-04, 4.6700e-04, 4.6112e-04, 6.0248e-04,\n                      5.0538e-04, 8.4996e-06, 5.5110e-04, 7.0355e-04, 5.4398e-04, 3.4948e-04,\n                      6.3113e-04, 5.5005e-04, 4.2905e-04, 5.5216e-04, 6.0422e-04, 4.0965e-04,\n                      6.5315e-04, 6.2393e-04, 4.5480e-04, 5.9531e-04, 5.4451e-04, 3.1834e-04,\n                      4.8023e-04, 7.1613e-04, 7.0119e-04, 8.0446e-04, 7.2976e-04, 5.0214e-04,\n                      5.3526e-04, 7.6250e-04, 5.7945e-04, 6.0680e-04, 5.0642e-04, 6.9149e-04,\n                      5.2165e-04, 5.4920e-04, 7.2101e-04, 6.7576e-04, 6.0499e-04, 3.6889e-04,\n                      5.4081e-04, 6.0232e-04, 4.1575e-04, 5.2472e-04, 5.6716e-04, 6.0453e-04,\n                      5.9433e-04, 5.4750e-04, 6.5902e-04, 5.0629e-04, 5.7107e-04, 4.7465e-04,\n                      5.0025e-04, 5.1288e-04, 5.6264e-04, 3.4733e-04, 5.4096e-04, 3.2988e-04,\n                      6.0873e-04, 6.1178e-04, 4.2769e-04, 3.7961e-04, 6.5567e-04, 3.1673e-04,\n                      4.4429e-04, 7.3249e-04, 6.5950e-04, 6.8802e-04, 5.7992e-04, 5.0609e-04,\n                      5.6501e-04, 5.2057e-04, 5.0248e-04, 5.8197e-04, 5.0664e-04, 4.2740e-04,\n                      5.6581e-04, 6.8585e-04, 6.7264e-04, 6.0226e-04, 6.4943e-04, 5.0737e-04,\n                      6.4847e-04, 8.2929e-04, 4.4182e-04, 8.4849e-04, 3.7522e-04, 5.9692e-04,\n                      5.2152e-04, 5.0301e-04, 4.7874e-04, 8.1586e-04, 6.1125e-04, 7.8577e-04,\n                      5.2089e-04, 5.5642e-04, 4.1476e-04, 5.7157e-04, 4.1705e-04, 6.3424e-04,\n                      4.1519e-04, 6.2328e-04, 1.9065e-05, 5.3421e-04, 6.6765e-04, 4.8606e-04,\n                      4.8290e-04, 6.3688e-04, 5.6568e-04, 5.0300e-04, 4.8668e-04, 4.8341e-04,\n                      1.7305e-05, 5.3462e-04, 5.0364e-04, 4.2413e-04, 5.3236e-04, 5.2146e-04,\n                      5.7293e-04, 6.0189e-04, 7.6220e-04, 3.4354e-04, 6.3623e-04, 5.8618e-04,\n                      6.9484e-04, 5.9351e-04, 5.7438e-04, 1.8723e-04, 3.5864e-04, 5.1693e-04,\n                      5.3445e-04, 3.2292e-04, 4.3265e-04, 6.3968e-04, 4.6562e-04, 5.6217e-04,\n                      5.1132e-04, 4.3959e-04, 3.1189e-04, 6.2681e-04, 5.6693e-04, 5.8391e-04,\n                      5.0520e-04, 5.4850e-04, 2.5018e-04, 6.5097e-04, 6.1254e-04, 3.9361e-04,\n                      4.2923e-04, 6.4112e-04, 4.5044e-04, 7.0705e-04, 9.3410e-04, 5.6354e-04,\n                      5.7760e-04, 5.1280e-04, 6.9448e-04, 6.0172e-04, 5.8017e-04, 6.6529e-04,\n                      7.3896e-04, 4.7160e-04, 5.1060e-04, 9.2929e-04, 5.6893e-04, 3.7708e-04,\n                      4.8589e-04, 4.5976e-04, 4.0658e-04, 7.2179e-04, 3.8600e-04, 6.5427e-04,\n                      4.9272e-04, 6.6222e-04, 7.2570e-04, 5.0911e-04, 6.4604e-04, 1.0411e-03,\n                      4.9101e-04, 2.2200e-04, 4.5656e-04, 5.2803e-04, 5.5485e-04, 1.9559e-03,\n                      3.8848e-04, 4.3424e-04, 5.0668e-04, 7.2752e-04, 4.8174e-04, 5.1479e-04,\n                      7.2599e-04, 6.1699e-04, 6.1468e-04, 5.0192e-04, 5.5792e-04, 8.4275e-04,\n                      7.1412e-04, 4.9945e-04, 4.3873e-04, 7.3760e-04, 5.8453e-04, 3.9549e-04,\n                      5.8149e-04, 5.4726e-04, 5.1357e-04, 4.3525e-04, 5.8572e-04, 8.7513e-04,\n                      6.9560e-04, 7.0890e-04, 7.5265e-04, 3.3539e-04, 6.8051e-04, 8.1757e-04,\n                      5.4309e-04, 5.7638e-04, 4.4992e-04, 4.8804e-04, 7.8961e-04, 5.0173e-04,\n                      4.3831e-04, 5.0591e-04, 6.9300e-04, 4.3469e-04, 7.2185e-04, 5.1210e-04,\n                      5.5561e-04, 4.9253e-04, 7.9620e-04, 5.2337e-04, 6.7165e-04, 4.5820e-04,\n                      5.8744e-04, 6.3979e-04, 4.6932e-04, 5.7612e-04, 3.7089e-04, 5.5601e-04,\n                      6.2547e-04, 3.2587e-04, 4.6057e-04, 5.4400e-04, 6.2706e-04, 4.9324e-04,\n                      4.4327e-04, 5.5211e-04, 7.6020e-04, 3.7769e-04, 5.3740e-04, 5.8065e-04,\n                      6.7324e-04, 5.2167e-04, 4.7182e-04, 5.3257e-04, 6.5309e-04, 5.6292e-04,\n                      5.2356e-04, 4.5123e-04, 5.9545e-04, 3.8252e-04, 8.0345e-06, 1.4069e-05,\n                      6.7427e-04, 4.2966e-04, 5.8325e-04, 4.5622e-04, 7.7346e-04, 4.4481e-04,\n                      8.0062e-04, 5.4592e-04, 7.2446e-04, 5.2462e-04, 5.8844e-04, 6.0098e-04,\n                      4.3961e-04, 5.9524e-04, 5.5816e-04, 5.8995e-04, 3.8360e-04, 4.9217e-04,\n                      4.8605e-04, 4.4889e-04, 6.1410e-04, 5.9697e-04, 5.3531e-04, 5.3884e-04,\n                      8.5835e-04, 5.7608e-04, 3.4853e-04, 6.0050e-04, 6.5660e-04, 6.4491e-04,\n                      5.8024e-04, 5.2438e-04, 5.3810e-04, 4.8530e-04, 6.4382e-04, 4.4465e-04,\n                      4.9739e-04, 6.0413e-04, 4.6963e-04, 4.2064e-04, 5.6876e-04, 8.3122e-04,\n                      5.4619e-04, 4.7526e-04, 5.1339e-04, 5.4692e-04, 5.1097e-04, 5.6147e-04,\n                      5.9867e-04, 8.3916e-04, 6.7684e-04, 6.9852e-04, 5.0203e-04, 5.8568e-04,\n                      2.8801e-04, 6.6720e-04, 5.4286e-04, 6.5380e-04, 5.7110e-04, 5.5164e-04,\n                      7.3550e-04, 4.5215e-04, 6.0895e-04, 5.8355e-04, 5.4074e-04, 6.9974e-04,\n                      5.1540e-04, 5.0632e-04, 5.5241e-04, 5.6363e-04, 5.5961e-04, 2.2073e-04,\n                      8.2365e-04, 5.5552e-04, 5.3983e-04, 6.5843e-04, 8.6409e-04, 5.2467e-04,\n                      4.6232e-04, 5.5414e-04, 4.9104e-04, 7.6132e-04, 1.1903e-03, 6.3542e-04,\n                      4.5536e-04, 4.5644e-04, 6.6361e-04, 5.9869e-04, 4.9551e-04, 5.4495e-04,\n                      4.6167e-04, 4.4560e-04, 5.7161e-04, 4.6202e-04, 4.6593e-04, 4.8572e-04,\n                      5.1022e-04, 6.9018e-04, 6.1490e-04, 5.8990e-04, 6.3902e-04, 4.8020e-04,\n                      7.1545e-04, 5.6890e-04, 8.5278e-04, 4.7281e-04, 3.7444e-04, 6.0411e-04,\n                      4.8813e-04, 5.5276e-04, 5.9626e-04, 4.5969e-04, 4.8681e-04, 6.4721e-04,\n                      3.0270e-04, 5.4133e-04, 5.3962e-04, 5.0117e-04, 5.8228e-04, 7.1189e-04,\n                      2.5274e-04, 2.6379e-04, 3.0337e-04, 3.5173e-04, 5.9641e-04, 4.6066e-04,\n                      4.7551e-04, 4.6017e-04, 4.4468e-04, 4.9293e-04, 3.8429e-04, 4.5920e-04,\n                      8.8558e-04, 5.9835e-04, 3.1763e-04, 6.9241e-04, 5.4406e-04, 6.2829e-04,\n                      6.2303e-04, 5.6675e-04, 4.5409e-04, 8.7912e-04, 5.4270e-04, 5.9577e-04,\n                      5.7789e-04, 5.8362e-04, 3.8321e-04, 3.7402e-04, 5.8277e-04, 5.8805e-04,\n                      3.8892e-04, 5.9335e-04, 4.2373e-04, 7.5935e-04, 7.1536e-04, 4.9811e-04,\n                      5.9561e-04, 8.0526e-04, 5.0260e-04, 2.0847e-04, 5.1227e-04, 5.2123e-04,\n                      5.6099e-04, 4.1684e-04, 4.9831e-04, 3.7981e-04, 4.4790e-04, 7.3899e-04,\n                      7.5894e-04, 4.2736e-04, 6.3887e-04, 5.4208e-04, 3.4443e-04, 5.3259e-04,\n                      6.4941e-04, 4.5606e-04, 6.3138e-04, 5.6967e-04, 5.2770e-04, 5.6832e-04,\n                      5.7514e-04, 6.0058e-04, 5.4902e-04, 2.5366e-04, 4.6664e-04, 5.7972e-04,\n                      6.4370e-04, 4.3598e-04, 3.0499e-04, 8.1730e-04, 5.0641e-04, 6.5003e-04,\n                      4.8510e-04, 9.4825e-04, 5.9269e-04, 7.7079e-04, 4.4586e-04, 4.8205e-04,\n                      4.0175e-04, 7.5797e-04, 5.0490e-04, 6.3870e-04, 7.9596e-04, 6.9006e-04,\n                      7.4953e-04, 6.6012e-04, 5.8331e-04, 5.2388e-04, 5.7270e-04, 5.9729e-04,\n                      5.3342e-04, 4.4549e-04, 6.4112e-04, 4.1045e-04, 4.7355e-04, 5.6094e-04,\n                      5.9808e-04, 5.5237e-04, 4.5162e-04, 6.5889e-04, 5.0821e-04, 1.3179e-03,\n                      6.5926e-04, 5.5262e-04, 7.2236e-04, 3.4177e-04, 7.9578e-04, 3.7471e-04,\n                      3.2415e-04, 7.9344e-04, 7.8152e-04, 5.9847e-04, 6.7552e-04, 4.8856e-04,\n                      6.3079e-04, 6.3655e-04, 5.3349e-04, 7.2965e-04, 7.0718e-04, 5.1000e-04,\n                      5.5647e-04, 6.4272e-04, 5.1539e-04, 6.4636e-04, 6.8254e-04, 6.2816e-04,\n                      4.5597e-04, 3.7069e-04, 4.8389e-04, 4.8537e-04, 5.3392e-04, 6.3173e-04,\n                      8.3317e-06, 4.3254e-04, 5.0573e-04, 4.5507e-04, 5.0145e-04, 6.7036e-04,\n                      4.7403e-04, 6.1745e-04, 7.9925e-04, 5.3133e-04, 1.6316e-05, 4.6542e-04,\n                      8.5933e-04, 5.0545e-04, 3.6952e-04, 5.2099e-04, 6.2819e-04, 6.8335e-04,\n                      5.4893e-04, 4.6986e-04, 6.8934e-04, 8.0606e-04, 5.4401e-04, 5.8836e-04,\n                      6.7112e-04, 7.7387e-04, 7.1418e-04, 4.8623e-04, 4.4848e-04, 5.9756e-04,\n                      9.8286e-04, 4.8218e-04, 5.5157e-04, 5.2691e-04, 3.5742e-04, 5.9295e-04,\n                      4.0918e-04, 4.8889e-04, 6.8168e-04, 3.2879e-04, 7.0806e-04, 5.4964e-04],\n                     dtype=torch.float64),\n                     zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n                     axis=0)),\n             ('mnet.features.13.conv.0.0.bias',\n              Parameter containing:\n              tensor([ 2.1716e-01, -8.0947e-02, -7.1568e-02,  4.5823e-02,  1.6562e-02,\n                       9.0684e-02,  1.0164e-01,  4.0636e-02,  2.3885e-01, -9.1099e-02,\n                       4.7848e-02,  1.8220e-01, -8.2640e-02, -5.2165e-02,  4.6413e-02,\n                      -1.0211e-01, -4.6797e-02,  1.2954e-01,  4.5886e-02, -7.2239e-02,\n                      -5.0296e-02,  7.6178e-02, -9.9411e-02, -9.9322e-02, -8.3476e-02,\n                      -1.7817e-02, -3.8263e-02, -1.8623e-02,  8.4770e-03,  7.2260e-02,\n                       9.6874e-02, -3.9478e-02, -5.6129e-03, -2.1425e-04, -2.4612e-02,\n                       2.9475e-02, -7.4717e-02, -8.4751e-02,  1.0814e-02, -3.2658e-02,\n                       8.5473e-02,  1.4307e-01,  3.3777e-02, -7.5809e-02, -1.5342e-01,\n                      -1.0197e-01, -2.6779e-02,  4.8902e-02, -9.3196e-02,  1.9118e-02,\n                       3.2690e-02, -8.1210e-02, -2.5061e-02,  3.5601e-02, -1.0626e-01,\n                       6.7528e-02,  1.1023e-01, -5.7499e-02,  3.9909e-03,  2.0987e-02,\n                      -1.9691e-02,  2.8259e-02, -1.0368e-01, -1.0336e-01, -9.8385e-02,\n                      -9.0644e-02, -3.1384e-02, -4.4046e-02,  1.5895e-01, -6.9049e-02,\n                       3.7327e-03, -5.5590e-02, -3.4611e-02, -1.0291e-02,  3.7997e-03,\n                       8.8846e-02,  1.9576e-01,  1.0732e-01, -3.7064e-02, -1.1382e-01,\n                      -8.8165e-02,  1.7566e-01,  3.1402e-02,  1.6093e-01, -3.4350e-02,\n                       7.6691e-02,  8.2659e-02, -3.4937e-02,  4.8530e-02, -5.2065e-02,\n                       2.9477e-02,  5.5481e-02, -2.5301e-02, -6.1471e-02, -1.1278e-01,\n                       6.1764e-02,  1.0702e-01, -3.3493e-02, -1.1970e-02, -5.1111e-02,\n                       1.2466e-01, -4.4155e-02, -9.9025e-02,  3.0973e-02,  1.3073e-01,\n                      -8.4049e-02,  1.1520e-01,  2.2072e-02, -4.5322e-02,  3.4718e-02,\n                      -7.1952e-02, -1.7208e-03, -4.2022e-02, -6.2843e-02, -3.4785e-02,\n                      -6.2528e-02,  7.3250e-02,  6.3167e-02, -5.7493e-02, -7.0220e-02,\n                       4.2243e-02, -5.2162e-02, -3.1469e-02,  1.7628e-02,  1.7238e-01,\n                      -1.4147e-01,  1.8465e-02,  5.6377e-02,  6.9682e-02,  7.3391e-02,\n                      -9.5770e-02,  9.5703e-02, -2.2737e-02,  2.0127e-01, -3.9380e-02,\n                      -4.8221e-02,  2.2198e-02, -4.0654e-02,  3.6471e-02,  6.9336e-03,\n                       6.1378e-02, -4.1330e-02, -1.4430e-01, -7.9639e-02,  2.4080e-05,\n                      -2.0751e-01,  3.4315e-02, -2.6188e-02,  6.9926e-02, -6.2821e-02,\n                       2.1951e-02,  9.9877e-02,  4.5049e-02,  3.2197e-02, -7.8763e-02,\n                       7.1437e-02,  4.2641e-02, -1.0784e-01,  8.5723e-02,  2.6764e-02,\n                      -8.8244e-03,  1.0211e-01,  2.6046e-02, -7.4970e-03, -6.4657e-03,\n                      -1.8759e-02, -9.5811e-02, -7.6364e-03,  1.0312e-01,  1.6868e-02,\n                       1.6670e-01,  1.2537e-02, -6.2888e-02, -2.6548e-02,  4.1958e-02,\n                      -1.3900e-01,  5.3258e-03,  4.5614e-04,  1.6364e-02, -7.2797e-02,\n                       2.2823e-02, -9.2747e-02, -2.9927e-02, -7.1540e-03, -2.4546e-02,\n                       8.2714e-02, -5.9219e-02,  7.6868e-02,  1.3586e-01, -2.5240e-02,\n                       1.2546e-01, -1.4605e-02, -1.7529e-01, -4.3247e-02,  2.4028e-02,\n                      -1.6217e-02, -5.5390e-02, -9.2213e-02, -1.2924e-03,  1.3155e-01,\n                      -3.6930e-02, -6.4945e-02, -9.6645e-03, -3.9888e-01, -8.4312e-02,\n                       8.4447e-02, -2.7296e-02,  6.2683e-02,  1.9377e-02,  2.6098e-02,\n                      -7.0840e-02, -6.3611e-02,  2.5843e-02, -6.8355e-02, -9.5355e-02,\n                       3.3229e-02, -3.4589e-02, -1.0549e-01,  8.6805e-02,  1.1054e-01,\n                       3.2992e-03, -1.7523e-03, -1.9667e-03,  4.5147e-02, -1.3706e-02,\n                      -9.3255e-02, -4.4278e-02,  4.0475e-02, -1.4884e-01, -4.7574e-03,\n                       3.6648e-02,  9.5979e-02, -1.0773e-01, -1.1241e-02,  7.9238e-02,\n                       6.9063e-02, -4.1006e-02,  1.0747e-01, -1.0012e-01,  2.0535e-01,\n                       3.1320e-02,  2.0622e-03, -8.5053e-02,  7.1219e-02, -5.8101e-02,\n                      -3.2865e-02,  8.7582e-02,  5.5517e-02, -2.9542e-02, -2.4093e-02,\n                      -1.0357e-01, -9.0225e-03, -4.3863e-02, -9.2587e-02,  2.1289e-02,\n                      -5.9551e-02,  2.6172e-02, -1.7939e-01, -4.5456e-02, -7.9905e-02,\n                       2.5749e-02, -1.0211e-01, -2.8293e-02,  2.6963e-02,  9.3842e-02,\n                       1.0323e-01, -3.7841e-02,  1.1235e-01,  6.4150e-02,  3.1923e-03,\n                       6.6921e-04, -3.4310e-04,  9.3773e-02, -2.3942e-02,  5.6671e-02,\n                      -4.0037e-02, -6.5016e-02, -1.2737e-02, -6.0100e-02,  9.0459e-02,\n                      -1.3456e-02, -2.7811e-02,  2.8401e-02,  5.1654e-02,  1.7660e-01,\n                       5.7458e-02, -1.6373e-01,  1.5234e-01,  2.7175e-02,  1.6663e-01,\n                      -2.7170e-02,  1.0655e-01, -7.4444e-02,  5.8111e-02,  6.1952e-02,\n                      -1.4206e-01,  3.6718e-02,  9.4773e-02,  1.6319e-02, -5.2566e-02,\n                      -6.4340e-03,  3.5101e-02,  5.7439e-02,  1.8250e-02, -4.6803e-02,\n                      -7.2581e-02,  3.7789e-03,  2.1760e-02,  8.9473e-02, -5.3362e-02,\n                       5.2816e-02, -1.2434e-01, -5.9243e-02, -1.7182e-02,  2.4856e-02,\n                      -2.3679e-02, -5.6361e-02, -1.0740e-01, -3.9931e-03, -6.1807e-02,\n                       7.9161e-02,  7.8662e-02,  2.9250e-02, -7.2990e-02,  4.7377e-03,\n                      -3.9761e-02,  2.9868e-02,  5.6794e-02,  8.9943e-02,  1.3214e-01,\n                      -3.2369e-02,  2.5484e-02, -1.3862e-01,  6.0176e-02, -2.7832e-04,\n                      -1.3509e-02, -2.4670e-02, -1.4565e-02,  2.9550e-02, -7.9871e-02,\n                       6.1973e-02,  1.1323e-01, -6.4571e-02,  3.2418e-02, -1.2447e-02,\n                      -5.3232e-02,  3.8962e-02, -4.7634e-02,  8.0395e-02,  7.4165e-02,\n                       3.3571e-02,  1.0077e-02,  1.0501e-02, -2.1101e-02,  4.1461e-02,\n                      -1.4643e-02, -3.7059e-02,  1.3945e-01, -5.1114e-02,  2.9902e-02,\n                      -9.6427e-02, -3.1986e-02, -6.4544e-02, -3.7657e-04, -2.0349e-02,\n                       1.7072e-01,  7.5652e-02, -5.3851e-02,  9.9221e-03, -1.0652e-01,\n                       1.2770e-01, -3.7617e-02,  9.6090e-02,  5.4529e-02, -1.7552e-02,\n                       9.2616e-02,  7.4855e-02, -6.9022e-02,  1.9193e-02, -6.2546e-02,\n                      -2.7771e-02, -3.8863e-02, -6.3439e-02, -5.4608e-02, -2.6021e-02,\n                       8.3848e-02,  1.1306e-01, -8.2624e-02, -7.5017e-02, -1.3271e-02,\n                       9.0404e-02, -1.3296e-01, -1.2398e-02, -4.4817e-02,  4.6831e-02,\n                      -5.6488e-02,  1.3098e-01, -1.2558e-02,  7.4991e-02,  6.6570e-02,\n                      -1.8322e-02,  7.9400e-03,  1.3510e-01, -3.5820e-02,  1.0285e-01,\n                       4.2469e-02,  5.2947e-03,  1.0702e-01, -1.8966e-02,  6.3033e-02,\n                      -2.3483e-02,  9.4709e-02,  1.1459e-01, -4.5369e-02, -1.8089e-02,\n                      -9.4649e-02,  1.0541e-01,  1.2582e-02,  9.6817e-02, -5.7818e-02,\n                      -9.4337e-02, -6.2226e-02, -2.9858e-02,  3.3162e-02,  9.3606e-03,\n                      -1.2486e-01, -2.6612e-02,  1.3473e-01,  7.2882e-02,  2.3485e-02,\n                      -2.4234e-02, -3.2271e-02,  8.6177e-03, -1.7380e-02, -3.8339e-02,\n                      -4.7971e-02,  4.4819e-03,  1.0775e-02, -1.3560e-01,  7.9129e-02,\n                      -9.9212e-04, -1.4062e-02, -2.3496e-02,  6.5539e-02, -3.7206e-02,\n                       2.3945e-02,  7.0115e-02,  3.9610e-02, -1.6724e-01,  6.3995e-03,\n                      -7.9910e-02,  1.4157e-01, -3.9935e-02, -3.5553e-02, -1.3581e-01,\n                      -1.0319e-01,  2.5606e-02,  3.0109e-02, -4.7242e-02,  3.8201e-02,\n                       5.3012e-02, -1.2782e-01, -6.2709e-02, -6.7482e-02,  3.3169e-03,\n                       1.8362e-01,  3.2857e-03, -7.0487e-02, -1.2576e-01, -6.4692e-02,\n                       7.9178e-02, -2.2238e-02,  1.6317e-02, -6.0307e-02,  1.6822e-01,\n                       5.6235e-02,  1.2904e-02,  3.2777e-02, -3.3359e-02, -5.2945e-02,\n                      -1.1050e-01, -2.8864e-02,  3.4306e-02,  2.4532e-01, -2.2543e-02,\n                      -2.3797e-02,  1.9238e-02,  7.2798e-03,  5.7854e-02, -6.4588e-02,\n                      -1.6163e-01, -1.5155e-03, -1.5897e-02, -1.8240e-01,  4.9936e-02,\n                       1.0110e-01,  5.0594e-02, -8.7295e-03,  4.1458e-02, -6.0215e-03,\n                       8.1019e-02,  2.6599e-02, -3.3230e-02,  4.0891e-02, -7.7030e-02,\n                       3.9995e-02, -6.6799e-03, -6.0338e-02, -2.0058e-02, -1.6449e-02,\n                      -2.8765e-02, -4.7590e-02, -1.1996e-01, -6.3530e-02, -2.2560e-02,\n                      -8.2338e-02, -1.5521e-02, -1.1989e-01,  3.0352e-03, -1.3723e-04,\n                      -4.9417e-02, -6.3108e-02,  4.8410e-02, -1.9803e-02, -5.1286e-02,\n                      -3.8321e-02,  1.0989e-01,  5.7763e-02, -1.1061e-01, -1.6842e-02,\n                      -7.2348e-02,  3.1326e-02,  2.9422e-02, -1.3360e-01, -1.4167e-02,\n                       3.0529e-02,  4.4689e-02,  3.4353e-02, -3.3304e-02,  7.4713e-03,\n                      -7.5319e-02, -3.6920e-02, -1.4383e-01, -2.4284e-02, -1.6388e-02,\n                       3.2347e-02,  2.5835e-03, -3.2220e-02,  9.9877e-02,  1.2557e-01,\n                      -2.3911e-02, -1.4696e-03,  4.7109e-02, -2.1117e-02, -1.8245e-02,\n                      -3.4114e-02,  3.2044e-02, -3.6911e-02, -8.8228e-03,  6.7280e-03,\n                      -1.3172e-01,  2.1023e-02,  1.2267e-01, -6.0654e-02,  2.8398e-02,\n                      -1.4964e-01, -8.5710e-02,  1.1699e-01, -9.6887e-02, -1.7665e-01,\n                      -9.6690e-03,  3.9036e-02, -3.7245e-02,  9.3314e-02, -3.9205e-02,\n                      -1.1910e-01], requires_grad=True)),\n             ('mnet.features.13.conv.0.0.scale', tensor(0.0216)),\n             ('mnet.features.13.conv.0.0.zero_point', tensor(57)),\n             ('mnet.features.13.conv.1.0.weight',\n              tensor([[[[-0.4000, -0.5214, -0.4714],\n                        [-0.8357, -0.9143, -0.6785],\n                        [-0.2428, -0.3714, -0.4428]]],\n              \n              \n                      [[[ 1.2018,  0.9488,  1.1807],\n                        [ 0.7169, -2.6988,  0.6536],\n                        [-0.4849, -0.3795, -0.5904]]],\n              \n              \n                      [[[-0.1020,  0.3060, -0.0728],\n                        [ 0.3060,  1.8503,  0.3642],\n                        [-0.1894,  0.5245, -0.1457]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-0.4630, -0.4449, -0.4329],\n                        [-0.3668, -0.7636, -0.4089],\n                        [-0.7275, -0.4149, -0.6554]]],\n              \n              \n                      [[[-0.1673,  0.0558, -0.1533],\n                        [ 0.3206,  1.7705,  0.2091],\n                        [ 0.1255,  0.9619,  0.1673]]],\n              \n              \n                      [[[ 0.3592,  0.4951,  0.4757],\n                        [ 0.9126,  0.9708,  0.8446],\n                        [ 0.1456,  1.2329,  0.3980]]]], size=(576, 1, 3, 3),\n                     dtype=torch.qint8, quantization_scheme=torch.per_channel_affine,\n                     scale=tensor([0.0071, 0.0211, 0.0146, 0.0041, 0.0135, 0.0048, 0.0222, 0.0043, 0.0125,\n                      0.0244, 0.0059, 0.0096, 0.0123, 0.0363, 0.0055, 0.0213, 0.0131, 0.0150,\n                      0.0036, 0.0121, 0.0160, 0.0160, 0.0235, 0.0190, 0.0119, 0.0005, 0.0174,\n                      0.0105, 0.0172, 0.0156, 0.0057, 0.0170, 0.0111, 0.0187, 0.0048, 0.0193,\n                      0.0222, 0.0290, 0.0152, 0.0080, 0.0183, 0.0074, 0.0159, 0.0160, 0.0263,\n                      0.0186, 0.0120, 0.0097, 0.0088, 0.0074, 0.0259, 0.0216, 0.0091, 0.0042,\n                      0.0126, 0.0181, 0.0083, 0.0152, 0.0042, 0.0179, 0.0046, 0.0142, 0.0223,\n                      0.0228, 0.0159, 0.0195, 0.0107, 0.0114, 0.0061, 0.0155, 0.0158, 0.0111,\n                      0.0139, 0.0135, 0.0050, 0.0263, 0.0057, 0.0127, 0.0097, 0.0339, 0.0176,\n                      0.0088, 0.0188, 0.0234, 0.0160, 0.0054, 0.0044, 0.0124, 0.0035, 0.0081,\n                      0.0186, 0.0083, 0.0122, 0.0123, 0.0235, 0.0179, 0.0095, 0.0095, 0.0084,\n                      0.0035, 0.0144, 0.0189, 0.0100, 0.0036, 0.0083, 0.0047, 0.0060, 0.0047,\n                      0.0208, 0.0197, 0.0234, 0.0047, 0.0134, 0.0089, 0.0214, 0.0187, 0.0043,\n                      0.0041, 0.0154, 0.0116, 0.0150, 0.0155, 0.0011, 0.0038, 0.0103, 0.0256,\n                      0.0132, 0.0185, 0.0230, 0.0036, 0.0268, 0.0054, 0.0014, 0.0151, 0.0200,\n                      0.0182, 0.0115, 0.0145, 0.0184, 0.0164, 0.0186, 0.0250, 0.0167, 0.0235,\n                      0.0158, 0.0441, 0.0064, 0.0277, 0.0152, 0.0092, 0.0069, 0.0225, 0.0124,\n                      0.0040, 0.0125, 0.0041, 0.0059, 0.0127, 0.0191, 0.0226, 0.0056, 0.0061,\n                      0.0111, 0.0180, 0.0233, 0.0042, 0.0137, 0.0173, 0.0147, 0.0138, 0.0139,\n                      0.0048, 0.0039, 0.0176, 0.0048, 0.0396, 0.0053, 0.0042, 0.0043, 0.0119,\n                      0.0086, 0.0240, 0.0097, 0.0036, 0.0033, 0.0214, 0.0168, 0.0150, 0.0241,\n                      0.0139, 0.0061, 0.0195, 0.0304, 0.0128, 0.0044, 0.0158, 0.0134, 0.0159,\n                      0.0162, 0.0314, 0.0137, 0.0185, 0.0045, 0.0072, 0.0293, 0.0139, 0.0104,\n                      0.0039, 0.0123, 0.0161, 0.0228, 0.0047, 0.0047, 0.0147, 0.0185, 0.0075,\n                      0.0184, 0.0144, 0.0136, 0.0055, 0.0055, 0.0152, 0.0047, 0.0096, 0.0030,\n                      0.0745, 0.0094, 0.0075, 0.0266, 0.0176, 0.0075, 0.0206, 0.0121, 0.0036,\n                      0.0078, 0.0052, 0.0133, 0.0139, 0.0142, 0.0140, 0.0124, 0.0165, 0.0265,\n                      0.0042, 0.0146, 0.0124, 0.0077, 0.0155, 0.0101, 0.0051, 0.0174, 0.0141,\n                      0.0088, 0.0110, 0.0135, 0.0149, 0.0169, 0.0314, 0.0117, 0.0296, 0.0195,\n                      0.0152, 0.0247, 0.0055, 0.0200, 0.0045, 0.0053, 0.0048, 0.0046, 0.0087,\n                      0.0055, 0.0186, 0.0199, 0.0079, 0.0051, 0.0163, 0.0218, 0.0139, 0.0156,\n                      0.0148, 0.0003, 0.0017, 0.0070, 0.0058, 0.0055, 0.0104, 0.0084, 0.0081,\n                      0.0210, 0.0096, 0.0048, 0.0083, 0.0165, 0.0047, 0.0145, 0.0233, 0.0202,\n                      0.0129, 0.0057, 0.0200, 0.0169, 0.0128, 0.0048, 0.0220, 0.0181, 0.0128,\n                      0.0072, 0.0057, 0.0164, 0.0126, 0.0053, 0.0268, 0.0187, 0.0143, 0.0048,\n                      0.0192, 0.0055, 0.0272, 0.0169, 0.0165, 0.0160, 0.0059, 0.0161, 0.0212,\n                      0.0186, 0.0126, 0.0065, 0.0049, 0.0060, 0.0164, 0.0053, 0.0098, 0.0328,\n                      0.0108, 0.0051, 0.0032, 0.0257, 0.0048, 0.0143, 0.0152, 0.0228, 0.0128,\n                      0.0155, 0.0043, 0.0040, 0.0181, 0.0143, 0.0138, 0.0056, 0.0038, 0.0055,\n                      0.0113, 0.0205, 0.0263, 0.0101, 0.0173, 0.0113, 0.0159, 0.0039, 0.0072,\n                      0.0359, 0.0127, 0.0217, 0.0091, 0.0083, 0.0150, 0.0074, 0.0084, 0.0157,\n                      0.0092, 0.0134, 0.0172, 0.0198, 0.0133, 0.0115, 0.0053, 0.0154, 0.0129,\n                      0.0109, 0.0132, 0.0178, 0.0142, 0.0170, 0.0186, 0.0047, 0.0080, 0.0042,\n                      0.0223, 0.0120, 0.0081, 0.0091, 0.0182, 0.0203, 0.0040, 0.0129, 0.0037,\n                      0.0066, 0.0105, 0.0151, 0.0213, 0.0053, 0.0146, 0.0204, 0.0224, 0.0128,\n                      0.0115, 0.0084, 0.0169, 0.0114, 0.0170, 0.0106, 0.0044, 0.0132, 0.0191,\n                      0.0038, 0.0170, 0.0250, 0.0102, 0.0059, 0.0093, 0.0214, 0.0169, 0.0105,\n                      0.0043, 0.0143, 0.0158, 0.0237, 0.0109, 0.0110, 0.0184, 0.0057, 0.0221,\n                      0.0118, 0.0057, 0.0170, 0.0122, 0.0142, 0.0150, 0.0139, 0.0086, 0.0054,\n                      0.0315, 0.0145, 0.0174, 0.0095, 0.0139, 0.0057, 0.0154, 0.0308, 0.0043,\n                      0.0110, 0.0142, 0.0079, 0.0051, 0.0223, 0.0137, 0.0033, 0.0055, 0.0121,\n                      0.0154, 0.0195, 0.0168, 0.0109, 0.0217, 0.0110, 0.0142, 0.0173, 0.0073,\n                      0.0166, 0.0154, 0.0162, 0.0059, 0.0054, 0.0194, 0.0129, 0.0167, 0.0104,\n                      0.0063, 0.0182, 0.0230, 0.0247, 0.0045, 0.0102, 0.0119, 0.0030, 0.0094,\n                      0.0101, 0.0134, 0.0112, 0.0181, 0.0444, 0.0041, 0.0060, 0.0344, 0.0206,\n                      0.0125, 0.0167, 0.0096, 0.0052, 0.0193, 0.0225, 0.0040, 0.0119, 0.0100,\n                      0.0241, 0.0095, 0.0070, 0.0343, 0.0145, 0.0128, 0.0240, 0.0082, 0.0074,\n                      0.0121, 0.0156, 0.0162, 0.0132, 0.0142, 0.0140, 0.0138, 0.0037, 0.0223,\n                      0.0172, 0.0104, 0.0213, 0.0226, 0.0042, 0.0138, 0.0233, 0.0105, 0.0166,\n                      0.0132, 0.0050, 0.0113, 0.0006, 0.0150, 0.0047, 0.0201, 0.0155, 0.0054,\n                      0.0194, 0.0230, 0.0052, 0.0076, 0.0006, 0.0123, 0.0052, 0.0133, 0.0064,\n                      0.0098, 0.0084, 0.0115, 0.0071, 0.0110, 0.0122, 0.0135, 0.0040, 0.0037,\n                      0.0154, 0.0096, 0.0261, 0.0044, 0.0132, 0.0113, 0.0075, 0.0292, 0.0178,\n                      0.0136, 0.0244, 0.0223, 0.0156, 0.0082, 0.0053, 0.0060, 0.0139, 0.0097],\n                     dtype=torch.float64),\n                     zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n                     axis=0)),\n             ('mnet.features.13.conv.1.0.bias',\n              Parameter containing:\n              tensor([ 8.4997e-01,  1.7974e-01, -1.6785e-01,  2.8109e-01, -3.9656e-01,\n                       3.2297e-01, -6.0722e-02,  2.4013e-01,  1.0666e+00, -1.5279e-01,\n                       3.3694e-01,  2.1243e-01,  2.1255e-01, -3.2906e-01,  2.9921e-01,\n                      -1.7349e-01, -3.5829e-01, -1.1270e-01,  2.6886e-01, -7.4133e-02,\n                      -2.7233e-01, -2.4582e-02, -7.7920e-02,  6.5998e-02, -1.3465e-01,\n                      -1.0108e-02, -2.0719e-02,  2.7379e-01, -1.9984e-02, -1.5141e-01,\n                       4.3830e-01, -8.5661e-02,  1.3346e-01, -1.6046e-01,  2.7687e-01,\n                      -7.5400e-02, -1.3070e-01, -5.5151e-02, -1.6577e-01,  2.4996e-01,\n                      -1.5345e-01,  4.1174e-01, -1.9509e-01, -2.5288e-01, -4.5549e-02,\n                      -3.0831e-02,  1.0925e-01, -3.3040e-01, -7.1596e-02,  1.8856e-01,\n                      -4.0834e-01, -3.6974e-01,  1.5576e-01,  2.6894e-01, -4.0208e-02,\n                       1.7147e-01, -4.7266e-01, -2.4887e-01,  2.5133e-01, -3.4900e-01,\n                       2.2207e-01, -3.4258e-01, -1.4582e-01, -6.7596e-02, -1.8028e-01,\n                      -2.1887e-01,  8.0317e-02, -2.9212e-01,  5.3354e-01, -1.8722e-01,\n                       4.4342e-02, -1.1753e-01, -2.0853e-01, -6.8260e-03,  2.9376e-01,\n                      -1.4714e-01,  6.7135e-01, -1.0390e-01, -2.8347e-03, -7.2000e-02,\n                      -2.6627e-01,  6.5738e-01, -4.5006e-01, -8.1705e-02, -1.6239e-02,\n                       3.3832e-01,  3.2658e-01, -2.0303e-01,  2.6742e-01, -7.4932e-02,\n                      -2.4333e-01,  1.9526e-01, -1.6430e-01, -1.2749e-01, -1.7062e-02,\n                      -7.4527e-02, -6.1057e-01,  3.4861e-01,  2.4128e-01,  2.7327e-01,\n                       2.6927e-01, -2.3791e-01,  2.2638e-01,  2.6960e-01,  7.4059e-04,\n                       3.8437e-01,  4.7050e-01,  2.6877e-01, -1.8575e-01,  7.5019e-03,\n                      -1.6527e-01,  2.6220e-01, -2.0148e-01, -2.1121e-03, -1.7137e-01,\n                      -1.3860e-01,  3.3926e-01,  3.2739e-01, -2.1230e-01, -4.8516e-02,\n                       5.9429e-02, -1.8853e-01, -8.6876e-04,  3.1210e-01, -4.7487e-01,\n                       2.2911e-01, -2.3426e-01,  3.6647e-01, -1.2732e-01,  2.6851e-01,\n                      -4.6842e-02,  3.3955e-01, -1.4515e-02,  3.1820e-01, -1.0921e-01,\n                      -3.1437e-01,  2.3645e-01, -1.7341e-01, -7.4748e-01, -2.5402e-02,\n                       3.2386e-01, -1.7912e-01, -2.4363e-02, -2.2956e-01, -2.2618e-01,\n                      -2.8992e-01,  1.7521e-01, -1.8543e-01, -9.1047e-02, -1.2276e-01,\n                       2.7783e-01, -5.5077e-02, -4.7689e-02,  2.3764e-01,  7.9434e-02,\n                       3.3005e-01,  2.4191e-01, -7.6438e-02, -4.9931e-01, -5.0976e-01,\n                       2.8549e-01,  4.0901e-01,  9.1951e-02, -2.6603e-01, -2.6491e-01,\n                       2.6231e-01, -7.7406e-02, -2.8267e-01, -4.2720e-02, -9.8061e-02,\n                      -1.5483e-01,  2.7647e-01,  2.4320e-01, -2.0497e-01,  2.5284e-01,\n                      -2.1921e-01,  2.4397e-01,  2.6125e-01,  2.9436e-01, -1.3017e-01,\n                       2.0959e-01, -1.9520e-01,  1.7582e-01,  2.2020e-01,  2.6089e-01,\n                      -5.1365e-01, -4.9489e-02, -2.7116e-02, -2.9387e-01,  2.8094e-01,\n                       4.5368e-01, -8.0807e-02, -1.1657e-01,  1.4109e-01,  2.6182e-01,\n                      -2.5856e-01, -2.7728e-01, -7.1859e-02, -2.8906e-01, -5.8346e-01,\n                      -1.4131e-01, -1.7245e-01,  2.7222e-01, -1.9791e-01, -2.1642e-01,\n                      -1.0787e-01, -2.7005e-01,  3.4772e-01,  2.5515e-03, -3.1748e-02,\n                      -1.1393e-01,  2.6543e-01,  3.0975e-01,  2.3965e-01, -1.7026e-01,\n                       2.3371e-01, -2.6499e-01, -9.9336e-02, -7.7231e-02,  4.0533e-01,\n                       2.6456e-01,  6.3030e-02,  2.6492e-01,  2.0291e-02,  2.6562e-01,\n                      -3.5856e-01, -1.4212e-01,  3.4316e-01, -2.0741e-01, -1.7793e-01,\n                       3.0050e-01, -5.8903e-02, -7.8428e-02,  2.6543e-01,  2.4840e-01,\n                       3.1619e-01, -1.4994e-02, -3.1902e-02,  4.7715e-03,  4.3818e-01,\n                      -2.7151e-01, -8.7743e-02, -3.8246e-01,  2.6296e-01, -1.8850e-01,\n                      -8.1489e-02,  3.0627e-01, -2.7251e-01,  2.4578e-01,  2.5235e-01,\n                      -1.7037e-01, -1.0070e-01, -1.1942e-01,  2.3428e-01,  1.0525e-01,\n                      -8.6744e-02, -3.0793e-01, -1.5520e-01, -2.2089e-01, -9.7421e-02,\n                      -1.8922e-02, -2.7524e-02, -1.4358e-01,  2.8171e-01, -1.5329e-01,\n                       3.3236e-01,  4.3249e-01,  3.2513e-01,  2.9525e-01,  1.9004e-01,\n                       2.6382e-01, -3.2369e-01,  6.8353e-02,  3.3360e-01,  2.9279e-01,\n                      -1.8509e-01, -1.8247e-01, -2.6791e-01,  1.6215e-01, -9.0010e-02,\n                      -2.2828e-03, -2.4900e-02, -2.6145e-01,  3.0992e-01,  5.0719e-01,\n                      -7.4398e-02,  2.6553e-01,  5.4653e-01, -9.8269e-01,  5.0546e-01,\n                       3.2184e-01,  3.9135e-01, -1.3020e-01,  3.3721e-01, -1.2663e-01,\n                      -3.2372e-01,  3.0766e-01, -9.7146e-02,  2.8658e-01, -3.4041e-02,\n                      -2.9069e-01,  2.8723e-01,  3.0636e-01,  3.8859e-01, -2.6603e-01,\n                      -6.2628e-02,  3.4812e-01,  2.3444e-01, -3.6434e-02,  1.5102e-01,\n                       2.8621e-01, -2.6305e-01, -2.2356e-02, -9.0263e-02,  2.2578e-01,\n                      -7.9139e-02,  2.3812e-01, -1.8238e-01, -3.3900e-01, -2.1623e-01,\n                      -6.3001e-02,  3.3869e-01, -2.2155e-02, -6.4093e-02, -2.7393e-01,\n                       3.1167e-03,  2.8603e-01,  3.8781e-01,  3.1778e-01, -8.3907e-01,\n                       3.6828e-01, -1.0054e-01, -2.1369e-01, -1.9953e-01,  2.3176e-01,\n                       2.4267e-01, -2.9344e-01,  2.4756e-01, -1.4033e-01, -2.1323e-01,\n                       1.2350e-01,  1.6681e-01,  7.9823e-02,  2.3224e-01,  3.4260e-01,\n                       2.1453e-01, -1.1729e-02, -1.1055e-01,  3.5921e-01,  2.7683e-01,\n                       2.9055e-01,  1.3141e-01, -1.4246e-01, -3.7373e-01,  1.1877e-01,\n                      -3.5844e-01, -1.9987e-01,  4.2187e-01,  2.5910e-01, -1.2238e-01,\n                      -6.9466e-02, -2.8298e-01, -1.9488e-01, -1.9158e-01, -9.5206e-02,\n                       2.5098e-01,  3.6314e-01,  2.0566e-01, -7.1752e-02, -3.4557e-02,\n                       1.5686e-01, -1.7852e-01, -2.7224e-01, -7.6740e-02, -2.8699e-01,\n                       3.3962e-01, -4.5401e-01, -1.0621e-01, -2.0248e-01, -1.6939e-01,\n                      -1.2493e-01, -2.2042e-01, -2.6784e-01, -1.0791e-01,  2.2637e-01,\n                       1.9783e-01,  3.7342e-01, -2.0095e-01,  1.2544e-03,  2.5639e-01,\n                      -2.1292e-01,  1.4200e-01, -6.1245e-02,  2.1467e-01, -2.1689e-02,\n                       2.4910e-01,  5.7468e-01, -6.8468e-02, -3.2620e-01, -1.5001e-01,\n                       2.2137e-01,  2.3108e-01,  2.2886e-01, -2.1713e-01, -1.5111e-02,\n                       9.1806e-02,  2.2389e-01, -1.3420e-01, -5.4765e-02, -1.7071e-01,\n                       1.3330e-01,  3.4891e-01, -7.9892e-02, -1.1809e-01,  2.6963e-01,\n                      -1.2393e-01, -6.2134e-01, -2.1830e-01,  3.6009e-01, -1.4173e-01,\n                       2.4261e-01, -1.3526e-01, -1.0774e-01,  2.5510e-01, -3.5867e-01,\n                       1.1489e-01, -4.1496e-01, -1.6962e-01, -5.4321e-02, -5.2495e-01,\n                       2.4020e-01, -1.7796e-01,  3.5343e-03,  2.1148e-01, -1.0689e-01,\n                       2.4570e-01, -5.0371e-01, -5.2696e-02, -1.4153e-02, -4.4700e-02,\n                       2.5819e-01, -2.3250e-01, -2.9575e-01, -4.5459e-02, -1.1723e-01,\n                      -6.2327e-02,  3.7183e-01, -9.9377e-02, -2.0700e-01,  2.1621e-01,\n                      -4.7649e-01, -4.8707e-02, -2.2644e-01,  2.9616e-01, -8.4902e-02,\n                       7.6779e-03,  2.6706e-01,  2.8712e-01,  1.4915e-01, -1.2519e-04,\n                      -8.8887e-02, -1.2064e-01,  2.8813e-01, -8.2539e-02, -2.5585e-01,\n                       8.2363e-01, -1.5816e-02, -2.1701e-01,  1.0116e-01, -7.2999e-02,\n                      -4.2413e-02,  3.3260e-01,  2.6290e-01, -6.9857e-02,  6.3347e-01,\n                      -5.9596e-01,  3.1418e-01,  3.1977e-01, -9.9587e-02, -1.6916e-02,\n                      -6.4516e-02,  2.5389e-01,  1.8140e-01,  4.2930e-01,  3.2130e-01,\n                       2.7175e-01,  1.7510e-01, -3.1352e-02,  1.4013e-01, -2.3051e-01,\n                      -1.2627e-01,  2.3659e-01,  2.3204e-01, -2.1281e-01,  3.8073e-01,\n                      -1.5334e-01, -6.5286e-02, -8.5633e-02,  2.4526e-01, -2.4006e-01,\n                      -2.5051e-01,  2.4642e-01, -2.3646e-01,  3.3801e-01, -4.7868e-03,\n                      -2.6580e-01,  2.9120e-01, -3.1323e-01, -4.5159e-01, -2.2836e-01,\n                      -2.5392e-01,  2.6657e-01, -5.6495e-03, -9.9286e-02, -3.5188e-01,\n                      -2.0416e-01, -1.9721e-01,  5.2059e-02, -1.4376e-02, -3.5284e-02,\n                       3.5373e-01, -2.2734e-01,  2.5064e-01, -1.9519e-01, -1.2242e-01,\n                      -2.4692e-01,  3.4010e-01, -2.8916e-01, -6.5566e-02, -2.4160e-01,\n                      -1.4814e-01, -9.8754e-03,  2.5530e-01, -5.4531e-02, -3.9497e-03,\n                      -9.0443e-02,  2.6584e-01, -1.5298e-01, -6.4299e-02,  2.2694e-01,\n                      -1.5145e-01, -6.2117e-02,  3.4393e-01, -2.8992e-01, -1.6200e-02,\n                      -4.6515e-02,  3.0224e-01, -1.0781e-01,  4.5104e-01,  9.8293e-02,\n                       2.6163e-01, -3.0720e-01,  2.9720e-01, -1.6725e-01, -3.7091e-01,\n                      -8.6914e-02,  3.0422e-01,  2.3840e-01, -3.0042e-01, -1.0980e-02,\n                      -8.5263e-02,  2.7622e-01,  7.2904e-02, -8.7635e-02,  2.1557e-01,\n                      -1.6747e-01,  2.3779e-01, -5.7996e-03, -7.0866e-02, -6.0445e-02,\n                      -5.7719e-02,  2.6534e-01,  2.4628e-01,  4.1018e-01, -2.0695e-01,\n                      -8.3786e-02], requires_grad=True)),\n             ('mnet.features.13.conv.1.0.scale', tensor(0.0432)),\n             ('mnet.features.13.conv.1.0.zero_point', tensor(56)),\n             ('mnet.features.13.conv.2.weight',\n              tensor([[[[-0.1247]],\n              \n                       [[ 0.1702]],\n              \n                       [[-0.2973]],\n              \n                       ...,\n              \n                       [[-0.0216]],\n              \n                       [[-0.1319]],\n              \n                       [[ 0.0935]]],\n              \n              \n                      [[[ 0.0623]],\n              \n                       [[ 0.0882]],\n              \n                       [[-0.0363]],\n              \n                       ...,\n              \n                       [[ 0.0830]],\n              \n                       [[ 0.1661]],\n              \n                       [[ 0.0675]]],\n              \n              \n                      [[[ 0.0131]],\n              \n                       [[ 0.0261]],\n              \n                       [[ 0.0294]],\n              \n                       ...,\n              \n                       [[ 0.1175]],\n              \n                       [[-0.0229]],\n              \n                       [[ 0.0457]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-0.1562]],\n              \n                       [[ 0.1414]],\n              \n                       [[ 0.2046]],\n              \n                       ...,\n              \n                       [[ 0.1042]],\n              \n                       [[ 0.0149]],\n              \n                       [[-0.0558]]],\n              \n              \n                      [[[-0.0143]],\n              \n                       [[ 0.0286]],\n              \n                       [[ 0.0787]],\n              \n                       ...,\n              \n                       [[-0.0930]],\n              \n                       [[-0.0644]],\n              \n                       [[-0.0715]]],\n              \n              \n                      [[[-0.0832]],\n              \n                       [[ 0.0863]],\n              \n                       [[ 0.3916]],\n              \n                       ...,\n              \n                       [[-0.0401]],\n              \n                       [[ 0.1449]],\n              \n                       [[ 0.0956]]]], size=(96, 576, 1, 1), dtype=torch.qint8,\n                     quantization_scheme=torch.per_channel_affine,\n                     scale=tensor([0.0024, 0.0052, 0.0033, 0.0024, 0.0035, 0.0054, 0.0029, 0.0031, 0.0024,\n                      0.0045, 0.0028, 0.0031, 0.0026, 0.0021, 0.0035, 0.0055, 0.0032, 0.0028,\n                      0.0038, 0.0067, 0.0030, 0.0039, 0.0034, 0.0031, 0.0053, 0.0035, 0.0027,\n                      0.0030, 0.0025, 0.0025, 0.0030, 0.0030, 0.0083, 0.0033, 0.0029, 0.0046,\n                      0.0028, 0.0034, 0.0028, 0.0034, 0.0020, 0.0040, 0.0037, 0.0027, 0.0026,\n                      0.0028, 0.0028, 0.0028, 0.0044, 0.0036, 0.0018, 0.0029, 0.0019, 0.0035,\n                      0.0083, 0.0047, 0.0031, 0.0020, 0.0041, 0.0027, 0.0032, 0.0037, 0.0033,\n                      0.0054, 0.0025, 0.0024, 0.0028, 0.0033, 0.0027, 0.0034, 0.0055, 0.0047,\n                      0.0033, 0.0031, 0.0028, 0.0049, 0.0097, 0.0037, 0.0020, 0.0020, 0.0035,\n                      0.0055, 0.0055, 0.0030, 0.0030, 0.0027, 0.0080, 0.0036, 0.0026, 0.0030,\n                      0.0045, 0.0030, 0.0030, 0.0037, 0.0072, 0.0031], dtype=torch.float64),\n                     zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n                     axis=0)),\n             ('mnet.features.13.conv.2.bias',\n              Parameter containing:\n              tensor([ 0.0577, -0.0591,  0.4164, -0.2860,  0.2794, -0.0217,  0.1551,  0.5570,\n                      -0.1032,  0.1907, -0.1639,  1.0602, -0.1046,  0.2962,  0.0649,  0.1847,\n                       0.7214, -0.1677,  0.0759, -0.6898,  0.1858, -0.8483,  0.0596, -0.2591,\n                      -0.6246,  0.0572,  0.3753,  0.6922,  0.2114, -0.2404,  0.2154,  0.0718,\n                       0.6823, -0.4250,  0.1424, -0.3552,  0.0737,  0.0766,  0.0920, -0.7666,\n                      -0.0459, -0.3374,  0.0593,  0.1404,  0.1497, -0.1780, -0.1336,  0.1625,\n                      -0.3668,  0.7818,  0.1083,  0.4155,  0.0642,  0.0835,  0.2192, -0.0077,\n                       0.0361, -0.2545, -0.9555, -0.3494,  0.8695, -0.0555, -0.2657,  0.5212,\n                       0.1201, -0.5948,  0.0307,  0.2389,  0.0638,  0.4077, -0.4163, -0.1865,\n                      -0.4970, -0.1431, -0.2153,  0.2967, -0.5490,  0.5626, -0.1782, -0.1300,\n                      -0.4153,  0.4274,  0.2145,  0.9921, -0.0499, -0.2878, -1.0644, -0.0020,\n                       0.1658, -0.0247, -0.8592, -0.1431,  0.7086, -0.2590,  0.4424, -0.1010],\n                     requires_grad=True)),\n             ('mnet.features.13.conv.2.scale', tensor(0.0968)),\n             ('mnet.features.13.conv.2.zero_point', tensor(66)),\n             ('mnet.features.14.conv.0.0.weight',\n              tensor([[[[ 0.0132]],\n              \n                       [[ 0.0214]],\n              \n                       [[-0.0163]],\n              \n                       ...,\n              \n                       [[ 0.0190]],\n              \n                       [[-0.0031]],\n              \n                       [[-0.0089]]],\n              \n              \n                      [[[-0.0063]],\n              \n                       [[ 0.0411]],\n              \n                       [[ 0.0045]],\n              \n                       ...,\n              \n                       [[ 0.0250]],\n              \n                       [[ 0.0125]],\n              \n                       [[ 0.0134]]],\n              \n              \n                      [[[ 0.0336]],\n              \n                       [[ 0.0148]],\n              \n                       [[ 0.0369]],\n              \n                       ...,\n              \n                       [[ 0.0607]],\n              \n                       [[ 0.0385]],\n              \n                       [[ 0.0041]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-0.0754]],\n              \n                       [[-0.0167]],\n              \n                       [[ 0.0094]],\n              \n                       ...,\n              \n                       [[ 0.0167]],\n              \n                       [[-0.0116]],\n              \n                       [[-0.0123]]],\n              \n              \n                      [[[ 0.0213]],\n              \n                       [[ 0.0373]],\n              \n                       [[ 0.0258]],\n              \n                       ...,\n              \n                       [[ 0.0068]],\n              \n                       [[ 0.0403]],\n              \n                       [[-0.0547]]],\n              \n              \n                      [[[-0.0013]],\n              \n                       [[ 0.0599]],\n              \n                       [[-0.0380]],\n              \n                       ...,\n              \n                       [[-0.0019]],\n              \n                       [[ 0.0315]],\n              \n                       [[ 0.0026]]]], size=(576, 96, 1, 1), dtype=torch.qint8,\n                     quantization_scheme=torch.per_channel_affine,\n                     scale=tensor([3.8855e-04, 8.9337e-04, 8.1967e-04, 8.3004e-04, 5.5780e-04, 4.3281e-04,\n                      4.7303e-04, 6.8456e-04, 7.9206e-04, 7.2421e-04, 5.8324e-04, 5.5094e-04,\n                      5.6011e-04, 4.4745e-04, 6.7806e-04, 6.6724e-04, 3.8162e-04, 5.4928e-04,\n                      6.2382e-04, 8.7799e-04, 5.0241e-04, 7.6213e-04, 7.2445e-04, 6.0699e-04,\n                      5.0944e-04, 2.4072e-04, 6.1495e-04, 6.0726e-04, 7.3790e-04, 6.3439e-04,\n                      6.0372e-04, 5.4826e-04, 8.8451e-04, 8.1779e-04, 9.2213e-04, 8.4724e-04,\n                      6.4281e-04, 6.2268e-04, 8.0025e-04, 7.6777e-04, 4.1106e-04, 5.2336e-04,\n                      5.6614e-04, 9.6327e-04, 7.2489e-04, 5.5349e-04, 5.8105e-04, 5.3640e-04,\n                      6.2436e-04, 6.3433e-04, 5.8005e-04, 7.1866e-04, 5.3080e-04, 6.2410e-04,\n                      6.1554e-04, 5.9864e-04, 6.3684e-04, 5.5769e-04, 9.0720e-04, 4.5371e-04,\n                      5.6647e-04, 9.8695e-04, 5.6387e-04, 4.9338e-04, 4.1136e-04, 6.3129e-04,\n                      6.7667e-04, 7.3446e-04, 5.4203e-04, 7.6182e-04, 4.5340e-04, 6.2868e-04,\n                      6.4927e-04, 5.3766e-04, 5.4457e-04, 7.0825e-04, 6.5135e-04, 8.5609e-04,\n                      2.3564e-04, 5.1065e-04, 5.4145e-04, 5.2419e-04, 7.4213e-04, 6.4518e-04,\n                      8.9382e-04, 6.2792e-04, 5.8422e-04, 7.2956e-04, 4.8816e-04, 4.9457e-04,\n                      5.0502e-04, 7.7793e-04, 9.1485e-04, 8.9953e-04, 5.3260e-04, 6.7910e-04,\n                      4.3500e-04, 4.5947e-04, 6.6205e-04, 7.5624e-04, 9.2203e-04, 7.5075e-04,\n                      8.8191e-04, 1.0360e-03, 6.2231e-04, 9.2800e-04, 7.1602e-04, 5.7905e-04,\n                      1.0884e-03, 6.0713e-04, 6.0938e-04, 8.5696e-04, 8.0080e-04, 4.2995e-04,\n                      5.5000e-04, 8.2973e-04, 8.1483e-04, 7.4254e-04, 7.4082e-04, 9.2242e-04,\n                      7.5677e-04, 2.6364e-04, 8.0750e-04, 5.1352e-04, 5.1790e-04, 4.9834e-04,\n                      1.0119e-03, 4.9690e-04, 5.8492e-04, 4.4023e-04, 4.8362e-04, 7.6040e-04,\n                      7.5822e-04, 6.2421e-04, 3.9043e-04, 2.7599e-04, 5.7194e-04, 7.9476e-04,\n                      1.1149e-03, 5.2745e-04, 6.5036e-04, 6.9361e-04, 3.0266e-04, 5.6402e-04,\n                      4.1647e-04, 5.8950e-04, 5.5167e-04, 2.4612e-04, 6.4302e-04, 7.0334e-04,\n                      5.0560e-04, 4.8299e-04, 6.4477e-04, 5.8051e-04, 7.4651e-04, 5.7223e-04,\n                      6.1329e-04, 9.5129e-04, 7.4877e-04, 6.3232e-04, 4.9482e-04, 7.3743e-04,\n                      6.0776e-04, 5.4851e-04, 1.1960e-03, 7.1687e-04, 4.5874e-04, 5.9322e-04,\n                      5.9256e-04, 5.7842e-04, 5.4967e-04, 1.0614e-03, 5.6815e-04, 6.3692e-04,\n                      5.2857e-04, 6.5184e-04, 5.3218e-04, 5.8370e-04, 5.3911e-04, 4.8672e-04,\n                      2.5110e-04, 7.4523e-04, 6.8794e-04, 6.9717e-04, 7.0496e-04, 6.1444e-04,\n                      6.5374e-04, 5.1580e-04, 7.8633e-04, 5.4324e-04, 7.1968e-04, 5.2482e-04,\n                      5.5730e-04, 5.2423e-04, 1.0745e-03, 4.9544e-04, 5.1063e-04, 4.6347e-04,\n                      4.5000e-04, 6.5802e-04, 5.5199e-04, 9.1677e-04, 6.3722e-04, 1.2796e-03,\n                      8.3209e-04, 5.1059e-04, 7.3251e-04, 5.4755e-04, 6.4630e-04, 8.7458e-04,\n                      8.2663e-04, 6.2149e-04, 5.5194e-04, 5.4616e-04, 7.4779e-04, 5.7120e-04,\n                      4.5070e-04, 7.4600e-04, 5.6269e-04, 6.8278e-04, 4.8308e-04, 4.4798e-04,\n                      5.6989e-04, 6.1594e-04, 7.3377e-04, 4.3943e-04, 7.3285e-04, 5.4430e-04,\n                      1.0265e-03, 5.2578e-04, 8.0882e-04, 4.8283e-04, 5.1530e-04, 7.3773e-04,\n                      6.7028e-04, 4.3552e-04, 7.7535e-04, 5.5237e-04, 3.6934e-04, 5.2098e-04,\n                      6.4083e-04, 8.4491e-04, 6.3403e-04, 5.5771e-04, 4.9909e-04, 4.9288e-04,\n                      5.1571e-04, 6.4852e-04, 4.7289e-04, 5.4152e-04, 4.0030e-04, 5.0981e-04,\n                      1.1194e-03, 5.7264e-04, 6.3668e-04, 4.0379e-04, 5.4672e-04, 3.8705e-04,\n                      6.4299e-04, 3.6801e-04, 6.7099e-04, 5.9021e-04, 4.8587e-04, 7.1853e-04,\n                      3.4214e-04, 6.0662e-04, 6.5874e-04, 3.0098e-04, 7.4255e-04, 8.2012e-04,\n                      8.0661e-04, 6.5375e-04, 1.0104e-03, 5.8876e-04, 7.2139e-04, 4.7494e-04,\n                      7.8422e-04, 6.8649e-04, 5.8130e-04, 7.6057e-04, 5.0898e-04, 7.8068e-04,\n                      7.8837e-04, 4.5109e-04, 4.7959e-04, 6.5156e-04, 6.9938e-04, 6.4917e-04,\n                      6.1401e-04, 7.8727e-04, 7.7794e-04, 7.6207e-04, 6.8683e-04, 6.2680e-04,\n                      5.5511e-04, 5.9256e-04, 4.4765e-04, 3.9946e-04, 5.3762e-04, 4.8700e-04,\n                      7.4345e-04, 4.0602e-04, 5.0836e-04, 5.8531e-04, 8.6241e-06, 8.0371e-04,\n                      9.6368e-04, 7.1495e-04, 6.1181e-04, 6.6998e-04, 7.0247e-04, 6.2275e-04,\n                      5.4724e-04, 4.7525e-04, 4.7205e-04, 4.2629e-04, 1.4255e-03, 7.9070e-04,\n                      5.4270e-04, 4.6862e-04, 5.7377e-04, 5.5831e-04, 6.6411e-04, 5.1197e-04,\n                      5.7169e-04, 6.4793e-04, 5.4467e-04, 7.3229e-04, 1.0599e-03, 9.4418e-04,\n                      5.6429e-04, 6.0967e-04, 6.6725e-04, 2.2797e-04, 3.5754e-04, 4.9542e-04,\n                      5.5297e-04, 5.9840e-04, 6.1950e-04, 5.4474e-04, 7.3953e-04, 6.3505e-04,\n                      5.8758e-04, 9.1205e-04, 5.4724e-04, 5.7496e-04, 7.8431e-04, 7.6733e-04,\n                      6.7229e-04, 4.2780e-04, 4.6391e-04, 6.0308e-04, 4.6550e-04, 5.9483e-04,\n                      6.6172e-04, 5.6087e-04, 7.2096e-04, 3.8915e-04, 6.5927e-04, 5.3650e-04,\n                      5.2209e-04, 7.5550e-04, 6.9103e-04, 7.1904e-04, 5.5799e-04, 5.4035e-04,\n                      5.7248e-04, 4.0732e-04, 7.7562e-04, 5.6505e-04, 2.1468e-04, 5.9789e-04,\n                      5.7058e-04, 7.6875e-04, 7.9400e-04, 5.8696e-04, 6.7780e-04, 6.3047e-04,\n                      6.9506e-04, 6.7029e-04, 7.6104e-04, 8.4857e-04, 6.0588e-04, 5.7037e-04,\n                      6.0305e-04, 7.5133e-04, 5.9899e-04, 7.4144e-04, 6.0240e-04, 5.7980e-04,\n                      5.8469e-04, 5.9328e-04, 5.7279e-04, 5.6915e-04, 4.7629e-04, 5.1868e-04,\n                      7.7480e-04, 9.8699e-04, 1.1687e-03, 5.3276e-04, 7.1287e-04, 5.3988e-04,\n                      2.2499e-04, 6.8026e-04, 5.9108e-04, 5.6127e-04, 5.5382e-04, 7.1454e-04,\n                      5.6094e-04, 6.4430e-04, 7.9478e-04, 3.6270e-04, 6.4773e-04, 7.3810e-04,\n                      6.6199e-04, 5.3389e-04, 3.5066e-04, 7.0355e-04, 7.2169e-04, 7.4329e-04,\n                      7.3878e-04, 6.1498e-04, 5.9092e-04, 1.1042e-04, 6.5696e-04, 5.4991e-04,\n                      5.4903e-04, 9.1333e-04, 6.0887e-04, 3.8036e-04, 6.7815e-04, 3.9636e-04,\n                      2.6093e-04, 8.1462e-04, 4.1184e-04, 6.2014e-04, 5.3150e-04, 7.4423e-04,\n                      4.1193e-04, 4.1120e-04, 6.7931e-04, 7.6620e-04, 7.1389e-04, 6.1933e-04,\n                      4.9573e-04, 3.2781e-04, 6.2979e-04, 5.8238e-04, 5.7646e-04, 5.6594e-04,\n                      6.0218e-04, 6.9980e-04, 6.4840e-04, 5.7068e-04, 7.4809e-04, 1.5753e-04,\n                      3.1346e-04, 6.4607e-04, 6.0091e-04, 5.6050e-04, 4.7767e-04, 5.7528e-04,\n                      5.3155e-04, 8.5877e-04, 2.0989e-04, 6.3196e-04, 4.1847e-04, 4.5998e-04,\n                      6.1087e-04, 6.4659e-04, 8.0540e-04, 5.9063e-04, 6.5255e-04, 4.8825e-04,\n                      9.1643e-04, 4.1992e-04, 6.5418e-04, 9.5078e-04, 4.8534e-04, 6.1244e-04,\n                      5.2372e-04, 5.2307e-04, 4.8953e-04, 5.6319e-04, 6.3281e-04, 6.3594e-04,\n                      7.3248e-04, 5.8170e-04, 5.9136e-04, 6.4255e-04, 6.5608e-04, 8.7871e-04,\n                      5.5812e-04, 8.1562e-04, 4.6464e-04, 7.5098e-04, 5.3991e-04, 6.2632e-04,\n                      5.3445e-04, 7.4029e-04, 5.1429e-04, 5.9080e-04, 7.1162e-04, 8.3814e-04,\n                      7.0944e-04, 5.7028e-04, 4.9477e-04, 4.9126e-04, 1.8852e-04, 6.7575e-04,\n                      5.0176e-04, 5.1657e-04, 4.5933e-04, 5.9886e-04, 6.9796e-04, 7.0675e-04,\n                      7.7200e-04, 7.6141e-04, 6.0368e-04, 5.5799e-04, 6.5018e-04, 6.4181e-04,\n                      6.5288e-04, 8.8020e-04, 7.8795e-04, 5.3862e-04, 4.6494e-04, 7.5034e-04,\n                      5.0437e-04, 6.0130e-04, 6.1985e-04, 5.8760e-04, 8.4996e-04, 7.8670e-04,\n                      6.6593e-04, 5.2452e-04, 5.2305e-04, 6.3127e-04, 6.8937e-04, 4.6169e-04,\n                      7.8283e-04, 4.7116e-04, 9.9056e-04, 1.3047e-05, 6.2686e-04, 4.5322e-04,\n                      8.6066e-04, 3.3827e-04, 1.4101e-03, 6.1416e-04, 6.2962e-04, 5.1375e-04,\n                      6.4087e-04, 5.3597e-04, 4.6841e-04, 7.2919e-04, 5.3327e-04, 5.9204e-04,\n                      5.9602e-04, 4.7585e-04, 6.5273e-04, 1.0426e-03, 7.9285e-04, 3.8141e-04,\n                      4.8475e-04, 8.4303e-04, 3.8327e-04, 7.0434e-04, 6.6023e-04, 4.7123e-04,\n                      5.7679e-04, 6.3178e-04, 7.0413e-04, 7.2522e-04, 7.6024e-04, 6.4361e-04],\n                     dtype=torch.float64),\n                     zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n                     axis=0)),\n             ('mnet.features.14.conv.0.0.bias',\n              Parameter containing:\n              tensor([ 0.1157,  0.0010, -0.3125, -0.1309, -0.1267,  0.0450, -0.1396, -0.1778,\n                      -0.1568, -0.1855, -0.2510, -0.0154, -0.1553,  0.0036, -0.0962, -0.2190,\n                       0.1506,  0.0137, -0.1823, -0.2239, -0.0595, -0.1636, -0.0243, -0.0945,\n                       0.0344,  0.1662, -0.1748, -0.0109, -0.0915, -0.1492, -0.0210,  0.0551,\n                      -0.1088, -0.2490, -0.1235,  0.0187, -0.0270, -0.0907,  0.0161, -0.0261,\n                       0.1644, -0.1749, -0.1509, -0.1282, -0.0252, -0.1541, -0.1813,  0.0393,\n                      -0.1709, -0.2360, -0.1021, -0.1131, -0.0628,  0.0354, -0.0567, -0.1397,\n                      -0.1482, -0.0813, -0.1435, -0.1601, -0.1481, -0.1132, -0.1210, -0.0333,\n                       0.1183, -0.0565, -0.1300,  0.0419, -0.0631, -0.0421,  0.0042, -0.2130,\n                      -0.1149, -0.0104, -0.1745, -0.1559, -0.0740, -0.0475,  0.0628,  0.0721,\n                      -0.1094,  0.1276, -0.1751, -0.2407, -0.1441,  0.0899, -0.0727, -0.1614,\n                      -0.0845, -0.0097, -0.1479, -0.1955, -0.1660, -0.1934, -0.0454, -0.1452,\n                       0.1197,  0.1946, -0.1346,  0.1593, -0.1325, -0.2470, -0.2122, -0.3645,\n                      -0.1672, -0.1716, -0.1556, -0.2366, -0.1965, -0.2485, -0.1683, -0.2310,\n                      -0.1255,  0.0845, -0.0519, -0.1750, -0.0980, -0.1253, -0.1848, -0.2120,\n                      -0.1244,  0.2167, -0.2494, -0.0263, -0.0923,  0.0183, -0.1935,  0.0493,\n                      -0.1403,  0.0083, -0.0962, -0.1037, -0.2409, -0.1201,  0.1500,  0.1539,\n                      -0.1614, -0.0679, -0.2236, -0.0582, -0.0995, -0.1106,  0.0891, -0.0998,\n                       0.0255, -0.1174, -0.2393,  0.1395, -0.1167,  0.0317, -0.1112,  0.0327,\n                      -0.2287, -0.1700, -0.1303, -0.2237, -0.1095, -0.0592, -0.0241, -0.1830,\n                      -0.1395, -0.1150, -0.1196,  0.1078, -0.1940, -0.1368,  0.0112,  0.1685,\n                      -0.0498, -0.1799, -0.0189, -0.1495,  0.0150, -0.1328,  0.0186, -0.0794,\n                      -0.1077, -0.0090,  0.0007, -0.1169,  0.1016,  0.0759, -0.1012, -0.0891,\n                       0.0737, -0.1648, -0.0494, -0.1239, -0.0680, -0.0547,  0.0745,  0.0993,\n                      -0.1265,  0.0875,  0.0045, -0.0953, -0.0178,  0.1129,  0.1707,  0.0346,\n                      -0.1178, -0.1905, -0.0898, -0.1285, -0.1799,  0.0338, -0.0853, -0.0814,\n                       0.0040, -0.0337, -0.1009, -0.1953,  0.0596,  0.1422, -0.1266,  0.1124,\n                      -0.1126, -0.1950, -0.0564, -0.2243,  0.1301, -0.0180, -0.1455, -0.2321,\n                      -0.1348,  0.1013, -0.1538, -0.1812, -0.1276,  0.0047, -0.1669, -0.1237,\n                      -0.0294, -0.2525, -0.0127,  0.0749, -0.2598, -0.0901,  0.1279,  0.0058,\n                      -0.1585, -0.1629, -0.1522, -0.1441, -0.0697, -0.0952,  0.0751, -0.1857,\n                      -0.0453,  0.0052,  0.0959,  0.0484, -0.1985, -0.1113, -0.2075,  0.1294,\n                      -0.2183,  0.0618, -0.1055,  0.1212, -0.1389,  0.0665, -0.0305, -0.1718,\n                       0.2153, -0.1523, -0.2910,  0.1723, -0.1454, -0.2272,  0.0257, -0.1524,\n                      -0.2583, -0.1001, -0.0495, -0.1157,  0.0270, -0.1003, -0.1073, -0.0205,\n                       0.0474, -0.1960, -0.1275, -0.1717,  0.0069, -0.1428, -0.0711, -0.1043,\n                      -0.0094, -0.1223, -0.0987,  0.0389, -0.1299, -0.0649, -0.1407,  0.0635,\n                      -0.0618,  0.1563,  0.1103,  0.0977, -0.1639,  0.1227,  0.0895, -0.0098,\n                      -0.0374, -0.1527, -0.2041, -0.0972,  0.0109, -0.2192, -0.1703, -0.0099,\n                      -0.1764, -0.0081,  0.0182,  0.0017, -0.2990, -0.1678, -0.2072,  0.0612,\n                      -0.1722, -0.0825,  0.0171, -0.0150, -0.1510, -0.1516, -0.0903, -0.1492,\n                      -0.1148, -0.1453,  0.0293, -0.2258, -0.2002,  0.0683,  0.0965, -0.1709,\n                      -0.1910, -0.0803, -0.1279,  0.0443, -0.0252, -0.1645,  0.0115, -0.1135,\n                      -0.1320, -0.1009, -0.0389, -0.0277, -0.1203, -0.0326, -0.0009,  0.0372,\n                       0.1027, -0.1266, -0.0832, -0.0311, -0.1615, -0.0182,  0.0169,  0.0406,\n                       0.0140,  0.0490,  0.0461, -0.1284,  0.0314, -0.1066, -0.1527, -0.0309,\n                      -0.1450, -0.1083,  0.1647, -0.2478, -0.1696, -0.1362, -0.0794,  0.1067,\n                      -0.0319, -0.1487, -0.1931, -0.1390, -0.1278, -0.3282, -0.1390, -0.1671,\n                      -0.1622, -0.2052, -0.1479, -0.1673, -0.0944, -0.1270, -0.1391, -0.1069,\n                       0.0029, -0.0911, -0.1693, -0.1297, -0.1088,  0.0017, -0.1651, -0.0905,\n                      -0.1649, -0.0827,  0.2719, -0.1621, -0.2911,  0.0338,  0.0026, -0.0991,\n                      -0.1021, -0.1423,  0.0150, -0.0422, -0.0321, -0.1685,  0.0555, -0.1220,\n                       0.1215, -0.1232, -0.1110, -0.1471, -0.0451, -0.0159, -0.1228,  0.2573,\n                      -0.1835, -0.1592, -0.1920, -0.2458, -0.1653,  0.0325, -0.1668, -0.0193,\n                       0.0453, -0.1711,  0.0917, -0.0231, -0.1228, -0.1836,  0.1327,  0.1412,\n                       0.0854,  0.1771,  0.0551, -0.2145, -0.0244,  0.1137, -0.1415, -0.0294,\n                      -0.0871, -0.1570, -0.1367,  0.0704,  0.0190, -0.1289, -0.0458,  0.1175,\n                       0.1620,  0.0089, -0.1340, -0.0052,  0.0338,  0.1157,  0.0176, -0.1255,\n                       0.1747,  0.0248, -0.0677, -0.1290, -0.0068,  0.0405, -0.1668, -0.1024,\n                      -0.1462, -0.0012, -0.0477,  0.0270, -0.2178,  0.0072, -0.0488, -0.1187,\n                       0.0330, -0.1127, -0.0310, -0.0191,  0.1559, -0.1565, -0.0916, -0.1763,\n                      -0.0246, -0.1993, -0.1860, -0.1398, -0.0905, -0.1316, -0.0146, -0.0684,\n                      -0.0408, -0.1341, -0.2273, -0.0485,  0.0325,  0.0035, -0.2084, -0.2360,\n                      -0.0986, -0.0970, -0.0596, -0.0134,  0.2686, -0.1931, -0.0050, -0.1623,\n                       0.1615, -0.1433,  0.0026, -0.0113, -0.2071,  0.0005, -0.1543, -0.1169,\n                      -0.1362, -0.1492, -0.0055, -0.2153, -0.1325, -0.0708, -0.1475, -0.0089,\n                      -0.0411,  0.0016, -0.2038, -0.0273, -0.0755, -0.0247, -0.1733, -0.1199,\n                      -0.1984, -0.0914, -0.1898,  0.0444, -0.1257,  0.0151, -0.1995, -0.0406,\n                      -0.0355,  0.1093, -0.1073,  0.1560, -0.1676, -0.1685, -0.1060, -0.0939,\n                      -0.0035, -0.0355, -0.1392, -0.0061, -0.0139, -0.1501, -0.0227, -0.0229,\n                      -0.1303, -0.1809, -0.1319,  0.0822, -0.0239, -0.1821,  0.1364,  0.1009,\n                      -0.1311,  0.0243, -0.0566, -0.1842, -0.0995,  0.1925, -0.1715,  0.0878],\n                     requires_grad=True)),\n             ('mnet.features.14.conv.0.0.scale', tensor(0.0332)),\n             ('mnet.features.14.conv.0.0.zero_point', tensor(37)),\n             ('mnet.features.14.conv.1.0.weight',\n              tensor([[[[-0.1428, -0.3186, -0.2710],\n                        [-0.3003, -0.4358, -0.4687],\n                        [-0.2673, -0.4138, -0.4211]]],\n              \n              \n                      [[[-0.0991, -0.2813, -0.2100],\n                        [-0.2853, -0.5072, -0.4002],\n                        [-0.2021, -0.3645, -0.2972]]],\n              \n              \n                      [[[ 0.1973,  0.4791,  0.3241],\n                        [ 0.4368,  0.8948,  0.6552],\n                        [ 0.2818,  0.6129,  0.4932]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-0.1444, -0.3331, -0.2702],\n                        [-0.2591, -0.4738, -0.4109],\n                        [-0.2110, -0.4405, -0.3220]]],\n              \n              \n                      [[[ 0.1656,  0.3948,  0.2929],\n                        [ 0.3694,  0.8088,  0.5668],\n                        [ 0.2547,  0.5477,  0.4076]]],\n              \n              \n                      [[[-0.1936, -0.3137, -0.2336],\n                        [-0.2336, -0.4272, -0.3705],\n                        [-0.2103, -0.3538, -0.3104]]]], size=(576, 1, 3, 3),\n                     dtype=torch.qint8, quantization_scheme=torch.per_channel_affine,\n                     scale=tensor([0.0037, 0.0040, 0.0070, 0.0056, 0.0065, 0.0039, 0.0101, 0.0059, 0.0066,\n                      0.0069, 0.0058, 0.0036, 0.0063, 0.0036, 0.0064, 0.0089, 0.0044, 0.0038,\n                      0.0074, 0.0070, 0.0045, 0.0061, 0.0044, 0.0047, 0.0031, 0.0078, 0.0097,\n                      0.0041, 0.0057, 0.0056, 0.0036, 0.0029, 0.0049, 0.0080, 0.0049, 0.0041,\n                      0.0040, 0.0063, 0.0027, 0.0047, 0.0059, 0.0065, 0.0054, 0.0060, 0.0031,\n                      0.0069, 0.0087, 0.0045, 0.0075, 0.0078, 0.0060, 0.0054, 0.0046, 0.0040,\n                      0.0050, 0.0061, 0.0080, 0.0048, 0.0060, 0.0073, 0.0059, 0.0051, 0.0062,\n                      0.0051, 0.0038, 0.0042, 0.0072, 0.0026, 0.0057, 0.0049, 0.0035, 0.0061,\n                      0.0053, 0.0042, 0.0058, 0.0056, 0.0060, 0.0034, 0.0170, 0.0029, 0.0060,\n                      0.0044, 0.0065, 0.0074, 0.0074, 0.0025, 0.0049, 0.0057, 0.0054, 0.0043,\n                      0.0075, 0.0092, 0.0054, 0.0078, 0.0039, 0.0057, 0.0046, 0.0040, 0.0066,\n                      0.0050, 0.0051, 0.0071, 0.0082, 0.0075, 0.0047, 0.0064, 0.0052, 0.0077,\n                      0.0064, 0.0085, 0.0081, 0.0076, 0.0055, 0.0035, 0.0040, 0.0074, 0.0056,\n                      0.0069, 0.0063, 0.0055, 0.0060, 0.0117, 0.0098, 0.0051, 0.0061, 0.0036,\n                      0.0065, 0.0039, 0.0058, 0.0037, 0.0065, 0.0051, 0.0076, 0.0061, 0.0041,\n                      0.0063, 0.0059, 0.0052, 0.0079, 0.0051, 0.0055, 0.0057, 0.0087, 0.0056,\n                      0.0035, 0.0060, 0.0086, 0.0083, 0.0058, 0.0045, 0.0056, 0.0032, 0.0075,\n                      0.0055, 0.0048, 0.0128, 0.0049, 0.0031, 0.0043, 0.0075, 0.0075, 0.0061,\n                      0.0066, 0.0034, 0.0067, 0.0052, 0.0055, 0.0031, 0.0043, 0.0058, 0.0032,\n                      0.0063, 0.0037, 0.0064, 0.0035, 0.0059, 0.0054, 0.0044, 0.0048, 0.0046,\n                      0.0172, 0.0037, 0.0052, 0.0060, 0.0029, 0.0063, 0.0037, 0.0081, 0.0033,\n                      0.0050, 0.0038, 0.0037, 0.0084, 0.0032, 0.0034, 0.0075, 0.0044, 0.0041,\n                      0.0037, 0.0032, 0.0051, 0.0057, 0.0055, 0.0049, 0.0051, 0.0041, 0.0044,\n                      0.0067, 0.0040, 0.0049, 0.0053, 0.0072, 0.0029, 0.0057, 0.0054, 0.0044,\n                      0.0057, 0.0073, 0.0045, 0.0079, 0.0036, 0.0044, 0.0063, 0.0077, 0.0051,\n                      0.0034, 0.0065, 0.0065, 0.0055, 0.0038, 0.0054, 0.0058, 0.0039, 0.0063,\n                      0.0034, 0.0042, 0.0075, 0.0057, 0.0046, 0.0036, 0.0081, 0.0068, 0.0062,\n                      0.0071, 0.0054, 0.0049, 0.0034, 0.0082, 0.0047, 0.0039, 0.0038, 0.0034,\n                      0.0056, 0.0066, 0.0079, 0.0045, 0.0076, 0.0049, 0.0059, 0.0094, 0.0062,\n                      0.0036, 0.0044, 0.0065, 0.0101, 0.0060, 0.0082, 0.0091, 0.0057, 0.0088,\n                      0.0042, 0.0066, 0.0065, 0.0050, 0.0039, 0.0058, 0.0036, 0.0061, 0.0060,\n                      0.0032, 0.0033, 0.0068, 0.0068, 0.0058, 0.0040, 0.0072, 0.0047, 0.0071,\n                      0.0038, 0.0047, 0.0054, 0.0042, 0.0059, 0.0044, 0.0078, 0.0039, 0.0045,\n                      0.0043, 0.0046, 0.0042, 0.0054, 0.0035, 0.0033, 0.0037, 0.0016, 0.0057,\n                      0.0051, 0.0060, 0.0038, 0.0081, 0.0084, 0.0038, 0.0078, 0.0043, 0.0031,\n                      0.0040, 0.0064, 0.0098, 0.0093, 0.0045, 0.0068, 0.0057, 0.0031, 0.0039,\n                      0.0077, 0.0065, 0.0060, 0.0057, 0.0048, 0.0066, 0.0042, 0.0094, 0.0071,\n                      0.0173, 0.0061, 0.0071, 0.0104, 0.0086, 0.0077, 0.0038, 0.0044, 0.0073,\n                      0.0032, 0.0057, 0.0072, 0.0073, 0.0036, 0.0039, 0.0062, 0.0047, 0.0043,\n                      0.0032, 0.0039, 0.0060, 0.0047, 0.0045, 0.0071, 0.0045, 0.0034, 0.0038,\n                      0.0030, 0.0033, 0.0038, 0.0050, 0.0038, 0.0069, 0.0079, 0.0044, 0.0056,\n                      0.0058, 0.0052, 0.0082, 0.0062, 0.0050, 0.0050, 0.0029, 0.0040, 0.0048,\n                      0.0058, 0.0065, 0.0061, 0.0098, 0.0071, 0.0078, 0.0074, 0.0062, 0.0064,\n                      0.0050, 0.0067, 0.0062, 0.0062, 0.0055, 0.0035, 0.0053, 0.0077, 0.0058,\n                      0.0056, 0.0038, 0.0059, 0.0055, 0.0089, 0.0053, 0.0107, 0.0075, 0.0098,\n                      0.0039, 0.0044, 0.0060, 0.0056, 0.0064, 0.0054, 0.0098, 0.0045, 0.0064,\n                      0.0039, 0.0065, 0.0030, 0.0058, 0.0070, 0.0078, 0.0040, 0.0043, 0.0055,\n                      0.0147, 0.0078, 0.0077, 0.0106, 0.0062, 0.0078, 0.0037, 0.0062, 0.0039,\n                      0.0112, 0.0079, 0.0036, 0.0029, 0.0057, 0.0061, 0.0043, 0.0043, 0.0036,\n                      0.0043, 0.0030, 0.0081, 0.0044, 0.0044, 0.0054, 0.0049, 0.0060, 0.0081,\n                      0.0060, 0.0026, 0.0038, 0.0058, 0.0045, 0.0102, 0.0062, 0.0036, 0.0069,\n                      0.0041, 0.0036, 0.0028, 0.0039, 0.0048, 0.0063, 0.0041, 0.0043, 0.0073,\n                      0.0035, 0.0039, 0.0050, 0.0048, 0.0066, 0.0037, 0.0049, 0.0048, 0.0082,\n                      0.0035, 0.0037, 0.0071, 0.0036, 0.0064, 0.0045, 0.0029, 0.0056, 0.0071,\n                      0.0040, 0.0072, 0.0041, 0.0058, 0.0073, 0.0064, 0.0051, 0.0055, 0.0039,\n                      0.0042, 0.0046, 0.0062, 0.0094, 0.0038, 0.0035, 0.0037, 0.0080, 0.0095,\n                      0.0063, 0.0059, 0.0044, 0.0043, 0.0135, 0.0070, 0.0035, 0.0058, 0.0029,\n                      0.0066, 0.0043, 0.0031, 0.0073, 0.0039, 0.0053, 0.0053, 0.0055, 0.0064,\n                      0.0034, 0.0062, 0.0048, 0.0056, 0.0077, 0.0036, 0.0044, 0.0043, 0.0070,\n                      0.0038, 0.0041, 0.0033, 0.0056, 0.0070, 0.0084, 0.0055, 0.0129, 0.0041,\n                      0.0057, 0.0042, 0.0081, 0.0026, 0.0041, 0.0040, 0.0055, 0.0041, 0.0060,\n                      0.0076, 0.0058, 0.0062, 0.0047, 0.0045, 0.0075, 0.0048, 0.0047, 0.0072,\n                      0.0047, 0.0040, 0.0048, 0.0066, 0.0067, 0.0043, 0.0035, 0.0071, 0.0040,\n                      0.0031, 0.0061, 0.0040, 0.0047, 0.0074, 0.0057, 0.0037, 0.0064, 0.0033],\n                     dtype=torch.float64),\n                     zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n                     axis=0)),\n             ('mnet.features.14.conv.1.0.bias',\n              Parameter containing:\n              tensor([ 4.4699e-01,  3.0620e-01, -1.0453e-02,  1.9853e-02,  9.5884e-03,\n                       2.9806e-01, -1.6032e-02, -1.5591e-02,  1.3682e-02,  2.2771e-02,\n                       3.0110e-02,  3.1452e-01, -3.2777e-02,  5.1524e-01,  2.0136e-03,\n                       1.5072e-02,  5.6914e-01,  5.2638e-01,  2.2212e-02,  2.2637e-02,\n                       6.0593e-01,  3.2918e-02,  3.0497e-01, -1.9267e-02,  5.1473e-01,\n                       7.9525e-01, -1.0097e-02,  4.1149e-01,  1.7215e-02,  2.3773e-02,\n                       5.8518e-01,  3.5399e-01,  3.3507e-02, -1.9528e-02,  9.4528e-03,\n                       4.5758e-01,  5.1884e-01, -7.2311e-04,  4.6165e-01,  3.5949e-01,\n                       6.2624e-01,  1.8322e-02,  2.2167e-02,  8.2459e-03,  2.5912e-01,\n                       2.7757e-02,  3.7407e-03,  4.0361e-01,  2.0852e-02,  3.0751e-02,\n                      -3.4908e-02, -2.2194e-02,  5.2441e-01,  3.5656e-01,  2.3729e-03,\n                       2.4207e-02, -5.5280e-03,  6.1873e-01,  2.6805e-02,  2.1097e-02,\n                       2.7540e-02,  3.1442e-02,  1.6463e-02,  4.8378e-01,  3.9424e-01,\n                       4.5578e-01,  6.4703e-03,  2.6807e-01,  2.5034e-02,  6.1102e-01,\n                       5.0284e-01,  3.5111e-02,  2.8034e-02,  4.1949e-01,  1.6409e-02,\n                       1.0562e-02, -5.1619e-03,  4.5221e-01, -2.8433e-01,  3.0838e-01,\n                       4.5171e-03,  4.6327e-01,  1.0713e-02, -3.2830e-02,  1.1330e-02,\n                       3.0328e-01,  6.1294e-01,  2.9203e-02, -2.4916e-03,  5.4578e-01,\n                      -3.7248e-03, -6.2701e-02,  2.8090e-02,  2.5623e-02,  3.1434e-01,\n                       2.0294e-02,  3.8828e-01,  5.4455e-01,  1.5187e-02,  4.6152e-01,\n                      -5.8064e-02,  2.1419e-02,  1.6126e-02, -2.1742e-02,  3.1565e-02,\n                      -2.7952e-02,  2.4333e-02,  1.9502e-02, -3.3341e-03, -6.9817e-02,\n                       6.7939e-03, -1.4519e-01,  7.7067e-03,  3.3629e-01,  5.1877e-01,\n                       2.7034e-02,  2.3438e-02,  1.4053e-02,  3.1493e-02,  3.8843e-02,\n                       2.2934e-02,  3.7169e-01, -1.8193e-01, -1.6079e-02, -1.6761e-03,\n                       4.3437e-01,  2.6374e-02,  4.9415e-01,  2.8836e-02,  4.4484e-01,\n                      -9.3531e-03,  1.7743e-02,  3.0796e-02,  1.7707e-02,  4.6615e-01,\n                       6.9167e-01,  2.4588e-02, -3.7620e-02,  2.4236e-02, -3.2039e-03,\n                       2.5507e-02,  6.5778e-03, -2.3225e-02,  2.6550e-02,  3.8141e-01,\n                       2.3556e-02,  3.0295e-02,  5.6051e-01, -2.6461e-02,  2.9025e-01,\n                      -1.3110e-02,  3.5528e-01,  2.0369e-02,  2.6396e-02,  3.9070e-02,\n                      -7.7189e-03,  3.5834e-02,  3.2353e-01,  3.2055e-01,  2.6746e-02,\n                      -2.2039e-03,  1.8825e-02,  9.1468e-03,  3.2934e-01,  1.9392e-02,\n                       3.2885e-02, -3.3635e-02,  3.7339e-01,  6.1326e-01,  3.1028e-02,\n                       3.1991e-01, -1.4028e-01,  2.8347e-01,  2.2202e-02,  3.9254e-01,\n                       1.0985e-02,  2.0623e-02,  4.8305e-01,  5.8953e-01,  3.3303e-01,\n                      -4.6227e-01,  3.4308e-01, -2.4130e-02, -1.0782e-02,  2.8275e-01,\n                       5.7340e-04, -4.9006e-02, -1.8171e-03,  5.5441e-02,  5.5480e-01,\n                       3.1005e-01,  3.4282e-01,  8.7114e-03,  2.8262e-01,  2.7356e-01,\n                       1.0900e-02,  5.0505e-01,  3.7236e-01,  4.1722e-01,  3.2872e-01,\n                       3.4478e-02,  2.5894e-02,  7.6112e-03,  3.4692e-02, -6.4335e-02,\n                       4.1618e-01,  2.6999e-01,  9.3251e-03,  2.7070e-01, -2.2402e-02,\n                      -3.9779e-02,  2.4481e-02,  3.0515e-01,  4.3089e-01,  1.8139e-02,\n                       3.6356e-02,  3.0297e-02, -4.6659e-02,  5.8408e-01, -1.7455e-01,\n                       4.1340e-01,  4.2885e-01, -1.4593e-02,  1.6219e-02,  3.4252e-02,\n                       3.6213e-01,  2.7100e-02,  2.4357e-02,  1.9731e-02,  2.6437e-01,\n                       2.6555e-02,  2.4706e-02,  5.5520e-01,  2.6751e-02,  4.0780e-01,\n                       4.4207e-01, -5.0346e-02,  3.0587e-03,  5.2889e-01,  3.6405e-01,\n                       1.0442e-02,  3.0144e-02,  2.0593e-02, -2.9021e-02,  1.6942e-02,\n                       4.2090e-01,  3.1134e-01,  1.4787e-02,  4.6042e-01,  4.3269e-01,\n                       4.0048e-01,  3.8096e-01,  2.9912e-02, -3.1139e-02,  4.1534e-03,\n                       5.2449e-01,  2.9052e-02,  3.5030e-01, -9.7863e-03, -1.6250e-01,\n                       2.8430e-02,  3.2401e-01,  5.9205e-01,  1.2686e-02,  4.9803e-01,\n                      -2.0211e-02,  2.8963e-02,  6.3580e-01,  1.1181e-02, -1.5268e-02,\n                       3.2880e-01,  2.2132e-02,  3.5033e-02,  3.1388e-02,  4.3191e-01,\n                       2.8572e-03,  3.8553e-01,  1.1144e-02,  1.7688e-02,  4.3666e-01,\n                       3.5367e-01,  2.3942e-02, -1.9925e-03,  3.5627e-02,  4.5231e-01,\n                      -9.2174e-03,  3.9233e-01, -6.7739e-03,  3.2207e-01,  3.2153e-02,\n                      -5.7243e-05,  3.4934e-01,  3.0614e-02,  4.3130e-01, -5.2613e-03,\n                       3.1084e-01,  2.8799e-02,  4.9921e-01,  3.9954e-01,  4.3610e-01,\n                       4.2492e-02,  4.0862e-01,  3.4202e-01,  5.1698e-01,  6.0725e-03,\n                      -3.2802e-02,  4.6302e-02,  2.1425e-02,  5.0217e-01, -5.6031e-02,\n                       2.0595e-03,  5.5537e-01,  6.3577e-03,  4.8166e-01,  3.5185e-01,\n                       3.8194e-01, -1.2746e-01,  8.8678e-04, -2.9877e-02,  4.0121e-01,\n                       2.1540e-02,  8.1389e-03,  3.7969e-01,  4.2134e-01,  6.1730e-03,\n                      -2.3271e-02,  3.0972e-02,  3.1719e-02,  3.4919e-02,  1.6832e-02,\n                       3.0843e-01,  2.2157e-02,  3.7834e-02, -3.3422e-01,  2.3965e-01,\n                       1.5976e-02, -5.5295e-03, -2.8367e-03,  1.1871e-02,  2.8612e-01,\n                      -1.2395e-02,  3.1858e-02,  3.1202e-01,  1.8181e-03,  1.0997e-02,\n                      -1.8733e-02,  4.4688e-01,  5.1992e-01, -2.3446e-03,  5.4159e-01,\n                       2.9877e-01,  3.0462e-01,  3.7187e-01,  3.4840e-02,  1.1165e-02,\n                      -5.0830e-03,  1.5996e-02,  4.8986e-01,  3.8966e-01,  3.2431e-01,\n                       3.0046e-01,  2.8458e-01,  3.2781e-01,  4.0682e-02,  2.9353e-01,\n                       2.1997e-03,  1.5790e-02,  4.5958e-01,  2.6594e-02,  2.6287e-02,\n                       6.5100e-01,  1.2879e-02,  1.0459e-02,  2.5887e-02,  2.6353e-02,\n                       3.1564e-01,  3.8774e-01,  3.8784e-02, -1.5780e-02,  1.5541e-02,\n                       1.1924e-02,  2.1665e-02, -4.1449e-03,  1.5260e-02,  1.0285e-02,\n                       3.4561e-02,  2.0753e-02,  4.7008e-02, -6.3019e-03,  7.4831e-03,\n                       2.0079e-02,  1.1293e-02,  2.8483e-01,  6.9862e-03,  1.3398e-02,\n                       1.5809e-02,  2.8807e-02,  4.5025e-01,  2.4763e-02,  2.1749e-02,\n                      -3.8023e-03, -5.6114e-03, -2.7854e-01,  1.7502e-02,  1.3604e-02,\n                       2.7445e-01,  4.3538e-01, -4.7429e-02,  1.7308e-02,  5.5835e-03,\n                      -1.5533e-02, -3.8530e-02, -1.4377e-02, -1.0596e-02,  3.2077e-01,\n                       8.2199e-03,  3.5085e-01,  3.7773e-03,  1.7222e-02, -5.9668e-03,\n                       1.5278e-02,  3.6394e-01,  2.1529e-02, -1.0858e-02,  2.6810e-02,\n                      -4.5121e-02, -4.6102e-03,  3.3107e-02, -3.7836e-03,  3.5894e-01,\n                       1.0153e-02,  4.6905e-01, -5.5042e-02,  3.7594e-03,  3.3571e-01,\n                       3.5257e-01,  2.7936e-02,  3.3167e-02,  4.1543e-01,  4.9456e-01,\n                       4.7410e-01,  4.3714e-01,  3.0906e-01,  2.2009e-02,  4.6423e-01,\n                       4.3355e-01,  1.5853e-02,  6.3004e-01,  8.1088e-03,  1.4767e-02,\n                       7.1073e-03,  3.5505e-01,  3.3455e-01,  1.6248e-02, -5.0001e-03,\n                       6.5132e-01,  6.1074e-01,  3.3971e-01,  8.4749e-03,  5.2735e-01,\n                       3.7271e-01,  3.5236e-01,  4.0183e-01,  2.8444e-02,  7.8366e-01,\n                       3.1561e-01,  4.5360e-01, -6.4289e-03,  2.6298e-01,  3.0599e-01,\n                       3.7859e-02,  6.5493e-01,  2.7158e-02,  2.8875e-01, -1.7969e-01,\n                       4.7976e-01, -1.4214e-02,  3.2008e-01,  5.3740e-01,  4.2066e-03,\n                       3.5721e-01,  1.6730e-02,  5.6034e-01,  3.7755e-01,  2.2918e-02,\n                       2.8703e-03,  5.2481e-01,  2.2145e-02,  4.8228e-01,  3.8157e-02,\n                       1.8689e-02, -3.3770e-02,  5.8791e-03, -8.9075e-03,  5.4506e-01,\n                       5.6845e-01,  5.2821e-01,  1.3576e-02, -1.0036e-02,  3.0307e-01,\n                       3.4771e-01,  3.1562e-01,  1.3862e-02,  1.3691e-02, -2.6604e-03,\n                       2.0736e-02, -2.1170e-02,  7.0252e-01, -4.9934e-01,  2.8317e-02,\n                       5.1216e-01,  2.3012e-02,  4.6725e-01,  1.5078e-02,  4.1232e-01,\n                       2.8859e-01, -6.3572e-02,  3.5284e-01,  2.8819e-02,  6.0047e-01,\n                       3.5663e-02,  1.5419e-02,  3.6784e-01,  2.3741e-02,  4.2052e-02,\n                      -1.6620e-02,  9.6264e-03,  3.4643e-01,  4.4616e-01,  4.4087e-01,\n                       2.0472e-02,  3.1746e-01,  2.1746e-02,  3.2437e-01,  2.9206e-02,\n                      -5.5087e-02,  9.5807e-03,  2.1024e-02, -1.7018e-01,  3.1343e-01,\n                       3.2131e-02,  2.6303e-01, -1.7500e-02, -6.9261e-03,  5.6626e-01,\n                       4.0468e-01,  1.3962e-02,  6.0292e-01,  3.0898e-02,  9.6617e-03,\n                       2.0760e-02, -7.2058e-03,  3.7019e-01,  5.2330e-01,  1.1763e-02,\n                       2.8156e-01,  6.0194e-01,  1.7621e-02,  4.5849e-01,  5.1322e-01,\n                       3.4791e-02, -4.3643e-02,  2.6768e-04,  3.9930e-01,  4.1281e-01,\n                       1.9979e-02,  4.7326e-01,  3.2356e-01,  1.4610e-02,  4.2875e-01,\n                       5.3458e-01,  1.1870e-02,  9.9398e-03,  4.6492e-01,  2.6180e-02,\n                       3.7675e-01], requires_grad=True)),\n             ('mnet.features.14.conv.1.0.scale', tensor(0.0632)),\n             ('mnet.features.14.conv.1.0.zero_point', tensor(35)),\n             ('mnet.features.14.conv.2.weight',\n              tensor([[[[-0.0031]],\n              \n                       [[ 0.0353]],\n              \n                       [[-0.0492]],\n              \n                       ...,\n              \n                       [[-0.0430]],\n              \n                       [[ 0.0584]],\n              \n                       [[-0.0446]]],\n              \n              \n                      [[[-0.0174]],\n              \n                       [[ 0.0395]],\n              \n                       [[-0.0411]],\n              \n                       ...,\n              \n                       [[ 0.1074]],\n              \n                       [[ 0.0095]],\n              \n                       [[ 0.0237]]],\n              \n              \n                      [[[-0.0558]],\n              \n                       [[-0.1636]],\n              \n                       [[-0.0481]],\n              \n                       ...,\n              \n                       [[-0.0019]],\n              \n                       [[-0.0712]],\n              \n                       [[ 0.0308]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-0.0207]],\n              \n                       [[-0.0148]],\n              \n                       [[ 0.0948]],\n              \n                       ...,\n              \n                       [[ 0.0815]],\n              \n                       [[-0.0148]],\n              \n                       [[ 0.0637]]],\n              \n              \n                      [[[ 0.0295]],\n              \n                       [[ 0.0155]],\n              \n                       [[ 0.0202]],\n              \n                       ...,\n              \n                       [[-0.0761]],\n              \n                       [[-0.0357]],\n              \n                       [[ 0.0093]]],\n              \n              \n                      [[[-0.0029]],\n              \n                       [[-0.0393]],\n              \n                       [[ 0.0976]],\n              \n                       ...,\n              \n                       [[ 0.0393]],\n              \n                       [[-0.0918]],\n              \n                       [[ 0.0786]]]], size=(160, 576, 1, 1), dtype=torch.qint8,\n                     quantization_scheme=torch.per_channel_affine,\n                     scale=tensor([0.0015, 0.0016, 0.0019, 0.0020, 0.0019, 0.0017, 0.0017, 0.0022, 0.0019,\n                      0.0017, 0.0017, 0.0012, 0.0015, 0.0017, 0.0011, 0.0013, 0.0016, 0.0015,\n                      0.0016, 0.0017, 0.0018, 0.0015, 0.0017, 0.0015, 0.0014, 0.0017, 0.0022,\n                      0.0018, 0.0015, 0.0019, 0.0021, 0.0018, 0.0016, 0.0017, 0.0017, 0.0020,\n                      0.0023, 0.0018, 0.0019, 0.0018, 0.0019, 0.0017, 0.0017, 0.0024, 0.0016,\n                      0.0017, 0.0013, 0.0015, 0.0019, 0.0015, 0.0020, 0.0014, 0.0016, 0.0014,\n                      0.0017, 0.0020, 0.0020, 0.0016, 0.0017, 0.0017, 0.0018, 0.0025, 0.0017,\n                      0.0019, 0.0020, 0.0013, 0.0017, 0.0016, 0.0022, 0.0016, 0.0016, 0.0015,\n                      0.0017, 0.0015, 0.0016, 0.0022, 0.0019, 0.0020, 0.0017, 0.0015, 0.0016,\n                      0.0021, 0.0017, 0.0024, 0.0018, 0.0014, 0.0016, 0.0020, 0.0019, 0.0018,\n                      0.0016, 0.0016, 0.0018, 0.0014, 0.0017, 0.0021, 0.0021, 0.0022, 0.0017,\n                      0.0018, 0.0016, 0.0021, 0.0016, 0.0020, 0.0022, 0.0013, 0.0016, 0.0016,\n                      0.0017, 0.0018, 0.0015, 0.0019, 0.0019, 0.0015, 0.0021, 0.0016, 0.0015,\n                      0.0016, 0.0013, 0.0017, 0.0017, 0.0015, 0.0020, 0.0021, 0.0020, 0.0015,\n                      0.0016, 0.0015, 0.0015, 0.0017, 0.0021, 0.0016, 0.0014, 0.0015, 0.0018,\n                      0.0020, 0.0016, 0.0022, 0.0020, 0.0013, 0.0013, 0.0017, 0.0016, 0.0016,\n                      0.0013, 0.0017, 0.0022, 0.0018, 0.0014, 0.0020, 0.0018, 0.0017, 0.0022,\n                      0.0016, 0.0016, 0.0019, 0.0016, 0.0015, 0.0016, 0.0015],\n                     dtype=torch.float64),\n                     zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n                     axis=0)),\n             ('mnet.features.14.conv.2.bias',\n              Parameter containing:\n              tensor([ 0.1255,  0.0761, -0.1711, -0.0100, -0.1320,  0.4492, -0.1705, -0.3759,\n                      -0.0301, -0.4514, -0.5847, -0.1675, -0.1254, -0.2214,  0.0158, -0.1552,\n                      -0.1966,  0.4459,  0.2734, -0.5303,  0.1801, -0.0643, -0.1023, -0.4262,\n                       0.1198,  0.1029, -0.2650,  0.1724,  1.1490,  0.4591,  0.0053,  0.0771,\n                      -0.1472,  0.2679,  0.2430, -0.1993,  0.0236, -0.0208, -0.3541,  0.1594,\n                       0.0219,  0.2886,  0.2130,  0.4253, -0.1082, -0.2826, -0.2032,  0.5059,\n                       0.0891,  0.2573,  0.2995,  0.1374,  0.1058,  0.2130,  0.4147, -0.1581,\n                       0.3486,  0.1314, -0.0150, -0.1518,  0.0956,  0.5219,  0.3248, -0.1225,\n                       0.3800,  0.3057, -0.0716, -0.3951, -0.0138,  0.2197,  0.4017,  0.2691,\n                      -0.6471,  0.1490,  0.2369, -0.3180,  0.1546, -0.0944, -0.0219,  0.1885,\n                      -0.0954,  0.0021,  0.1002, -0.0840,  0.0902,  0.0585,  0.2515, -0.1809,\n                       0.4025, -0.1752,  0.3614, -0.3247, -0.3180, -0.1275, -0.0347,  0.2309,\n                       0.0790, -0.0044, -0.0245, -0.1250,  0.2869, -0.1514, -0.1164, -0.2356,\n                       0.3758, -0.1236,  0.5532,  0.3263,  0.1234, -0.5773,  0.1384,  0.0412,\n                       0.3135,  0.0145, -0.3334, -0.1529, -0.1417,  0.3756,  0.6349,  0.0810,\n                       0.0033, -0.2836,  0.1913, -0.1261, -0.3272, -0.2288, -0.0152, -0.0753,\n                       0.0078,  0.1596,  0.3302, -0.0520,  0.1825,  0.3666,  0.4040, -0.0330,\n                      -0.2007,  0.0604, -0.1042,  0.0673, -0.1723, -0.0902, -0.1984,  0.5280,\n                       0.1554,  0.1081,  0.1048,  0.1708,  0.3541,  0.5976, -0.1665,  0.0988,\n                      -0.4779, -0.3549, -0.6204, -0.5696, -0.5209,  0.0628, -0.1385,  0.2394],\n                     requires_grad=True)),\n             ('mnet.features.14.conv.2.scale', tensor(0.1464)),\n             ('mnet.features.14.conv.2.zero_point', tensor(67)),\n             ('mnet.features.15.conv.0.0.weight',\n              tensor([[[[ 0.0311]],\n              \n                       [[ 0.0396]],\n              \n                       [[-0.0589]],\n              \n                       ...,\n              \n                       [[ 0.0000]],\n              \n                       [[ 0.0032]],\n              \n                       [[ 0.0150]]],\n              \n              \n                      [[[-0.0154]],\n              \n                       [[ 0.0077]],\n              \n                       [[-0.0608]],\n              \n                       ...,\n              \n                       [[-0.0608]],\n              \n                       [[-0.0351]],\n              \n                       [[ 0.0248]]],\n              \n              \n                      [[[ 0.0105]],\n              \n                       [[-0.0041]],\n              \n                       [[-0.0299]],\n              \n                       ...,\n              \n                       [[-0.0006]],\n              \n                       [[-0.0123]],\n              \n                       [[-0.0187]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-0.0148]],\n              \n                       [[-0.0070]],\n              \n                       [[ 0.0026]],\n              \n                       ...,\n              \n                       [[ 0.0000]],\n              \n                       [[ 0.0611]],\n              \n                       [[ 0.0183]]],\n              \n              \n                      [[[ 0.0161]],\n              \n                       [[ 0.0464]],\n              \n                       [[ 0.0275]],\n              \n                       ...,\n              \n                       [[ 0.0090]],\n              \n                       [[-0.0009]],\n              \n                       [[-0.0033]]],\n              \n              \n                      [[[ 0.0497]],\n              \n                       [[ 0.0309]],\n              \n                       [[ 0.0666]],\n              \n                       ...,\n              \n                       [[-0.0400]],\n              \n                       [[-0.0636]],\n              \n                       [[ 0.0103]]]], size=(960, 160, 1, 1), dtype=torch.qint8,\n                     quantization_scheme=torch.per_channel_affine,\n                     scale=tensor([0.0011, 0.0009, 0.0006, 0.0013, 0.0010, 0.0010, 0.0008, 0.0006, 0.0013,\n                      0.0010, 0.0006, 0.0007, 0.0012, 0.0013, 0.0009, 0.0009, 0.0014, 0.0011,\n                      0.0006, 0.0014, 0.0005, 0.0011, 0.0014, 0.0008, 0.0012, 0.0010, 0.0007,\n                      0.0007, 0.0007, 0.0010, 0.0005, 0.0009, 0.0007, 0.0009, 0.0016, 0.0007,\n                      0.0010, 0.0008, 0.0006, 0.0009, 0.0011, 0.0009, 0.0012, 0.0011, 0.0005,\n                      0.0009, 0.0010, 0.0009, 0.0009, 0.0008, 0.0014, 0.0010, 0.0011, 0.0012,\n                      0.0007, 0.0009, 0.0005, 0.0009, 0.0015, 0.0011, 0.0013, 0.0011, 0.0010,\n                      0.0008, 0.0008, 0.0011, 0.0005, 0.0011, 0.0011, 0.0008, 0.0010, 0.0008,\n                      0.0010, 0.0005, 0.0009, 0.0010, 0.0011, 0.0007, 0.0009, 0.0009, 0.0010,\n                      0.0009, 0.0008, 0.0010, 0.0011, 0.0010, 0.0010, 0.0007, 0.0009, 0.0005,\n                      0.0013, 0.0012, 0.0004, 0.0007, 0.0007, 0.0009, 0.0010, 0.0007, 0.0007,\n                      0.0007, 0.0013, 0.0006, 0.0009, 0.0008, 0.0008, 0.0014, 0.0007, 0.0007,\n                      0.0007, 0.0012, 0.0011, 0.0008, 0.0006, 0.0007, 0.0007, 0.0016, 0.0006,\n                      0.0009, 0.0006, 0.0013, 0.0012, 0.0006, 0.0008, 0.0010, 0.0009, 0.0007,\n                      0.0007, 0.0010, 0.0017, 0.0009, 0.0012, 0.0008, 0.0007, 0.0012, 0.0005,\n                      0.0007, 0.0007, 0.0011, 0.0010, 0.0008, 0.0008, 0.0010, 0.0006, 0.0011,\n                      0.0007, 0.0017, 0.0006, 0.0011, 0.0012, 0.0007, 0.0006, 0.0009, 0.0010,\n                      0.0006, 0.0010, 0.0013, 0.0007, 0.0009, 0.0008, 0.0008, 0.0010, 0.0014,\n                      0.0010, 0.0016, 0.0013, 0.0013, 0.0011, 0.0012, 0.0013, 0.0009, 0.0007,\n                      0.0006, 0.0009, 0.0009, 0.0011, 0.0004, 0.0009, 0.0005, 0.0009, 0.0015,\n                      0.0010, 0.0008, 0.0007, 0.0008, 0.0013, 0.0009, 0.0012, 0.0007, 0.0005,\n                      0.0010, 0.0008, 0.0011, 0.0005, 0.0011, 0.0006, 0.0016, 0.0009, 0.0012,\n                      0.0008, 0.0008, 0.0008, 0.0005, 0.0012, 0.0008, 0.0004, 0.0005, 0.0010,\n                      0.0003, 0.0008, 0.0008, 0.0011, 0.0007, 0.0012, 0.0006, 0.0009, 0.0007,\n                      0.0007, 0.0008, 0.0008, 0.0010, 0.0005, 0.0010, 0.0015, 0.0011, 0.0008,\n                      0.0011, 0.0005, 0.0010, 0.0007, 0.0006, 0.0008, 0.0013, 0.0007, 0.0010,\n                      0.0009, 0.0006, 0.0008, 0.0013, 0.0007, 0.0008, 0.0009, 0.0005, 0.0013,\n                      0.0008, 0.0010, 0.0010, 0.0009, 0.0007, 0.0006, 0.0010, 0.0008, 0.0007,\n                      0.0007, 0.0005, 0.0008, 0.0005, 0.0013, 0.0010, 0.0007, 0.0006, 0.0011,\n                      0.0010, 0.0008, 0.0008, 0.0009, 0.0010, 0.0010, 0.0010, 0.0008, 0.0005,\n                      0.0009, 0.0010, 0.0006, 0.0005, 0.0009, 0.0008, 0.0011, 0.0009, 0.0004,\n                      0.0008, 0.0007, 0.0011, 0.0006, 0.0007, 0.0019, 0.0009, 0.0009, 0.0007,\n                      0.0012, 0.0013, 0.0007, 0.0005, 0.0008, 0.0008, 0.0008, 0.0008, 0.0005,\n                      0.0013, 0.0007, 0.0007, 0.0009, 0.0010, 0.0008, 0.0011, 0.0009, 0.0008,\n                      0.0023, 0.0007, 0.0016, 0.0013, 0.0008, 0.0011, 0.0005, 0.0009, 0.0007,\n                      0.0010, 0.0007, 0.0006, 0.0014, 0.0008, 0.0010, 0.0010, 0.0004, 0.0010,\n                      0.0012, 0.0008, 0.0008, 0.0008, 0.0013, 0.0008, 0.0006, 0.0011, 0.0008,\n                      0.0010, 0.0011, 0.0012, 0.0011, 0.0005, 0.0008, 0.0011, 0.0006, 0.0006,\n                      0.0010, 0.0007, 0.0009, 0.0013, 0.0006, 0.0010, 0.0007, 0.0006, 0.0011,\n                      0.0008, 0.0011, 0.0008, 0.0009, 0.0007, 0.0011, 0.0008, 0.0006, 0.0007,\n                      0.0011, 0.0010, 0.0016, 0.0011, 0.0009, 0.0007, 0.0008, 0.0014, 0.0005,\n                      0.0008, 0.0006, 0.0012, 0.0006, 0.0007, 0.0010, 0.0011, 0.0009, 0.0010,\n                      0.0016, 0.0009, 0.0010, 0.0011, 0.0008, 0.0009, 0.0010, 0.0007, 0.0008,\n                      0.0009, 0.0010, 0.0015, 0.0014, 0.0005, 0.0008, 0.0009, 0.0007, 0.0007,\n                      0.0007, 0.0008, 0.0009, 0.0009, 0.0008, 0.0007, 0.0009, 0.0009, 0.0010,\n                      0.0006, 0.0011, 0.0008, 0.0012, 0.0011, 0.0010, 0.0008, 0.0008, 0.0010,\n                      0.0013, 0.0005, 0.0008, 0.0009, 0.0008, 0.0007, 0.0010, 0.0009, 0.0014,\n                      0.0009, 0.0010, 0.0010, 0.0007, 0.0008, 0.0006, 0.0007, 0.0010, 0.0010,\n                      0.0009, 0.0007, 0.0009, 0.0010, 0.0008, 0.0008, 0.0012, 0.0007, 0.0006,\n                      0.0008, 0.0008, 0.0012, 0.0005, 0.0008, 0.0009, 0.0006, 0.0006, 0.0009,\n                      0.0009, 0.0015, 0.0011, 0.0012, 0.0012, 0.0005, 0.0012, 0.0008, 0.0011,\n                      0.0014, 0.0011, 0.0006, 0.0011, 0.0007, 0.0014, 0.0008, 0.0005, 0.0010,\n                      0.0011, 0.0009, 0.0005, 0.0006, 0.0005, 0.0011, 0.0007, 0.0007, 0.0010,\n                      0.0010, 0.0007, 0.0004, 0.0011, 0.0009, 0.0009, 0.0011, 0.0006, 0.0006,\n                      0.0006, 0.0005, 0.0012, 0.0009, 0.0007, 0.0014, 0.0008, 0.0008, 0.0008,\n                      0.0009, 0.0009, 0.0007, 0.0012, 0.0010, 0.0009, 0.0008, 0.0007, 0.0005,\n                      0.0009, 0.0005, 0.0015, 0.0013, 0.0005, 0.0011, 0.0012, 0.0007, 0.0006,\n                      0.0017, 0.0012, 0.0011, 0.0007, 0.0007, 0.0007, 0.0008, 0.0007, 0.0012,\n                      0.0024, 0.0011, 0.0010, 0.0007, 0.0009, 0.0006, 0.0007, 0.0008, 0.0006,\n                      0.0007, 0.0007, 0.0006, 0.0014, 0.0016, 0.0013, 0.0012, 0.0009, 0.0011,\n                      0.0010, 0.0011, 0.0011, 0.0009, 0.0012, 0.0010, 0.0006, 0.0010, 0.0011,\n                      0.0010, 0.0013, 0.0007, 0.0008, 0.0009, 0.0003, 0.0009, 0.0007, 0.0006,\n                      0.0009, 0.0006, 0.0005, 0.0012, 0.0013, 0.0008, 0.0007, 0.0010, 0.0005,\n                      0.0009, 0.0008, 0.0008, 0.0005, 0.0012, 0.0012, 0.0014, 0.0005, 0.0014,\n                      0.0014, 0.0008, 0.0006, 0.0007, 0.0005, 0.0007, 0.0014, 0.0010, 0.0012,\n                      0.0009, 0.0012, 0.0009, 0.0011, 0.0004, 0.0010, 0.0009, 0.0010, 0.0010,\n                      0.0008, 0.0009, 0.0009, 0.0010, 0.0008, 0.0010, 0.0009, 0.0010, 0.0008,\n                      0.0007, 0.0006, 0.0009, 0.0011, 0.0009, 0.0005, 0.0010, 0.0006, 0.0007,\n                      0.0010, 0.0007, 0.0009, 0.0011, 0.0006, 0.0004, 0.0013, 0.0010, 0.0007,\n                      0.0008, 0.0012, 0.0014, 0.0006, 0.0011, 0.0010, 0.0010, 0.0007, 0.0012,\n                      0.0007, 0.0005, 0.0011, 0.0011, 0.0009, 0.0005, 0.0009, 0.0007, 0.0007,\n                      0.0006, 0.0007, 0.0011, 0.0009, 0.0015, 0.0011, 0.0013, 0.0010, 0.0007,\n                      0.0011, 0.0007, 0.0009, 0.0010, 0.0012, 0.0009, 0.0008, 0.0012, 0.0009,\n                      0.0006, 0.0005, 0.0011, 0.0011, 0.0008, 0.0008, 0.0009, 0.0010, 0.0007,\n                      0.0013, 0.0008, 0.0013, 0.0010, 0.0011, 0.0005, 0.0010, 0.0007, 0.0012,\n                      0.0006, 0.0009, 0.0007, 0.0009, 0.0010, 0.0012, 0.0010, 0.0007, 0.0010,\n                      0.0005, 0.0004, 0.0006, 0.0010, 0.0011, 0.0008, 0.0008, 0.0013, 0.0008,\n                      0.0011, 0.0008, 0.0008, 0.0010, 0.0008, 0.0006, 0.0009, 0.0007, 0.0009,\n                      0.0013, 0.0009, 0.0008, 0.0009, 0.0011, 0.0007, 0.0009, 0.0007, 0.0008,\n                      0.0008, 0.0012, 0.0011, 0.0009, 0.0007, 0.0008, 0.0008, 0.0007, 0.0010,\n                      0.0010, 0.0006, 0.0010, 0.0011, 0.0016, 0.0017, 0.0007, 0.0009, 0.0008,\n                      0.0012, 0.0008, 0.0009, 0.0009, 0.0013, 0.0014, 0.0007, 0.0014, 0.0008,\n                      0.0010, 0.0011, 0.0010, 0.0005, 0.0009, 0.0011, 0.0007, 0.0006, 0.0012,\n                      0.0011, 0.0009, 0.0007, 0.0007, 0.0010, 0.0007, 0.0009, 0.0006, 0.0008,\n                      0.0006, 0.0007, 0.0012, 0.0011, 0.0006, 0.0009, 0.0008, 0.0005, 0.0009,\n                      0.0007, 0.0013, 0.0010, 0.0010, 0.0006, 0.0006, 0.0004, 0.0008, 0.0010,\n                      0.0009, 0.0009, 0.0008, 0.0006, 0.0005, 0.0010, 0.0009, 0.0010, 0.0008,\n                      0.0012, 0.0008, 0.0005, 0.0007, 0.0008, 0.0012, 0.0008, 0.0011, 0.0007,\n                      0.0013, 0.0012, 0.0010, 0.0009, 0.0009, 0.0008, 0.0005, 0.0006, 0.0005,\n                      0.0011, 0.0006, 0.0011, 0.0006, 0.0009, 0.0007, 0.0005, 0.0012, 0.0004,\n                      0.0009, 0.0005, 0.0008, 0.0008, 0.0010, 0.0013, 0.0006, 0.0017, 0.0009,\n                      0.0007, 0.0006, 0.0008, 0.0012, 0.0010, 0.0010, 0.0008, 0.0007, 0.0007,\n                      0.0008, 0.0010, 0.0010, 0.0012, 0.0006, 0.0012, 0.0009, 0.0010, 0.0006,\n                      0.0008, 0.0009, 0.0010, 0.0005, 0.0015, 0.0006, 0.0008, 0.0009, 0.0009,\n                      0.0009, 0.0011, 0.0011, 0.0008, 0.0008, 0.0010, 0.0011, 0.0016, 0.0006,\n                      0.0013, 0.0013, 0.0005, 0.0005, 0.0013, 0.0012, 0.0030, 0.0010, 0.0006,\n                      0.0009, 0.0012, 0.0008, 0.0010, 0.0009, 0.0010, 0.0013, 0.0008, 0.0006,\n                      0.0009, 0.0013, 0.0011, 0.0008, 0.0006, 0.0012, 0.0004, 0.0012, 0.0008,\n                      0.0006, 0.0007, 0.0007, 0.0011, 0.0013, 0.0007, 0.0009, 0.0006, 0.0010,\n                      0.0008, 0.0007, 0.0011, 0.0010, 0.0008, 0.0008, 0.0013, 0.0011, 0.0010,\n                      0.0009, 0.0008, 0.0007, 0.0013, 0.0010, 0.0007, 0.0011, 0.0010, 0.0010,\n                      0.0010, 0.0004, 0.0010, 0.0012, 0.0007, 0.0009, 0.0009, 0.0011, 0.0004,\n                      0.0005, 0.0014, 0.0011, 0.0010, 0.0005, 0.0010, 0.0008, 0.0010, 0.0009,\n                      0.0008, 0.0010, 0.0009, 0.0006, 0.0010, 0.0004, 0.0007, 0.0011, 0.0014,\n                      0.0007, 0.0011, 0.0009, 0.0007, 0.0009, 0.0010, 0.0009, 0.0007, 0.0009,\n                      0.0009, 0.0003, 0.0006, 0.0013, 0.0010, 0.0009, 0.0007, 0.0008, 0.0008,\n                      0.0009, 0.0007, 0.0011, 0.0009, 0.0005, 0.0006], dtype=torch.float64),\n                     zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n                     axis=0)),\n             ('mnet.features.15.conv.0.0.bias',\n              Parameter containing:\n              tensor([-9.3952e-03, -3.6496e-02,  1.5111e-01, -2.3915e-02,  1.1575e-01,\n                       7.8541e-02,  1.9701e-02,  3.4584e-01,  8.7781e-02,  4.6066e-02,\n                       1.8502e-01, -5.9827e-02, -2.2715e-02,  6.4350e-02,  6.5672e-02,\n                       9.8547e-02,  8.1740e-02,  1.4608e-01,  8.0139e-02,  3.2211e-02,\n                       1.0674e-01, -2.8262e-02, -2.5695e-02,  1.2891e-01, -7.1406e-02,\n                       5.8735e-02, -5.8824e-02,  1.6301e-01,  1.3740e-01, -2.3037e-02,\n                       1.1668e-01,  5.4503e-02,  2.6537e-02,  9.0827e-02,  2.0868e-02,\n                       5.2027e-02,  3.3893e-02,  8.4898e-03,  1.8047e-01,  6.1560e-02,\n                       5.7627e-03, -9.5370e-03, -6.6688e-02, -2.6605e-02,  6.1330e-02,\n                       8.8794e-02, -7.9655e-03, -5.4384e-03,  1.0946e-02,  1.2027e-01,\n                       1.3419e-02,  1.2284e-02, -2.5161e-02,  5.3197e-02,  9.2105e-02,\n                       3.6639e-02,  1.7613e-01,  7.4494e-02,  3.3249e-02, -4.6375e-02,\n                      -3.8577e-02, -1.6435e-02,  7.9496e-02,  6.6197e-02,  8.9374e-03,\n                       8.1832e-02,  1.4275e-01,  9.2395e-02,  6.9452e-02, -1.5504e-03,\n                       1.0385e-02,  5.5232e-02,  2.4484e-02, -3.8758e-02,  5.0370e-02,\n                      -3.7921e-02, -4.1190e-02, -6.9397e-02,  6.0536e-02, -3.8191e-02,\n                      -1.3583e-02,  7.4048e-02,  9.3237e-02,  6.7330e-02,  5.9497e-02,\n                      -6.8266e-02,  5.5542e-02,  3.7526e-02, -4.5205e-02,  1.4042e-01,\n                       2.2568e-02,  5.9072e-02, -7.3585e-03,  1.1159e-01,  2.1749e-02,\n                      -4.3519e-02,  6.3166e-02, -3.9805e-02, -3.3806e-02,  3.8418e-02,\n                       5.6847e-02,  7.7038e-02, -4.4706e-02,  2.0880e-02,  1.7982e-01,\n                      -3.2159e-02,  4.2963e-02,  5.7924e-02,  1.0133e-01, -7.3281e-02,\n                       5.0532e-02,  8.7776e-02, -2.1244e-02,  1.2112e-01, -3.7938e-02,\n                       1.1057e-01,  3.8204e-02,  6.4421e-03,  1.7940e-02, -6.3015e-03,\n                       5.9761e-03,  6.7452e-02, -3.6205e-02,  5.4835e-03,  7.5029e-02,\n                       5.8734e-02,  9.8154e-02,  9.6092e-02, -5.6330e-02,  4.6885e-02,\n                       1.4732e-01,  6.1857e-02,  1.1143e-01, -1.7001e-02,  1.1348e-01,\n                       6.4308e-02, -3.7793e-02, -5.2014e-02,  7.7340e-02,  6.7838e-02,\n                      -9.7771e-02,  9.6881e-02,  1.2219e-01, -5.0353e-03,  5.2678e-02,\n                      -2.1722e-02,  1.0695e-01,  5.2655e-02,  1.9638e-02, -2.6258e-03,\n                       9.6476e-02,  1.5960e-02, -1.1039e-01,  1.1715e-01,  4.5698e-02,\n                       8.3585e-02,  2.2871e-02,  8.1092e-02,  2.8178e-02,  7.3396e-02,\n                       4.7031e-03, -7.8341e-02,  6.8329e-02,  4.0356e-02,  6.6241e-02,\n                       1.9625e-02,  3.3233e-02,  4.6494e-03,  1.0933e-02,  8.4588e-03,\n                       3.6110e-02,  7.6063e-02,  1.1979e-01,  1.8864e-02,  1.0177e-01,\n                      -1.8580e-02,  5.8841e-02,  3.5193e-03,  1.4016e-02,  2.0682e-02,\n                      -3.2535e-02,  9.5005e-02,  9.7265e-03,  9.5843e-02,  1.3968e-01,\n                       2.3154e-02, -1.1397e-01,  4.9913e-02,  1.4268e-01,  8.6420e-02,\n                      -5.0241e-02,  5.7544e-02,  1.1419e-01, -1.7841e-03,  9.5686e-02,\n                      -1.4139e-02,  5.1578e-03, -4.6598e-02, -2.8922e-02, -5.8262e-02,\n                       1.9271e-01,  1.5587e-02,  2.9460e-02,  5.9790e-02,  1.0525e-01,\n                       1.1567e-01, -3.5897e-02, -6.1596e-03,  1.3380e-02, -5.8407e-02,\n                      -4.7660e-02,  2.0225e-01,  3.3237e-02,  9.7106e-02,  3.1887e-04,\n                       1.1135e-02,  1.3075e-01,  1.4446e-02,  1.0629e-01, -2.1596e-02,\n                       4.4589e-02,  1.5940e-01, -3.5623e-03,  3.3297e-02,  1.1112e-01,\n                       5.2028e-02,  4.3283e-02,  3.0515e-02,  1.0281e-01,  2.9562e-02,\n                       7.0200e-02,  5.4439e-03,  1.0346e-01,  7.9710e-02, -5.0024e-02,\n                       1.1446e-01, -3.3287e-02, -1.3378e-01,  1.5337e-01, -1.2153e-02,\n                       7.0216e-03,  6.9175e-02,  8.7669e-02,  1.2383e-01, -6.5278e-02,\n                      -3.5727e-03,  1.4833e-01, -2.9402e-03,  5.5770e-02, -2.5034e-02,\n                      -2.5110e-02,  4.7449e-02,  1.8191e-01,  2.2762e-02, -1.5926e-02,\n                       7.9638e-02,  8.8326e-02, -1.4124e-02,  9.3047e-02, -4.5028e-02,\n                       1.2160e-02,  8.5041e-02,  5.7729e-02,  1.4796e-01,  6.6277e-02,\n                       9.2847e-04, -5.9398e-03,  4.6569e-02,  5.7032e-02,  4.4916e-02,\n                      -1.8902e-03,  6.5122e-02,  1.0063e-01,  1.3282e-01,  5.2505e-02,\n                       1.2473e-01, -1.5472e-02, -6.1541e-02,  1.5423e-02,  4.4917e-02,\n                      -4.7735e-02, -6.7754e-03,  7.5054e-02,  3.5625e-02,  8.3969e-02,\n                      -4.4354e-02,  5.6635e-02,  1.0620e-01,  4.3976e-02,  7.9251e-02,\n                       2.1234e-02,  1.3906e-01, -2.9080e-02,  4.4215e-02,  7.7067e-02,\n                       2.0112e-02,  1.1433e-01,  9.1892e-02,  5.8589e-03, -1.1884e-02,\n                       1.0232e-02, -5.9216e-02,  1.5453e-01, -3.1950e-02,  9.2516e-02,\n                       9.5114e-02,  8.8482e-02,  6.6140e-02,  3.7731e-02, -7.1823e-04,\n                      -5.0001e-02,  9.3503e-02, -1.7222e-02, -8.2961e-03, -1.3299e-02,\n                      -2.5877e-02, -3.6950e-03,  1.2174e-01, -4.2515e-02, -4.5624e-02,\n                      -2.3361e-02, -6.8234e-02, -2.3400e-02,  1.2682e-01,  6.5366e-02,\n                      -1.1752e-02,  1.0344e-01, -1.0566e-02,  4.5998e-02, -2.3574e-02,\n                       4.3188e-02, -3.9973e-02,  1.3882e-01, -4.0766e-02,  9.1792e-02,\n                       2.8580e-02, -2.6962e-02,  5.6216e-02,  7.9186e-05, -3.9465e-02,\n                       9.5852e-02,  1.3995e-04,  1.0836e-01,  3.5089e-02,  1.9659e-02,\n                      -6.3445e-03,  2.1677e-02, -9.2483e-03,  1.4112e-02,  1.1386e-01,\n                       5.0770e-02,  1.5670e-01, -7.7197e-03,  8.3809e-02, -1.5783e-01,\n                       1.1971e-01,  4.5849e-04,  7.1389e-02,  1.8989e-01,  1.4644e-01,\n                       7.8879e-02, -3.1818e-02, -1.0186e-01,  6.9514e-02, -2.6904e-02,\n                       9.5556e-02,  1.4261e-01,  1.0254e-01,  1.5023e-01, -4.7427e-02,\n                       1.7998e-02,  1.2513e-01,  6.8123e-02, -1.0916e-01, -5.7204e-02,\n                      -5.1166e-02,  1.3481e-02, -4.6826e-02, -1.1512e-02, -4.3405e-02,\n                       9.4055e-02,  3.7847e-02,  3.3529e-02,  5.0378e-02, -4.7116e-02,\n                       7.0407e-02, -4.6982e-02, -8.9964e-02,  9.6738e-03,  3.0702e-02,\n                       3.2746e-03,  1.2490e-01, -1.6523e-02,  5.4609e-02, -5.9747e-02,\n                       2.2160e-02,  8.6210e-02,  1.3775e-01, -2.8421e-02, -2.0576e-02,\n                       1.1785e-01, -3.4669e-02,  6.9775e-02,  7.0229e-03, -6.9188e-03,\n                       2.2535e-02,  1.2704e-01,  6.8272e-02,  2.8537e-02, -8.1881e-03,\n                       2.9498e-02,  6.0740e-02,  1.6703e-02, -4.6836e-02,  2.6509e-02,\n                      -1.6999e-02,  8.5207e-02,  1.4387e-01,  4.7964e-02, -1.6975e-02,\n                      -2.4017e-03,  4.1001e-02,  7.8118e-03, -2.0707e-02,  2.7532e-02,\n                       9.8196e-02,  1.1901e-02,  5.7498e-02,  9.8367e-02, -5.5805e-03,\n                       2.3817e-02,  1.2017e-01,  4.8785e-02,  8.5225e-02,  7.9274e-02,\n                       1.1617e-01,  7.3165e-02,  1.1588e-01,  6.1953e-02,  2.8219e-02,\n                       1.3357e-01,  2.6790e-02,  4.9007e-02,  3.2393e-02, -6.6959e-02,\n                       1.4266e-02, -3.0441e-02,  1.8008e-01,  1.1341e-01,  5.5544e-02,\n                       7.0942e-03, -4.9076e-02,  1.4109e-02,  8.0702e-02,  1.6437e-02,\n                       1.2067e-01,  2.4625e-02, -3.3650e-02,  7.2374e-02,  3.3209e-02,\n                       2.2314e-02,  1.1543e-01,  2.9337e-02,  3.4522e-02, -7.4588e-02,\n                       1.4403e-01,  1.3155e-01,  4.5390e-03,  1.9267e-03,  2.5845e-02,\n                       8.9170e-02,  1.4508e-01,  1.6788e-01, -2.0085e-02,  2.2124e-02,\n                       6.1666e-02,  1.4701e-01, -5.0272e-02,  1.3243e-01,  1.0526e-01,\n                      -6.9000e-02,  9.5998e-02,  5.9799e-02,  9.1607e-02,  1.1336e-01,\n                       1.0269e-01,  1.1122e-01, -5.4764e-03,  8.3639e-02,  1.6378e-02,\n                       1.2706e-01, -3.4545e-02,  7.7905e-02,  7.7033e-02,  4.0190e-02,\n                       7.2266e-02, -3.5128e-03, -6.6765e-02,  3.9291e-02,  1.3803e-02,\n                       5.8393e-02, -7.2412e-03, -4.2276e-02,  8.3599e-02,  7.0015e-02,\n                       1.4476e-01,  8.4403e-02, -8.5782e-03,  6.5529e-02,  6.3374e-02,\n                       3.4220e-02,  1.3942e-01,  8.4011e-02, -4.0391e-02,  3.6890e-02,\n                      -8.6205e-03, -5.3328e-02,  1.5608e-03,  1.0804e-01,  5.5910e-02,\n                       1.0217e-01,  1.3658e-01,  2.6294e-02,  2.1170e-02,  1.2377e-01,\n                       8.3523e-02,  1.1213e-01, -4.2547e-02,  1.2993e-01, -1.8759e-02,\n                       3.8880e-02,  3.9444e-02,  3.2488e-02,  7.1474e-02, -1.0453e-02,\n                       2.6282e-03, -1.5724e-02,  3.3601e-02, -1.5012e-02,  6.2033e-02,\n                       4.2559e-02,  1.4301e-02,  3.5298e-02,  2.3353e-02,  1.7634e-01,\n                       1.3704e-02, -5.2459e-02,  4.2464e-02,  1.7798e-02, -2.3470e-02,\n                       2.0662e-02,  1.2859e-02,  6.3163e-02, -8.1403e-02, -2.5515e-02,\n                       3.4121e-02,  6.2841e-02,  6.9419e-02,  2.6792e-03,  1.2434e-01,\n                       4.8556e-02,  6.2493e-02,  5.2989e-02, -1.5746e-02,  7.3833e-02,\n                      -6.5680e-02, -3.5754e-02,  5.1043e-02,  7.3880e-02,  8.7104e-02,\n                       2.1398e-02,  7.8497e-02,  4.8453e-02,  1.2166e-02,  1.1162e-01,\n                       4.7296e-02, -3.8375e-02,  3.0934e-02, -4.8862e-02,  9.2888e-02,\n                      -2.7322e-02,  1.8814e-02,  1.9982e-02,  7.3930e-02,  1.6213e-02,\n                       4.3357e-02, -1.0737e-02,  9.2829e-02,  8.0973e-02,  9.9451e-02,\n                       3.5559e-02,  1.0477e-01,  3.5471e-02, -1.3979e-02,  3.4829e-02,\n                       4.5895e-02,  3.4194e-02, -5.1229e-02,  3.8014e-02,  2.0121e-02,\n                      -1.5397e-02, -1.1105e-02, -4.1818e-02,  1.3638e-01,  1.4055e-01,\n                      -8.5693e-02,  5.6480e-02,  6.8547e-03,  7.8597e-02,  6.8861e-02,\n                       6.6422e-02,  1.7882e-01, -3.3708e-02, -5.4036e-02,  6.5875e-02,\n                       1.9351e-02, -3.6416e-02, -2.7253e-02,  1.2343e-01,  4.0131e-03,\n                       8.0601e-02,  8.0705e-04,  6.8661e-02,  3.7746e-02,  9.7605e-02,\n                       7.7080e-02,  1.4468e-01,  1.1971e-01,  7.6488e-02,  3.4905e-02,\n                       5.0915e-02, -1.4119e-03,  8.5968e-02,  2.9262e-02,  9.4414e-02,\n                       1.1524e-01,  1.2045e-01,  6.6332e-02, -7.1824e-03,  2.1221e-02,\n                       8.1056e-02,  5.9030e-02,  6.0399e-02, -2.2002e-02,  5.5103e-02,\n                      -1.6392e-02,  2.2321e-02,  5.2376e-02, -4.1696e-03,  1.4903e-01,\n                       3.1613e-03,  9.5713e-02,  4.6091e-02, -2.2735e-02,  1.1934e-01,\n                      -1.5150e-03,  6.3438e-02,  9.7788e-02,  1.1562e-01,  4.5689e-02,\n                      -1.6795e-02,  2.1529e-02,  8.4944e-02,  5.8696e-02,  1.0023e-01,\n                      -3.4194e-02,  2.1716e-02,  4.0424e-02, -2.5759e-03, -1.9504e-02,\n                       6.4487e-02,  7.5260e-02, -5.6865e-02,  7.5038e-02,  3.2815e-02,\n                      -5.1402e-02,  2.7023e-02,  1.0989e-01, -3.6438e-02,  8.8214e-02,\n                      -2.8861e-03,  5.4015e-02,  6.6532e-02,  7.4200e-03,  8.3978e-02,\n                       1.2314e-01,  1.2639e-01,  7.9038e-02, -9.5115e-02, -1.7274e-02,\n                      -3.8937e-02, -4.0991e-02, -1.7285e-02, -8.6332e-02,  1.3421e-01,\n                      -2.6571e-02,  1.0399e-01, -1.0262e-02,  7.5865e-02,  1.1388e-01,\n                       1.0261e-01,  5.3052e-02,  6.4154e-02, -1.8004e-02, -1.5221e-01,\n                      -3.3312e-02,  1.9567e-02,  1.2764e-01,  6.0112e-02, -8.1563e-02,\n                      -5.8941e-02, -1.4674e-02,  5.0529e-02, -5.2796e-03,  4.2728e-03,\n                       8.6695e-02,  9.1913e-02,  1.3502e-02,  1.4444e-01,  5.4226e-02,\n                       1.1840e-01,  8.3185e-02, -7.9275e-02, -7.4883e-02,  7.1110e-02,\n                      -2.6417e-02,  3.7256e-02,  8.3310e-02, -5.5261e-02,  1.0416e-01,\n                       9.6036e-02,  7.4822e-02,  6.3481e-02,  1.4501e-02, -1.8450e-03,\n                       4.7826e-03,  6.7870e-02,  8.8524e-02,  1.0318e-01, -1.2140e-02,\n                       6.1513e-02,  7.7391e-02,  3.6119e-02,  2.3163e-02,  9.5588e-02,\n                       9.0856e-03,  2.3778e-02,  3.8202e-03,  3.3415e-03,  6.5507e-02,\n                      -1.3237e-03,  7.7877e-02, -2.0609e-02, -2.7320e-02, -1.6498e-02,\n                      -6.6594e-02, -4.7283e-02,  1.5757e-01,  5.8757e-02, -2.6323e-02,\n                       6.9078e-02,  9.3790e-02,  8.1615e-02,  1.0046e-01,  4.8154e-02,\n                      -1.3761e-02, -4.8227e-02,  7.5557e-02,  7.4096e-02,  8.9250e-02,\n                       1.0031e-01,  1.0152e-01,  2.7772e-02,  6.1302e-02,  1.2222e-01,\n                       6.1989e-02,  1.2384e-01,  8.2650e-03,  8.4937e-02,  8.5265e-02,\n                       1.2893e-01,  5.5323e-02, -5.8239e-02,  1.7366e-02,  4.4033e-02,\n                       1.2080e-01, -3.3354e-02,  1.2878e-01,  2.9970e-02,  1.2914e-01,\n                       9.6196e-02,  6.5932e-02, -1.4701e-02,  9.1455e-03,  1.0761e-01,\n                       1.4640e-02,  6.4418e-02,  1.5049e-01,  7.5001e-02,  1.6806e-02,\n                       1.1782e-01,  2.3372e-02,  1.8605e-02, -4.8512e-02,  1.0518e-01,\n                       1.1825e-01,  1.2795e-01,  8.0573e-02, -7.9341e-02,  8.1258e-02,\n                       1.5928e-01,  8.5917e-02,  3.2903e-02,  3.3998e-02,  3.8165e-02,\n                       9.3052e-02,  9.1199e-02,  1.1707e-01,  7.1617e-02,  4.0814e-02,\n                       1.1653e-01,  7.2113e-02,  4.7163e-02, -3.6359e-02,  9.0396e-02,\n                       6.3397e-02, -1.1389e-02,  7.1884e-02,  4.0131e-02,  9.0671e-02,\n                       2.0879e-02,  7.6479e-02,  1.8413e-01,  1.3507e-01, -2.0267e-02,\n                      -3.9111e-03,  8.6260e-02,  6.0404e-02,  4.4386e-02,  5.6168e-02,\n                       1.5017e-01,  3.5201e-02,  9.4628e-02,  1.6552e-01,  7.0414e-03,\n                       1.6689e-01,  4.4720e-02,  4.2000e-02,  4.4305e-02, -2.8494e-02,\n                      -2.4786e-02, -8.2765e-03, -1.6901e-02,  5.9106e-02,  5.5190e-02,\n                      -1.0412e-01, -1.1578e-02,  2.6397e-02,  1.0517e-01,  3.0737e-02,\n                       1.7409e-03, -2.7606e-02, -3.0881e-02, -3.0584e-02, -4.0453e-02,\n                      -1.3855e-03,  6.7126e-02, -2.4527e-02, -1.9181e-02,  5.5916e-02,\n                       7.6824e-02, -2.8942e-02, -1.2684e-02, -2.6828e-02, -2.2691e-02,\n                      -5.0545e-02, -9.3894e-02,  8.5882e-02,  6.6064e-04, -2.5420e-02,\n                      -4.2498e-03, -3.1610e-02,  7.2529e-02,  8.8400e-02,  1.2213e-01,\n                      -4.7429e-02,  6.2468e-02,  3.2225e-02, -6.1644e-02,  6.6971e-02,\n                      -5.4353e-02, -4.1154e-02,  7.2831e-02,  3.0210e-02,  3.1831e-02,\n                      -1.0163e-02,  5.6151e-02,  5.2397e-02,  4.4743e-02,  4.9695e-02,\n                       1.0478e-02,  5.6789e-02,  1.1298e-01,  2.1868e-02, -1.1525e-02,\n                       7.8464e-02,  1.1857e-02,  1.1778e-01,  1.0827e-01, -2.7199e-03,\n                      -6.2945e-02,  8.0126e-02, -2.9398e-03,  3.0911e-02,  5.8737e-02,\n                       2.5408e-03, -1.0624e-01, -8.9216e-03,  2.2851e-02,  8.1949e-02,\n                       1.5583e-01,  7.9727e-02,  1.5369e-01,  4.2091e-02,  1.1743e-02,\n                      -2.0958e-02,  2.1444e-02, -6.0952e-02,  3.9664e-02,  2.5585e-02,\n                       8.9271e-02,  1.1194e-01,  1.3584e-01,  8.6949e-02,  7.4729e-02,\n                       3.3741e-02,  3.0860e-02,  6.3955e-02, -7.4882e-03,  7.8769e-02,\n                       8.0039e-02, -2.1177e-02, -2.8212e-02,  1.3637e-01, -3.6006e-02,\n                      -1.0446e-01, -5.9753e-03,  1.1128e-01,  1.3844e-02,  3.4053e-02,\n                       9.1381e-02, -2.9698e-02,  1.5034e-02, -2.0765e-02,  5.1502e-02,\n                       6.0118e-02,  4.0852e-02,  1.4098e-01,  1.1111e-01,  2.3039e-02],\n                     requires_grad=True)),\n             ('mnet.features.15.conv.0.0.scale', tensor(0.0494)),\n             ('mnet.features.15.conv.0.0.zero_point', tensor(57)),\n             ('mnet.features.15.conv.1.0.weight',\n              tensor([[[[ 0.1044,  0.6089,  0.1218],\n                        [ 0.4523,  2.2094,  0.7829],\n                        [ 0.3131,  0.2957,  0.4001]]],\n              \n              \n                      [[[-0.7795, -0.1782, -1.0022],\n                        [-0.5568,  2.8284,  0.1559],\n                        [ 0.4900,  1.7816,  0.6013]]],\n              \n              \n                      [[[-0.4367,  0.2912, -0.5176],\n                        [-0.8088, -2.0705, -0.8411],\n                        [ 1.1808,  1.5205,  1.2779]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-0.1623, -0.3344, -0.2262],\n                        [-0.3295, -0.6294, -0.2754],\n                        [-0.1918, -0.2803, -0.2409]]],\n              \n              \n                      [[[-0.5847,  2.8561, -2.4289],\n                        [ 0.2249, -0.8546,  1.0795],\n                        [ 0.1799, -1.0795,  0.9221]]],\n              \n              \n                      [[[ 0.6497,  1.7937,  0.7768],\n                        [-0.1836,  0.7062, -0.1836],\n                        [-0.9039, -1.3700, -0.9039]]]], size=(960, 1, 3, 3),\n                     dtype=torch.qint8, quantization_scheme=torch.per_channel_affine,\n                     scale=tensor([0.0174, 0.0223, 0.0162, 0.0033, 0.0137, 0.0041, 0.0131, 0.0151, 0.0040,\n                      0.0045, 0.0057, 0.0222, 0.0062, 0.0041, 0.0198, 0.0085, 0.0035, 0.0046,\n                      0.0102, 0.0038, 0.0125, 0.0126, 0.0033, 0.0103, 0.0115, 0.0170, 0.0126,\n                      0.0064, 0.0048, 0.0090, 0.0096, 0.0113, 0.0133, 0.0043, 0.0040, 0.0121,\n                      0.0104, 0.0049, 0.0134, 0.0099, 0.0040, 0.0124, 0.0073, 0.0131, 0.0171,\n                      0.0048, 0.0143, 0.0140, 0.0168, 0.0047, 0.0032, 0.0035, 0.0156, 0.0092,\n                      0.0106, 0.0118, 0.0238, 0.0123, 0.0121, 0.0186, 0.0071, 0.0034, 0.0089,\n                      0.0085, 0.0041, 0.0089, 0.0113, 0.0040, 0.0044, 0.0115, 0.0117, 0.0111,\n                      0.0122, 0.0172, 0.0103, 0.0173, 0.0048, 0.0212, 0.0106, 0.0129, 0.0111,\n                      0.0037, 0.0053, 0.0041, 0.0040, 0.0094, 0.0113, 0.0107, 0.0056, 0.0165,\n                      0.0032, 0.0031, 0.0479, 0.0085, 0.0089, 0.0125, 0.0087, 0.0185, 0.0153,\n                      0.0163, 0.0084, 0.0118, 0.0220, 0.0190, 0.0058, 0.0129, 0.0157, 0.0118,\n                      0.0149, 0.0206, 0.0035, 0.0050, 0.0218, 0.0114, 0.0273, 0.0039, 0.0144,\n                      0.0124, 0.0290, 0.0059, 0.0026, 0.0108, 0.0125, 0.0032, 0.0132, 0.0143,\n                      0.0126, 0.0048, 0.0074, 0.0173, 0.0083, 0.0119, 0.0182, 0.0164, 0.0125,\n                      0.0121, 0.0137, 0.0120, 0.0037, 0.0090, 0.0111, 0.0093, 0.0117, 0.0071,\n                      0.0125, 0.0080, 0.0133, 0.0035, 0.0045, 0.0154, 0.0133, 0.0150, 0.0164,\n                      0.0107, 0.0037, 0.0040, 0.0122, 0.0138, 0.0160, 0.0126, 0.0121, 0.0079,\n                      0.0168, 0.0039, 0.0035, 0.0036, 0.0040, 0.0044, 0.0116, 0.0153, 0.0119,\n                      0.0117, 0.0105, 0.0105, 0.0075, 0.0294, 0.0100, 0.0275, 0.0049, 0.0039,\n                      0.0113, 0.0050, 0.0041, 0.0104, 0.0098, 0.0130, 0.0109, 0.0130, 0.0115,\n                      0.0043, 0.0151, 0.0128, 0.0134, 0.0030, 0.0051, 0.0096, 0.0048, 0.0083,\n                      0.0127, 0.0126, 0.0060, 0.0228, 0.0044, 0.0113, 0.0119, 0.0155, 0.0132,\n                      0.0372, 0.0205, 0.0069, 0.0163, 0.0147, 0.0092, 0.0314, 0.0097, 0.0146,\n                      0.0079, 0.0167, 0.0090, 0.0133, 0.0059, 0.0109, 0.0033, 0.0036, 0.0099,\n                      0.0041, 0.0186, 0.0136, 0.0088, 0.0176, 0.0067, 0.0042, 0.0129, 0.0101,\n                      0.0126, 0.0171, 0.0223, 0.0116, 0.0126, 0.0130, 0.0121, 0.0188, 0.0045,\n                      0.0113, 0.0075, 0.0130, 0.0155, 0.0271, 0.0117, 0.0125, 0.0067, 0.0093,\n                      0.0044, 0.0151, 0.0268, 0.0135, 0.0032, 0.0085, 0.0184, 0.0244, 0.0119,\n                      0.0110, 0.0043, 0.0075, 0.0084, 0.0211, 0.0189, 0.0031, 0.0139, 0.0067,\n                      0.0040, 0.0037, 0.0156, 0.0134, 0.0111, 0.0110, 0.0117, 0.0134, 0.0288,\n                      0.0085, 0.0176, 0.0040, 0.0118, 0.0152, 0.0083, 0.0088, 0.0135, 0.0103,\n                      0.0041, 0.0045, 0.0045, 0.0129, 0.0204, 0.0049, 0.0102, 0.0099, 0.0126,\n                      0.0043, 0.0056, 0.0169, 0.0039, 0.0105, 0.0062, 0.0143, 0.0097, 0.0118,\n                      0.0039, 0.0165, 0.0031, 0.0163, 0.0145, 0.0098, 0.0368, 0.0107, 0.0118,\n                      0.0053, 0.0145, 0.0146, 0.0037, 0.0103, 0.0152, 0.0076, 0.0240, 0.0087,\n                      0.0034, 0.0210, 0.0125, 0.0163, 0.0098, 0.0119, 0.0152, 0.0124, 0.0111,\n                      0.0167, 0.0050, 0.0042, 0.0086, 0.0169, 0.0043, 0.0043, 0.0216, 0.0201,\n                      0.0111, 0.0121, 0.0106, 0.0041, 0.0130, 0.0124, 0.0133, 0.0119, 0.0043,\n                      0.0045, 0.0052, 0.0196, 0.0100, 0.0120, 0.0129, 0.0077, 0.0110, 0.0117,\n                      0.0035, 0.0082, 0.0076, 0.0078, 0.0082, 0.0213, 0.0072, 0.0049, 0.0178,\n                      0.0176, 0.0145, 0.0067, 0.0133, 0.0558, 0.0181, 0.0288, 0.0149, 0.0176,\n                      0.0047, 0.0095, 0.0119, 0.0108, 0.0152, 0.0046, 0.0100, 0.0120, 0.0385,\n                      0.0187, 0.0031, 0.0036, 0.0144, 0.0125, 0.0167, 0.0260, 0.0071, 0.0124,\n                      0.0155, 0.0115, 0.0146, 0.0113, 0.0154, 0.0186, 0.0184, 0.0111, 0.0072,\n                      0.0153, 0.0036, 0.0128, 0.0044, 0.0105, 0.0038, 0.0032, 0.0110, 0.0088,\n                      0.0034, 0.0187, 0.0112, 0.0151, 0.0124, 0.0195, 0.0100, 0.0045, 0.0040,\n                      0.0093, 0.0039, 0.0114, 0.0043, 0.0147, 0.0171, 0.0079, 0.0123, 0.0059,\n                      0.0091, 0.0109, 0.0042, 0.0049, 0.0046, 0.0046, 0.0037, 0.0136, 0.0115,\n                      0.0133, 0.0132, 0.0042, 0.0664, 0.0044, 0.0080, 0.0159, 0.0050, 0.0259,\n                      0.0112, 0.0054, 0.0137, 0.0092, 0.0124, 0.0118, 0.0041, 0.0159, 0.0042,\n                      0.0036, 0.0041, 0.0125, 0.0082, 0.0082, 0.0068, 0.0107, 0.0106, 0.0137,\n                      0.0034, 0.0116, 0.0186, 0.0076, 0.0113, 0.0128, 0.0113, 0.0102, 0.0039,\n                      0.0137, 0.0122, 0.0128, 0.0120, 0.0105, 0.0037, 0.0049, 0.0207, 0.0104,\n                      0.0164, 0.0247, 0.0044, 0.0157, 0.0087, 0.0035, 0.0095, 0.0092, 0.0088,\n                      0.0114, 0.0105, 0.0065, 0.0119, 0.0033, 0.0045, 0.0141, 0.0113, 0.0133,\n                      0.0062, 0.0133, 0.0059, 0.0032, 0.0209, 0.0137, 0.0086, 0.0098, 0.0136,\n                      0.0191, 0.0095, 0.0116, 0.0134, 0.0060, 0.0122, 0.0166, 0.0147, 0.0044,\n                      0.0049, 0.0108, 0.0108, 0.0126, 0.0165, 0.0114, 0.0085, 0.0168, 0.0120,\n                      0.0128, 0.0039, 0.0113, 0.0095, 0.0041, 0.0147, 0.0031, 0.0161, 0.0099,\n                      0.0041, 0.0112, 0.0032, 0.0037, 0.0039, 0.0127, 0.0386, 0.0154, 0.0193,\n                      0.0102, 0.0037, 0.0257, 0.0119, 0.0099, 0.0335, 0.0121, 0.0126, 0.0185,\n                      0.0047, 0.0059, 0.0124, 0.0038, 0.0095, 0.0135, 0.0140, 0.0098, 0.0395,\n                      0.0051, 0.0141, 0.0113, 0.0150, 0.0074, 0.0039, 0.0028, 0.0147, 0.0035,\n                      0.0102, 0.0042, 0.0187, 0.0133, 0.0228, 0.0130, 0.0038, 0.0147, 0.0029,\n                      0.0033, 0.0145, 0.0090, 0.0042, 0.0204, 0.0039, 0.0043, 0.0044, 0.0102,\n                      0.0136, 0.0034, 0.0036, 0.0130, 0.0111, 0.0079, 0.0142, 0.0135, 0.0041,\n                      0.0088, 0.0158, 0.0267, 0.0042, 0.0110, 0.0158, 0.0041, 0.0128, 0.0077,\n                      0.0110, 0.0203, 0.0096, 0.0046, 0.0234, 0.0294, 0.0034, 0.0084, 0.0107,\n                      0.0065, 0.0035, 0.0047, 0.0117, 0.0033, 0.0044, 0.0042, 0.0094, 0.0039,\n                      0.0174, 0.0287, 0.0040, 0.0080, 0.0083, 0.0131, 0.0099, 0.0123, 0.0207,\n                      0.0117, 0.0095, 0.0083, 0.0031, 0.0034, 0.0075, 0.0034, 0.0125, 0.0160,\n                      0.0101, 0.0061, 0.0121, 0.0128, 0.0035, 0.0168, 0.0091, 0.0106, 0.0106,\n                      0.0115, 0.0106, 0.0115, 0.0266, 0.0098, 0.0144, 0.0174, 0.0108, 0.0140,\n                      0.0035, 0.0106, 0.0150, 0.0166, 0.0110, 0.0187, 0.0069, 0.0126, 0.0119,\n                      0.0341, 0.0040, 0.0127, 0.0126, 0.0059, 0.0068, 0.0104, 0.0124, 0.0093,\n                      0.0135, 0.0137, 0.0064, 0.0043, 0.0127, 0.0132, 0.0114, 0.0168, 0.0113,\n                      0.0067, 0.0045, 0.0197, 0.0104, 0.0132, 0.0101, 0.0054, 0.0166, 0.0084,\n                      0.0036, 0.0123, 0.0445, 0.0080, 0.0040, 0.0104, 0.0078, 0.0110, 0.0377,\n                      0.0045, 0.0100, 0.0146, 0.0046, 0.0109, 0.0126, 0.0121, 0.0110, 0.0049,\n                      0.0051, 0.0114, 0.0095, 0.0070, 0.0042, 0.0038, 0.0099, 0.0114, 0.0089,\n                      0.0045, 0.0079, 0.0132, 0.0096, 0.0041, 0.0080, 0.0166, 0.0035, 0.0057,\n                      0.0158, 0.0159, 0.0142, 0.0169, 0.0037, 0.0094, 0.0113, 0.0123, 0.0106,\n                      0.0043, 0.0132, 0.0111, 0.0103, 0.0089, 0.0262, 0.0090, 0.0092, 0.0121,\n                      0.0226, 0.0201, 0.0042, 0.0115, 0.0142, 0.0096, 0.0113, 0.0137, 0.0137,\n                      0.0083, 0.0097, 0.0099, 0.0048, 0.0171, 0.0108, 0.0244, 0.0105, 0.0031,\n                      0.0090, 0.0050, 0.0083, 0.0121, 0.0130, 0.0118, 0.0113, 0.0044, 0.0201,\n                      0.0038, 0.0141, 0.0118, 0.0264, 0.0108, 0.0041, 0.0090, 0.0043, 0.0127,\n                      0.0148, 0.0039, 0.0041, 0.0278, 0.0048, 0.0061, 0.0143, 0.0198, 0.0151,\n                      0.0141, 0.0132, 0.0109, 0.0184, 0.0071, 0.0147, 0.0179, 0.0056, 0.0274,\n                      0.0072, 0.0086, 0.0112, 0.0112, 0.0094, 0.0062, 0.0137, 0.0041, 0.0141,\n                      0.0047, 0.0150, 0.0135, 0.0029, 0.0096, 0.0043, 0.0049, 0.0128, 0.0121,\n                      0.0120, 0.0042, 0.0142, 0.0033, 0.0101, 0.0043, 0.0086, 0.0135, 0.0155,\n                      0.0049, 0.0032, 0.0042, 0.0153, 0.0051, 0.0104, 0.0043, 0.0073, 0.0129,\n                      0.0077, 0.0037, 0.0157, 0.0109, 0.0134, 0.0091, 0.0049, 0.0041, 0.0138,\n                      0.0068, 0.0029, 0.0204, 0.0112, 0.0040, 0.0047, 0.0241, 0.0162, 0.0129,\n                      0.0200, 0.0046, 0.0123, 0.0103, 0.0153, 0.0049, 0.0201, 0.0177, 0.0216,\n                      0.0189, 0.0045, 0.0199, 0.0197, 0.0162, 0.0036, 0.0837, 0.0047, 0.0148,\n                      0.0123, 0.0111, 0.0095, 0.0094, 0.0045, 0.0106, 0.0114, 0.0127, 0.0124,\n                      0.0105, 0.0145, 0.0108, 0.0077, 0.0035, 0.0168, 0.0044, 0.0038, 0.0038,\n                      0.0110, 0.0130, 0.0083, 0.0036, 0.0156, 0.0057, 0.0101, 0.0043, 0.0053,\n                      0.0040, 0.0407, 0.0126, 0.0177, 0.0193, 0.0115, 0.0115, 0.0092, 0.0235,\n                      0.0132, 0.0122, 0.0056, 0.0043, 0.0101, 0.0032, 0.0096, 0.0132, 0.0061,\n                      0.0115, 0.0137, 0.0044, 0.0125, 0.0040, 0.0168, 0.0178, 0.0099, 0.0036,\n                      0.0040, 0.0085, 0.0162, 0.0124, 0.0044, 0.0081, 0.0182, 0.0055, 0.0274,\n                      0.0084, 0.0405, 0.0126, 0.0036, 0.0092, 0.0113, 0.0149, 0.0136, 0.0043,\n                      0.0099, 0.0128, 0.0077, 0.0049, 0.0225, 0.0141], dtype=torch.float64),\n                     zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n                     axis=0)),\n             ('mnet.features.15.conv.1.0.bias',\n              Parameter containing:\n              tensor([-1.6574e-01, -1.4101e-01,  1.6416e-01,  2.2353e-01, -4.0804e-01,\n                       2.6167e-01, -2.4637e-01,  3.4546e-01,  3.0182e-01,  2.8429e-01,\n                       5.6054e-01, -1.2030e-01, -1.2720e-01,  2.5992e-01, -2.4750e-02,\n                      -2.6933e-02,  2.8652e-01,  3.3492e-01, -8.8148e-03,  1.9536e-01,\n                      -3.8082e-02,  2.3078e-02,  2.3395e-01, -9.5717e-03,  1.0988e-01,\n                       2.9071e-01,  1.9350e-01,  4.7650e-01,  3.8313e-01, -8.6306e-02,\n                       7.2141e-03, -2.7583e-02, -6.9737e-02,  2.7515e-01,  2.1469e-01,\n                      -5.3648e-02, -2.8025e-02,  2.0451e-01,  7.9762e-01,  7.7105e-03,\n                       2.2269e-01, -1.9263e-01, -4.1704e-02,  5.0189e-03, -8.4077e-02,\n                       3.0379e-01,  1.2936e-02, -2.1201e-01, -3.0806e-01,  3.3455e-01,\n                       2.2677e-01,  2.3276e-01, -2.4160e-01,  1.3446e-01, -3.6994e-02,\n                      -9.8457e-03,  7.1369e-01, -1.7786e-02, -2.2676e-03, -6.9881e-02,\n                      -1.6712e-01,  1.8909e-01, -8.6980e-03,  9.1515e-04,  2.0463e-01,\n                       2.1510e-01, -2.6218e-02,  2.9221e-01,  2.5091e-01, -1.9794e-02,\n                       1.5374e-01,  2.1782e-02,  1.0035e-01, -1.0737e-01, -2.1173e-02,\n                      -1.7275e-01,  2.0517e-01,  6.3713e-02, -4.5072e-02, -2.2733e-01,\n                      -1.7853e-02,  2.4278e-01,  3.1915e-01,  2.6473e-01,  2.3643e-01,\n                      -4.1151e-02, -1.7516e-02,  1.5791e-02, -2.4701e-02,  1.1063e-01,\n                       2.2589e-01,  2.3150e-01, -4.0218e-01, -3.6371e-02, -4.8356e-02,\n                      -9.1937e-02, -1.5674e-02, -1.1647e-01, -6.6107e-02, -3.2966e-02,\n                       1.8289e-03, -1.7211e-03, -7.0417e-02, -1.8429e-01,  6.0099e-01,\n                      -1.7622e-01,  2.3437e-04, -4.1706e-02, -8.0958e-02,  3.0215e-01,\n                       2.2583e-01,  2.9748e-01, -1.1547e-01, -1.5980e-02, -1.5170e-01,\n                       3.0601e-01, -5.8552e-03, -4.2819e-02,  3.4620e-01,  1.9301e-01,\n                       1.9111e-01, -4.8621e-02, -7.5375e-02,  1.8237e-01,  2.0768e-02,\n                      -2.9277e-03, -3.9563e-03,  3.4724e-01, -6.0790e-02, -4.2238e-01,\n                       2.8390e-01, -2.5634e-03, -4.0317e-02, -4.1452e-01, -3.6881e-02,\n                      -3.3526e-02, -5.9249e-02, -1.2785e-01,  2.5996e-01,  2.6437e-01,\n                       4.5140e-02, -3.4416e-02, -2.8426e-03, -1.1396e-01,  3.4311e-01,\n                       1.3640e-01, -2.8836e-03,  2.3160e-01,  2.0318e-01, -4.0234e-02,\n                      -1.8678e-01, -3.3055e-02,  1.2458e-02, -2.1078e-02,  2.6017e-01,\n                       2.8726e-01,  9.4247e-03,  5.0592e-04, -4.5480e-02,  2.1612e-02,\n                      -2.7677e-02, -7.0586e-02, -2.6086e-02,  2.3567e-01,  2.6031e-01,\n                       2.0360e-01,  2.2405e-01,  1.9017e-01,  2.1773e-02, -1.8095e-01,\n                       1.9260e-02, -5.4343e-03, -1.1067e-02, -7.4189e-02,  1.0942e-01,\n                      -1.3735e-01, -2.3356e-01, -1.2605e-01,  1.7308e-01,  2.4948e-01,\n                       1.6510e-02,  2.9200e-01,  1.9815e-01, -1.7536e-02, -2.1115e-01,\n                      -2.8443e-02, -8.0960e-02,  2.7360e-02,  6.0977e-02,  2.8697e-01,\n                      -1.3250e-02, -2.0152e-02, -5.7651e-02,  2.7575e-01,  3.2421e-01,\n                       2.2998e-01,  1.9039e-01, -1.1684e-01, -1.2724e-01,  3.3771e-02,\n                       6.7304e-01, -2.1713e-01,  2.1085e-01,  1.5015e-01,  4.6643e-03,\n                      -6.2891e-02, -9.2395e-02, -1.9025e-01,  1.5118e-01, -3.4176e-02,\n                      -1.7495e-01,  7.8872e-01, -4.6475e-02,  8.5511e-03,  1.5710e-02,\n                      -1.8538e-01, -1.0663e-02, -4.8145e-03, -1.1617e-02, -1.8246e-02,\n                       2.0532e-01,  2.5134e-01,  1.7835e-01,  2.0532e-01, -1.1822e-02,\n                       2.3453e-01, -1.4594e-01, -2.7154e-02,  3.1095e-01, -6.2874e-02,\n                       3.0336e-01,  2.1364e-01, -1.0955e-01, -1.9378e-02, -4.6469e-02,\n                      -1.8306e-02, -1.6718e-01, -1.1167e-01,  1.7623e-01, -2.2916e-01,\n                      -5.8495e-02, -8.6389e-02,  2.7851e-01, -1.2962e-02,  2.1662e-02,\n                       2.1988e-01,  2.9305e-01,  2.9624e-01, -2.0210e-02, -7.9821e-02,\n                      -7.3419e-02,  2.2855e-01,  4.9717e-01,  1.0535e-03,  2.4265e-02,\n                      -1.3081e-01,  2.4153e-01, -2.0393e-01, -1.6611e-02, -5.4487e-02,\n                      -1.7956e-01, -1.7452e-02,  2.2707e-01,  2.6933e-02, -6.1008e-02,\n                       2.5742e-01, -2.5116e-01,  2.1465e-01, -9.8598e-02,  2.2243e-01,\n                       2.2441e-01,  2.1700e-01, -4.8293e-02, -5.6443e-02,  7.4861e-03,\n                      -1.1878e-02,  2.3755e-01, -7.3493e-02, -4.4090e-01,  5.5053e-02,\n                      -8.5631e-02,  2.1248e-01, -2.5782e-01, -3.1397e-02,  2.7742e-01,\n                      -8.3602e-02, -3.1021e-01, -4.5256e-03,  2.6596e-01,  2.7691e-01,\n                       2.0840e-01,  1.8580e-01,  1.5835e-02,  2.3346e-01, -2.3952e-02,\n                      -2.7499e-01, -3.2466e-02,  3.3610e-01,  2.0859e-01, -2.2151e-01,\n                       1.9775e-01, -1.1735e-01,  5.4087e-01,  1.8226e-01, -5.0673e-03,\n                      -9.4855e-03,  2.9604e-01, -4.6459e-02,  2.0574e-01,  1.5200e-01,\n                      -7.0844e-02, -1.8730e-02,  3.0293e-01, -2.2741e-01, -2.5395e-02,\n                       2.1643e-01, -1.3297e-01, -3.7884e-02,  2.1331e-01, -3.3253e-02,\n                       9.0045e-02,  6.6218e-02, -6.0877e-02, -3.6998e-02,  2.3010e-01,\n                       4.2302e-02, -1.5014e-03, -2.1427e-01,  9.9189e-02, -8.4311e-02,\n                      -4.2343e-02, -1.4032e-01, -2.5881e-02, -1.5554e-01,  2.3815e-01,\n                       2.2971e-01, -5.0726e-02, -4.6371e-01,  2.4130e-01,  2.1071e-01,\n                      -1.9596e-01,  1.3254e-01, -4.5491e-03,  1.0923e-02,  1.3891e-01,\n                       2.1337e-01, -1.1048e-01,  3.7464e-02, -3.3696e-02, -2.2985e-02,\n                       2.2288e-01,  4.4303e-01,  4.0571e-01,  2.2147e-01,  1.1503e-02,\n                      -2.3762e-01, -3.4840e-02, -1.2462e-01,  8.2606e-01, -2.4918e-02,\n                       2.5552e-01, -9.3929e-02,  1.5526e-01,  2.2376e-01, -5.1682e-02,\n                       8.8749e-03,  4.9567e-01,  3.0336e-01,  1.5100e-01, -1.9354e-01,\n                      -2.3889e-02,  2.9858e-01, -1.0215e-01, -3.2015e-02, -2.0987e-01,\n                      -4.0641e-02, -1.6269e-02, -1.5741e-02,  2.1161e-01, -8.4031e-02,\n                       5.5539e-02, -1.2929e-02,  2.6535e-02,  2.2449e-01, -1.5824e-01,\n                      -3.2624e-03, -1.8474e-01, -1.3402e-01,  2.3215e-01,  2.0229e-01,\n                      -3.1464e-01, -2.1580e-02, -1.6243e-01, -6.5044e-02, -2.5735e-02,\n                      -4.1784e-03, -6.4608e-02, -6.8398e-02, -5.3921e-02,  1.2994e-03,\n                      -8.0502e-02, -1.4774e-01, -2.0416e-02, -1.3315e-02, -1.9279e-02,\n                      -3.0415e-01,  3.7546e-01, -1.2286e-02,  2.2052e-01, -1.3147e-01,\n                       2.3494e-01,  2.1883e-01, -1.8164e-01, -8.9059e-02,  2.2600e-01,\n                      -5.2466e-02,  1.4175e-02,  2.6389e-01, -4.1117e-03, -7.0355e-04,\n                       1.6247e-01,  2.2623e-01,  2.4499e-01, -4.5580e-02,  2.1669e-01,\n                      -2.0949e-02,  1.9663e-01,  1.4364e-02,  3.2598e-02, -1.1408e-01,\n                      -1.6291e-02,  3.1391e-01, -3.0019e-02, -6.7321e-03,  2.7653e-01,\n                       4.1654e-01,  2.6178e-01,  3.3878e-01,  2.4288e-01, -1.5929e-02,\n                      -1.5979e-02,  7.5467e-02, -8.7869e-03,  1.8870e-01, -6.7823e-01,\n                       2.5060e-01, -5.7393e-02,  8.0121e-01,  3.8853e-01, -2.5055e-01,\n                      -2.3261e-02,  2.0308e-01,  3.8656e-02, -2.8147e-03,  2.3415e-02,\n                      -5.2354e-02,  2.3291e-01, -6.0122e-02,  2.6803e-01,  2.4488e-01,\n                       2.2480e-01, -5.9569e-02,  5.6048e-04,  1.4783e-02,  5.8796e-02,\n                      -1.3097e-02, -3.3433e-02, -2.0178e-01,  2.0118e-01, -5.8439e-02,\n                       1.6875e-02, -7.0186e-02,  5.8206e-01, -2.5953e-01, -2.2182e-01,\n                      -8.9220e-03,  4.3338e-01, -5.7578e-02, -1.4122e-01, -2.4254e-02,\n                      -6.4002e-02, -1.2712e-03,  2.2748e-01,  2.7718e-01, -5.1894e-02,\n                      -9.5358e-03, -6.0802e-02, -1.6346e-01,  2.8304e-01, -2.5649e-01,\n                      -7.3623e-03,  2.6824e-01,  4.3133e-02, -7.4992e-02,  1.6977e-02,\n                      -3.3627e-01, -1.1173e-01, -2.7719e-02,  1.8091e-02,  1.9164e-01,\n                       2.3933e-01,  5.8024e-02,  5.4889e-02, -9.0742e-03, -8.5136e-02,\n                      -4.3377e-02,  3.4620e-01,  2.3958e-01, -1.4805e-01,  4.7023e-03,\n                       1.0924e-01, -9.2742e-04, -4.8920e-02,  1.5034e-01, -2.7572e-02,\n                       1.9529e-03,  3.3913e-03,  2.2070e-01,  2.4841e-02, -5.4590e-02,\n                       1.1670e-01,  4.5572e-01,  5.0171e-01,  1.7558e-01, -5.1692e-02,\n                      -3.6450e-02, -3.0231e-02, -3.9717e-02,  3.1757e-01, -1.7465e-01,\n                      -2.7741e-02, -2.8773e-02,  2.0400e-01, -5.6259e-03, -1.1616e-01,\n                       2.1330e-01, -9.8947e-03,  2.0800e-01, -1.2749e-01,  4.6222e-02,\n                       2.1124e-01,  1.1954e-02,  2.3419e-01,  2.1441e-01,  5.1531e-01,\n                      -1.4576e-01, -2.2154e-01,  1.2181e-02, -2.3249e-01, -9.8325e-02,\n                       1.9070e-01, -7.0773e-02,  5.7633e-02, -2.3777e-02, -1.8087e-01,\n                      -2.0535e-02, -1.4955e-03, -8.8298e-02, -8.2737e-02,  4.3364e-01,\n                      -3.3785e-03,  2.1056e-01, -4.7259e-02, -2.0662e-01,  3.8339e-03,\n                       1.0614e-02, -1.6319e-01,  2.4202e-01, -1.8208e-03,  1.0209e-02,\n                      -1.2207e-01,  2.4207e-01,  2.3329e-01,  2.0186e-01,  2.9863e-02,\n                       2.6860e-01, -1.4692e-01,  2.0899e-01, -2.3565e-02, -8.9891e-02,\n                      -8.3222e-02,  2.3936e-02,  2.1577e-01, -2.7476e-02,  2.9019e-01,\n                       1.9759e-01, -2.0912e-02, -2.0838e-02,  2.5119e-01, -4.3476e-02,\n                       2.0449e-01,  3.5844e-01,  2.0749e-01, -3.6824e-03,  4.5110e-03,\n                       2.3194e-01,  2.1614e-01, -5.3848e-02, -4.2604e-02,  4.5292e-02,\n                      -4.2834e-02, -2.1951e-02,  2.0046e-01, -4.0037e-04, -4.7788e-03,\n                      -1.8114e-01,  2.1339e-01,  2.0832e-02, -5.7578e-02,  2.9317e-01,\n                      -5.4941e-02,  5.5508e-01, -1.6630e-01, -6.0724e-02, -4.8769e-02,\n                       2.1257e-01, -1.6522e-01, -2.3440e-02,  3.0629e-01,  1.5276e-01,\n                      -2.5673e-02, -9.1494e-02,  2.3621e-01,  2.2214e-01, -1.9270e-02,\n                       2.5759e-01,  3.7945e-01,  3.4922e-01, -3.7149e-02,  2.4924e-01,\n                      -5.8687e-02, -1.9299e-01,  2.6445e-01, -3.5235e-02, -2.2484e-02,\n                      -1.4878e-02, -9.5612e-02, -6.0167e-02, -1.9107e-01, -1.3600e-02,\n                      -3.6629e-03,  3.6051e-02,  2.2713e-01,  2.1301e-01,  3.2704e-02,\n                       3.8281e-01,  2.7631e-02, -1.0623e-02,  2.3138e-01,  4.5059e-01,\n                      -1.9751e-01,  3.4704e-01,  1.9522e-01,  2.0823e-03, -4.7663e-03,\n                      -1.3589e-01, -8.2839e-03, -2.1643e-02, -6.5862e-02, -2.5897e-02,\n                      -4.6756e-02, -1.4970e-01, -4.8689e-02,  1.1313e-01, -1.9694e-03,\n                      -1.3477e-01,  2.0371e-01, -6.7723e-02, -2.9723e-01, -1.7688e-01,\n                       2.6068e-01, -2.9588e-02,  7.9394e-02, -3.4299e-02, -2.6865e-02,\n                      -5.0481e-02,  2.2401e-01,  1.9713e-02, -7.9425e-02,  2.9764e-01,\n                       2.9621e-01, -5.5551e-02, -5.0544e-02,  2.6859e-02, -7.0922e-04,\n                      -2.5911e-02,  4.7287e-01,  2.6669e-01,  1.2063e-01, -2.3496e-02,\n                      -1.0007e-01, -7.2472e-02, -1.2402e-01,  1.0997e-01,  4.1388e-01,\n                      -2.6926e-01, -3.7176e-02, -1.6123e-01, -1.1950e-01,  3.5767e-01,\n                      -1.1245e-02, -1.1512e-02,  2.2524e-01, -1.0819e-01, -1.1636e-01,\n                      -8.8144e-02,  2.2332e-01, -2.7690e-02, -4.4695e-04, -3.0969e-02,\n                      -2.4393e-01,  2.3491e-01,  9.2027e-02, -5.1195e-03,  2.0606e-01,\n                      -3.5362e-02,  5.3567e-02, -1.5935e-01, -4.3174e-02,  2.4088e-01,\n                       4.0356e-01, -3.7383e-02, -3.8893e-02, -1.0843e-02,  2.9620e-01,\n                       1.8554e-01, -8.8302e-02, -3.0112e-03, -7.5866e-02,  3.3297e-01,\n                       3.9206e-04, -1.4006e-01, -1.0386e-02,  2.0640e-01, -1.1112e-01,\n                      -2.7318e-01,  2.2608e-01,  3.5647e-01, -3.7254e-02,  3.9632e-01,\n                       7.0683e-03, -2.2382e-01,  2.3020e-01,  4.5773e-02, -8.0138e-02,\n                      -1.6477e-01,  2.0747e-04,  2.3040e-01, -3.2819e-02, -3.4283e-02,\n                       1.3752e-02,  1.4690e-01, -3.5256e-01, -1.0373e-01, -1.1996e-01,\n                      -4.7874e-02, -1.0874e-01,  4.2122e-01,  2.4642e-01, -1.5246e-03,\n                      -1.9184e-03, -2.8085e-02, -2.3384e-03, -2.1011e-02, -8.8700e-02,\n                      -6.8238e-02, -2.1663e-01,  5.8761e-03,  2.5767e-01, -2.8876e-02,\n                      -3.0776e-02,  1.1906e-01, -1.8098e-02,  2.2917e-01,  2.6731e-01,\n                       2.5140e-01, -2.4372e-02, -1.0285e-01, -1.6859e-02,  7.5510e-03,\n                      -2.9461e-02,  2.5380e-01,  5.0217e-03,  2.0351e-01, -3.1733e-01,\n                      -4.1657e-02, -2.4098e-01, -3.2512e-02,  2.3829e-01,  7.5803e-03,\n                       3.1015e-01, -6.6576e-02, -3.5224e-01,  2.0331e-01,  3.3709e-01,\n                       2.3677e-02,  2.2339e-01,  4.5452e-01, -2.6259e-02, -2.6353e-02,\n                      -3.9066e-02, -2.4292e-02, -5.9419e-02, -1.3080e-01, -3.9700e-02,\n                       2.7689e-01, -3.7706e-02, -1.8264e-02,  4.8560e-02, -5.8228e-01,\n                       4.6425e-01, -6.8494e-03, -7.6711e-02, -1.9035e-03, -3.2342e-02,\n                       3.4456e-01, -3.8117e-02,  3.6224e-01, -1.6544e-01,  2.3051e-01,\n                      -4.9462e-01,  4.3273e-02,  2.1597e-01, -9.6178e-02,  3.2954e-01,\n                       2.4854e-01,  8.4693e-02,  5.9349e-03, -3.1676e-04,  3.0191e-01,\n                      -2.8414e-03,  2.7088e-01,  7.7142e-01,  3.6620e-01, -1.9120e-01,\n                      -2.5837e-01, -3.3746e-03,  3.1116e-01,  1.9105e-01,  2.2209e-01,\n                      -5.0715e-02,  2.3011e-01,  8.3382e-04,  5.0719e-01,  1.7893e-03,\n                       1.9480e-01, -2.9074e-02,  2.3054e-01, -7.6362e-02,  9.0242e-02,\n                      -9.5421e-02, -4.9330e-02,  2.0379e-01,  2.3973e-01,  6.9871e-03,\n                       8.1113e-02,  2.0987e-01, -2.1461e-03,  1.5050e-02,  2.2003e-01,\n                       2.0737e-01,  1.5473e-01, -6.0654e-02, -1.7957e-01, -5.4031e-02,\n                       1.9941e-01,  3.0252e-02, -1.5349e-01, -7.9353e-03,  2.6877e-01,\n                      -1.1527e-01,  3.8727e-01, -1.0275e-01,  1.2356e-01,  2.1317e-01,\n                      -1.7438e-01, -3.1916e-03, -1.1005e-02,  1.9925e-01, -3.7223e-01,\n                       2.1295e-01, -2.5340e-01, -5.0381e-04, -1.7374e-02, -1.1950e-02,\n                      -3.9618e-02,  2.8377e-01,  1.5297e-04, -6.8283e-02,  1.8977e-03,\n                       3.0476e-03, -9.6021e-02,  3.0472e-01,  2.9475e-02,  2.6713e-01,\n                       1.8863e-01, -3.9626e-01,  2.3134e-01,  2.3266e-01,  2.1960e-01,\n                      -1.0614e-02, -5.8839e-02,  2.1327e-02,  1.9057e-01, -1.9075e-01,\n                       3.2436e-01, -1.6225e-02,  3.4458e-01,  3.3342e-01,  1.9920e-01,\n                      -1.8385e-01, -1.3303e-02,  3.4530e-01, -6.0143e-01, -7.5130e-03,\n                      -2.4131e-01,  6.8761e-03, -1.2829e-01, -1.5870e-01, -1.8517e-01,\n                       3.7910e-01,  2.7870e-01, -1.8567e-02,  2.1626e-01, -3.3101e-02,\n                      -5.0669e-02,  1.9684e-01, -8.2843e-02,  3.1874e-03,  2.1484e-01,\n                      -6.0788e-02,  3.2835e-01,  5.2951e-02,  3.9214e-01, -5.5913e-02,\n                       1.9538e-01,  2.1856e-01, -1.9896e-02,  1.8282e-02, -4.0528e-02,\n                       2.7940e-01, -1.1378e-01, -2.7823e-03,  4.7421e-01, -2.7015e-01,\n                       1.3077e-03, -2.8451e-01, -9.8613e-02,  2.3603e-01,  3.1536e-01,\n                       1.6428e-01, -1.5851e-01,  3.0963e-02,  1.9418e-01,  6.0756e-04,\n                      -4.5180e-02, -1.1066e-01,  3.5873e-01, -3.8674e-03, -1.3888e-02],\n                     requires_grad=True)),\n             ('mnet.features.15.conv.1.0.scale', tensor(0.0629)),\n             ('mnet.features.15.conv.1.0.zero_point', tensor(49)),\n             ('mnet.features.15.conv.2.weight',\n              tensor([[[[-0.0780]],\n              \n                       [[ 0.0312]],\n              \n                       [[-0.0201]],\n              \n                       ...,\n              \n                       [[ 0.1137]],\n              \n                       [[ 0.0713]],\n              \n                       [[ 0.0669]]],\n              \n              \n                      [[[ 0.0996]],\n              \n                       [[ 0.0069]],\n              \n                       [[ 0.0429]],\n              \n                       ...,\n              \n                       [[-0.0069]],\n              \n                       [[-0.0292]],\n              \n                       [[ 0.0206]]],\n              \n              \n                      [[[-0.1230]],\n              \n                       [[-0.0277]],\n              \n                       [[-0.1611]],\n              \n                       ...,\n              \n                       [[ 0.0312]],\n              \n                       [[ 0.0312]],\n              \n                       [[ 0.0312]]],\n              \n              \n                      ...,\n              \n              \n                      [[[ 0.0087]],\n              \n                       [[-0.0226]],\n              \n                       [[ 0.0469]],\n              \n                       ...,\n              \n                       [[-0.0765]],\n              \n                       [[-0.0052]],\n              \n                       [[-0.0226]]],\n              \n              \n                      [[[ 0.0267]],\n              \n                       [[-0.0401]],\n              \n                       [[-0.0995]],\n              \n                       ...,\n              \n                       [[ 0.1292]],\n              \n                       [[-0.0327]],\n              \n                       [[-0.0698]]],\n              \n              \n                      [[[ 0.0098]],\n              \n                       [[ 0.0225]],\n              \n                       [[-0.0197]],\n              \n                       ...,\n              \n                       [[-0.0731]],\n              \n                       [[ 0.0225]],\n              \n                       [[-0.0169]]]], size=(160, 960, 1, 1), dtype=torch.qint8,\n                     quantization_scheme=torch.per_channel_affine,\n                     scale=tensor([0.0022, 0.0017, 0.0017, 0.0017, 0.0013, 0.0016, 0.0014, 0.0023, 0.0017,\n                      0.0012, 0.0017, 0.0018, 0.0015, 0.0016, 0.0016, 0.0012, 0.0018, 0.0014,\n                      0.0016, 0.0018, 0.0017, 0.0024, 0.0016, 0.0020, 0.0017, 0.0018, 0.0017,\n                      0.0014, 0.0016, 0.0020, 0.0016, 0.0018, 0.0014, 0.0017, 0.0014, 0.0016,\n                      0.0015, 0.0015, 0.0016, 0.0014, 0.0017, 0.0015, 0.0020, 0.0014, 0.0014,\n                      0.0016, 0.0014, 0.0015, 0.0019, 0.0017, 0.0019, 0.0019, 0.0021, 0.0014,\n                      0.0016, 0.0020, 0.0019, 0.0015, 0.0017, 0.0019, 0.0017, 0.0014, 0.0014,\n                      0.0015, 0.0015, 0.0016, 0.0017, 0.0018, 0.0024, 0.0016, 0.0017, 0.0014,\n                      0.0014, 0.0015, 0.0020, 0.0015, 0.0017, 0.0024, 0.0020, 0.0019, 0.0017,\n                      0.0015, 0.0019, 0.0014, 0.0017, 0.0017, 0.0011, 0.0018, 0.0018, 0.0018,\n                      0.0020, 0.0017, 0.0014, 0.0019, 0.0020, 0.0014, 0.0015, 0.0014, 0.0020,\n                      0.0017, 0.0014, 0.0015, 0.0016, 0.0017, 0.0017, 0.0020, 0.0015, 0.0015,\n                      0.0016, 0.0018, 0.0012, 0.0024, 0.0020, 0.0015, 0.0016, 0.0015, 0.0014,\n                      0.0014, 0.0023, 0.0016, 0.0013, 0.0013, 0.0016, 0.0016, 0.0021, 0.0021,\n                      0.0018, 0.0015, 0.0012, 0.0015, 0.0018, 0.0017, 0.0014, 0.0014, 0.0014,\n                      0.0019, 0.0013, 0.0016, 0.0015, 0.0016, 0.0017, 0.0015, 0.0016, 0.0017,\n                      0.0019, 0.0017, 0.0019, 0.0014, 0.0016, 0.0017, 0.0014, 0.0019, 0.0015,\n                      0.0017, 0.0022, 0.0025, 0.0018, 0.0017, 0.0015, 0.0014],\n                     dtype=torch.float64),\n                     zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n                     axis=0)),\n             ('mnet.features.15.conv.2.bias',\n              Parameter containing:\n              tensor([-1.1593e-01, -6.0083e-02,  1.0149e-01,  9.3940e-02,  1.5695e-01,\n                      -9.5538e-02,  1.4227e-01, -3.4495e-01, -9.4181e-05, -1.5851e-01,\n                       2.3303e-01,  1.7467e-01, -6.8416e-02, -3.8620e-01, -1.9339e-02,\n                      -2.1773e-01, -2.5727e-03,  6.0358e-02, -7.8410e-02,  3.9730e-02,\n                      -3.3890e-02, -1.4845e-01, -5.4508e-01,  7.1873e-03, -3.6123e-02,\n                       9.5661e-02, -7.6472e-02, -9.4976e-02, -1.1319e-01, -7.0488e-02,\n                       1.6999e-01, -2.8260e-01,  2.0382e-01, -1.0349e-01, -3.4571e-01,\n                      -3.3854e-01,  1.3428e-01, -2.1227e-02, -1.0415e-01,  5.8819e-02,\n                      -1.8138e-02,  8.9115e-02,  1.3844e-01,  7.7664e-02,  2.0262e-01,\n                       1.7480e-01, -1.4089e-01,  1.7414e-02,  1.4850e-01,  1.3818e-01,\n                      -2.0388e-02,  1.3906e-01, -3.2820e-01,  2.6292e-01, -1.9763e-01,\n                      -2.6628e-01, -3.2827e-01,  7.6188e-02,  2.6478e-02,  6.9272e-02,\n                      -1.5875e-01, -4.3445e-02,  1.0114e-01, -5.0652e-02, -1.2946e-01,\n                      -2.5664e-03, -6.7381e-03, -2.9310e-01,  6.2873e-02,  2.5919e-01,\n                       1.0821e-01,  1.4291e-01,  1.0224e-01, -2.7483e-01, -1.1873e-01,\n                      -2.0535e-01,  3.2731e-01, -2.1454e-01,  4.4612e-01, -5.8121e-02,\n                      -1.8250e-01,  7.1144e-02, -4.9759e-02,  1.6297e-01,  4.1758e-02,\n                       2.7284e-02,  1.1189e-01,  2.1496e-02,  2.1663e-01, -2.0606e-01,\n                       2.3454e-01,  1.3226e-01, -2.8681e-01,  6.3417e-02,  2.1203e-02,\n                      -1.7112e-02, -4.8078e-02,  2.5446e-02, -2.5267e-01,  1.2207e-01,\n                      -3.6409e-01, -1.0519e-01,  1.4861e-01, -1.9302e-01,  1.1296e-01,\n                      -1.6032e-01,  6.4942e-02,  2.3763e-02,  2.3576e-01, -8.3679e-02,\n                       5.5106e-02,  8.3445e-02,  1.9606e-01,  6.5509e-02,  1.6846e-01,\n                       7.0176e-02,  1.8867e-01, -1.4756e-01,  7.6958e-02,  8.9119e-03,\n                      -1.4276e-01,  3.8403e-01,  6.8226e-02,  2.3995e-02, -4.7660e-02,\n                       1.9287e-02,  2.6534e-02,  2.1629e-01, -5.4030e-02,  7.4001e-02,\n                       3.9169e-02, -2.6885e-03, -3.8280e-02, -1.2600e-01, -5.8355e-03,\n                       1.3154e-01, -1.4251e-02,  2.0486e-02,  2.7936e-02, -2.0535e-01,\n                       8.3624e-03,  1.5343e-01, -2.2024e-01,  6.5361e-02, -1.1926e-01,\n                       1.9433e-01,  1.5316e-01,  1.5343e-01, -8.7544e-02,  5.7416e-02,\n                      -4.2638e-02,  2.8269e-01,  8.6792e-02, -2.0576e-01,  1.2891e-01,\n                      -1.2920e-01,  2.6533e-01,  1.2912e-01,  8.1426e-02, -3.9246e-02],\n                     requires_grad=True)),\n             ('mnet.features.15.conv.2.scale', tensor(0.1238)),\n             ('mnet.features.15.conv.2.zero_point', tensor(58)),\n             ('mnet.features.16.conv.0.0.weight',\n              tensor([[[[-0.0302]],\n              \n                       [[ 0.0373]],\n              \n                       [[ 0.0053]],\n              \n                       ...,\n              \n                       [[ 0.0022]],\n              \n                       [[ 0.0195]],\n              \n                       [[-0.0080]]],\n              \n              \n                      [[[ 0.0034]],\n              \n                       [[-0.0041]],\n              \n                       [[ 0.0096]],\n              \n                       ...,\n              \n                       [[-0.0089]],\n              \n                       [[-0.0007]],\n              \n                       [[-0.0062]]],\n              \n              \n                      [[[-0.0164]],\n              \n                       [[-0.0046]],\n              \n                       [[ 0.0082]],\n              \n                       ...,\n              \n                       [[ 0.0169]],\n              \n                       [[-0.0026]],\n              \n                       [[-0.0031]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-0.0334]],\n              \n                       [[ 0.0136]],\n              \n                       [[ 0.0303]],\n              \n                       ...,\n              \n                       [[ 0.0077]],\n              \n                       [[-0.0253]],\n              \n                       [[-0.0027]]],\n              \n              \n                      [[[-0.0267]],\n              \n                       [[ 0.0361]],\n              \n                       [[ 0.0094]],\n              \n                       ...,\n              \n                       [[ 0.0180]],\n              \n                       [[-0.0290]],\n              \n                       [[ 0.0063]]],\n              \n              \n                      [[[-0.0366]],\n              \n                       [[ 0.0012]],\n              \n                       [[ 0.0133]],\n              \n                       ...,\n              \n                       [[-0.0157]],\n              \n                       [[ 0.0412]],\n              \n                       [[ 0.0116]]]], size=(960, 160, 1, 1), dtype=torch.qint8,\n                     quantization_scheme=torch.per_channel_affine,\n                     scale=tensor([4.4383e-04, 6.8344e-04, 5.1099e-04, 5.5128e-04, 8.7334e-04, 6.0449e-04,\n                      8.4531e-04, 3.6659e-04, 6.1718e-04, 5.4262e-04, 7.9843e-04, 5.1344e-04,\n                      6.1781e-04, 6.2668e-04, 7.5662e-04, 6.9785e-04, 6.7191e-04, 9.4603e-04,\n                      4.9663e-04, 6.2599e-04, 8.7140e-04, 6.3911e-04, 4.5381e-04, 6.1203e-04,\n                      3.9611e-04, 7.6696e-04, 6.6837e-04, 5.0592e-04, 7.0997e-04, 5.4005e-04,\n                      3.4409e-04, 1.0909e-03, 6.2462e-04, 1.0840e-03, 4.5086e-04, 4.6389e-04,\n                      4.8116e-04, 6.2484e-04, 6.0726e-04, 9.7003e-04, 8.1494e-04, 7.9909e-04,\n                      4.9394e-04, 8.7850e-04, 1.1090e-03, 6.3391e-04, 8.1139e-04, 1.1993e-03,\n                      8.5676e-04, 7.0231e-04, 6.7467e-04, 7.8091e-04, 6.9858e-04, 4.7340e-04,\n                      1.0680e-03, 3.5323e-04, 6.6886e-04, 5.7868e-04, 4.8608e-04, 6.3599e-04,\n                      1.0197e-03, 7.5547e-04, 6.4194e-04, 6.4911e-04, 6.4268e-04, 7.4725e-04,\n                      7.3759e-04, 7.6806e-04, 6.6520e-04, 8.2352e-04, 7.2382e-04, 5.4110e-04,\n                      1.0280e-03, 7.7633e-04, 4.4479e-04, 6.4436e-04, 8.6507e-04, 5.5986e-04,\n                      9.1161e-04, 7.5533e-04, 1.1791e-05, 7.8451e-04, 5.1758e-04, 6.9249e-04,\n                      5.6357e-04, 7.7822e-04, 7.3080e-04, 5.0671e-04, 1.0706e-03, 4.9833e-04,\n                      6.7342e-04, 7.2845e-04, 6.8705e-04, 5.6864e-04, 6.7283e-04, 8.4581e-04,\n                      9.6758e-04, 8.7682e-04, 8.0160e-04, 5.8308e-04, 8.0666e-04, 6.8498e-04,\n                      7.3011e-04, 1.0499e-03, 6.3246e-04, 8.5744e-04, 5.8486e-04, 8.7325e-04,\n                      5.5820e-04, 5.0014e-04, 6.4168e-04, 8.6077e-04, 9.7156e-04, 5.8300e-04,\n                      4.7974e-04, 8.2780e-04, 5.7744e-04, 3.2668e-04, 4.2701e-04, 5.3842e-04,\n                      8.3362e-04, 5.7815e-04, 9.9284e-04, 9.6969e-04, 6.4570e-04, 6.1468e-04,\n                      6.5317e-04, 5.7989e-04, 7.7984e-04, 6.9812e-04, 8.2016e-04, 5.7186e-04,\n                      5.7547e-04, 6.3097e-04, 8.4084e-04, 8.4759e-04, 4.4168e-04, 6.6607e-04,\n                      6.3973e-04, 5.8385e-04, 9.6865e-04, 5.7394e-04, 6.4555e-04, 8.0116e-04,\n                      8.9602e-04, 5.2273e-04, 5.4165e-04, 5.2192e-04, 5.7796e-04, 6.8192e-04,\n                      5.3085e-04, 5.1460e-04, 6.9046e-04, 7.7779e-04, 9.1951e-04, 1.0059e-03,\n                      7.6138e-04, 5.2815e-04, 1.0191e-03, 7.9721e-04, 7.6838e-04, 3.4198e-04,\n                      8.5776e-04, 9.6457e-04, 7.5160e-04, 1.0808e-03, 5.6424e-04, 7.1816e-04,\n                      7.0407e-04, 6.4040e-04, 6.2449e-04, 7.0923e-04, 7.4402e-04, 8.5258e-04,\n                      6.1885e-04, 8.0168e-04, 8.1543e-04, 6.4866e-04, 9.9201e-04, 2.8089e-04,\n                      6.2490e-04, 4.7441e-04, 8.9520e-04, 8.3819e-04, 7.8458e-04, 8.7631e-04,\n                      6.4662e-04, 5.4263e-04, 8.5897e-04, 7.7033e-04, 4.9543e-04, 8.2481e-04,\n                      9.5402e-04, 8.0447e-04, 8.5916e-04, 1.0200e-03, 5.7559e-04, 5.5386e-04,\n                      5.9750e-04, 7.5070e-04, 7.9789e-04, 5.8664e-04, 8.0059e-04, 5.1798e-04,\n                      5.7548e-04, 7.7983e-04, 9.6668e-04, 9.8467e-04, 5.1107e-04, 8.4607e-04,\n                      4.6198e-04, 6.7146e-04, 7.0290e-04, 5.4123e-04, 9.2110e-04, 5.6120e-04,\n                      5.3392e-04, 4.5813e-04, 9.9533e-04, 6.2898e-04, 5.4950e-04, 6.3226e-04,\n                      7.0034e-04, 8.9625e-04, 5.8914e-04, 8.6066e-04, 1.1639e-03, 7.5850e-04,\n                      6.7858e-04, 6.0008e-04, 9.0973e-04, 5.7592e-04, 6.8385e-04, 1.9291e-03,\n                      6.8112e-04, 1.1260e-03, 1.1426e-03, 7.8071e-04, 5.2727e-04, 5.9041e-04,\n                      5.6964e-04, 7.7938e-04, 6.2672e-04, 5.6296e-04, 4.8341e-04, 7.2450e-04,\n                      4.2799e-04, 5.5427e-04, 6.0524e-04, 5.5297e-04, 1.0487e-03, 4.9334e-04,\n                      4.2448e-04, 7.1591e-04, 8.3182e-04, 5.4382e-04, 6.6722e-04, 6.2841e-04,\n                      5.2687e-04, 5.4099e-04, 3.5227e-04, 9.1324e-04, 6.9663e-04, 6.2383e-04,\n                      5.6417e-04, 3.6533e-04, 6.9210e-04, 8.8342e-04, 4.8285e-04, 5.0153e-04,\n                      7.3482e-04, 5.3921e-04, 8.2198e-04, 4.5715e-04, 6.8205e-04, 8.3279e-04,\n                      6.8761e-04, 4.6323e-04, 5.4101e-04, 7.2539e-04, 9.5026e-04, 7.0132e-04,\n                      9.2696e-04, 7.8951e-04, 6.1856e-04, 5.1034e-04, 7.3691e-04, 4.8827e-04,\n                      8.7422e-04, 8.2567e-04, 8.9333e-04, 5.2884e-04, 8.2133e-04, 6.2516e-04,\n                      5.3896e-04, 8.0249e-04, 1.0425e-03, 5.0874e-04, 9.4232e-04, 4.8070e-04,\n                      7.6760e-04, 9.2759e-04, 4.0724e-04, 6.3641e-04, 6.3284e-04, 8.3891e-04,\n                      4.2462e-04, 9.9396e-04, 5.1687e-04, 6.3693e-04, 5.7671e-04, 5.6309e-04,\n                      7.6314e-04, 8.7081e-04, 7.2133e-04, 8.5481e-04, 5.8283e-04, 8.7557e-04,\n                      6.2401e-04, 7.8818e-04, 5.9953e-04, 5.5732e-04, 4.5389e-04, 1.2191e-03,\n                      8.1673e-04, 9.9669e-04, 5.9112e-04, 8.5687e-04, 1.0300e-03, 9.4961e-04,\n                      7.6004e-04, 8.4108e-04, 4.4043e-04, 1.0028e-03, 6.7824e-04, 6.2549e-04,\n                      6.9272e-04, 6.2622e-04, 8.1333e-04, 6.4695e-04, 7.0774e-04, 4.6736e-04,\n                      6.0873e-04, 3.9605e-04, 7.5771e-04, 6.0889e-04, 7.8285e-04, 9.4698e-04,\n                      1.0129e-05, 5.2455e-04, 6.0941e-04, 8.1856e-04, 1.0018e-03, 1.1183e-03,\n                      5.5859e-04, 7.7538e-04, 4.5595e-04, 8.1986e-04, 9.0726e-04, 6.5966e-04,\n                      8.7185e-04, 7.9379e-04, 9.3733e-04, 5.7207e-04, 7.0815e-04, 7.4547e-04,\n                      8.0841e-04, 5.1005e-04, 5.3666e-04, 8.5944e-04, 4.3628e-04, 7.5814e-04,\n                      9.7276e-04, 8.4123e-04, 4.9669e-04, 7.0827e-04, 5.0877e-04, 7.2492e-04,\n                      6.2937e-04, 8.0217e-04, 4.9134e-04, 7.2359e-04, 9.3665e-04, 5.9403e-04,\n                      6.7040e-04, 6.1840e-04, 5.6447e-04, 5.6552e-04, 5.5438e-04, 5.3003e-04,\n                      7.8219e-04, 5.8712e-04, 1.0192e-03, 7.3093e-04, 7.2066e-04, 3.4868e-04,\n                      6.8630e-04, 4.3038e-04, 6.8071e-04, 6.7485e-04, 7.0872e-04, 7.3286e-04,\n                      1.2326e-03, 7.0583e-04, 9.4219e-06, 6.2725e-04, 3.2807e-04, 6.0001e-04,\n                      6.5232e-04, 5.2692e-04, 6.4472e-04, 8.6290e-04, 5.4512e-04, 4.6833e-04,\n                      6.3992e-04, 6.9345e-04, 8.3386e-04, 6.1530e-04, 5.6749e-04, 3.3362e-04,\n                      7.6411e-04, 5.0527e-04, 6.5406e-04, 3.9704e-04, 6.6824e-04, 7.0360e-04,\n                      5.4117e-04, 5.9761e-04, 7.0771e-04, 9.4735e-04, 1.0968e-03, 4.7456e-04,\n                      5.3128e-04, 6.8667e-04, 6.1948e-04, 7.9940e-04, 7.1798e-04, 8.8774e-04,\n                      4.8917e-04, 7.7798e-04, 6.1997e-04, 8.2879e-04, 6.4219e-04, 4.2506e-04,\n                      6.5349e-04, 5.4812e-04, 6.8603e-04, 4.9199e-04, 5.6697e-04, 1.1663e-03,\n                      9.7205e-04, 5.6801e-04, 5.6519e-04, 6.3766e-04, 1.4192e-03, 5.0533e-04,\n                      4.4373e-04, 6.8312e-04, 6.4500e-04, 7.7770e-04, 8.0415e-04, 6.7973e-04,\n                      4.4589e-04, 8.2067e-04, 6.7311e-04, 5.4206e-04, 4.9492e-04, 6.1938e-04,\n                      6.3851e-04, 8.1546e-04, 5.3527e-04, 7.3415e-04, 5.2144e-04, 6.0606e-04,\n                      8.5895e-04, 6.5100e-04, 8.3788e-04, 1.0357e-03, 4.3019e-04, 3.5540e-04,\n                      8.0321e-04, 8.9545e-04, 8.7116e-04, 7.4423e-04, 6.3668e-04, 7.7525e-04,\n                      8.4815e-04, 9.3600e-04, 7.3636e-04, 7.4420e-04, 4.3675e-04, 6.7550e-04,\n                      9.3342e-04, 5.5481e-04, 6.6685e-04, 4.9993e-04, 6.8173e-04, 7.5412e-04,\n                      3.6700e-04, 5.3616e-04, 6.7298e-04, 7.5487e-04, 7.5713e-04, 2.8351e-04,\n                      7.7376e-04, 5.8682e-04, 8.9918e-04, 7.6708e-04, 6.0522e-04, 6.3201e-04,\n                      1.0169e-03, 5.8027e-04, 4.6288e-04, 8.5900e-04, 7.0728e-04, 6.2456e-04,\n                      5.7249e-04, 8.1068e-04, 4.6388e-04, 5.4971e-04, 8.3134e-04, 6.2395e-04,\n                      8.6390e-04, 5.9647e-04, 5.4990e-04, 5.5063e-04, 8.7325e-04, 7.6329e-04,\n                      8.7917e-04, 9.2594e-04, 8.3268e-04, 7.9875e-04, 7.7918e-04, 7.5754e-04,\n                      4.4671e-04, 6.6158e-04, 4.9497e-04, 5.5457e-04, 8.0725e-04, 6.7814e-04,\n                      8.2422e-04, 9.4185e-04, 5.1926e-04, 7.3993e-04, 3.7637e-04, 4.7265e-04,\n                      9.8494e-04, 5.3769e-04, 6.5123e-04, 6.8195e-04, 5.9039e-04, 8.9221e-04,\n                      5.7573e-04, 7.0200e-04, 9.4255e-04, 8.1409e-04, 7.7729e-04, 7.2872e-04,\n                      7.5089e-04, 6.4892e-04, 9.1126e-04, 8.1046e-04, 9.9706e-04, 6.7541e-04,\n                      6.3654e-04, 9.0024e-04, 6.2207e-04, 3.9052e-04, 6.4386e-04, 6.4174e-04,\n                      5.3535e-04, 7.1889e-04, 7.2713e-04, 3.3324e-04, 7.7898e-04, 6.6925e-04,\n                      1.0238e-03, 3.4245e-04, 6.1153e-04, 5.3545e-04, 6.7602e-04, 8.5615e-04,\n                      7.0128e-04, 9.4337e-04, 6.4127e-04, 3.0916e-04, 6.4452e-04, 9.0626e-04,\n                      9.8781e-04, 6.1687e-04, 9.3157e-04, 5.8502e-04, 7.8032e-04, 7.0994e-04,\n                      7.2376e-04, 7.4405e-04, 6.5170e-04, 7.0525e-04, 4.8575e-04, 5.6526e-04,\n                      5.0011e-04, 6.8238e-04, 8.0863e-04, 4.3179e-04, 8.8241e-04, 9.3157e-04,\n                      8.8738e-04, 8.6281e-04, 6.8062e-04, 4.0635e-04, 9.1454e-04, 6.5108e-04,\n                      4.6432e-04, 6.6881e-04, 8.5842e-04, 5.5343e-04, 8.4233e-04, 6.3914e-04,\n                      5.0335e-04, 6.4568e-04, 5.8445e-04, 8.3871e-04, 8.3455e-04, 7.6722e-04,\n                      1.1228e-03, 7.9819e-04, 7.6173e-04, 5.8875e-04, 4.9607e-04, 6.8913e-04,\n                      7.2582e-04, 4.4522e-04, 1.0452e-03, 7.0899e-04, 7.7532e-04, 7.3422e-04,\n                      4.7491e-04, 7.5761e-04, 6.1734e-04, 3.7831e-04, 5.8621e-04, 8.7731e-04,\n                      3.9140e-04, 5.8246e-04, 6.2245e-04, 8.9754e-04, 7.4831e-04, 8.8008e-04,\n                      5.5806e-04, 6.3137e-04, 6.7228e-04, 6.5779e-04, 4.2875e-04, 7.3871e-04,\n                      5.7119e-04, 5.9044e-04, 5.7677e-04, 5.5195e-04, 8.0067e-04, 7.9541e-04,\n                      6.9754e-04, 3.6600e-04, 5.8402e-04, 3.9780e-04, 7.2177e-04, 1.0337e-03,\n                      8.4655e-04, 6.7829e-04, 4.9743e-04, 7.9401e-04, 6.9033e-04, 6.8261e-04,\n                      9.5989e-04, 3.0993e-04, 5.9532e-04, 6.7953e-04, 9.9380e-04, 6.0555e-04,\n                      7.9094e-04, 5.5331e-04, 5.3989e-04, 7.4569e-04, 1.0580e-03, 6.3185e-04,\n                      9.0180e-04, 5.6096e-04, 4.3029e-04, 6.4493e-04, 6.1440e-04, 7.0693e-04,\n                      5.8336e-04, 6.6361e-04, 4.6507e-04, 5.9803e-04, 6.7446e-04, 8.5605e-04,\n                      6.4905e-04, 6.8877e-04, 9.5026e-04, 5.9842e-04, 7.6932e-04, 6.1783e-04,\n                      5.6568e-04, 7.8042e-04, 7.1624e-04, 7.9218e-04, 6.1873e-04, 3.8332e-04,\n                      1.4259e-03, 1.0607e-03, 5.8878e-04, 7.7605e-04, 4.9241e-04, 5.2680e-04,\n                      6.8295e-04, 5.5547e-04, 1.2552e-03, 5.1283e-04, 7.1525e-04, 5.4329e-04,\n                      4.7885e-04, 5.7914e-04, 6.7376e-04, 5.1873e-04, 5.7305e-04, 3.5155e-04,\n                      5.2443e-04, 5.2023e-04, 9.5546e-04, 6.8854e-04, 1.0170e-03, 3.8468e-04,\n                      7.2180e-04, 6.5419e-04, 1.1085e-03, 5.8088e-04, 6.6212e-04, 4.1558e-04,\n                      6.6510e-04, 6.7367e-04, 4.3108e-04, 6.9110e-04, 6.4120e-04, 4.9972e-04,\n                      6.6723e-04, 4.8322e-04, 7.6349e-04, 4.0946e-04, 7.9868e-04, 7.9219e-04,\n                      3.3517e-04, 5.6598e-04, 7.3860e-04, 9.6764e-04, 5.9452e-04, 4.2662e-04,\n                      9.7289e-04, 5.3010e-04, 4.5774e-04, 6.2979e-04, 5.0480e-04, 3.7803e-04,\n                      4.7196e-04, 6.3114e-04, 5.1774e-04, 4.8656e-04, 5.8902e-04, 7.1785e-04,\n                      6.1050e-04, 7.4328e-04, 7.9499e-04, 7.7910e-04, 1.0295e-03, 7.3015e-04,\n                      7.8478e-04, 3.1042e-04, 4.7635e-04, 7.7840e-04, 6.6282e-04, 5.9211e-04,\n                      4.6011e-04, 8.3836e-04, 7.6195e-04, 3.3439e-04, 2.9513e-04, 6.6387e-04,\n                      8.5018e-04, 8.1750e-04, 5.0472e-04, 6.8230e-04, 8.7633e-04, 4.2723e-04,\n                      4.8386e-04, 9.2240e-04, 4.8282e-04, 7.6033e-04, 7.1117e-04, 6.6834e-04,\n                      5.0688e-04, 4.6539e-04, 4.1963e-04, 8.8001e-04, 9.1868e-04, 5.7520e-04,\n                      8.2568e-04, 5.3137e-04, 8.3386e-04, 7.0898e-04, 2.4675e-04, 4.2347e-04,\n                      4.4483e-04, 2.3397e-04, 2.4847e-04, 8.1856e-04, 7.4039e-06, 5.0639e-04,\n                      4.4867e-04, 1.1693e-03, 8.5849e-04, 1.0037e-03, 6.5628e-04, 6.6462e-04,\n                      8.8375e-04, 5.8697e-04, 6.0749e-04, 6.9554e-04, 4.8350e-04, 4.5218e-04,\n                      7.4300e-04, 7.3465e-04, 7.9218e-04, 8.1988e-04, 5.0860e-04, 1.0909e-03,\n                      1.1859e-03, 6.1095e-04, 7.3680e-04, 8.0882e-04, 7.8699e-04, 9.8550e-04,\n                      7.3411e-04, 8.3877e-04, 8.3880e-04, 6.4392e-04, 4.1614e-04, 7.9600e-04,\n                      9.6617e-04, 8.0561e-04, 7.9173e-04, 6.7053e-04, 1.7427e-03, 5.4897e-04,\n                      4.9867e-04, 7.1313e-04, 7.6329e-04, 7.6855e-04, 4.2658e-04, 1.4684e-03,\n                      6.5537e-04, 6.7649e-04, 6.3640e-04, 8.4154e-04, 7.9048e-04, 9.5242e-04,\n                      4.9804e-04, 1.2113e-03, 7.6219e-04, 3.7585e-04, 3.2888e-04, 8.2652e-04,\n                      6.2093e-04, 8.5562e-04, 1.0148e-03, 8.6170e-04, 2.7480e-04, 7.5732e-04,\n                      4.1381e-04, 5.9241e-04, 8.1573e-04, 5.1040e-04, 5.8808e-04, 4.8539e-04,\n                      6.8205e-04, 9.6074e-04, 7.2489e-04, 1.2847e-03, 6.3789e-04, 5.8907e-04,\n                      6.9339e-04, 8.3100e-04, 6.2005e-04, 4.2289e-04, 3.5877e-04, 8.2614e-04,\n                      5.1454e-04, 6.8413e-04, 7.5148e-04, 7.2119e-04, 1.3483e-03, 8.7815e-04,\n                      7.5785e-04, 7.7447e-04, 7.9774e-04, 7.8812e-04, 3.9660e-04, 6.7264e-04,\n                      5.2818e-04, 1.0646e-03, 9.5518e-04, 6.3858e-04, 8.0822e-04, 6.6996e-04,\n                      6.0428e-04, 1.2159e-03, 5.2183e-04, 8.7107e-04, 1.0188e-03, 8.8981e-04,\n                      7.4308e-04, 5.2453e-04, 7.2082e-04, 5.1807e-04, 5.3550e-04, 6.2700e-04,\n                      6.6241e-04, 1.0036e-03, 6.1235e-04, 6.8131e-04, 7.8563e-04, 9.7152e-04,\n                      5.4969e-04, 4.0531e-04, 3.4079e-04, 8.4501e-04, 7.8925e-04, 5.7956e-04,\n                      7.9157e-04, 6.9928e-04, 6.0173e-04, 2.5020e-04, 9.0545e-04, 4.4618e-04,\n                      7.1425e-04, 6.5045e-04, 1.0306e-03, 5.2559e-04, 7.6193e-04, 6.6183e-04,\n                      7.7468e-04, 7.8523e-04, 6.9696e-04, 6.5602e-04, 8.0904e-04, 6.8437e-04,\n                      6.9158e-04, 5.7012e-04, 6.9984e-04, 4.5193e-04, 7.8402e-04, 5.8022e-04],\n                     dtype=torch.float64),\n                     zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n                     axis=0)),\n             ('mnet.features.16.conv.0.0.bias',\n              Parameter containing:\n              tensor([-3.4374e-02, -1.2401e-02,  3.7047e-02,  1.3002e-02, -7.2276e-02,\n                       3.6618e-03,  1.9603e-02, -2.2505e-02,  2.0380e-01, -1.9746e-02,\n                      -3.3490e-02,  7.1303e-02,  3.4287e-02,  1.9528e-03,  7.9085e-02,\n                       3.1298e-02,  1.9147e-01,  5.1424e-02,  4.0869e-02,  6.4203e-02,\n                       6.5358e-02,  9.1413e-02,  4.4779e-04,  4.9089e-03,  6.9392e-02,\n                       2.9259e-02,  1.5185e-01,  1.5390e-01, -1.2748e-02,  1.7321e-01,\n                      -2.2581e-02, -3.3508e-02,  1.0560e-01, -5.1348e-02,  1.2144e-01,\n                      -5.5554e-02,  2.3825e-02, -2.8841e-02,  8.2547e-02, -8.5364e-02,\n                      -4.2058e-02,  9.0768e-02, -3.0078e-02, -6.2383e-02, -3.8280e-02,\n                      -5.9107e-02,  4.7340e-02,  7.2078e-02,  1.8555e-02, -2.7107e-02,\n                       6.8357e-02, -7.1507e-03, -3.4244e-02, -7.0101e-02, -6.6536e-02,\n                       1.0771e-01,  9.0240e-02,  3.2008e-02,  1.9540e-01, -5.1910e-02,\n                      -5.7881e-02, -1.4143e-03,  3.0084e-02, -2.9498e-02, -5.7408e-02,\n                       8.1853e-02, -3.7406e-02, -9.3698e-02,  1.0671e-01,  3.0838e-02,\n                       6.4394e-02,  1.3092e-01,  4.2656e-02, -1.0797e-01, -3.1267e-02,\n                      -9.8114e-03, -4.8520e-02, -4.9919e-02, -1.0454e-01, -8.1006e-02,\n                      -1.4321e-02, -4.8374e-02,  8.8596e-03, -5.2042e-02,  8.3750e-02,\n                      -1.1357e-02, -7.0486e-02, -3.8907e-02,  7.3293e-02,  9.6010e-02,\n                       1.0963e-01,  1.0398e-01, -4.7314e-04,  5.4688e-02,  1.0237e-01,\n                      -3.8137e-02, -3.3866e-02, -7.5237e-02, -4.9184e-02, -3.7445e-02,\n                       7.6857e-02, -5.3641e-02,  2.7306e-02, -2.7748e-02,  4.4919e-02,\n                       4.5509e-02,  1.2602e-01,  1.4883e-02,  5.1205e-02,  6.8719e-03,\n                      -4.0234e-02,  7.6008e-02, -3.8648e-02, -1.0647e-02,  1.9304e-01,\n                       4.7815e-03,  7.7170e-02,  4.4879e-02,  3.0300e-03, -5.3686e-02,\n                       4.9438e-02,  5.9827e-03, -6.6182e-02, -8.5302e-02,  9.0322e-03,\n                      -3.5733e-02,  6.3158e-02,  1.4427e-01, -6.6243e-02,  1.7956e-01,\n                       1.1669e-01,  1.5072e-02,  1.7803e-01,  2.2667e-02,  3.7432e-02,\n                       4.6655e-03, -2.1285e-02, -4.6544e-03,  8.1268e-02, -3.4961e-02,\n                      -4.3415e-02, -1.8225e-02,  6.6814e-02,  9.1814e-02,  3.7419e-02,\n                       4.8766e-03,  2.0118e-01,  1.4678e-01,  1.5981e-01,  1.4641e-01,\n                      -1.9164e-02, -5.1887e-02, -4.5516e-02, -2.0266e-02, -2.3505e-02,\n                       8.2653e-03,  1.7698e-02,  4.5178e-02, -6.5355e-02,  2.5183e-02,\n                       6.1481e-02, -3.0868e-03,  1.1495e-01,  5.5698e-02, -6.7039e-02,\n                       5.6208e-02, -5.8299e-02, -2.8987e-02, -8.7230e-03, -4.2252e-02,\n                       1.7334e-02,  5.2716e-02, -2.3473e-02, -1.4544e-03, -4.0468e-02,\n                      -4.0827e-02,  2.7610e-02,  9.8379e-02,  1.5740e-02, -3.5770e-03,\n                      -7.3804e-03, -3.2297e-02,  3.0971e-02,  1.0442e-01,  3.7050e-02,\n                       6.7711e-02, -4.9436e-03,  2.5107e-01, -2.8950e-02, -1.2136e-02,\n                      -8.1390e-02,  1.4284e-02,  8.5303e-02,  1.9731e-02,  9.1568e-02,\n                       4.4156e-02,  9.5624e-02,  5.4229e-02,  4.3503e-02,  8.8802e-02,\n                      -6.1881e-02, -3.2798e-02, -5.7417e-02,  8.2261e-03,  1.0296e-01,\n                      -2.6722e-04, -5.3133e-02, -6.4955e-02, -2.1654e-02,  4.5813e-03,\n                       1.7835e-02, -1.1232e-02,  5.9343e-02,  1.4022e-01, -7.7248e-02,\n                       8.8645e-02,  4.5467e-02, -6.1462e-02, -1.2830e-01, -6.9017e-02,\n                      -5.6771e-02,  1.4607e-01,  1.5790e-01,  5.5930e-02, -3.0003e-03,\n                       6.4181e-02, -4.1781e-02, -2.8695e-02,  1.5201e-01, -2.9705e-02,\n                      -2.8313e-02, -2.3328e-02,  4.8606e-02,  3.0512e-02,  1.0272e-01,\n                       9.1612e-02, -3.6126e-03, -5.8865e-02,  3.5520e-02, -1.5218e-02,\n                      -2.8557e-02,  7.9257e-03,  1.3004e-01,  1.6633e-01,  1.7937e-01,\n                      -3.8573e-02, -6.3066e-02,  6.4650e-02,  5.9296e-02,  2.4364e-02,\n                       8.4821e-02,  7.5136e-02, -1.2601e-02,  3.9025e-02,  9.4572e-02,\n                       1.1202e-01, -4.6117e-02, -2.6764e-03,  2.0193e-01,  1.6690e-01,\n                      -8.9671e-03, -5.5646e-02, -5.5075e-02,  1.0716e-01,  1.3943e-01,\n                       2.8514e-02,  4.6318e-02,  6.6857e-02,  1.4433e-02,  2.1106e-02,\n                       2.5836e-02,  3.4749e-03,  4.2267e-02, -2.6299e-02, -6.0350e-02,\n                       1.0194e-01, -2.4440e-03, -3.0733e-02, -5.8259e-02, -7.0903e-02,\n                       2.1074e-02,  3.1091e-04, -3.6412e-02,  3.7656e-04,  7.7816e-02,\n                       9.4370e-02,  7.0570e-02,  3.3834e-02, -6.1048e-03,  1.3542e-01,\n                      -3.8021e-02,  1.8307e-01, -7.2876e-02, -4.6434e-02, -6.2861e-03,\n                       1.3700e-01,  7.8855e-02,  9.2610e-02,  8.0630e-02,  2.2338e-01,\n                      -7.7906e-02,  1.2661e-02,  1.0412e-01,  1.0141e-01,  4.9662e-02,\n                      -1.1235e-01,  4.5069e-02,  1.2773e-01,  9.8907e-02, -3.4413e-02,\n                      -5.1019e-02,  1.8901e-01, -5.0829e-02, -8.6564e-02,  6.5911e-02,\n                      -1.0129e-01, -2.2626e-03,  6.5134e-02,  2.2126e-02, -5.3296e-02,\n                       1.2236e-01,  1.2767e-01,  7.0594e-02,  1.0204e-03,  1.2364e-01,\n                      -8.2940e-02,  5.0474e-02, -1.3554e-01,  1.7059e-02, -8.5768e-02,\n                      -3.1389e-02, -1.1862e-01, -3.4909e-02,  8.7949e-02, -2.7523e-02,\n                      -5.5466e-03,  3.7950e-02,  2.3310e-03,  3.9688e-02, -7.7350e-02,\n                      -1.2486e-02, -7.1271e-02, -3.6597e-02, -1.3346e-02, -1.7930e-02,\n                       6.4490e-02,  8.5260e-02, -4.4507e-02, -2.1131e-02,  6.9997e-02,\n                      -2.3019e-02, -2.0994e-02, -3.4484e-02,  6.0969e-02,  3.3472e-03,\n                       4.4202e-03,  1.8211e-01, -2.6776e-02,  4.5804e-02, -2.2339e-02,\n                       1.6917e-02, -5.1649e-02,  6.8300e-02, -1.8712e-02,  1.9408e-01,\n                       6.6046e-03,  1.1760e-01, -3.0101e-02,  2.5144e-02, -7.4581e-02,\n                       5.5616e-02, -1.2010e-01,  5.5337e-02, -4.8202e-02,  1.1181e-02,\n                      -5.5361e-03,  1.3176e-01, -8.5362e-02, -1.1676e-02, -1.3428e-02,\n                      -2.9930e-04,  1.0064e-02, -6.7154e-02,  3.9934e-02,  4.6679e-02,\n                       1.2473e-01,  8.7921e-02,  2.3276e-02,  7.7959e-02, -3.3365e-02,\n                      -4.1871e-02, -5.1902e-02, -1.4115e-02, -1.4129e-02, -5.4580e-02,\n                      -3.0221e-02, -8.5185e-03,  1.0263e-01,  1.1005e-01,  6.6109e-02,\n                       2.8552e-02,  2.5231e-03,  1.7993e-01, -2.7633e-02, -2.0701e-02,\n                       1.7439e-02,  2.1600e-01, -1.2579e-02, -4.5048e-02, -7.4532e-02,\n                       6.3910e-02, -6.2595e-02,  1.7938e-01,  1.4563e-01, -6.9770e-02,\n                      -4.4703e-03,  6.7468e-02,  8.8198e-02, -4.3655e-02,  6.9904e-02,\n                      -7.6687e-02,  8.8459e-02, -1.5949e-03, -2.0486e-02, -1.5566e-02,\n                       6.5910e-02, -6.8173e-02, -7.5181e-02,  1.0184e-01, -4.8757e-02,\n                       4.9170e-02,  1.8666e-01, -2.0545e-03,  9.8009e-02, -3.9647e-02,\n                       3.8130e-03,  1.2751e-01,  6.1429e-02, -2.1421e-02,  1.5744e-01,\n                       1.0446e-01, -1.7021e-02, -2.2043e-02,  9.1910e-02,  1.6986e-02,\n                      -3.4008e-02,  1.9438e-02, -4.8532e-02, -3.1987e-02, -6.1935e-02,\n                       6.4724e-02,  7.9798e-02, -3.3018e-02,  4.5662e-03,  3.2096e-02,\n                       6.1256e-02, -1.1579e-02, -1.5864e-02, -5.4630e-02, -3.1997e-02,\n                       5.7170e-02, -4.5599e-02,  1.8963e-01, -4.4026e-02, -5.0724e-02,\n                       2.2988e-02, -1.8271e-02,  6.5283e-02,  6.8399e-03,  1.5003e-01,\n                       3.1517e-02,  3.9444e-02,  1.4358e-01, -2.7071e-02,  3.2196e-02,\n                       1.9143e-02,  3.8710e-02,  3.3703e-02,  4.8643e-02,  5.3629e-02,\n                      -2.0113e-02,  7.3366e-02, -8.1967e-03,  3.5722e-04,  9.0853e-02,\n                       1.2498e-01,  3.9254e-03,  2.4980e-02,  1.2353e-01, -2.5328e-02,\n                      -2.6331e-02,  6.1638e-02, -3.8075e-02, -4.9332e-02, -1.4810e-02,\n                      -2.4366e-02,  5.5831e-02,  1.8398e-02,  1.5167e-02,  3.0285e-02,\n                      -6.7024e-03,  8.6040e-02,  8.9688e-02, -1.0074e-02, -2.7097e-02,\n                      -1.3782e-02,  1.0908e-02,  1.3646e-02, -7.6642e-02,  9.7641e-02,\n                       8.0606e-02,  3.2069e-02,  1.8379e-01,  3.5969e-02,  8.9364e-02,\n                      -3.6122e-02,  1.4918e-02,  1.2937e-01,  7.4140e-03, -4.0577e-02,\n                       4.8452e-02,  7.1326e-03,  1.1557e-02,  9.8524e-02, -1.0963e-02,\n                       2.2363e-02,  1.2360e-02, -3.8736e-02, -6.1604e-02,  5.5335e-02,\n                       6.4517e-02, -1.0051e-01,  1.1900e-01,  1.7622e-02,  4.0241e-02,\n                      -1.2544e-01,  1.8207e-02,  3.9765e-03,  2.3469e-02, -3.3524e-02,\n                       1.2733e-01, -5.1652e-02,  4.2710e-02,  4.4545e-03,  1.5269e-02,\n                       1.2098e-01,  5.9824e-02, -4.3860e-02,  1.2418e-01,  4.9585e-02,\n                      -4.1625e-02,  2.3471e-02,  5.8760e-02, -5.3611e-02, -1.0456e-01,\n                      -6.8481e-02, -9.8787e-02,  4.9069e-02, -5.7046e-02,  5.1805e-02,\n                      -7.8847e-02, -4.0450e-02,  2.3319e-02, -3.5852e-02,  8.1153e-02,\n                       2.0393e-02, -7.0406e-02,  5.6204e-02, -6.7786e-02,  1.1040e-01,\n                       1.2541e-01, -3.1366e-02, -1.5574e-02,  8.5247e-02, -1.5002e-02,\n                      -5.3279e-03, -1.7997e-02, -4.0425e-02,  9.9560e-02,  1.9986e-02,\n                       1.7920e-02, -1.2304e-03,  1.2337e-01, -3.6070e-02, -6.9428e-02,\n                      -1.5444e-02,  3.5784e-03,  1.2758e-01,  1.8261e-02,  1.1762e-01,\n                       3.6459e-02, -7.1605e-02,  1.1904e-02, -3.0219e-02,  9.4822e-02,\n                      -4.2161e-02,  1.0682e-01,  1.1754e-01, -4.7114e-02,  1.9647e-01,\n                      -7.1690e-03,  5.5662e-03,  7.1545e-02, -6.3784e-02,  7.7887e-02,\n                      -1.4544e-01, -1.0246e-01,  7.7465e-02, -1.6846e-02,  1.5593e-01,\n                       7.2951e-02,  4.7176e-02,  9.5857e-02, -1.0479e-02, -1.0614e-01,\n                      -5.9213e-02,  6.5742e-02, -1.1067e-02,  3.0246e-02,  2.2102e-02,\n                       7.3091e-02, -6.2591e-02, -2.4386e-02, -4.0404e-02,  5.9142e-02,\n                      -2.5033e-02,  4.4787e-02,  7.9249e-02,  7.6701e-02,  1.8907e-03,\n                      -6.9762e-02, -9.0272e-02, -6.9743e-02, -2.2995e-02,  6.7152e-02,\n                      -6.4134e-02, -5.2053e-02,  3.5266e-02, -1.8594e-02, -2.8966e-03,\n                       1.5858e-01, -5.2737e-02, -2.5198e-02,  1.0319e-01,  1.0756e-01,\n                      -8.3400e-02,  4.9812e-02,  3.9980e-03, -7.8213e-02,  4.8953e-03,\n                       6.9572e-02, -9.0073e-02,  3.9119e-02, -5.1169e-02,  4.1283e-02,\n                      -2.1247e-02,  9.9887e-02,  1.8167e-01, -4.1027e-02, -1.1646e-02,\n                       2.3876e-02,  1.5290e-01,  1.5262e-02, -4.1667e-02,  7.9564e-02,\n                      -1.6041e-02, -1.7771e-02, -5.8516e-03, -1.4948e-02,  3.9510e-02,\n                       1.2827e-01,  3.0102e-02,  3.1395e-02,  7.2862e-02, -2.7146e-02,\n                       1.5267e-01, -6.2152e-02,  5.5486e-02, -1.8736e-03,  5.3166e-03,\n                      -2.6180e-02,  9.7001e-02,  3.7401e-02,  4.5879e-02,  3.9469e-02,\n                       1.0789e-01, -1.2875e-02,  1.1540e-01, -2.4963e-02, -4.5733e-02,\n                      -5.2166e-02,  1.3490e-01,  1.9344e-01,  1.1027e-01, -5.1426e-02,\n                       4.5756e-02, -8.7031e-02,  1.4934e-01, -4.6200e-02, -2.7427e-02,\n                       4.3626e-02, -5.7775e-02,  6.3253e-02,  2.8755e-02, -3.1532e-02,\n                      -6.0023e-02, -8.2701e-02, -2.7086e-02,  3.3989e-02,  3.0342e-02,\n                       4.8395e-02,  2.0555e-02,  1.2221e-01,  1.5094e-01, -2.2385e-02,\n                       1.5266e-01,  1.0987e-01,  3.8444e-02,  3.6766e-03, -5.0622e-02,\n                       6.4975e-02, -1.0614e-01,  3.4904e-03,  9.6289e-02,  1.7543e-02,\n                       1.1718e-01,  4.0054e-02, -1.0243e-01,  2.5644e-02, -2.2154e-02,\n                      -3.5510e-03, -2.6049e-02,  2.8693e-03, -3.3488e-02, -4.0342e-02,\n                       2.4062e-02, -3.4777e-02, -4.7008e-02, -7.7532e-03,  6.4595e-02,\n                       1.0160e-02,  3.8418e-02, -1.1026e-01, -5.3493e-02,  1.2498e-01,\n                       8.4349e-02,  3.7247e-02, -2.0499e-02,  4.3388e-02, -6.9804e-02,\n                       2.5081e-02,  3.1594e-02,  3.1271e-02,  8.4552e-02, -2.9749e-02,\n                      -5.8579e-02,  1.4520e-01,  9.4637e-02, -2.2171e-02, -8.5358e-02,\n                       4.2630e-02, -3.7563e-02,  2.4745e-02,  1.6922e-01,  6.0850e-02,\n                       6.2679e-02,  1.2418e-03,  1.7363e-01,  1.8617e-01,  3.9587e-02,\n                      -7.6240e-02,  9.0693e-02, -1.3128e-01, -1.9781e-02, -6.8915e-02,\n                       6.9173e-03, -3.6342e-02, -1.0652e-01,  1.0053e-01, -2.1437e-02,\n                       1.6787e-01,  3.5110e-02,  2.9552e-02,  1.2914e-01,  4.8182e-02,\n                      -3.2432e-02, -5.7444e-02,  1.1337e-01,  8.9565e-02,  4.7119e-02,\n                      -7.1043e-03,  6.6243e-02,  3.8964e-02, -1.8160e-02,  6.0918e-02,\n                       4.2301e-02, -5.7465e-02, -4.5060e-02,  2.8387e-03,  6.3096e-02,\n                      -4.3694e-03,  7.9049e-02, -2.1023e-02,  1.2486e-02, -1.6787e-02,\n                       1.9934e-01, -5.3301e-02,  2.7089e-02,  5.5910e-03,  1.3895e-01,\n                      -1.2279e-02, -1.3744e-02, -1.6350e-02,  5.0071e-02, -1.7773e-02,\n                      -5.2071e-02, -8.5815e-03, -9.2784e-02,  7.5781e-02,  2.8999e-02,\n                       9.8794e-02,  8.1081e-02,  1.2092e-01, -4.0005e-02, -4.7528e-02,\n                       2.3582e-02,  1.2007e-01,  5.4109e-02, -3.3907e-02, -5.9446e-02,\n                      -7.4515e-02,  6.0088e-02,  7.8623e-03, -7.5768e-02,  2.3045e-02,\n                       1.7619e-01,  1.1232e-01,  3.2360e-02,  6.6004e-02, -4.7644e-02,\n                      -2.0274e-02,  4.2462e-02,  4.3133e-02,  1.2832e-01,  2.3419e-02,\n                      -3.5963e-02, -1.6292e-02, -7.9794e-02,  4.6476e-02,  2.9975e-02,\n                       8.5039e-02,  1.7081e-01,  2.0552e-02,  1.7500e-01, -6.2947e-02,\n                       6.2457e-02, -1.7648e-02, -4.8912e-02, -3.3480e-02, -4.1374e-02,\n                      -4.1594e-02,  5.9101e-02,  2.9534e-02,  1.4698e-02,  9.0502e-02,\n                       3.7981e-02, -4.0385e-04,  1.0933e-02,  6.3222e-03,  4.2741e-03,\n                       2.0595e-02, -1.1359e-01, -4.0987e-02, -5.9020e-02, -3.3729e-02,\n                      -1.5698e-02,  7.3317e-06,  1.3137e-01,  7.7175e-02,  1.1223e-01,\n                      -3.2842e-02,  2.0681e-02,  1.3981e-02, -3.6197e-02,  6.5326e-02,\n                       2.5727e-03,  4.9352e-02,  8.7724e-02,  1.3311e-01,  4.2305e-02,\n                       1.8281e-02, -3.0951e-02, -3.4159e-02, -5.0705e-02,  7.5576e-02,\n                      -5.9339e-02, -7.6860e-03, -1.1418e-03, -1.1009e-01,  4.6172e-02,\n                       1.5497e-01,  1.0116e-01, -1.4367e-02, -1.1478e-02,  8.5858e-02,\n                       2.7557e-02, -5.9766e-02,  1.1074e-02, -1.3231e-01,  1.6690e-01,\n                       6.4339e-02, -6.2150e-02,  4.4695e-02, -7.0533e-03,  1.1750e-01,\n                      -7.1665e-02, -5.2282e-02, -6.3991e-02, -1.5589e-02, -5.3790e-02,\n                      -4.0506e-02,  7.7798e-03, -2.2609e-02, -4.4511e-02,  1.0305e-01,\n                       9.3358e-02, -2.2751e-02, -2.9334e-02,  3.7329e-02,  3.9799e-03,\n                       2.5987e-02,  3.6257e-03,  8.1689e-02, -4.9971e-02, -5.6693e-03,\n                       5.9022e-02,  7.2489e-02,  1.0515e-01,  8.0317e-03, -7.1345e-03,\n                       3.8391e-02,  8.6277e-02,  1.0273e-01, -3.2181e-02, -9.6173e-02,\n                       5.7756e-02, -4.7018e-02,  6.8096e-02, -2.5845e-02,  4.1460e-02,\n                      -1.0367e-02, -6.1754e-02,  8.2572e-02, -7.3644e-04, -5.1878e-02,\n                      -2.2432e-02, -4.6522e-02,  7.6848e-03,  4.1172e-02, -2.3191e-02],\n                     requires_grad=True)),\n             ('mnet.features.16.conv.0.0.scale', tensor(0.0535)),\n             ('mnet.features.16.conv.0.0.zero_point', tensor(54)),\n             ('mnet.features.16.conv.1.0.weight',\n              tensor([[[[ 2.0556,  2.9667,  2.0089],\n                        [ 1.4016,  0.3971,  1.4016],\n                        [ 0.6307, -1.4950,  0.2102]]],\n              \n              \n                      [[[ 0.4533,  0.2357,  0.4895],\n                        [ 0.3445,  2.3025,  0.5620],\n                        [ 0.4714,  0.7977,  0.3807]]],\n              \n              \n                      [[[ 1.4317, -0.4596, -0.7424],\n                        [ 2.2447, -0.8838, -1.1666],\n                        [ 1.0959, -0.8131, -0.7954]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-0.8712, -1.7251, -0.9932],\n                        [ 0.5576, -0.7144,  0.8015],\n                        [ 0.9061,  2.2130,  0.9932]]],\n              \n              \n                      [[[ 0.3815, -0.3164,  0.4560],\n                        [-0.0372, -1.1074,  0.0093],\n                        [ 1.1632, -0.5770,  1.1818]]],\n              \n              \n                      [[[ 0.1776,  0.4441,  0.2411],\n                        [ 0.2411,  1.6114,  0.3680],\n                        [ 0.4695,  0.4821,  0.4821]]]], size=(960, 1, 3, 3),\n                     dtype=torch.qint8, quantization_scheme=torch.per_channel_affine,\n                     scale=tensor([0.0234, 0.0181, 0.0177, 0.0056, 0.0120, 0.0107, 0.0134, 0.0198, 0.0090,\n                      0.0109, 0.0083, 0.0040, 0.0170, 0.0228, 0.0046, 0.0138, 0.0181, 0.0039,\n                      0.0180, 0.0044, 0.0035, 0.0128, 0.0120, 0.0119, 0.0105, 0.0110, 0.0027,\n                      0.0093, 0.0083, 0.0135, 0.0155, 0.0101, 0.0114, 0.0106, 0.0141, 0.0413,\n                      0.0135, 0.0085, 0.0068, 0.0101, 0.0126, 0.0038, 0.0172, 0.0150, 0.0083,\n                      0.0146, 0.0122, 0.0037, 0.0040, 0.0104, 0.0036, 0.0052, 0.0077, 0.0098,\n                      0.0118, 0.0107, 0.0144, 0.0045, 0.0159, 0.0165, 0.0115, 0.0222, 0.0174,\n                      0.0104, 0.0126, 0.0126, 0.0131, 0.0114, 0.0052, 0.0047, 0.0039, 0.0088,\n                      0.0031, 0.0105, 0.0321, 0.0060, 0.0072, 0.0135, 0.0072, 0.0107, 0.0002,\n                      0.0115, 0.0141, 0.0140, 0.0052, 0.0157, 0.0193, 0.0103, 0.0045, 0.0157,\n                      0.0044, 0.0075, 0.0174, 0.0065, 0.0047, 0.0212, 0.0103, 0.0093, 0.0137,\n                      0.0096, 0.0050, 0.0080, 0.0043, 0.0039, 0.0045, 0.0040, 0.0051, 0.0060,\n                      0.0135, 0.0173, 0.0081, 0.0044, 0.0075, 0.0174, 0.0191, 0.0094, 0.0103,\n                      0.0285, 0.0157, 0.0148, 0.0035, 0.0098, 0.0096, 0.0251, 0.0278, 0.0138,\n                      0.0048, 0.0080, 0.0082, 0.0134, 0.0047, 0.0228, 0.0118, 0.0093, 0.0091,\n                      0.0049, 0.0196, 0.0163, 0.0041, 0.0116, 0.0112, 0.0117, 0.0049, 0.0053,\n                      0.0037, 0.0230, 0.0160, 0.0038, 0.0061, 0.0109, 0.0091, 0.0086, 0.0105,\n                      0.0213, 0.0095, 0.0043, 0.0043, 0.0050, 0.0123, 0.0159, 0.0033, 0.0148,\n                      0.0033, 0.0037, 0.0187, 0.0049, 0.0144, 0.0164, 0.0055, 0.0081, 0.0048,\n                      0.0036, 0.0122, 0.0026, 0.0092, 0.0124, 0.0156, 0.0039, 0.0047, 0.0322,\n                      0.0126, 0.0095, 0.0166, 0.0057, 0.0044, 0.0046, 0.0044, 0.0132, 0.0075,\n                      0.0049, 0.0240, 0.0038, 0.0031, 0.0035, 0.0040, 0.0089, 0.0048, 0.0117,\n                      0.0114, 0.0045, 0.0168, 0.0164, 0.0142, 0.0119, 0.0097, 0.0091, 0.0180,\n                      0.0234, 0.0200, 0.0084, 0.0154, 0.0128, 0.0046, 0.0047, 0.0090, 0.0047,\n                      0.0047, 0.0167, 0.0150, 0.0080, 0.0149, 0.0103, 0.0101, 0.0031, 0.0311,\n                      0.0031, 0.0033, 0.0123, 0.0090, 0.0118, 0.0088, 0.0056, 0.0144, 0.0211,\n                      0.0047, 0.0029, 0.0038, 0.0061, 0.0098, 0.0144, 0.0109, 0.0036, 0.0063,\n                      0.0060, 0.0140, 0.0128, 0.0212, 0.0077, 0.0059, 0.0107, 0.0055, 0.0136,\n                      0.0302, 0.0043, 0.0058, 0.0035, 0.0210, 0.0125, 0.0123, 0.0074, 0.0299,\n                      0.0169, 0.0164, 0.0050, 0.0058, 0.0317, 0.0052, 0.0038, 0.0049, 0.0064,\n                      0.0032, 0.0049, 0.0045, 0.0227, 0.0078, 0.0039, 0.0160, 0.0110, 0.0158,\n                      0.0132, 0.0033, 0.0138, 0.0085, 0.0124, 0.0048, 0.0101, 0.0124, 0.0051,\n                      0.0072, 0.0063, 0.0092, 0.0166, 0.0191, 0.0142, 0.0178, 0.0055, 0.0032,\n                      0.0087, 0.0044, 0.0132, 0.0092, 0.0040, 0.0108, 0.0055, 0.0032, 0.0165,\n                      0.0156, 0.0052, 0.0088, 0.0104, 0.0196, 0.0168, 0.0125, 0.0111, 0.0183,\n                      0.0235, 0.0141, 0.0036, 0.0042, 0.0168, 0.0054, 0.0051, 0.0171, 0.0037,\n                      0.0046, 0.0081, 0.0112, 0.0412, 0.0087, 0.0106, 0.0124, 0.0198, 0.0229,\n                      0.0039, 0.0105, 0.0147, 0.0039, 0.0061, 0.0040, 0.0098, 0.0112, 0.0626,\n                      0.0128, 0.0167, 0.0150, 0.0045, 0.0107, 0.0124, 0.0010, 0.0141, 0.0091,\n                      0.0106, 0.0107, 0.0035, 0.0233, 0.0053, 0.0108, 0.0106, 0.0047, 0.0097,\n                      0.0035, 0.0111, 0.0036, 0.0134, 0.0141, 0.0056, 0.0049, 0.0122, 0.0165,\n                      0.0093, 0.0153, 0.0168, 0.0036, 0.0113, 0.0227, 0.0130, 0.0088, 0.0077,\n                      0.0061, 0.0098, 0.0384, 0.0101, 0.0125, 0.0042, 0.0079, 0.0096, 0.0160,\n                      0.0042, 0.0090, 0.0239, 0.0091, 0.0250, 0.0100, 0.0182, 0.0130, 0.0214,\n                      0.0132, 0.0152, 0.0033, 0.0036, 0.0029, 0.0041, 0.0049, 0.0091, 0.0004,\n                      0.0047, 0.0196, 0.0129, 0.0073, 0.0153, 0.0043, 0.0065, 0.0073, 0.0101,\n                      0.0107, 0.0103, 0.0031, 0.0055, 0.0171, 0.0183, 0.0223, 0.0101, 0.0142,\n                      0.0133, 0.0044, 0.0090, 0.0194, 0.0144, 0.0116, 0.0098, 0.0094, 0.0203,\n                      0.0088, 0.0106, 0.0146, 0.0038, 0.0077, 0.0043, 0.0167, 0.0055, 0.0253,\n                      0.0068, 0.0091, 0.0137, 0.0044, 0.0142, 0.0137, 0.0211, 0.0147, 0.0125,\n                      0.0050, 0.0129, 0.0323, 0.0120, 0.0040, 0.0129, 0.0114, 0.0143, 0.0089,\n                      0.0136, 0.0032, 0.0044, 0.0109, 0.0189, 0.0151, 0.0166, 0.0138, 0.0087,\n                      0.0144, 0.0061, 0.0053, 0.0038, 0.0108, 0.0151, 0.0044, 0.0207, 0.0049,\n                      0.0049, 0.0134, 0.0172, 0.0238, 0.0046, 0.0030, 0.0206, 0.0078, 0.0047,\n                      0.0039, 0.0114, 0.0057, 0.0099, 0.0154, 0.0040, 0.0063, 0.0083, 0.0047,\n                      0.0121, 0.0059, 0.0040, 0.0234, 0.0053, 0.0170, 0.0045, 0.0038, 0.0317,\n                      0.0088, 0.0049, 0.0044, 0.0041, 0.0133, 0.0127, 0.0041, 0.0033, 0.0130,\n                      0.0142, 0.0045, 0.0101, 0.0250, 0.0087, 0.0115, 0.0144, 0.0058, 0.0174,\n                      0.0036, 0.0036, 0.0121, 0.0110, 0.0035, 0.0102, 0.0048, 0.0034, 0.0046,\n                      0.0099, 0.0051, 0.0141, 0.0102, 0.0305, 0.0048, 0.0151, 0.0089, 0.0114,\n                      0.0065, 0.0109, 0.0142, 0.0081, 0.0174, 0.0148, 0.0050, 0.0127, 0.0086,\n                      0.0050, 0.0090, 0.0045, 0.0192, 0.0106, 0.0116, 0.0045, 0.0096, 0.0035,\n                      0.0066, 0.0285, 0.0118, 0.0112, 0.0044, 0.0106, 0.0142, 0.0031, 0.0088,\n                      0.0145, 0.0070, 0.0040, 0.0110, 0.0105, 0.0105, 0.0165, 0.0072, 0.0115,\n                      0.0110, 0.0152, 0.0058, 0.0243, 0.0096, 0.0095, 0.0095, 0.0089, 0.0096,\n                      0.0225, 0.0158, 0.0056, 0.0027, 0.0147, 0.0116, 0.0137, 0.0113, 0.0147,\n                      0.0054, 0.0140, 0.0088, 0.0083, 0.0143, 0.0166, 0.0085, 0.0039, 0.0053,\n                      0.0302, 0.0047, 0.0118, 0.0168, 0.0120, 0.0189, 0.0148, 0.0056, 0.0067,\n                      0.0130, 0.0124, 0.0153, 0.0142, 0.0042, 0.0049, 0.0048, 0.0184, 0.0152,\n                      0.0201, 0.0100, 0.0110, 0.0092, 0.0125, 0.0121, 0.0059, 0.0149, 0.0053,\n                      0.0080, 0.0631, 0.0100, 0.0110, 0.0139, 0.0124, 0.0200, 0.0219, 0.0080,\n                      0.0235, 0.0049, 0.0112, 0.0150, 0.0161, 0.0090, 0.0080, 0.0111, 0.0238,\n                      0.0156, 0.0042, 0.0084, 0.0101, 0.0178, 0.0088, 0.0145, 0.0098, 0.0073,\n                      0.0120, 0.0100, 0.0129, 0.0088, 0.0147, 0.0164, 0.0358, 0.0080, 0.0043,\n                      0.0190, 0.0107, 0.0189, 0.0088, 0.0066, 0.0122, 0.0037, 0.0205, 0.0125,\n                      0.0075, 0.0074, 0.0144, 0.0042, 0.0090, 0.0098, 0.0038, 0.0098, 0.0149,\n                      0.0038, 0.0086, 0.0159, 0.0046, 0.0123, 0.0143, 0.0106, 0.0074, 0.0115,\n                      0.0046, 0.0139, 0.0045, 0.0211, 0.0103, 0.0062, 0.0041, 0.0037, 0.0181,\n                      0.0122, 0.0058, 0.0101, 0.0107, 0.0155, 0.0214, 0.0109, 0.0040, 0.0144,\n                      0.0038, 0.0084, 0.0063, 0.0076, 0.0057, 0.0038, 0.0184, 0.0117, 0.0094,\n                      0.0160, 0.0204, 0.0049, 0.0177, 0.0058, 0.0138, 0.0188, 0.0229, 0.0141,\n                      0.0138, 0.0120, 0.0241, 0.0038, 0.0113, 0.0117, 0.0181, 0.0067, 0.0374,\n                      0.0127, 0.0043, 0.0081, 0.0044, 0.0104, 0.0179, 0.0123, 0.0138, 0.0035,\n                      0.0256, 0.0094, 0.0117, 0.0070, 0.0178, 0.0107, 0.0043, 0.0093, 0.0255,\n                      0.0063, 0.0098, 0.0243, 0.0126, 0.0060, 0.0186, 0.0054, 0.0065, 0.0172,\n                      0.0216, 0.0108, 0.0077, 0.0074, 0.0116, 0.0104, 0.0036, 0.0215, 0.0089,\n                      0.0143, 0.0292, 0.0067, 0.0113, 0.0041, 0.0134, 0.0179, 0.0038, 0.0207,\n                      0.0321, 0.0216, 0.0138, 0.0097, 0.0092, 0.0053, 0.0183, 0.0038, 0.0057,\n                      0.0109, 0.0036, 0.0166, 0.0110, 0.0108, 0.0089, 0.0122, 0.0181, 0.0146,\n                      0.0041, 0.0034, 0.0130, 0.0094, 0.0164, 0.0088, 0.0038, 0.0346, 0.0089,\n                      0.0121, 0.0396, 0.0330, 0.0040, 0.0004, 0.0144, 0.0146, 0.0094, 0.0036,\n                      0.0032, 0.0070, 0.0229, 0.0056, 0.0116, 0.0152, 0.0047, 0.0093, 0.0129,\n                      0.0095, 0.0105, 0.0082, 0.0044, 0.0178, 0.0108, 0.0097, 0.0062, 0.0167,\n                      0.0059, 0.0056, 0.0085, 0.0154, 0.0149, 0.0056, 0.0149, 0.0359, 0.0139,\n                      0.0141, 0.0099, 0.0029, 0.0128, 0.0173, 0.0072, 0.0100, 0.0077, 0.0128,\n                      0.0044, 0.0189, 0.0056, 0.0128, 0.0081, 0.0112, 0.0122, 0.0042, 0.0036,\n                      0.0124, 0.0049, 0.0036, 0.0151, 0.0550, 0.0126, 0.0101, 0.0157, 0.0091,\n                      0.0090, 0.0506, 0.0260, 0.0213, 0.0118, 0.0041, 0.0092, 0.0120, 0.0055,\n                      0.0059, 0.0130, 0.0051, 0.0055, 0.0053, 0.0127, 0.0088, 0.0044, 0.0054,\n                      0.0276, 0.0169, 0.0080, 0.0102, 0.0235, 0.0169, 0.0089, 0.0091, 0.0036,\n                      0.0091, 0.0046, 0.0138, 0.0060, 0.0150, 0.0168, 0.0151, 0.0049, 0.0167,\n                      0.0079, 0.0039, 0.0118, 0.0103, 0.0118, 0.0102, 0.0079, 0.0123, 0.0086,\n                      0.0191, 0.0294, 0.0105, 0.0135, 0.0202, 0.0102, 0.0053, 0.0117, 0.0115,\n                      0.0130, 0.0043, 0.0033, 0.0180, 0.0114, 0.0308, 0.0119, 0.0039, 0.0128,\n                      0.0113, 0.0029, 0.0038, 0.0331, 0.0030, 0.0121, 0.0046, 0.0130, 0.0078,\n                      0.0135, 0.0103, 0.0046, 0.0118, 0.0037, 0.0160, 0.0137, 0.0045, 0.0072,\n                      0.0070, 0.0091, 0.0099, 0.0174, 0.0093, 0.0127], dtype=torch.float64),\n                     zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n                     axis=0)),\n             ('mnet.features.16.conv.1.0.bias',\n              Parameter containing:\n              tensor([-8.6665e-02, -2.3348e-01, -4.5236e-02,  2.1194e-01, -9.9233e-02,\n                      -1.2162e-01, -2.5998e-01, -8.9892e-02,  6.1135e-01,  2.5740e-01,\n                       4.1811e-01,  2.6153e-01, -3.1627e-03, -2.5952e-01,  3.1972e-01,\n                       2.7780e-02,  9.5520e-01,  2.7103e-01, -7.7639e-02,  2.8397e-01,\n                       2.5257e-01, -4.7498e-02, -1.6348e-01, -8.7244e-03,  1.4669e-01,\n                       2.8629e-03,  3.1515e-01,  7.3545e-01, -1.6196e-01,  1.6821e-01,\n                      -6.0771e-02, -4.3728e-02, -2.1213e-02, -1.4751e-01, -4.4518e-02,\n                      -5.4111e-01, -3.6800e-01, -1.2051e-01, -9.5222e-04,  1.8160e-01,\n                      -1.2626e-01,  3.8619e-01, -3.4421e-02, -2.0767e-01, -8.3788e-02,\n                      -1.1704e-01, -1.7001e-01,  2.1573e-01,  2.1655e-01, -6.8619e-02,\n                       2.5200e-01,  2.1290e-01, -9.0069e-02,  2.7394e-01, -9.7964e-02,\n                      -4.6488e-03, -2.0305e-02,  2.5148e-01,  7.2707e-01, -4.5598e-02,\n                      -1.2055e-01, -3.6044e-02, -2.9126e-01, -1.1611e-01, -5.4474e-02,\n                      -5.5450e-02, -1.2149e-01,  1.7588e-02,  4.0178e-01,  2.3312e-01,\n                       3.0690e-01, -8.8873e-03,  2.3452e-01, -7.5441e-02, -1.9819e-01,\n                       2.3322e-01, -6.3526e-02,  6.6778e-03, -3.3518e-02, -6.3337e-02,\n                      -2.1148e-03, -1.1720e-01, -1.7670e-01, -9.1825e-02,  2.9135e-01,\n                      -2.3394e-01, -2.1265e-01, -9.7827e-02,  3.4256e-01, -6.2563e-02,\n                       3.3894e-01,  4.1483e-01, -2.8710e-01,  3.1078e-01,  3.8780e-01,\n                      -4.3475e-01, -7.8611e-02,  1.8141e-02, -1.0787e-01, -4.4605e-02,\n                       3.6115e-01, -3.2558e-02,  2.5529e-01,  3.9695e-01,  2.6607e-01,\n                       2.7526e-01,  3.7395e-01,  2.3655e-01, -1.1071e-02, -2.7429e-01,\n                      -2.6933e-02,  3.0207e-01, -1.0377e-01, -1.3991e-01,  9.4518e-01,\n                       4.2816e-01, -2.5779e-03,  1.0461e-01, -1.4492e-01, -7.4612e-02,\n                       2.2274e-01, -1.3963e-02,  1.0278e-01, -2.3314e-01,  3.7076e-02,\n                      -1.3033e-01,  2.8208e-01,  8.6066e-03, -4.9163e-02,  7.0133e-01,\n                       3.9033e-01, -4.3512e-01,  7.2408e-01, -2.3407e-02,  1.1072e-01,\n                       2.3113e-01,  5.2600e-03, -2.0031e-01,  2.6759e-01, -8.3030e-02,\n                      -1.2043e-01, -1.3121e-01,  2.8720e-01,  3.5157e-01,  2.4226e-01,\n                      -6.8133e-02,  8.3537e-01,  4.3083e-01,  6.7954e-01, -5.4743e-02,\n                      -3.7494e-03,  1.8928e-02, -5.9035e-03, -4.0565e-01, -7.7546e-02,\n                       2.3308e-01,  2.6033e-01,  2.3317e-01, -1.4231e-01, -1.1967e-01,\n                       2.5575e-01, -2.6173e-01,  2.8907e-01,  2.8794e-01, -7.7122e-02,\n                       2.8904e-01, -4.8781e-02, -2.7351e-01,  2.2765e-01, -3.7655e-02,\n                       2.6349e-01,  2.6541e-01, -1.7976e-01,  2.1966e-01, -4.7154e-02,\n                      -1.9041e-01, -1.2163e-02,  3.1226e-01,  2.3900e-01, -1.4575e-01,\n                      -4.0433e-02,  6.3308e-03,  3.1533e-01,  3.4751e-01,  2.5743e-01,\n                       2.7658e-01,  2.3787e-01,  9.8431e-01, -3.1442e-02,  2.2339e-01,\n                      -1.6324e-01,  2.6375e-01,  3.1500e-01,  2.5576e-01,  2.9216e-01,\n                      -5.9466e-02,  3.4842e-01, -3.5677e-02, -7.7550e-02,  3.3412e-01,\n                       1.0169e-02, -1.1217e-01, -1.0290e-01, -1.9376e-01,  1.4833e-02,\n                      -9.9706e-02, -3.6869e-01, -1.7171e-01, -9.1752e-02, -6.0243e-02,\n                      -2.3234e-01, -1.7170e-01,  2.4847e-01,  4.0245e-01, -6.2915e-02,\n                       3.4785e-01,  2.2722e-01, -4.3787e-02,  3.4062e-02, -3.0456e-02,\n                      -4.2618e-02, -1.8163e-02,  6.0799e-01,  2.3485e-01, -1.9218e-02,\n                       2.6597e-01,  1.9407e-01, -1.2037e-01,  5.2580e-01, -8.0967e-02,\n                       2.2072e-01, -5.9633e-02, -3.1093e-02,  1.7608e-01,  3.7363e-01,\n                       2.9536e-01,  2.3543e-01,  2.3418e-02, -4.8240e-01, -1.4233e-01,\n                      -1.2960e-01,  2.5526e-01,  4.2380e-01,  5.2965e-01,  7.3365e-01,\n                      -1.5450e-01, -3.5927e-02,  2.5677e-01,  2.7413e-01,  2.8731e-02,\n                       2.9539e-01, -8.5697e-02, -3.5974e-01,  2.5618e-01,  3.0575e-01,\n                       3.3370e-01, -2.8046e-01, -2.5825e-01,  7.6582e-01,  6.2377e-01,\n                      -1.4614e-01, -2.6756e-01, -9.4862e-02,  3.2095e-01,  4.7258e-01,\n                      -3.4583e-01,  2.7202e-01,  2.7746e-01,  2.2008e-01,  2.6918e-01,\n                       2.2895e-01,  2.0682e-01,  2.1897e-01, -1.0658e-01, -3.2782e-02,\n                       2.7274e-01,  1.1856e-01, -5.4601e-02, -2.3076e-02,  2.9984e-02,\n                       2.2081e-01, -8.0721e-02, -9.0503e-02, -5.3412e-02,  3.2442e-01,\n                      -3.8216e-02,  4.3768e-01,  2.4308e-01, -1.2411e-01,  4.0718e-01,\n                      -8.7282e-02,  6.9085e-01, -2.2847e-01, -7.9222e-02, -2.2773e-01,\n                       4.7781e-01,  2.9842e-01, -2.6234e-02,  2.6352e-01,  9.6375e-01,\n                       4.7269e-02,  2.3868e-01, -4.6400e-03,  3.5206e-01,  2.2847e-01,\n                       6.0633e-02,  8.2919e-03,  4.3160e-01, -3.4195e-02, -4.7987e-02,\n                       4.4709e-03,  7.1171e-01, -5.8441e-02, -6.6828e-02, -3.4413e-01,\n                       3.1992e-04, -1.4406e-01,  2.6637e-01,  2.3583e-01, -1.6182e-01,\n                       4.0649e-01,  4.2608e-01, -3.1287e-02,  2.4490e-01,  3.3582e-01,\n                      -2.3798e-02,  8.6898e-03, -2.9314e-01, -1.9182e-01, -1.4632e-01,\n                      -4.7974e-02, -1.4468e-01, -2.0369e-01,  3.0681e-01, -4.3162e-02,\n                      -1.3052e-01,  2.3550e-01,  2.5069e-01,  2.2148e-01, -9.4970e-02,\n                       6.8438e-02,  4.4675e-03, -1.0066e-01, -1.1573e-01, -1.4477e-01,\n                       2.4441e-01,  2.7959e-01, -1.8918e-01, -1.0403e-02, -1.7806e-01,\n                      -3.0481e-02, -1.9124e-01, -1.8293e-01,  2.4048e-01, -5.5616e-02,\n                       2.1404e-01, -1.5105e-02, -1.3080e-01,  2.0175e-01, -8.3602e-02,\n                       2.4273e-01, -1.4864e-01,  2.8987e-01,  1.4934e-01,  8.8628e-01,\n                       2.3280e-01,  4.4750e-01,  7.7012e-02, -2.6539e-01, -1.9414e-02,\n                      -2.4758e-02,  1.8441e-02,  2.7834e-01, -7.6945e-02, -4.0609e-01,\n                       6.2906e-02,  5.8783e-01, -1.3810e-02,  2.5944e-01, -1.0674e-01,\n                      -5.0192e-01, -1.3962e-01, -1.2894e-01,  2.2258e-01,  1.5102e-01,\n                       4.6786e-01,  3.1294e-01,  2.3243e-01,  8.4224e-02, -1.8315e-01,\n                      -1.4093e-01, -1.2851e-01, -1.5410e-01, -2.5791e-01,  2.8268e-02,\n                      -1.0379e-01, -3.5082e-02, -7.4306e-02,  3.1543e-01,  2.7819e-01,\n                       2.2449e-01,  2.0335e-01,  2.7722e-01, -8.4589e-02, -1.0305e-02,\n                       2.2620e-01,  1.0462e+00, -1.6951e-01, -5.9785e-02,  8.3007e-03,\n                       2.6564e-01, -5.5881e-02,  5.7852e-01, -4.1369e-02, -6.1882e-02,\n                      -1.0037e-01,  2.5032e-01,  3.7005e-01, -7.9835e-02, -4.4547e-02,\n                      -2.1135e-01, -2.5232e-02, -1.9470e-01, -5.1710e-02,  3.0079e-01,\n                      -2.4071e-02,  1.9774e-04,  1.0232e-02,  3.7194e-01, -9.2198e-02,\n                      -1.6324e-01,  9.3794e-01,  4.0408e-02, -4.7955e-02, -4.3696e-03,\n                       2.0797e-01,  4.3897e-01,  2.2182e-01, -1.1719e-01,  5.6002e-01,\n                      -8.1653e-01,  2.1096e-01, -1.0428e-01, -5.3238e-02,  2.3309e-01,\n                      -1.8251e-02, -5.6317e-02, -1.6303e-01, -9.5182e-02, -2.9761e-01,\n                       2.6136e-01, -2.9803e-02, -1.8806e-01, -8.4681e-03,  2.4219e-01,\n                      -2.8389e-02, -1.5195e-01,  1.5963e-02, -6.9707e-02, -2.1143e-01,\n                       2.3952e-01,  4.1618e-01,  8.5407e-01, -1.6630e-01,  7.7561e-02,\n                      -3.6734e-01,  1.4842e-02, -3.1646e-02, -1.1546e-01,  5.1715e-01,\n                       2.3961e-01,  2.6812e-01, -2.4617e-03,  1.2218e-01,  2.3996e-01,\n                      -3.2185e-01,  2.6639e-01,  2.3887e-01, -1.4113e-01, -5.5644e-02,\n                      -1.4069e-01,  3.7362e-01,  2.4481e-01, -3.2846e-01,  4.2480e-01,\n                       4.4498e-01,  2.3081e-01, -2.0028e-01,  3.5831e-01, -6.0647e-02,\n                      -3.6072e-02,  2.3057e-01, -3.9388e-02, -3.6965e-02,  2.5250e-01,\n                      -8.2703e-02,  2.9403e-01,  2.2680e-01, -3.6092e-01,  2.2299e-01,\n                      -8.2173e-03,  2.9769e-01,  3.2304e-01, -2.1979e-01, -6.6373e-02,\n                       2.9545e-01,  2.5832e-01,  2.1751e-01, -4.6902e-02, -1.4458e-01,\n                       2.3649e-01,  2.3276e-01,  9.3123e-01,  1.0010e-01,  3.3674e-01,\n                      -8.5008e-02, -4.1223e-01,  5.1555e-01, -1.6527e-01, -1.1728e-01,\n                       2.5812e-01, -2.7829e-01,  2.4251e-01,  3.5697e-01, -1.6007e-01,\n                      -1.7494e-01,  2.1617e-01, -1.0144e-01,  4.6540e-01,  2.5055e-01,\n                       2.7940e-01, -3.8387e-02,  3.7864e-01, -3.8107e-01,  1.2569e-03,\n                       7.7152e-02,  2.0872e-01, -2.0750e-01,  3.2103e-01, -3.8629e-02,\n                       3.4524e-01, -8.7526e-02,  2.7250e-02,  7.6183e-02, -1.4630e-01,\n                      -1.0280e-02,  2.8300e-01, -2.6682e-02,  9.3357e-03,  2.5342e-01,\n                      -6.0638e-02,  2.4753e-01, -4.4457e-01, -4.9557e-02, -2.5954e-01,\n                       2.8703e-01,  4.9500e-04,  2.3731e-01, -5.5925e-02, -1.5667e-01,\n                      -9.6649e-02, -1.2096e-01,  2.3067e-01, -8.4420e-02,  1.4648e-02,\n                       2.2737e-01,  2.1446e-01, -1.9205e-02,  2.2905e-01,  3.2699e-01,\n                       1.9375e-03, -7.7168e-02, -1.3162e-01, -9.6739e-02, -7.8778e-02,\n                      -1.9769e-01, -1.5857e-01, -6.5450e-02,  3.9673e-01, -3.6273e-01,\n                       2.4823e-01, -9.9641e-02, -4.1715e-02,  2.3797e-02, -6.9477e-02,\n                      -1.3171e-01, -2.3263e-01,  4.4107e-01,  2.2729e-01, -2.7071e-02,\n                      -2.0533e-01, -4.5225e-02,  3.0684e-02, -1.3727e-01,  4.1156e-01,\n                       1.3909e-02,  6.3655e-03, -4.1067e-02, -1.1301e-01,  8.2428e-01,\n                      -1.1247e-01,  2.2740e-01,  2.7454e-01, -7.5043e-02,  2.5653e-01,\n                       1.5445e-02, -9.4681e-02,  3.8261e-01, -2.2610e-01,  7.6429e-01,\n                       3.8854e-01,  2.5326e-01, -7.8410e-02, -2.5404e-01, -2.2482e-01,\n                      -5.5122e-02,  2.5549e-01,  2.5272e-01,  2.6504e-01, -3.2914e-01,\n                      -2.9026e-03, -2.4510e-01, -2.3621e-01, -1.2607e-01,  5.2853e-04,\n                       3.9366e-02,  4.6227e-03,  3.1384e-01, -2.5733e-02,  2.3397e-01,\n                      -1.8023e-02, -3.4785e-01, -2.1654e-01, -1.6577e-01, -4.5203e-02,\n                      -1.7707e-01,  7.1900e-03, -4.1879e-01,  1.1361e-01, -1.7014e-01,\n                       5.2838e-01,  7.5334e-02,  2.0783e-02, -8.3361e-02, -2.0798e-02,\n                       2.4378e-02, -7.4197e-05, -6.6309e-01, -5.9008e-02,  2.0165e-01,\n                      -2.3580e-01, -4.1925e-02, -1.2385e-01, -4.2975e-02, -1.3790e-02,\n                      -8.6999e-02,  3.7189e-01,  6.8597e-01, -1.6578e-01, -2.2449e-01,\n                      -7.9881e-02,  6.8338e-01, -1.5874e-01, -2.0445e-01,  2.9046e-01,\n                       2.2200e-01, -2.7137e-01, -1.3018e-01, -1.7005e-01,  2.3448e-01,\n                       5.0994e-01, -6.2718e-02,  2.4882e-01, -3.6685e-02, -1.1324e-01,\n                       4.4742e-01, -5.7038e-03, -3.8245e-01,  2.9643e-01, -1.7086e-01,\n                      -1.8184e-02,  2.9126e-01,  1.1089e-03,  9.1378e-03,  2.6796e-01,\n                      -4.6807e-02, -3.0444e-02,  3.7449e-01,  2.4405e-02,  2.9073e-02,\n                      -7.8016e-02,  5.8029e-01,  7.9069e-01,  3.3720e-01,  1.1074e-01,\n                       2.4777e-01, -7.0065e-02,  5.7383e-01, -7.2972e-02,  2.3146e-01,\n                       2.2769e-01, -1.4342e-01, -7.6077e-03,  2.4449e-01,  3.4951e-02,\n                       2.0503e-01, -1.6991e-02, -1.4950e-01,  1.8768e-01,  2.6861e-01,\n                       1.3674e-01,  2.4049e-01,  9.1810e-03,  4.3462e-01,  1.8260e-01,\n                       5.5473e-01,  3.9419e-01, -1.2495e-01, -2.1675e-01, -8.3109e-02,\n                      -7.0047e-02, -2.1650e-02,  2.0475e-01,  3.9113e-01,  2.5786e-01,\n                      -3.4283e-03, -2.6699e-01,  1.1060e-02, -2.1171e-01, -1.0303e-01,\n                      -2.1372e-01, -1.0787e-01,  2.2565e-01, -7.9399e-02, -8.7147e-02,\n                      -6.8702e-02, -4.8564e-02, -3.1263e-01, -1.6360e-01,  2.7582e-01,\n                      -1.6954e-01,  2.4242e-01,  9.6693e-02, -2.5524e-02, -1.5290e-01,\n                       2.5589e-02,  2.1958e-01, -1.4566e-01,  2.8642e-01, -6.8830e-02,\n                      -2.1915e-01, -2.7851e-01,  2.4930e-01,  2.9245e-01, -6.3215e-02,\n                      -7.0754e-02,  3.9634e-01, -1.4327e-02, -1.9341e-01,  3.0389e-02,\n                       3.0188e-01, -3.9336e-02,  2.2627e-01,  5.1866e-01,  1.3334e-04,\n                      -3.4353e-01, -2.9800e-02,  5.9210e-01,  7.6322e-01,  1.3379e-01,\n                       4.4818e-03,  3.0332e-01, -2.0429e-01, -7.4475e-02,  5.1584e-02,\n                      -3.1354e-01, -7.1529e-02, -1.3336e-01,  3.3590e-01, -7.2601e-02,\n                       8.0479e-01,  2.4546e-01, -2.7047e-02,  8.1885e-01, -2.1278e-02,\n                      -1.6045e-01,  1.5874e-02,  4.7152e-01,  3.5762e-01, -3.6662e-01,\n                       2.1512e-01,  3.0361e-01, -7.8312e-04,  2.3141e-01, -1.1365e-02,\n                       1.1031e-02, -1.6539e-01, -3.1574e-02, -5.9125e-02, -9.7528e-02,\n                      -1.2058e-01,  3.0224e-01,  2.4437e-01, -2.2065e-01, -1.1686e-01,\n                       8.6635e-01,  2.6535e-02,  2.3513e-01, -2.8931e-01, -1.8244e-02,\n                      -9.3594e-02, -2.2112e-01, -1.1124e-01,  2.1218e-01, -9.1236e-03,\n                       8.0177e-03,  1.4778e-03, -1.4441e-01,  2.6920e-01,  2.6324e-01,\n                       3.8985e-01, -2.5096e-01,  3.8762e-01, -7.2387e-02,  1.6667e-02,\n                       2.2629e-01, -2.3986e-04,  7.9829e-03, -2.1868e-01, -3.4982e-03,\n                       8.1039e-02,  3.0418e-01, -3.2052e-01, -1.8169e-01,  3.0047e-01,\n                       5.7384e-01, -1.5897e-01,  2.5785e-01,  2.3074e-01, -5.2038e-02,\n                      -2.2063e-01, -3.0706e-02,  2.6932e-01,  2.7167e-01,  9.9998e-02,\n                      -2.0099e-01, -1.6751e-01, -6.1525e-02,  2.1420e-01, -2.4044e-01,\n                       2.6350e-01,  5.5869e-01, -2.2739e-01,  5.9111e-01,  8.8928e-02,\n                       2.4117e-01, -1.5652e-01,  1.8475e-01, -1.4985e-02,  5.6295e-02,\n                      -9.4492e-02, -1.5293e-01,  2.2464e-01,  2.2884e-01, -3.2442e-02,\n                       2.3556e-01,  2.9601e-01, -1.3871e-01, -6.5600e-01, -2.3865e-01,\n                      -4.4613e-02, -2.9002e-01, -9.0996e-02, -6.6443e-02, -4.2385e-02,\n                       8.3259e-03, -1.3668e-01,  9.1258e-02,  2.8932e-01, -4.6493e-02,\n                       6.2465e-03,  2.3842e-01,  2.3581e-01, -1.6676e-01,  2.6753e-01,\n                       2.5000e-01,  2.6118e-01,  3.9748e-03,  6.3347e-03,  2.6694e-01,\n                       2.4567e-01, -1.9357e-01, -1.4940e-01, -7.5333e-02,  1.1460e-02,\n                      -7.7319e-02, -2.3605e-01,  7.2469e-02, -1.6549e-01,  2.7677e-01,\n                      -1.3564e-02,  3.5560e-01, -2.2787e-01,  2.6195e-01, -1.9059e-02,\n                      -3.7596e-02, -5.8823e-02,  3.9189e-01, -2.4552e-01,  5.7324e-01,\n                       2.7826e-01, -3.6410e-02, -2.0426e-02, -3.0305e-01, -2.1294e-02,\n                      -5.8689e-02, -1.0578e-01, -7.8204e-02, -2.6476e-01,  1.4079e-02,\n                      -2.0602e-01, -1.9175e-01, -7.5898e-02, -4.7804e-02,  3.3424e-01,\n                      -2.1661e-01,  1.4037e-03, -1.4187e-01,  2.5453e-01,  2.2866e-01,\n                      -3.5854e-01, -1.6252e-01,  1.1925e-01, -1.3630e-01,  2.3163e-01,\n                      -3.2454e-02, -1.8869e-01,  2.7209e-01,  2.0143e-01, -1.9558e-01,\n                       2.3681e-01,  1.0446e-02,  4.0973e-01, -7.2803e-02,  1.1134e-02,\n                      -1.9777e-02, -6.8567e-02,  2.7222e-01, -1.3720e-01,  2.9166e-01,\n                      -2.0531e-01, -2.1771e-02,  3.6258e-01, -8.2331e-02, -3.5844e-02,\n                       1.4309e-01,  5.8117e-02, -4.2910e-02,  2.3300e-01, -2.0871e-01],\n                     requires_grad=True)),\n             ('mnet.features.16.conv.1.0.scale', tensor(0.0777)),\n             ('mnet.features.16.conv.1.0.zero_point', tensor(50)),\n             ('mnet.features.16.conv.2.weight',\n              tensor([[[[ 0.0584]],\n              \n                       [[ 0.0738]],\n              \n                       [[-0.0062]],\n              \n                       ...,\n              \n                       [[-0.0031]],\n              \n                       [[ 0.0031]],\n              \n                       [[ 0.2738]]],\n              \n              \n                      [[[ 0.0809]],\n              \n                       [[ 0.0686]],\n              \n                       [[ 0.0466]],\n              \n                       ...,\n              \n                       [[-0.0686]],\n              \n                       [[-0.0172]],\n              \n                       [[ 0.0490]]],\n              \n              \n                      [[[ 0.0385]],\n              \n                       [[-0.0889]],\n              \n                       [[-0.0711]],\n              \n                       ...,\n              \n                       [[ 0.0267]],\n              \n                       [[ 0.0000]],\n              \n                       [[-0.0860]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-0.0071]],\n              \n                       [[ 0.0667]],\n              \n                       [[ 0.0310]],\n              \n                       ...,\n              \n                       [[-0.0882]],\n              \n                       [[ 0.0071]],\n              \n                       [[-0.1525]]],\n              \n              \n                      [[[-0.1384]],\n              \n                       [[-0.0319]],\n              \n                       [[ 0.0080]],\n              \n                       ...,\n              \n                       [[-0.0027]],\n              \n                       [[-0.0559]],\n              \n                       [[ 0.1464]]],\n              \n              \n                      [[[-0.0040]],\n              \n                       [[ 0.1272]],\n              \n                       [[-0.0626]],\n              \n                       ...,\n              \n                       [[ 0.0182]],\n              \n                       [[ 0.0061]],\n              \n                       [[ 0.2080]]]], size=(160, 960, 1, 1), dtype=torch.qint8,\n                     quantization_scheme=torch.per_channel_affine,\n                     scale=tensor([0.0031, 0.0025, 0.0030, 0.0026, 0.0025, 0.0027, 0.0026, 0.0032, 0.0026,\n                      0.0018, 0.0032, 0.0029, 0.0025, 0.0025, 0.0022, 0.0032, 0.0025, 0.0024,\n                      0.0028, 0.0026, 0.0029, 0.0028, 0.0029, 0.0023, 0.0025, 0.0030, 0.0033,\n                      0.0020, 0.0027, 0.0027, 0.0033, 0.0022, 0.0025, 0.0032, 0.0025, 0.0027,\n                      0.0026, 0.0029, 0.0023, 0.0025, 0.0026, 0.0030, 0.0025, 0.0033, 0.0020,\n                      0.0029, 0.0031, 0.0019, 0.0027, 0.0026, 0.0025, 0.0026, 0.0024, 0.0026,\n                      0.0025, 0.0038, 0.0023, 0.0024, 0.0031, 0.0026, 0.0028, 0.0035, 0.0018,\n                      0.0031, 0.0026, 0.0023, 0.0029, 0.0028, 0.0026, 0.0026, 0.0027, 0.0022,\n                      0.0023, 0.0022, 0.0028, 0.0024, 0.0025, 0.0022, 0.0024, 0.0032, 0.0027,\n                      0.0032, 0.0024, 0.0025, 0.0028, 0.0025, 0.0021, 0.0020, 0.0026, 0.0041,\n                      0.0034, 0.0021, 0.0024, 0.0028, 0.0021, 0.0020, 0.0028, 0.0024, 0.0029,\n                      0.0025, 0.0023, 0.0022, 0.0030, 0.0024, 0.0027, 0.0020, 0.0027, 0.0020,\n                      0.0028, 0.0024, 0.0019, 0.0025, 0.0025, 0.0023, 0.0025, 0.0025, 0.0023,\n                      0.0024, 0.0027, 0.0023, 0.0041, 0.0032, 0.0029, 0.0020, 0.0027, 0.0023,\n                      0.0027, 0.0028, 0.0024, 0.0023, 0.0024, 0.0030, 0.0025, 0.0021, 0.0046,\n                      0.0025, 0.0027, 0.0028, 0.0025, 0.0024, 0.0027, 0.0025, 0.0022, 0.0023,\n                      0.0025, 0.0022, 0.0025, 0.0026, 0.0028, 0.0021, 0.0020, 0.0029, 0.0023,\n                      0.0033, 0.0041, 0.0034, 0.0023, 0.0024, 0.0027, 0.0020],\n                     dtype=torch.float64),\n                     zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n                     axis=0)),\n             ('mnet.features.16.conv.2.bias',\n              Parameter containing:\n              tensor([-1.0036e-01, -1.4387e-01,  3.1871e-02,  7.4040e-01, -7.9527e-02,\n                      -7.8497e-02, -3.8163e-01,  4.3354e-01, -5.7921e-02, -1.6750e-02,\n                       9.3710e-02,  2.6371e-01,  3.4058e-01, -3.3131e-01, -2.4761e-01,\n                       1.3156e-02, -1.6170e-01, -3.9942e-02,  1.5999e-01, -2.4840e-01,\n                       9.9475e-02,  4.1044e-01, -5.1871e-01,  6.9838e-02, -2.0144e-01,\n                       1.9815e-01,  2.3415e-01, -2.5732e-01,  8.3254e-02, -3.2267e-01,\n                      -9.3023e-02,  1.6853e-01,  2.4025e-01, -1.5114e-01,  8.3975e-02,\n                       2.1094e-01, -2.8122e-01, -6.9558e-02,  3.5261e-01, -2.4701e-02,\n                      -1.8453e-01,  1.1188e-01,  4.5480e-01,  9.3345e-02,  1.2292e-01,\n                       3.0745e-01, -4.5908e-01,  1.7963e-01, -4.8490e-01, -2.6984e-01,\n                       1.8940e-01, -1.7624e-01,  2.4502e-01,  1.6378e-01, -1.0380e-01,\n                       3.7030e-01,  1.2139e-02, -1.9299e-01,  1.2783e-01,  9.4306e-02,\n                      -2.1687e-01, -2.6522e-01, -2.8443e-01,  6.6381e-02,  2.7178e-01,\n                      -6.6575e-02,  1.9575e-02, -1.1307e-01, -2.8523e-01, -8.2531e-02,\n                      -1.1005e-01,  1.0792e-01, -2.3180e-01,  3.7837e-01,  1.5560e-01,\n                       9.7763e-02,  2.9606e-01,  1.4368e-01, -1.1272e-01,  2.4891e-01,\n                       3.5619e-01,  1.2299e-01,  3.4670e-02, -2.9760e-01,  9.7394e-02,\n                      -4.8854e-01,  1.8388e-01, -2.1703e-01, -1.9166e-01, -1.6025e-01,\n                      -5.4068e-01,  2.7823e-02, -7.2668e-02,  7.0106e-02, -3.1138e-01,\n                      -2.1754e-01,  3.8803e-02, -2.6910e-01,  1.5843e-01,  1.8560e-01,\n                      -1.4273e-01,  4.2843e-01,  1.6997e-01,  1.5253e-01, -2.8463e-01,\n                      -1.2333e-01,  2.2702e-01, -2.5937e-01,  3.4513e-03, -8.5145e-02,\n                      -4.6346e-02,  3.6344e-01, -1.3251e-01,  1.1485e-01,  3.4461e-01,\n                       1.7554e-01, -5.7128e-02, -3.5281e-01, -5.5462e-04,  1.8170e-01,\n                       1.1467e-01,  1.0231e-02,  2.5500e-01, -7.4566e-02,  9.4698e-02,\n                       2.3060e-01, -6.1010e-02, -4.7405e-02,  1.0018e-01,  2.6840e-01,\n                       1.6035e-01,  2.1389e-01,  6.1952e-02, -1.1618e-01,  2.2777e-01,\n                      -3.8844e-03,  5.6280e-02, -1.8415e-02,  2.2974e-01,  5.2987e-02,\n                      -7.5868e-05,  1.6730e-01,  1.3156e-01,  2.3075e-02,  2.3369e-01,\n                       1.9800e-01,  1.4591e-01, -2.0984e-01,  8.2485e-01,  1.0040e-01,\n                       2.8688e-02, -3.2920e-01, -2.8927e-02,  1.8513e-01, -3.0360e-01,\n                      -1.2762e-01, -3.4754e-01, -4.9644e-02,  1.1086e-01, -6.9779e-02],\n                     requires_grad=True)),\n             ('mnet.features.16.conv.2.scale', tensor(0.2374)),\n             ('mnet.features.16.conv.2.zero_point', tensor(67)),\n             ('mnet.features.17.conv.0.0.weight',\n              tensor([[[[-0.0083]],\n              \n                       [[ 0.0036]],\n              \n                       [[ 0.0000]],\n              \n                       ...,\n              \n                       [[ 0.0144]],\n              \n                       [[-0.0138]],\n              \n                       [[-0.0006]]],\n              \n              \n                      [[[-0.0128]],\n              \n                       [[ 0.0018]],\n              \n                       [[ 0.0005]],\n              \n                       ...,\n              \n                       [[-0.0343]],\n              \n                       [[-0.0224]],\n              \n                       [[ 0.0018]]],\n              \n              \n                      [[[-0.0003]],\n              \n                       [[-0.0024]],\n              \n                       [[ 0.0049]],\n              \n                       ...,\n              \n                       [[ 0.0167]],\n              \n                       [[-0.0094]],\n              \n                       [[ 0.0082]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-0.0151]],\n              \n                       [[-0.0036]],\n              \n                       [[-0.0006]],\n              \n                       ...,\n              \n                       [[ 0.0043]],\n              \n                       [[ 0.0030]],\n              \n                       [[ 0.0040]]],\n              \n              \n                      [[[ 0.0021]],\n              \n                       [[-0.0119]],\n              \n                       [[ 0.0106]],\n              \n                       ...,\n              \n                       [[-0.0096]],\n              \n                       [[ 0.0130]],\n              \n                       [[-0.0251]]],\n              \n              \n                      [[[ 0.0199]],\n              \n                       [[-0.0065]],\n              \n                       [[-0.0005]],\n              \n                       ...,\n              \n                       [[ 0.0086]],\n              \n                       [[-0.0207]],\n              \n                       [[-0.0043]]]], size=(960, 160, 1, 1), dtype=torch.qint8,\n                     quantization_scheme=torch.per_channel_affine,\n                     scale=tensor([2.1243e-04, 4.5746e-04, 3.0404e-04, 2.7765e-04, 2.3514e-04, 3.0631e-04,\n                      2.8961e-04, 2.7938e-04, 3.1231e-04, 3.4679e-04, 4.7977e-04, 5.0024e-04,\n                      3.0550e-04, 2.4435e-04, 2.4925e-04, 2.8269e-04, 2.7481e-04, 3.2404e-04,\n                      3.1380e-04, 2.7817e-04, 2.6978e-04, 3.0583e-04, 2.8541e-04, 3.9742e-04,\n                      6.8364e-04, 3.3760e-04, 2.9768e-04, 2.6011e-04, 2.4481e-04, 2.7587e-04,\n                      1.9875e-04, 3.0405e-04, 5.8399e-04, 1.6896e-06, 3.0664e-04, 3.2653e-04,\n                      2.5766e-04, 3.1624e-04, 3.3160e-04, 6.6352e-04, 3.6184e-04, 2.3720e-04,\n                      2.6800e-04, 2.6772e-04, 2.5779e-04, 5.6114e-04, 3.0795e-04, 6.0397e-04,\n                      2.3522e-04, 2.7214e-04, 4.2160e-04, 3.1926e-04, 2.7582e-04, 2.7955e-04,\n                      2.9539e-04, 2.3128e-04, 3.5460e-04, 3.1919e-04, 3.0147e-04, 2.4434e-04,\n                      2.5419e-04, 2.7454e-04, 2.3950e-04, 2.8188e-04, 2.3378e-04, 2.8734e-04,\n                      2.6966e-04, 2.4953e-04, 2.9099e-04, 5.4369e-04, 4.4968e-04, 2.8773e-04,\n                      2.5282e-04, 2.4489e-04, 2.7193e-04, 2.2473e-04, 2.5745e-04, 2.1405e-04,\n                      2.9838e-04, 1.0280e-03, 2.8371e-04, 2.9559e-04, 2.8119e-04, 3.1624e-04,\n                      2.9908e-04, 3.1740e-04, 2.5357e-04, 5.9643e-04, 2.7416e-04, 2.1801e-04,\n                      2.4288e-04, 5.8872e-04, 4.3868e-04, 2.4376e-04, 7.5658e-04, 5.1077e-04,\n                      2.4386e-04, 2.7150e-04, 3.4058e-04, 2.7721e-04, 3.6025e-04, 3.0101e-04,\n                      2.5186e-04, 2.7349e-04, 5.1652e-04, 3.2285e-04, 4.7252e-04, 2.5561e-04,\n                      2.2960e-04, 5.9745e-04, 3.1380e-04, 5.4520e-04, 3.3384e-04, 3.1146e-04,\n                      2.5852e-04, 2.3449e-04, 2.3567e-04, 2.5451e-04, 2.9117e-04, 3.2896e-04,\n                      3.2373e-04, 5.3295e-04, 2.9930e-04, 3.1543e-04, 3.3999e-04, 4.5749e-04,\n                      2.9230e-04, 2.0676e-04, 2.6230e-04, 3.2105e-04, 2.7121e-04, 3.0390e-04,\n                      2.0008e-04, 2.4605e-04, 2.7032e-04, 3.2810e-04, 2.6227e-04, 3.0037e-04,\n                      3.7825e-04, 3.1380e-04, 1.9638e-04, 7.3922e-04, 2.6470e-04, 2.8186e-04,\n                      3.2964e-04, 2.6683e-04, 2.9170e-04, 2.5569e-04, 2.1899e-04, 3.1406e-04,\n                      3.2609e-04, 2.9829e-04, 2.6342e-04, 2.2298e-04, 2.5670e-04, 3.9209e-04,\n                      2.6930e-04, 2.8202e-04, 3.2988e-04, 2.9919e-04, 2.6351e-04, 2.6836e-04,\n                      3.0976e-04, 2.8345e-04, 3.1160e-04, 2.5843e-04, 3.6854e-04, 4.2359e-04,\n                      2.6724e-04, 2.2606e-04, 3.3774e-04, 3.1281e-04, 3.5909e-04, 2.5732e-04,\n                      5.2737e-04, 2.4847e-04, 4.1969e-04, 5.7732e-04, 2.4875e-04, 2.6955e-04,\n                      3.0535e-04, 2.3605e-04, 2.4656e-04, 3.0480e-04, 2.9921e-04, 2.8906e-04,\n                      3.0613e-04, 3.9833e-04, 2.4810e-04, 2.7627e-04, 2.7185e-04, 2.5406e-04,\n                      2.3438e-04, 2.2888e-04, 3.9170e-04, 2.6403e-04, 2.4316e-04, 2.5932e-04,\n                      2.5969e-04, 3.2463e-04, 3.9672e-04, 3.5084e-04, 5.5016e-04, 2.4154e-04,\n                      2.5229e-04, 2.8131e-04, 3.1558e-04, 3.2471e-04, 2.8907e-04, 2.5099e-04,\n                      2.6709e-04, 2.5884e-04, 2.9064e-04, 3.1090e-04, 2.4217e-04, 2.8878e-04,\n                      7.0415e-04, 2.5237e-04, 3.0703e-04, 3.2778e-04, 2.8778e-04, 2.6344e-04,\n                      2.6222e-04, 2.5771e-04, 2.2291e-04, 2.1132e-04, 3.4151e-04, 3.1315e-04,\n                      3.5316e-04, 2.7632e-04, 2.9499e-04, 3.3461e-04, 2.5669e-04, 4.9774e-04,\n                      2.3428e-04, 3.5444e-04, 2.9717e-04, 3.5498e-04, 3.0468e-04, 2.2827e-04,\n                      2.4599e-04, 3.0952e-04, 3.5260e-04, 3.5797e-04, 3.4208e-04, 2.7050e-04,\n                      2.7467e-04, 3.2993e-04, 2.8914e-04, 3.7119e-04, 4.7377e-04, 2.8666e-04,\n                      2.8704e-04, 2.4065e-04, 2.3437e-04, 2.7673e-04, 2.2852e-04, 3.1127e-04,\n                      3.0188e-04, 2.7715e-04, 3.7213e-04, 4.0507e-04, 2.7076e-04, 5.3120e-04,\n                      2.2232e-04, 2.5677e-04, 6.1378e-04, 2.5201e-04, 2.2885e-04, 3.2555e-04,\n                      2.8554e-04, 2.5960e-04, 2.5262e-04, 2.8064e-04, 2.7713e-04, 3.0422e-04,\n                      3.4735e-04, 4.8648e-04, 2.1975e-04, 3.7507e-04, 6.1039e-04, 2.5282e-04,\n                      2.2896e-04, 3.3142e-04, 2.1707e-04, 2.6554e-04, 2.7397e-04, 2.3636e-04,\n                      2.9771e-04, 2.9204e-04, 2.5308e-04, 3.3957e-04, 4.5734e-04, 3.1327e-04,\n                      3.5463e-04, 3.1369e-04, 4.8690e-04, 2.7593e-04, 5.7187e-04, 3.8136e-04,\n                      3.3635e-04, 3.7730e-04, 2.7088e-04, 2.7155e-04, 2.0932e-04, 3.8082e-04,\n                      6.3240e-04, 3.8421e-04, 3.2108e-04, 2.5273e-04, 2.1198e-04, 2.8315e-04,\n                      2.9925e-04, 4.1531e-04, 3.5754e-04, 2.3231e-04, 3.5218e-04, 2.7446e-04,\n                      2.6219e-04, 3.3342e-04, 2.6774e-04, 4.7382e-04, 3.5332e-04, 5.6758e-04,\n                      2.6936e-04, 8.5889e-04, 2.4640e-04, 3.0089e-04, 4.0109e-04, 2.9273e-04,\n                      2.4210e-04, 2.7747e-04, 2.6563e-04, 4.9563e-04, 5.0757e-04, 2.4285e-04,\n                      3.1341e-04, 2.9308e-04, 2.4181e-04, 2.3630e-04, 2.6446e-04, 5.0445e-04,\n                      2.9683e-04, 3.0588e-04, 2.6970e-04, 4.8242e-04, 3.0169e-04, 3.0511e-04,\n                      3.0421e-04, 2.8792e-04, 3.2641e-04, 2.7408e-04, 3.5459e-04, 3.0361e-04,\n                      5.5164e-04, 2.9136e-04, 3.4312e-04, 3.2374e-04, 2.9539e-04, 2.6810e-04,\n                      2.2327e-04, 2.5964e-04, 3.2723e-04, 2.9645e-04, 3.9471e-04, 4.4789e-04,\n                      3.4792e-04, 6.1769e-04, 2.7869e-04, 3.6328e-04, 3.4250e-04, 2.8246e-04,\n                      5.8769e-04, 2.9592e-04, 2.7572e-04, 2.6981e-04, 2.7059e-04, 6.1180e-04,\n                      3.1885e-04, 3.0295e-04, 3.2023e-04, 3.0192e-04, 2.5374e-04, 3.1045e-04,\n                      4.1478e-04, 2.3245e-04, 2.0510e-04, 3.4869e-04, 2.7922e-04, 2.7176e-04,\n                      1.8924e-04, 5.7436e-04, 4.3373e-04, 2.9409e-04, 3.6050e-04, 3.1024e-04,\n                      2.7048e-04, 2.5078e-04, 3.3355e-04, 2.8080e-04, 4.9924e-04, 4.8419e-04,\n                      3.3514e-04, 3.8367e-04, 5.3612e-04, 2.8569e-04, 7.7283e-04, 6.0868e-04,\n                      2.8303e-04, 2.2598e-04, 4.1767e-04, 2.6053e-04, 2.5787e-04, 3.4375e-04,\n                      3.0173e-04, 4.8847e-04, 3.2614e-04, 4.5094e-04, 3.0512e-04, 2.8704e-04,\n                      2.4235e-04, 6.1956e-04, 3.3587e-04, 3.3444e-04, 2.3731e-04, 2.4888e-04,\n                      2.2769e-04, 2.7301e-04, 2.9403e-04, 5.8663e-04, 2.5978e-04, 2.3161e-04,\n                      2.7920e-04, 2.1492e-04, 4.0349e-04, 2.9534e-04, 3.5965e-04, 3.1904e-04,\n                      2.6554e-04, 3.1395e-04, 3.6785e-04, 1.5150e-04, 2.1272e-04, 3.2967e-04,\n                      2.7685e-04, 2.8558e-04, 3.2128e-04, 3.1914e-04, 3.6154e-04, 3.7846e-04,\n                      3.5478e-04, 2.9028e-04, 6.6648e-04, 2.7916e-04, 2.6606e-04, 3.3477e-04,\n                      2.3936e-04, 2.5082e-04, 2.9681e-04, 2.1463e-04, 2.6051e-04, 2.9332e-04,\n                      4.6347e-04, 2.7517e-04, 2.3175e-04, 4.1992e-04, 2.9989e-04, 2.5264e-04,\n                      2.7884e-04, 2.8436e-04, 2.4464e-04, 4.0652e-04, 3.2500e-04, 3.0525e-04,\n                      2.8147e-04, 2.8206e-04, 2.4206e-04, 3.5227e-04, 4.2059e-04, 3.3192e-04,\n                      2.9900e-04, 3.1848e-04, 4.6895e-04, 4.2781e-04, 3.1953e-04, 3.1880e-04,\n                      3.0213e-04, 2.3657e-04, 3.5323e-04, 7.8001e-04, 2.5033e-04, 3.0852e-04,\n                      3.1364e-04, 7.2031e-04, 3.0822e-04, 2.5381e-04, 4.1171e-04, 5.5434e-04,\n                      2.5271e-04, 3.0880e-04, 2.5900e-04, 5.4700e-04, 3.0692e-04, 7.5308e-04,\n                      2.8600e-04, 3.0330e-04, 2.3454e-04, 5.6746e-04, 5.3418e-04, 4.3067e-04,\n                      2.9901e-04, 2.9915e-04, 2.8815e-04, 2.7610e-04, 3.3533e-04, 2.8986e-04,\n                      2.5075e-04, 2.0549e-04, 4.7267e-04, 2.3545e-04, 2.7237e-04, 2.3521e-04,\n                      2.5342e-04, 3.5140e-04, 4.9495e-04, 2.6662e-04, 2.6000e-04, 3.2159e-04,\n                      2.7664e-04, 2.8222e-04, 4.9868e-04, 2.1856e-04, 3.2850e-04, 2.5978e-04,\n                      2.6930e-04, 2.5246e-04, 2.7035e-04, 3.2195e-04, 2.6679e-04, 2.5513e-04,\n                      2.9437e-04, 2.1445e-04, 5.0034e-04, 2.3120e-04, 2.6201e-04, 5.2763e-04,\n                      3.5808e-04, 4.0329e-04, 3.3051e-04, 3.2456e-04, 6.7942e-04, 2.8161e-04,\n                      7.4626e-04, 2.6519e-04, 2.6087e-04, 3.6576e-04, 3.2684e-04, 2.3077e-04,\n                      6.0007e-04, 4.0440e-04, 3.3675e-04, 3.0724e-04, 4.9505e-04, 2.8301e-04,\n                      3.6859e-04, 2.1450e-04, 2.7115e-04, 3.0700e-04, 2.4033e-04, 2.4825e-04,\n                      3.2355e-04, 2.5968e-04, 2.4951e-04, 2.9597e-04, 2.3118e-04, 2.6171e-04,\n                      2.5392e-04, 2.9743e-04, 2.7959e-04, 3.8955e-04, 2.3754e-04, 3.4667e-04,\n                      2.5365e-04, 2.2134e-04, 2.6210e-04, 2.7978e-04, 3.1282e-04, 2.7070e-04,\n                      2.9407e-04, 2.6956e-04, 3.1840e-04, 2.3815e-04, 4.1685e-04, 2.9881e-04,\n                      3.3703e-04, 2.8313e-04, 3.6916e-04, 3.1159e-04, 5.7225e-04, 2.8205e-04,\n                      2.7473e-04, 3.2315e-04, 2.4201e-04, 2.6919e-04, 2.3957e-04, 5.5901e-04,\n                      2.9247e-04, 2.3298e-04, 2.9524e-04, 3.3214e-04, 2.6139e-04, 2.6680e-04,\n                      2.9251e-04, 2.7598e-04, 2.7215e-04, 3.9457e-04, 3.9946e-04, 2.8279e-04,\n                      2.5666e-04, 2.8675e-04, 2.8724e-04, 2.5277e-04, 2.6840e-04, 3.8862e-04,\n                      6.8397e-04, 2.6775e-04, 2.6366e-04, 3.1975e-04, 5.2402e-04, 7.5301e-04,\n                      3.3595e-04, 5.1427e-04, 3.0493e-04, 2.0719e-04, 2.8362e-04, 3.6701e-04,\n                      2.6306e-04, 4.3867e-04, 2.3705e-04, 2.6178e-04, 3.7698e-04, 4.6572e-04,\n                      3.5327e-04, 3.1596e-04, 2.0970e-04, 3.0222e-04, 2.9927e-04, 2.6689e-04,\n                      3.0733e-04, 2.9841e-04, 3.8803e-04, 2.5831e-04, 7.0096e-04, 2.5326e-04,\n                      2.6037e-04, 2.2242e-04, 2.6797e-04, 6.0627e-04, 2.2522e-04, 2.8409e-04,\n                      2.6831e-04, 2.8175e-04, 3.2494e-04, 4.9577e-04, 2.7762e-04, 3.6703e-04,\n                      2.3100e-04, 2.4083e-04, 2.7086e-04, 5.9979e-04, 5.0050e-04, 3.1515e-04,\n                      3.1225e-04, 2.6990e-04, 2.8615e-04, 2.9608e-04, 3.7806e-04, 4.1481e-04,\n                      2.2031e-04, 4.5867e-04, 9.7641e-05, 2.0090e-04, 2.9156e-04, 3.0094e-04,\n                      2.3510e-04, 4.9603e-04, 3.0164e-04, 2.0531e-04, 3.4070e-04, 2.5515e-04,\n                      5.1649e-04, 3.1935e-04, 2.5392e-04, 2.5796e-04, 3.2759e-04, 2.7120e-04,\n                      2.4457e-04, 1.9084e-04, 2.4947e-04, 3.9234e-04, 2.3951e-04, 3.2355e-04,\n                      2.9721e-04, 3.9623e-04, 2.6460e-04, 3.0404e-04, 2.2311e-04, 3.1532e-04,\n                      6.4545e-04, 3.4541e-04, 2.8772e-04, 2.9597e-04, 2.1319e-04, 8.2501e-04,\n                      2.6516e-04, 3.0587e-04, 2.9506e-04, 2.7371e-04, 3.4619e-04, 2.6529e-04,\n                      2.3846e-04, 4.9695e-04, 3.0224e-04, 2.5713e-04, 5.7841e-04, 2.8388e-04,\n                      1.1985e-03, 3.3552e-04, 2.4861e-04, 2.8287e-04, 3.0239e-04, 3.3762e-04,\n                      3.2401e-04, 4.6231e-04, 2.6965e-04, 7.9138e-04, 2.8472e-04, 4.3225e-04,\n                      3.5467e-04, 2.3599e-04, 3.0121e-04, 2.1399e-04, 2.5564e-04, 2.4211e-04,\n                      8.5824e-04, 2.3909e-04, 2.8852e-04, 2.9067e-04, 2.3503e-04, 2.2119e-04,\n                      2.8956e-04, 2.7409e-04, 2.9249e-04, 3.1639e-04, 2.6661e-04, 2.6619e-04,\n                      2.5318e-04, 2.7648e-04, 2.2211e-04, 3.4561e-04, 2.4939e-04, 3.9893e-04,\n                      3.2906e-04, 4.3454e-04, 2.3727e-04, 2.9143e-04, 2.5443e-04, 3.0878e-04,\n                      2.6642e-04, 5.4092e-04, 3.1583e-04, 3.2110e-04, 2.6322e-04, 5.1992e-04,\n                      2.2935e-04, 3.2516e-04, 2.4811e-04, 2.6531e-04, 3.5583e-04, 2.9804e-04,\n                      7.2982e-04, 3.2834e-04, 2.2397e-04, 6.5367e-04, 2.8178e-04, 4.5377e-04,\n                      2.8003e-04, 2.3988e-04, 2.7026e-04, 2.6852e-04, 3.0211e-04, 2.5824e-04,\n                      2.6269e-04, 3.3365e-04, 2.9001e-04, 3.6108e-04, 2.6123e-04, 2.4299e-04,\n                      2.5270e-04, 5.4658e-04, 2.7633e-04, 3.7961e-04, 6.5080e-04, 5.0817e-04,\n                      3.0035e-04, 2.8657e-04, 4.7644e-04, 5.7163e-04, 2.4690e-04, 4.4099e-04,\n                      2.6059e-04, 5.3745e-04, 2.6744e-04, 3.3433e-04, 2.9403e-04, 4.2447e-04,\n                      5.9515e-04, 2.9913e-04, 2.3871e-04, 3.5569e-04, 3.1850e-04, 3.7801e-04,\n                      2.5790e-04, 3.3708e-04, 2.5439e-04, 2.5689e-04, 3.1235e-04, 2.8960e-04,\n                      2.6396e-04, 3.3417e-04, 3.2090e-04, 6.7201e-04, 3.1777e-04, 5.6418e-04,\n                      6.0712e-04, 3.6211e-04, 2.6295e-04, 2.4851e-04, 2.9257e-04, 3.6093e-04,\n                      3.2217e-04, 2.8733e-04, 4.6894e-04, 2.8452e-04, 2.0686e-04, 2.8180e-04,\n                      2.1326e-04, 2.9424e-04, 3.9336e-04, 2.5616e-04, 7.1863e-04, 2.6530e-04,\n                      2.6744e-04, 2.4494e-04, 2.8223e-04, 3.8500e-04, 2.3797e-04, 2.2293e-04,\n                      2.6648e-04, 2.5225e-04, 2.7089e-04, 4.3662e-04, 4.0493e-04, 3.1348e-04,\n                      2.3625e-04, 4.7045e-04, 2.2467e-04, 4.8288e-04, 2.6542e-04, 2.4035e-04,\n                      3.9431e-04, 3.2026e-04, 2.6407e-04, 7.2162e-04, 5.8896e-04, 3.1929e-04,\n                      2.7331e-04, 3.8384e-04, 7.2913e-04, 2.8941e-04, 2.6917e-04, 4.0818e-04,\n                      5.2301e-04, 5.2143e-04, 2.4943e-04, 2.5915e-04, 2.6407e-04, 1.8394e-04,\n                      2.9151e-04, 2.4775e-04, 2.5557e-04, 2.2505e-04, 7.7932e-04, 2.6831e-04,\n                      2.6634e-04, 2.4472e-04, 5.0363e-04, 4.3237e-04, 2.7127e-04, 2.6822e-04,\n                      3.1306e-04, 2.8316e-04, 3.2806e-04, 3.0224e-04, 4.8970e-04, 2.4725e-04,\n                      5.3833e-04, 3.5034e-04, 2.9715e-04, 2.6604e-04, 4.0697e-04, 4.4893e-04,\n                      3.0170e-04, 5.5423e-04, 3.3577e-04, 3.3044e-04, 2.6542e-04, 3.2523e-04,\n                      6.2293e-04, 2.1753e-04, 6.0882e-04, 6.1545e-04, 2.6065e-04, 3.2713e-04,\n                      2.8017e-04, 2.7106e-04, 2.7711e-04, 2.7011e-04, 2.6903e-04, 2.6542e-04,\n                      6.0512e-04, 5.9084e-04, 3.5059e-04, 3.1059e-04, 2.7449e-04, 2.7995e-04,\n                      3.1661e-04, 7.2558e-04, 2.4370e-04, 2.6477e-04, 2.8397e-04, 7.2483e-04,\n                      3.0170e-04, 2.2965e-04, 4.3642e-04, 4.0104e-04, 2.8739e-04, 2.5864e-04,\n                      3.3543e-04, 2.4919e-04, 2.6439e-04, 2.5896e-04, 7.4247e-04, 3.0137e-04,\n                      4.2992e-04, 2.7230e-04, 2.2681e-04, 1.8881e-04, 2.5909e-04, 2.5186e-04],\n                     dtype=torch.float64),\n                     zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n                     axis=0)),\n             ('mnet.features.17.conv.0.0.bias',\n              Parameter containing:\n              tensor([-6.6301e-03,  3.6061e-02, -9.7154e-02, -1.2371e-01, -1.0071e-01,\n                      -1.2338e-01, -1.2666e-01, -7.5594e-02, -9.8105e-02, -1.0850e-01,\n                       4.1802e-02, -2.1277e-04, -1.1649e-01, -8.8319e-02, -6.7959e-02,\n                      -1.0089e-01, -1.2691e-01, -1.4272e-01, -1.1850e-01, -1.1750e-01,\n                      -1.1971e-01, -3.9153e-02, -9.2053e-02,  7.0240e-02,  3.4035e-02,\n                      -1.0935e-01, -8.4333e-02, -1.3203e-01, -4.4622e-02, -3.7139e-04,\n                      -9.2057e-02, -9.6396e-02,  5.0113e-02, -1.3961e-02, -4.1547e-03,\n                      -1.2782e-01, -9.4391e-02, -1.0619e-01,  2.4865e-02, -9.9587e-04,\n                      -1.0793e-01, -1.0097e-01, -4.6429e-02, -9.4039e-02, -3.9466e-02,\n                       3.4066e-02, -1.0273e-01,  9.8406e-02, -7.2951e-02, -1.1658e-01,\n                       1.2437e-03, -9.5940e-02, -9.4438e-02, -1.2263e-01, -7.8790e-03,\n                      -1.0665e-01, -3.6615e-02,  2.1873e-02, -5.0736e-02, -8.5943e-02,\n                      -4.0478e-02, -9.6011e-02, -5.3343e-02, -1.0647e-01, -7.1692e-02,\n                      -1.3871e-01, -3.4241e-02, -8.4192e-02, -4.6950e-02,  1.3441e-02,\n                      -1.8801e-02, -1.1045e-01, -1.0050e-01, -7.5941e-02, -1.3388e-01,\n                      -1.0530e-01, -1.0100e-01, -3.7190e-02, -1.2970e-01,  7.2345e-02,\n                       2.0322e-02, -1.0542e-01, -9.1997e-02, -9.7831e-02, -9.1423e-02,\n                      -1.0441e-01, -1.0972e-01,  1.1415e-02, -7.0958e-02, -5.1092e-02,\n                      -9.6340e-02, -1.3444e-02, -1.8296e-02, -8.1365e-02,  4.3659e-02,\n                       4.7528e-02, -1.0469e-01, -1.0989e-01, -2.7379e-02, -3.9721e-02,\n                      -1.6798e-01, -8.0130e-02, -1.0121e-01, -9.5465e-02,  6.3814e-02,\n                      -4.4669e-02,  7.3306e-03, -3.7604e-02, -1.1978e-01,  9.5975e-02,\n                      -1.0397e-01,  8.8338e-02, -1.0841e-01, -1.9040e-02, -8.3256e-02,\n                      -9.8815e-02, -1.1047e-01, -1.0369e-01, -6.8529e-02, -1.0373e-01,\n                      -4.2880e-04,  6.1088e-02, -3.6259e-02, -3.3778e-02, -9.1287e-02,\n                      -3.0672e-02, -1.1995e-01, -3.6570e-02, -1.1369e-01, -1.1620e-01,\n                      -1.0659e-01, -1.3965e-01, -8.8374e-02, -1.1696e-01, -1.4844e-02,\n                      -1.2378e-01, -1.1586e-01, -5.2009e-02, -2.0821e-01, -3.2863e-02,\n                      -3.0553e-02,  3.4045e-02, -9.0512e-02, -9.9738e-02, -6.7510e-02,\n                      -4.1531e-02, -2.5638e-02, -1.1536e-01, -4.3114e-02, -1.4297e-01,\n                      -7.8706e-02, -1.0039e-01, -9.7922e-02, -1.0827e-01, -1.4528e-01,\n                       3.2596e-02, -6.1854e-02, -1.0980e-01, -6.1594e-02, -1.1540e-01,\n                      -4.5938e-02, -2.0657e-02, -1.4214e-01, -1.3124e-01, -1.0009e-01,\n                      -2.6538e-02, -2.7264e-02, -4.5800e-02, -7.0415e-02, -1.0555e-01,\n                      -1.2368e-02, -5.8056e-02, -1.2602e-01, -5.0522e-02,  8.3605e-02,\n                      -6.6718e-02, -3.4329e-04,  8.8949e-02, -7.9821e-02, -7.9964e-02,\n                      -4.1903e-02, -1.0850e-01, -8.5529e-02, -1.1559e-01, -1.2307e-02,\n                      -8.1927e-02, -1.3322e-02, -1.0971e-01, -1.1191e-01, -2.7297e-02,\n                      -9.9294e-02, -7.7771e-02, -5.0448e-02, -8.5045e-02, -8.9831e-02,\n                      -9.5462e-02, -3.8215e-02, -3.7887e-02, -8.9494e-02, -1.2232e-01,\n                      -1.5558e-01, -1.3215e-01,  4.5552e-03, -8.3962e-02, -9.6814e-02,\n                      -1.2949e-01, -9.4650e-02, -9.8718e-02, -9.3512e-02, -1.0976e-01,\n                      -4.2119e-02, -1.0327e-01, -9.4904e-02, -1.0217e-01, -5.9388e-02,\n                      -1.2774e-01,  8.5893e-02, -1.0454e-01, -4.0291e-02, -4.0267e-03,\n                       2.2747e-02, -3.4484e-02, -4.4474e-03, -5.4433e-02, -2.5897e-02,\n                      -6.4093e-02,  4.9821e-02,  3.8897e-02, -3.2075e-02, -1.0092e-01,\n                      -1.2919e-01, -9.7303e-03, -1.7841e-02,  5.0532e-02, -8.7489e-02,\n                      -1.3017e-01, -9.3346e-02, -1.8084e-01, -1.3675e-01, -1.0924e-01,\n                      -1.0858e-01, -9.8447e-02, -9.4171e-02, -3.7865e-02, -3.6755e-03,\n                      -1.0409e-01, -1.2730e-01, -9.8225e-02, -8.2960e-02, -1.8830e-02,\n                       2.1714e-03, -8.2364e-02, -1.0786e-01, -8.7741e-02, -6.6499e-02,\n                      -8.8869e-02, -1.3002e-01, -1.3235e-01, -1.3321e-01, -1.2615e-01,\n                      -5.0563e-03,  5.1082e-03, -4.9282e-02,  5.0121e-02, -2.2753e-02,\n                      -1.1847e-01,  7.2065e-02, -1.2051e-01, -4.3633e-02, -1.1617e-01,\n                      -1.0615e-01, -1.0757e-01, -8.6946e-04, -1.1523e-01, -9.9048e-02,\n                      -8.9040e-02, -1.1516e-01, -4.2788e-02, -7.0241e-02, -2.7581e-03,\n                       4.6007e-02, -1.2182e-01, -9.4493e-02, -1.2137e-01, -7.8276e-02,\n                      -1.1217e-01, -6.1517e-02, -5.0721e-02, -9.1130e-02, -4.0037e-02,\n                      -9.6331e-02, -1.8567e-02,  6.1484e-02, -9.7711e-02, -3.1484e-02,\n                      -5.1237e-03,  6.2762e-02, -1.1000e-01,  9.3411e-02, -1.1402e-01,\n                       3.7411e-04, -2.4831e-02, -1.0838e-01, -2.7126e-02, -2.8964e-02,\n                      -1.3469e-01,  1.8914e-02, -1.3910e-01, -1.1114e-01, -1.0459e-01,\n                      -9.6106e-02, -7.7827e-02, -6.3937e-03, -3.7482e-02, -9.5194e-03,\n                      -1.1051e-01, -1.1686e-01, -9.6774e-02, -1.1039e-01, -4.8837e-02,\n                      -1.1571e-01,  3.3227e-02, -1.3801e-01, -8.1889e-03, -4.3011e-02,\n                      -3.8175e-03, -1.2215e-01, -3.1444e-02, -1.5603e-02, -6.7681e-02,\n                       8.1361e-03, -1.0329e-01, -8.6474e-02,  1.0850e-01, -4.8407e-03,\n                      -2.8399e-02, -1.1851e-01, -7.1825e-02, -1.0628e-01, -1.1483e-02,\n                      -9.1356e-02,  4.9941e-02, -4.5821e-02, -4.0165e-02, -1.1323e-01,\n                       5.8031e-02, -1.1025e-01, -9.9293e-02, -4.4017e-02, -1.0904e-01,\n                      -1.1640e-01, -3.0465e-02, -1.8803e-02, -1.0812e-01,  6.5759e-03,\n                      -1.2521e-01, -1.0444e-01, -2.9898e-02, -1.1061e-01, -1.0459e-01,\n                      -6.5275e-02, -1.3339e-01, -2.2113e-02, -4.4801e-02, -1.6117e-01,\n                      -3.8610e-02, -1.5637e-01,  1.3452e-01, -1.0379e-01,  3.5691e-02,\n                       2.0452e-01, -1.1212e-01,  7.5088e-02, -9.4110e-02, -9.0243e-02,\n                      -8.6855e-02, -9.1327e-02,  1.5848e-02, -1.1758e-02, -1.5168e-02,\n                      -1.3154e-01, -3.9398e-02, -1.2953e-01,  3.7066e-02,  4.3912e-02,\n                      -8.6284e-02, -8.3085e-02, -8.6882e-02, -6.4396e-02, -7.5770e-02,\n                      -7.5057e-02,  2.5319e-02, -1.8438e-01, -5.5248e-02, -1.1645e-01,\n                      -5.9441e-03, -1.1931e-01, -4.5291e-02, -9.6509e-02, -1.1100e-01,\n                       3.5479e-02,  2.7412e-02, -1.0818e-01, -2.2700e-01,  1.9193e-02,\n                      -1.0182e-01,  2.0004e-02, -1.1083e-02, -1.5441e-01, -9.7672e-02,\n                       1.9311e-02, -4.6211e-02, -9.5446e-02, -1.3556e-01, -1.1398e-01,\n                       6.1172e-02, -4.5863e-02,  1.7817e-03, -7.4123e-02, -8.7536e-02,\n                      -4.9734e-02,  2.1674e-02, -1.4180e-01, -1.0054e-01, -5.5061e-02,\n                      -2.4384e-02, -1.0620e-01, -1.2372e-01, -1.1999e-01,  5.1190e-02,\n                      -4.2145e-02, -3.4660e-02, -7.6609e-02, -8.3478e-02, -3.1825e-02,\n                      -8.4543e-02,  2.3410e-02, -1.1051e-01, -1.1648e-01,  4.8618e-02,\n                       9.4627e-02, -3.0848e-02, -7.3652e-02, -1.0154e-01,  7.3913e-03,\n                      -1.0434e-01,  6.1869e-02, -9.8492e-02, -1.0318e-01,  6.0744e-02,\n                      -1.0159e-01, -9.4141e-02, -4.2440e-03, -1.0820e-01, -3.0478e-02,\n                       1.7313e-03, -8.3293e-02, -2.5522e-02, -9.0662e-02, -8.9939e-02,\n                      -1.0218e-01,  8.2519e-03,  5.5818e-02, -1.3307e-01, -4.4308e-02,\n                       1.1731e-01, -1.2331e-01, -1.1336e-01, -9.5206e-02, -9.1163e-02,\n                      -8.2358e-02, -1.9426e-02, -5.0473e-02, -8.5913e-02, -9.6005e-02,\n                      -9.7071e-02, -3.2978e-02, -2.2927e-02, -1.5108e-01, -1.4639e-01,\n                      -9.9335e-02, -1.4448e-01,  3.3106e-02, -1.4714e-01, -4.4207e-02,\n                      -1.1417e-01, -9.9400e-02, -1.0658e-01, -8.0307e-02,  1.9040e-02,\n                      -7.5075e-02, -6.2984e-02, -2.5586e-02,  9.7914e-02, -2.9161e-02,\n                      -4.9136e-02,  1.0840e-01,  1.2058e-01, -5.6490e-02, -4.9746e-02,\n                      -1.0422e-01,  1.1405e-01, -1.5165e-01,  1.3741e-01, -1.9931e-02,\n                      -1.2686e-01, -1.0495e-01,  9.3928e-02,  3.2157e-02,  1.9540e-02,\n                      -1.3279e-01, -9.2391e-03, -1.2248e-01, -1.0066e-01, -1.7231e-01,\n                      -4.4227e-02, -1.2006e-01, -9.3570e-02,  7.7894e-02, -7.1483e-02,\n                      -5.7691e-02, -1.0815e-01, -1.0601e-01,  9.1297e-02,  1.3730e-02,\n                      -1.0988e-01, -1.0653e-01, -3.1231e-02, -7.3178e-02, -2.4864e-02,\n                       2.6994e-02, -4.4740e-02, -1.5022e-01, -3.3482e-02, -9.9646e-02,\n                      -4.4934e-02, -8.9076e-02, -9.4045e-02, -1.0604e-01, -1.1003e-02,\n                      -1.2086e-01, -8.2795e-02,  1.5196e-01, -6.0275e-02, -1.2731e-01,\n                       6.8649e-02,  7.5719e-02, -2.9213e-02, -2.2635e-02, -1.1745e-01,\n                       3.0609e-02, -9.5713e-02,  7.7407e-02, -7.6798e-02, -1.1036e-01,\n                      -9.3685e-02, -1.0758e-01, -1.2560e-01,  6.6309e-02,  6.4133e-02,\n                      -4.2176e-02, -1.0816e-01,  6.5276e-02, -1.0822e-01, -1.2394e-01,\n                      -6.5475e-03, -1.0495e-01, -1.1493e-01, -5.8485e-02, -9.2948e-02,\n                      -1.2297e-01, -1.0225e-01, -1.1450e-01, -1.4720e-01, -1.2691e-01,\n                      -1.0565e-01, -1.1740e-01, -2.2378e-02, -2.4529e-02, -8.0833e-02,\n                      -2.6413e-02, -1.0588e-01, -8.8648e-02, -6.0925e-02, -1.1463e-01,\n                      -2.2440e-02, -6.4753e-02, -9.9303e-02, -1.1559e-01, -1.2378e-01,\n                      -9.6598e-02, -1.0358e-01,  8.4680e-02, -7.5291e-02, -1.1826e-01,\n                      -2.5989e-02, -9.3693e-02, -1.0186e-01,  6.4431e-02, -1.0579e-01,\n                      -1.3022e-01, -1.2134e-01, -9.3592e-02, -1.0575e-01, -1.0742e-01,\n                      -1.1735e-02, -5.3796e-02, -1.0864e-01, -2.7438e-02, -1.2731e-01,\n                      -3.1403e-02, -9.9815e-02,  1.3551e-03, -1.2995e-01, -2.2773e-02,\n                       4.3656e-02,  7.5390e-03, -8.2431e-02, -1.0814e-01, -1.0468e-01,\n                      -1.0508e-01, -1.7324e-02, -7.8309e-02, -1.3224e-01,  1.3780e-02,\n                      -1.1007e-01, -1.0752e-01,  7.0629e-02,  8.0848e-02,  4.9118e-02,\n                      -1.3110e-01,  7.5056e-02, -1.2342e-01, -8.6450e-02, -6.3511e-02,\n                      -1.0834e-02, -9.3524e-02,  7.1462e-04, -6.5778e-02, -1.1794e-01,\n                      -3.9697e-03,  1.2242e-02, -1.8597e-01, -1.0221e-01, -1.0307e-01,\n                      -5.5118e-02, -9.9038e-02, -8.3607e-02, -1.2702e-01, -1.1323e-01,\n                      -1.1789e-01, -5.8136e-02,  6.3398e-02, -1.0246e-01, -7.8490e-02,\n                      -9.3524e-02, -9.3291e-02,  4.8403e-02, -1.0538e-01, -7.7960e-02,\n                      -4.4617e-02, -1.2719e-01, -1.3232e-01,  1.8644e-03,  4.0665e-03,\n                      -2.5048e-02, -5.8097e-02, -9.0713e-02, -1.2157e-01,  4.3030e-02,\n                       8.7588e-02, -1.5929e-01, -5.7578e-02, -1.2277e-01, -1.1453e-01,\n                      -1.1436e-01, -2.7455e-01,  9.0731e-02, -9.2147e-02,  2.6803e-02,\n                      -3.7518e-02, -5.2647e-02, -1.4192e-01, -3.3057e-02, -1.1756e-01,\n                      -1.3110e-02, -1.9392e-02, -3.4580e-03, -2.7912e-02, -7.1000e-02,\n                      -1.3311e-02, -1.0602e-02, -7.0998e-02, -1.1139e-01, -1.7364e-01,\n                       1.4116e-01, -1.0044e-01, -4.6674e-02, -1.0541e-01, -1.7318e-01,\n                      -1.0346e-01, -1.0926e-01, -6.6507e-02,  2.9411e-02, -1.0771e-01,\n                      -1.1413e-01, -9.3931e-02, -1.1144e-01,  1.1868e-01, -9.1615e-02,\n                      -1.1165e-01, -1.1252e-01, -4.1128e-02,  8.4541e-02, -1.4353e-01,\n                      -1.1938e-01, -1.1349e-01, -1.0328e-01, -1.4772e-01, -1.0986e-01,\n                      -4.9930e-02,  6.8731e-02, -2.0571e-02, -1.0519e-01,  5.1376e-03,\n                      -1.2555e-01,  1.4419e-01, -1.5987e-02, -1.2528e-01, -1.2351e-01,\n                      -1.9815e-02, -3.2965e-02, -9.2495e-02,  7.9837e-02, -9.3365e-02,\n                       2.6253e-02, -1.0467e-01, -2.1485e-01, -3.0228e-02, -1.0434e-01,\n                      -9.8790e-02, -5.4796e-02, -1.2124e-01, -1.0674e-01,  1.2919e-02,\n                      -8.5479e-02, -1.0795e-01, -8.7876e-02, -6.5614e-03, -8.6829e-02,\n                      -9.2790e-02, -1.0454e-01, -6.0298e-02, -1.6784e-01, -4.8192e-03,\n                      -1.2562e-01, -9.7606e-02, -1.0289e-01, -1.0758e-01, -1.3210e-01,\n                      -2.3683e-02, -1.1510e-01, -4.4845e-02, -1.9464e-01, -1.1192e-01,\n                      -1.3012e-01, -1.0411e-01, -1.0920e-01, -1.0722e-01,  1.1448e-01,\n                      -1.1213e-01, -1.4710e-01, -2.5573e-02,  3.3672e-02, -5.0915e-02,\n                      -1.2830e-01, -8.2944e-02, -1.2958e-01, -1.0256e-01, -1.0023e-01,\n                       7.4489e-02, -2.0930e-02, -1.0617e-01,  3.3938e-02, -9.3897e-02,\n                       8.2984e-02, -1.1386e-01, -1.1975e-01, -3.6115e-02, -5.1689e-02,\n                      -1.1106e-01, -9.4948e-02, -4.3388e-02, -4.4639e-02, -8.9739e-02,\n                      -8.6356e-03, -1.7494e-02, -5.5020e-02, -9.2112e-02,  1.0671e-03,\n                      -1.6065e-02, -1.0698e-01,  5.2879e-02, -9.3498e-05, -9.7217e-02,\n                      -1.0297e-01,  4.2389e-02,  8.4808e-02, -6.9130e-02,  3.1674e-02,\n                      -8.0579e-02,  8.8676e-02,  4.0828e-03, -9.9920e-02, -9.3411e-02,\n                       6.3169e-03, -3.4655e-02, -1.0733e-01, -1.0944e-01, -1.4983e-01,\n                      -1.1974e-01,  2.8928e-02, -5.9676e-02, -6.9608e-02, -4.4457e-02,\n                      -1.3269e-01, -1.3056e-01, -1.3408e-01, -7.5851e-02, -1.2670e-01,\n                      -1.3620e-01,  6.3729e-02, -1.0191e-01, -7.1987e-03,  3.4624e-02,\n                      -1.2615e-01, -1.0252e-01, -1.1060e-01, -9.2916e-02, -1.0103e-01,\n                      -1.1506e-01, -1.3440e-01,  1.9709e-02, -1.1705e-01, -1.0367e-01,\n                      -1.0979e-01, -6.3638e-02, -1.0649e-01,  1.2077e-01, -8.6476e-02,\n                       1.7827e-02, -9.5253e-02, -9.6379e-02, -1.0955e-01, -9.5027e-02,\n                      -3.4382e-02, -8.2295e-02, -1.0271e-01, -1.0855e-01, -5.1933e-02,\n                      -8.2005e-02,  8.6328e-02, -1.8454e-02, -7.8571e-02, -1.1628e-01,\n                      -4.0367e-03, -9.4291e-02,  2.7624e-02, -1.0301e-01, -3.8455e-02,\n                       1.2178e-01, -1.1132e-01, -1.3178e-01,  8.2527e-02,  1.2706e-02,\n                      -1.0314e-01, -1.1181e-01, -1.2256e-01,  2.7213e-02, -1.2967e-01,\n                      -6.6781e-02,  2.5804e-02,  2.6017e-02,  6.3458e-02, -9.6132e-02,\n                      -1.2099e-01, -3.6023e-02, -5.9528e-02, -3.7510e-02, -9.6131e-02,\n                      -9.9428e-02, -5.3544e-02,  9.0925e-03, -1.3015e-01, -1.1050e-01,\n                      -4.8076e-02,  1.0342e-03,  6.5348e-02, -1.1127e-01, -7.9953e-02,\n                      -1.0280e-01, -8.4378e-02, -9.3992e-02, -1.2445e-01,  1.1711e-01,\n                      -9.0213e-02,  1.4555e-01, -1.0144e-01, -9.2837e-02, -9.6344e-02,\n                      -5.6287e-04,  2.5613e-02, -8.7977e-02,  4.0081e-02, -9.6791e-02,\n                       1.9356e-02, -6.9166e-02, -4.2645e-02,  6.2007e-02, -4.9968e-02,\n                       4.1953e-02,  2.3904e-02, -1.0951e-01, -3.0905e-02, -8.6761e-02,\n                      -8.8224e-02, -3.1796e-02, -9.9159e-02, -9.2029e-02, -1.2755e-01,\n                       8.5037e-02,  3.0249e-02, -4.7201e-02, -1.0510e-01, -6.7361e-02,\n                      -1.1170e-01, -2.9673e-02,  4.0022e-02, -4.3964e-02, -9.7079e-02,\n                      -2.6705e-02,  1.0128e-01, -1.0909e-01, -1.1777e-01,  4.2895e-02,\n                      -5.8998e-02, -7.9376e-02, -8.2531e-02, -1.5748e-01, -1.0487e-01,\n                      -7.9887e-02, -1.0820e-01,  3.9447e-02, -1.1188e-01,  2.0030e-02,\n                      -1.1825e-01, -5.2651e-02, -1.4205e-02, -1.1718e-01, -9.8878e-02],\n                     requires_grad=True)),\n             ('mnet.features.17.conv.0.0.scale', tensor(0.0880)),\n             ('mnet.features.17.conv.0.0.zero_point', tensor(91)),\n             ('mnet.features.17.conv.1.0.weight',\n              tensor([[[[ 0.9664,  1.3393,  0.5595],\n                        [-0.0509,  0.7460, -0.0339],\n                        [-2.0344, -0.6781, -2.1531]]],\n              \n              \n                      [[[-0.4883, -0.3268, -0.4961],\n                        [-0.3150, -0.1614, -0.3268],\n                        [-0.4568, -0.3308, -0.5040]]],\n              \n              \n                      [[[ 1.2428,  0.9532,  1.4238],\n                        [ 0.9291,  0.6274,  1.0256],\n                        [ 1.3876,  1.0980,  1.5324]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-1.0024, -0.5740, -1.0624],\n                        [-0.6426, -0.2399, -0.6940],\n                        [-1.0710, -0.6083, -1.0881]]],\n              \n              \n                      [[[ 1.5303,  1.0285,  1.5679],\n                        [ 1.0662,  0.5770,  1.0662],\n                        [ 1.5553,  1.0536,  1.5930]]],\n              \n              \n                      [[[ 1.4931,  1.0576,  1.5802],\n                        [ 1.0327,  0.6470,  1.0825],\n                        [ 1.5304,  1.1074,  1.5553]]]], size=(960, 1, 3, 3),\n                     dtype=torch.qint8, quantization_scheme=torch.per_channel_affine,\n                     scale=tensor([0.0170, 0.0039, 0.0121, 0.0160, 0.0128, 0.0149, 0.0184, 0.0090, 0.0110,\n                      0.0121, 0.0039, 0.0030, 0.0129, 0.0127, 0.0100, 0.0119, 0.0180, 0.0164,\n                      0.0139, 0.0154, 0.0146, 0.0062, 0.0115, 0.0043, 0.0033, 0.0132, 0.0104,\n                      0.0144, 0.0083, 0.0072, 0.0129, 0.0097, 0.0027, 0.0003, 0.0047, 0.0156,\n                      0.0111, 0.0141, 0.0045, 0.0050, 0.0120, 0.0114, 0.0072, 0.0123, 0.0077,\n                      0.0041, 0.0121, 0.0033, 0.0098, 0.0143, 0.0054, 0.0118, 0.0120, 0.0148,\n                      0.0060, 0.0125, 0.0051, 0.0056, 0.0085, 0.0104, 0.0084, 0.0112, 0.0107,\n                      0.0144, 0.0115, 0.0183, 0.0067, 0.0113, 0.0068, 0.0031, 0.0035, 0.0120,\n                      0.0128, 0.0108, 0.0140, 0.0125, 0.0135, 0.0074, 0.0169, 0.0028, 0.0062,\n                      0.0117, 0.0116, 0.0128, 0.0108, 0.0111, 0.0133, 0.0034, 0.0120, 0.0088,\n                      0.0105, 0.0065, 0.0049, 0.0134, 0.0034, 0.0023, 0.0129, 0.0133, 0.0065,\n                      0.0088, 0.0239, 0.0102, 0.0120, 0.0105, 0.0030, 0.0073, 0.0048, 0.0087,\n                      0.0157, 0.0032, 0.0128, 0.0043, 0.0137, 0.0055, 0.0131, 0.0130, 0.0130,\n                      0.0133, 0.0090, 0.0140, 0.0057, 0.0032, 0.0095, 0.0057, 0.0110, 0.0049,\n                      0.0146, 0.0092, 0.0141, 0.0117, 0.0114, 0.0152, 0.0119, 0.0131, 0.0056,\n                      0.0148, 0.0122, 0.0087, 0.0227, 0.0078, 0.0098, 0.0035, 0.0126, 0.0135,\n                      0.0083, 0.0088, 0.0073, 0.0127, 0.0080, 0.0160, 0.0105, 0.0125, 0.0118,\n                      0.0127, 0.0178, 0.0041, 0.0111, 0.0148, 0.0083, 0.0115, 0.0062, 0.0100,\n                      0.0175, 0.0145, 0.0126, 0.0056, 0.0071, 0.0055, 0.0105, 0.0135, 0.0080,\n                      0.0086, 0.0155, 0.0075, 0.0035, 0.0087, 0.0068, 0.0032, 0.0117, 0.0101,\n                      0.0089, 0.0128, 0.0113, 0.0158, 0.0067, 0.0099, 0.0066, 0.0134, 0.0148,\n                      0.0063, 0.0112, 0.0103, 0.0093, 0.0104, 0.0098, 0.0119, 0.0064, 0.0063,\n                      0.0130, 0.0143, 0.0157, 0.0143, 0.0114, 0.0126, 0.0116, 0.0161, 0.0122,\n                      0.0150, 0.0104, 0.0146, 0.0076, 0.0131, 0.0108, 0.0129, 0.0091, 0.0159,\n                      0.0033, 0.0133, 0.0070, 0.0055, 0.0038, 0.0066, 0.0071, 0.0096, 0.0097,\n                      0.0102, 0.0051, 0.0044, 0.0055, 0.0122, 0.0193, 0.0063, 0.0074, 0.0034,\n                      0.0113, 0.0160, 0.0132, 0.0245, 0.0196, 0.0143, 0.0142, 0.0117, 0.0108,\n                      0.0066, 0.0057, 0.0121, 0.0183, 0.0123, 0.0109, 0.0059, 0.0057, 0.0116,\n                      0.0144, 0.0119, 0.0105, 0.0129, 0.0159, 0.0157, 0.0157, 0.0159, 0.0065,\n                      0.0038, 0.0068, 0.0028, 0.0064, 0.0145, 0.0030, 0.0167, 0.0087, 0.0134,\n                      0.0132, 0.0128, 0.0070, 0.0136, 0.0136, 0.0113, 0.0164, 0.0063, 0.0094,\n                      0.0054, 0.0035, 0.0134, 0.0122, 0.0166, 0.0117, 0.0132, 0.0104, 0.0076,\n                      0.0112, 0.0066, 0.0124, 0.0062, 0.0036, 0.0132, 0.0063, 0.0050, 0.0030,\n                      0.0145, 0.0033, 0.0136, 0.0062, 0.0071, 0.0137, 0.0090, 0.0100, 0.0139,\n                      0.0039, 0.0164, 0.0126, 0.0126, 0.0125, 0.0096, 0.0058, 0.0051, 0.0079,\n                      0.0146, 0.0143, 0.0124, 0.0142, 0.0074, 0.0145, 0.0037, 0.0199, 0.0054,\n                      0.0073, 0.0034, 0.0148, 0.0065, 0.0164, 0.0083, 0.0078, 0.0134, 0.0107,\n                      0.0033, 0.0039, 0.0077, 0.0129, 0.0098, 0.0143, 0.0091, 0.0117, 0.0035,\n                      0.0094, 0.0071, 0.0142, 0.0034, 0.0128, 0.0109, 0.0076, 0.0148, 0.0133,\n                      0.0073, 0.0067, 0.0130, 0.0056, 0.0159, 0.0126, 0.0060, 0.0139, 0.0137,\n                      0.0095, 0.0193, 0.0054, 0.0066, 0.0190, 0.0055, 0.0186, 0.0018, 0.0130,\n                      0.0039, 0.0142, 0.0144, 0.0035, 0.0098, 0.0109, 0.0121, 0.0118, 0.0042,\n                      0.0060, 0.0049, 0.0167, 0.0074, 0.0135, 0.0057, 0.0034, 0.0119, 0.0120,\n                      0.0122, 0.0093, 0.0103, 0.0120, 0.0037, 0.0226, 0.0095, 0.0132, 0.0039,\n                      0.0140, 0.0080, 0.0129, 0.0129, 0.0039, 0.0041, 0.0122, 0.0242, 0.0033,\n                      0.0116, 0.0032, 0.0039, 0.0218, 0.0127, 0.0028, 0.0059, 0.0120, 0.0158,\n                      0.0141, 0.0032, 0.0065, 0.0046, 0.0083, 0.0116, 0.0085, 0.0031, 0.0173,\n                      0.0133, 0.0087, 0.0083, 0.0131, 0.0151, 0.0144, 0.0043, 0.0075, 0.0077,\n                      0.0091, 0.0123, 0.0055, 0.0118, 0.0031, 0.0130, 0.0140, 0.0033, 0.0040,\n                      0.0117, 0.0119, 0.0129, 0.0070, 0.0122, 0.0047, 0.0125, 0.0129, 0.0035,\n                      0.0120, 0.0110, 0.0038, 0.0137, 0.0057, 0.0071, 0.0112, 0.0104, 0.0096,\n                      0.0133, 0.0135, 0.0075, 0.0035, 0.0159, 0.0074, 0.0033, 0.0140, 0.0145,\n                      0.0121, 0.0105, 0.0104, 0.0044, 0.0068, 0.0105, 0.0141, 0.0130, 0.0097,\n                      0.0067, 0.0178, 0.0159, 0.0124, 0.0195, 0.0032, 0.0187, 0.0057, 0.0138,\n                      0.0132, 0.0147, 0.0112, 0.0032, 0.0108, 0.0081, 0.0069, 0.0028, 0.0081,\n                      0.0077, 0.0031, 0.0043, 0.0090, 0.0091, 0.0140, 0.0030, 0.0175, 0.0032,\n                      0.0079, 0.0133, 0.0121, 0.0036, 0.0035, 0.0031, 0.0149, 0.0047, 0.0133,\n                      0.0122, 0.0257, 0.0085, 0.0145, 0.0121, 0.0036, 0.0102, 0.0086, 0.0147,\n                      0.0119, 0.0035, 0.0044, 0.0127, 0.0119, 0.0089, 0.0103, 0.0065, 0.0040,\n                      0.0094, 0.0194, 0.0081, 0.0131, 0.0076, 0.0117, 0.0118, 0.0137, 0.0075,\n                      0.0150, 0.0102, 0.0029, 0.0101, 0.0145, 0.0040, 0.0037, 0.0055, 0.0072,\n                      0.0136, 0.0040, 0.0145, 0.0033, 0.0105, 0.0124, 0.0119, 0.0139, 0.0159,\n                      0.0037, 0.0040, 0.0061, 0.0128, 0.0031, 0.0145, 0.0155, 0.0067, 0.0139,\n                      0.0138, 0.0114, 0.0130, 0.0148, 0.0127, 0.0139, 0.0158, 0.0153, 0.0127,\n                      0.0134, 0.0061, 0.0066, 0.0084, 0.0057, 0.0127, 0.0118, 0.0093, 0.0137,\n                      0.0074, 0.0074, 0.0138, 0.0151, 0.0134, 0.0104, 0.0130, 0.0036, 0.0111,\n                      0.0159, 0.0062, 0.0102, 0.0127, 0.0036, 0.0144, 0.0149, 0.0143, 0.0132,\n                      0.0138, 0.0128, 0.0027, 0.0080, 0.0128, 0.0092, 0.0168, 0.0084, 0.0126,\n                      0.0080, 0.0162, 0.0063, 0.0037, 0.0059, 0.0088, 0.0138, 0.0124, 0.0135,\n                      0.0059, 0.0105, 0.0130, 0.0035, 0.0140, 0.0128, 0.0039, 0.0025, 0.0028,\n                      0.0152, 0.0024, 0.0148, 0.0110, 0.0104, 0.0053, 0.0117, 0.0039, 0.0099,\n                      0.0136, 0.0061, 0.0041, 0.0228, 0.0120, 0.0132, 0.0092, 0.0142, 0.0102,\n                      0.0154, 0.0131, 0.0131, 0.0081, 0.0037, 0.0127, 0.0095, 0.0134, 0.0132,\n                      0.0037, 0.0118, 0.0104, 0.0081, 0.0148, 0.0168, 0.0042, 0.0073, 0.0056,\n                      0.0096, 0.0113, 0.0173, 0.0034, 0.0042, 0.0183, 0.0083, 0.0162, 0.0132,\n                      0.0128, 0.0395, 0.0034, 0.0130, 0.0032, 0.0541, 0.0111, 0.0152, 0.0059,\n                      0.0142, 0.0043, 0.0062, 0.0158, 0.0061, 0.0103, 0.0059, 0.0067, 0.0102,\n                      0.0136, 0.0251, 0.0067, 0.0129, 0.0113, 0.0116, 0.0194, 0.0135, 0.0131,\n                      0.0081, 0.0039, 0.0147, 0.0146, 0.0120, 0.0157, 0.0034, 0.0105, 0.0121,\n                      0.0133, 0.0082, 0.0029, 0.0177, 0.0154, 0.0129, 0.0130, 0.0178, 0.0132,\n                      0.0082, 0.0028, 0.0076, 0.0142, 0.0031, 0.0158, 0.0021, 0.0075, 0.0175,\n                      0.0149, 0.0064, 0.0104, 0.0120, 0.0040, 0.0121, 0.0027, 0.0137, 0.0244,\n                      0.0087, 0.0124, 0.0133, 0.0107, 0.0161, 0.0139, 0.0414, 0.0110, 0.0140,\n                      0.0122, 0.0082, 0.0117, 0.0105, 0.0118, 0.0085, 0.0184, 0.0071, 0.0151,\n                      0.0133, 0.0128, 0.0149, 0.0144, 0.0114, 0.0130, 0.0077, 0.0211, 0.0159,\n                      0.0144, 0.0145, 0.0118, 0.0126, 0.0028, 0.0130, 0.0175, 0.0063, 0.0030,\n                      0.0095, 0.0162, 0.0109, 0.0163, 0.0123, 0.0114, 0.0034, 0.0067, 0.0120,\n                      0.0028, 0.0115, 0.0035, 0.0130, 0.0132, 0.0084, 0.0077, 0.0133, 0.0113,\n                      0.0079, 0.0064, 0.0111, 0.0070, 0.0053, 0.0097, 0.0120, 0.0038, 0.0074,\n                      0.0120, 0.0033, 0.0064, 0.0120, 0.0123, 0.0038, 0.0031, 0.0087, 0.0039,\n                      0.0119, 0.0033, 0.0071, 0.0151, 0.0124, 0.0045, 0.0049, 0.0142, 0.0146,\n                      0.0161, 0.0142, 0.0050, 0.0109, 0.0089, 0.0070, 0.0167, 0.0164, 0.0154,\n                      0.0109, 0.0147, 0.0156, 0.0020, 0.0122, 0.0035, 0.0040, 0.0135, 0.0128,\n                      0.0168, 0.0125, 0.0122, 0.0146, 0.0146, 0.0037, 0.0129, 0.0145, 0.0125,\n                      0.0085, 0.0134, 0.0032, 0.0123, 0.0027, 0.0134, 0.0130, 0.0147, 0.0121,\n                      0.0080, 0.0106, 0.0132, 0.0147, 0.0081, 0.0096, 0.0034, 0.0040, 0.0102,\n                      0.0148, 0.0034, 0.0134, 0.0031, 0.0127, 0.0070, 0.0023, 0.0136, 0.0145,\n                      0.0034, 0.0028, 0.0130, 0.0140, 0.0159, 0.0025, 0.0149, 0.0097, 0.0035,\n                      0.0035, 0.0029, 0.0115, 0.0152, 0.0076, 0.0733, 0.0058, 0.0120, 0.0131,\n                      0.0103, 0.0036, 0.0188, 0.0134, 0.0077, 0.0042, 0.0045, 0.0135, 0.0111,\n                      0.0116, 0.0106, 0.0107, 0.0148, 0.0032, 0.0141, 0.0030, 0.0133, 0.0108,\n                      0.0118, 0.0044, 0.0049, 0.0114, 0.0042, 0.0111, 0.0056, 0.0103, 0.0072,\n                      0.0044, 0.0086, 0.0033, 0.0033, 0.0127, 0.0058, 0.0103, 0.0113, 0.0057,\n                      0.0118, 0.0118, 0.0149, 0.0033, 0.0031, 0.0078, 0.0119, 0.0105, 0.0128,\n                      0.0081, 0.0041, 0.0083, 0.0113, 0.0071, 0.0040, 0.0131, 0.0143, 0.0045,\n                      0.0075, 0.0107, 0.0101, 0.0212, 0.0133, 0.0119, 0.0124, 0.0036, 0.0133,\n                      0.0040, 0.0149, 0.0087, 0.0086, 0.0125, 0.0124], dtype=torch.float64),\n                     zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n                     axis=0)),\n             ('mnet.features.17.conv.1.0.bias',\n              Parameter containing:\n              tensor([ 0.2781,  0.2410,  0.0497,  0.0470,  0.0466,  0.0561,  0.0379,  0.0265,\n                       0.0422,  0.0413,  0.2639,  0.2652,  0.0408,  0.0436,  0.0243,  0.0422,\n                       0.0516,  0.0474,  0.0498,  0.0467,  0.0505,  0.2900,  0.0327,  0.3447,\n                       0.2141,  0.0483,  0.0348,  0.0473,  0.3651,  0.2319,  0.0450,  0.0315,\n                       0.3721, -0.0076,  0.2003,  0.0526,  0.0452,  0.0557,  0.1914,  0.2116,\n                       0.0489,  0.0526,  0.3046,  0.0471,  0.2839,  0.1968,  0.0442,  0.2762,\n                       0.2104,  0.0471,  0.2061,  0.0389,  0.0505,  0.0480,  0.2069,  0.0444,\n                       0.2992,  0.2348,  0.0158,  0.0391,  0.2176,  0.0428,  0.2552,  0.0427,\n                       0.0411,  0.0489,  0.3844,  0.0488,  0.3033,  0.3440,  0.3373,  0.0518,\n                       0.0436,  0.0441,  0.0463,  0.0398,  0.0523,  0.3777,  0.0494,  0.2419,\n                       0.3694,  0.0439,  0.0506,  0.0516,  0.0354,  0.0395,  0.0488,  0.2110,\n                       0.2056,  0.4720,  0.0391,  0.1907,  0.4188,  0.0410,  0.2242,  0.2927,\n                       0.0488,  0.0525,  0.1946,  0.2069,  0.0425,  0.0337,  0.0522,  0.0488,\n                       0.2272,  0.3414,  0.2331,  0.2368,  0.0512,  0.2928,  0.0495,  0.2983,\n                       0.0434,  0.3113,  0.0513,  0.0431,  0.0601,  0.0520,  0.0348,  0.0424,\n                       0.1880,  0.2491,  0.1817,  0.3358,  0.0457,  0.2560,  0.0466,  0.3300,\n                       0.0436,  0.0510,  0.0531,  0.0506,  0.0512,  0.0406,  0.2966,  0.0419,\n                       0.0503,  0.1946,  0.0430,  0.1940,  0.1836,  0.2346,  0.0454,  0.0540,\n                       0.3972,  0.1721,  0.1901,  0.0443,  0.2491,  0.0498,  0.0507,  0.0412,\n                       0.0444,  0.0486,  0.0456,  0.2186,  0.1732,  0.0495,  0.4228,  0.0519,\n                       0.3619,  0.1798,  0.0402,  0.0540,  0.0466,  0.2143,  0.2008,  0.2688,\n                       0.0392,  0.0480,  0.1996,  0.4114,  0.0470,  0.2392,  0.2863,  0.4679,\n                       0.2270,  0.3072,  0.0409,  0.0354,  0.1572,  0.0449,  0.0458,  0.0452,\n                       0.1795,  0.0336,  0.2594,  0.0534,  0.0484,  0.2425,  0.0492,  0.0271,\n                       0.3627,  0.0460,  0.0397,  0.0500,  0.4518,  0.2595,  0.0510,  0.0517,\n                       0.0467,  0.0416,  0.1787,  0.0453,  0.0501,  0.0512,  0.0465,  0.0482,\n                       0.0454,  0.0509,  0.3403,  0.0471,  0.0433,  0.0469,  0.3458,  0.0468,\n                       0.2717,  0.0452,  0.2525,  0.2418,  0.2369,  0.4105,  0.2065,  0.1979,\n                       0.1954,  0.0323,  0.2836,  0.2455,  0.3337,  0.0534,  0.0474,  0.2044,\n                       0.2489,  0.2440,  0.0481,  0.0529,  0.0461,  0.0536,  0.0488,  0.0448,\n                       0.0464,  0.0428,  0.0488,  0.2018,  0.2389,  0.0467,  0.0538,  0.0461,\n                       0.0406,  0.2041,  0.2152,  0.0486,  0.0536,  0.0520,  0.0300,  0.0533,\n                       0.0425,  0.0505,  0.0441,  0.0428,  0.2016,  0.3052,  0.4604,  0.2274,\n                       0.2718,  0.0502,  0.2468,  0.0539,  0.2963,  0.0415,  0.0426,  0.0424,\n                       0.2026,  0.0449,  0.0572,  0.0406,  0.0468,  0.3080,  0.2235,  0.1807,\n                       0.2314,  0.0519,  0.0488,  0.0532,  0.0488,  0.0454,  0.0394,  0.4377,\n                       0.0450,  0.2430,  0.0483,  0.2314,  0.2317,  0.0452,  0.2061,  0.2459,\n                       0.2082,  0.0556,  0.2658,  0.0507,  0.2339,  0.1997,  0.0481,  0.2264,\n                       0.1787,  0.0383,  0.2147,  0.0421,  0.0500,  0.0500,  0.0500,  0.0301,\n                       0.2214,  0.3590,  0.2107,  0.0433,  0.0427,  0.0464,  0.0468,  0.2583,\n                       0.0530,  0.2332,  0.0516,  0.2361,  0.4407,  0.2400,  0.0435,  0.3454,\n                       0.0196,  0.3632,  0.2130,  0.0519,  0.0320,  0.2925,  0.1847,  0.2018,\n                       0.0522,  0.0234,  0.0500,  0.2064,  0.0363,  0.2688,  0.2003,  0.1966,\n                       0.0415,  0.2261,  0.0482,  0.0483,  0.3335,  0.0508,  0.0423,  0.2234,\n                       0.1971,  0.0524,  0.2180,  0.0501,  0.0476,  0.3092,  0.0577,  0.0444,\n                       0.3828,  0.0479,  0.3967,  0.3601,  0.0385,  0.2862,  0.0450,  0.2678,\n                       0.0596,  0.2346, -0.1947,  0.0480,  0.3592,  0.0422,  0.0373,  0.0437,\n                       0.0441,  0.2241,  0.2095,  0.2755,  0.0491,  0.3380,  0.0495,  0.3066,\n                       0.2106,  0.0511,  0.0470,  0.0370,  0.0239,  0.0317,  0.0058,  0.2141,\n                       0.0428,  0.1831,  0.0405,  0.3695,  0.0545,  0.2335,  0.0410,  0.0472,\n                       0.2359,  0.2162,  0.0439,  0.0463,  0.3104,  0.0479,  0.2066,  0.2455,\n                       0.0445,  0.0408,  0.2789,  0.2782,  0.0424,  0.0472,  0.0546,  0.2379,\n                       0.1715,  0.2296,  0.0370,  0.0379,  0.4578,  0.1951,  0.0384,  0.0407,\n                       0.2502,  0.1630,  0.0513,  0.0419,  0.0467,  0.2438,  0.4138,  0.2058,\n                       0.0406,  0.0411,  0.3185,  0.0490,  0.3336,  0.0522,  0.0445,  0.2828,\n                       0.2650, -0.0881,  0.0369,  0.0558,  0.2623,  0.0382,  0.2284,  0.0357,\n                       0.0458,  0.2430,  0.0485,  0.0433,  0.1840,  0.0523,  0.4118,  0.2124,\n                       0.0467,  0.1607,  0.0479,  0.0463,  0.0481,  0.2361,  0.2090,  0.0527,\n                       0.3769,  0.3575,  0.0469,  0.0446,  0.0446,  0.0487,  0.0488,  0.3255,\n                       0.3283,  0.0433,  0.0504,  0.0416,  0.1995,  0.2855,  0.0449,  0.0479,\n                       0.0428,  0.0487,  0.1984,  0.0520,  0.2950,  0.0518,  0.0438,  0.0462,\n                       0.0447,  0.1821,  0.0593,  0.1745,  0.2895,  0.2726,  0.2632,  0.2856,\n                       0.2905,  0.4054,  0.3103,  0.1819,  0.0484,  0.3615,  0.0491,  0.3523,\n                       0.2021,  0.0479,  0.0485,  0.2615,  0.1936,  0.2830,  0.0428,  0.2193,\n                       0.0416,  0.0532,  0.0504,  0.2367,  0.0558,  0.0481,  0.2373,  0.2169,\n                       0.2446,  0.0513,  0.0525,  0.2756,  0.2104,  0.0519,  0.0588,  0.2045,\n                       0.2842,  0.2869,  0.2228, -0.0109,  0.0457,  0.2511,  0.0495,  0.3883,\n                       0.0505,  0.0370,  0.0444,  0.2152,  0.0438,  0.0428,  0.4217,  0.0334,\n                       0.0524,  0.2727,  0.2438,  0.2610,  0.2079,  0.0554,  0.2088,  0.0536,\n                       0.2302,  0.0313,  0.0413,  0.0508,  0.0403,  0.0535,  0.2926,  0.2783,\n                       0.2202,  0.0507,  0.2617,  0.0413,  0.0431,  0.2072,  0.0515,  0.0444,\n                       0.1580,  0.0509,  0.0406,  0.0520,  0.0399,  0.0447,  0.0523,  0.0542,\n                       0.0407,  0.2373,  0.3771,  0.0338,  0.3462,  0.0521,  0.0321,  0.2634,\n                       0.0453,  0.1840,  0.3966,  0.0491,  0.0530,  0.0496,  0.0339,  0.0564,\n                       0.2436,  0.0355,  0.0443,  0.3113,  0.0332,  0.0575,  0.2626,  0.0481,\n                       0.0492,  0.0469,  0.0482,  0.0457,  0.0446,  0.4118,  0.4505,  0.0442,\n                       0.1644,  0.0497,  0.2367,  0.0415,  0.2156,  0.0508,  0.2699,  0.2603,\n                       0.2681,  0.0373,  0.0498,  0.0484,  0.0472,  0.3661,  0.0419,  0.0397,\n                       0.1968,  0.0352,  0.0448,  0.5117,  0.4274,  0.2661,  0.0453,  0.2314,\n                       0.0510,  0.0458,  0.0385,  0.2047,  0.0505,  0.2768,  0.0265,  0.0507,\n                       0.2400,  0.2533,  0.0417,  0.0430,  0.0501,  0.0274,  0.0421,  0.0498,\n                       0.0533,  0.0533,  0.0522,  0.3432,  0.2455,  0.0396,  0.0446,  0.0413,\n                       0.0519,  0.2337,  0.0426,  0.0498,  0.2132,  0.0454,  0.0419,  0.2546,\n                       0.1831,  0.2337,  0.0187,  0.0379,  0.0446,  0.2287,  0.2864,  0.0405,\n                       0.2969,  0.0517,  0.0418,  0.0472,  0.0405,  0.2286,  0.0479,  0.3271,\n                      -0.0108,  0.0037,  0.0425,  0.2626,  0.0436,  0.2076,  0.2408,  0.2306,\n                       0.1981,  0.0355,  0.2119,  0.2215,  0.0278,  0.0465,  0.0570,  0.4605,\n                       0.0491,  0.1641,  0.0443,  0.0423,  0.0447,  0.0471,  0.4235,  0.2252,\n                       0.0529,  0.0554,  0.0441,  0.0503,  0.3152,  0.0387,  0.0519,  0.0537,\n                       0.2492,  0.2467,  0.0477,  0.0429,  0.0562,  0.0417,  0.0461,  0.0470,\n                       0.2569,  0.2489,  0.1860,  0.0442,  0.3144,  0.0505,  0.3036,  0.1767,\n                       0.0493,  0.0449,  0.3053,  0.1801,  0.0432,  0.2834,  0.0413,  0.1953,\n                       0.0491,  0.0493,  0.1717,  0.0526,  0.0475,  0.1974,  0.0531,  0.0494,\n                       0.1399,  0.0385,  0.0467,  0.0511,  0.2244,  0.0452,  0.0398,  0.0507,\n                       0.4021,  0.0350,  0.2139,  0.0513,  0.0334,  0.0495,  0.0433,  0.0515,\n                       0.1707,  0.0452,  0.2422,  0.0495,  0.0513,  0.0504,  0.0543,  0.0352,\n                       0.0464,  0.2822,  0.0450,  0.0497,  0.3806,  0.2216,  0.2262,  0.0426,\n                       0.0342,  0.0524,  0.0387,  0.0527,  0.2686,  0.2126,  0.0547,  0.1925,\n                       0.0411,  0.3423,  0.0533,  0.0457,  0.1955,  0.0359,  0.0572,  0.0492,\n                       0.3969,  0.4289,  0.5073,  0.1829,  0.3369,  0.0301,  0.0498,  0.2327,\n                       0.1815,  0.0508,  0.2164,  0.1818,  0.0461,  0.0493,  0.2452,  0.2498,\n                       0.4480,  0.2417,  0.0428,  0.2476,  0.2397,  0.0465,  0.0485,  0.2098,\n                       0.1876,  0.0476,  0.0439,  0.0426,  0.0517,  0.2248,  0.0281,  0.2131,\n                       0.2826,  0.0450,  0.0392,  0.0497,  0.0400,  0.0547,  0.0468,  0.3733,\n                       0.0478,  0.2025,  0.2412,  0.0538,  0.0483,  0.0500,  0.0491,  0.0472,\n                       0.0485,  0.0548,  0.1942,  0.0539,  0.0558,  0.0423,  0.3866,  0.0516,\n                       0.3249,  0.0339,  0.2554,  0.0400,  0.0424,  0.0518,  0.0597,  0.1827,\n                       0.0418,  0.0536,  0.0438,  0.3259,  0.0390,  0.2720,  0.3574,  0.0331,\n                       0.0521,  0.2184,  0.0547,  0.2420,  0.0439,  0.4211,  0.3319,  0.0553,\n                       0.0520,  0.2433,  0.3680,  0.0627,  0.0426,  0.0435,  0.2945,  0.0460,\n                       0.0306,  0.1928,  0.3129,  0.2109,  0.0436,  0.0520,  0.3361,  0.0055,\n                       0.4117,  0.0521,  0.0564,  0.0208,  0.2202,  0.0471,  0.0448,  0.3415,\n                       0.2298,  0.2816,  0.0524,  0.0475,  0.0469,  0.0433,  0.0437,  0.0458,\n                       0.2913,  0.0480,  0.4734,  0.0507,  0.0416,  0.0536,  0.3002,  0.2176,\n                       0.0470,  0.2272,  0.0370,  0.2275,  0.0406,  0.2660,  0.2330,  0.3239,\n                       0.1723,  0.2276,  0.0461,  0.0037,  0.0525,  0.0379,  0.1986,  0.0544,\n                       0.0431,  0.0475,  0.2328,  0.1922,  0.3926,  0.0500,  0.2493,  0.0462,\n                       0.2658,  0.2217,  0.2731,  0.0538,  0.3843,  0.2735,  0.0499,  0.0504,\n                       0.2865,  0.0281,  0.0315,  0.0354,  0.0504,  0.0415,  0.0299,  0.0508,\n                       0.2350,  0.0417,  0.2172,  0.0505,  0.0249,  0.2029,  0.0449,  0.0505],\n                     requires_grad=True)),\n             ('mnet.features.17.conv.1.0.scale', tensor(0.0226)),\n             ('mnet.features.17.conv.1.0.zero_point', tensor(91)),\n             ('mnet.features.17.conv.2.weight',\n              tensor([[[[ 0.0106]],\n              \n                       [[-0.0423]],\n              \n                       [[ 0.3067]],\n              \n                       ...,\n              \n                       [[-0.0423]],\n              \n                       [[-0.1657]],\n              \n                       [[ 0.0670]]],\n              \n              \n                      [[[ 0.0493]],\n              \n                       [[ 0.3800]],\n              \n                       [[ 0.1530]],\n              \n                       ...,\n              \n                       [[-0.0592]],\n              \n                       [[ 0.2369]],\n              \n                       [[-0.0987]]],\n              \n              \n                      [[[ 0.0789]],\n              \n                       [[-0.2662]],\n              \n                       [[-0.2120]],\n              \n                       ...,\n              \n                       [[-0.2761]],\n              \n                       [[-0.1430]],\n              \n                       [[ 0.0641]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-0.1250]],\n              \n                       [[ 0.0345]],\n              \n                       [[ 0.1724]],\n              \n                       ...,\n              \n                       [[-0.0345]],\n              \n                       [[-0.0043]],\n              \n                       [[-0.0603]]],\n              \n              \n                      [[[-0.0528]],\n              \n                       [[ 0.1187]],\n              \n                       [[-0.0704]],\n              \n                       ...,\n              \n                       [[ 0.1363]],\n              \n                       [[-0.0264]],\n              \n                       [[-0.1275]]],\n              \n              \n                      [[[ 0.1433]],\n              \n                       [[ 0.2823]],\n              \n                       [[ 0.0478]],\n              \n                       ...,\n              \n                       [[ 0.0478]],\n              \n                       [[-0.0869]],\n              \n                       [[-0.1216]]]], size=(320, 960, 1, 1), dtype=torch.qint8,\n                     quantization_scheme=torch.per_channel_affine,\n                     scale=tensor([0.0035, 0.0049, 0.0049, 0.0047, 0.0040, 0.0035, 0.0043, 0.0045, 0.0040,\n                      0.0038, 0.0047, 0.0045, 0.0040, 0.0052, 0.0039, 0.0050, 0.0038, 0.0043,\n                      0.0050, 0.0043, 0.0049, 0.0046, 0.0068, 0.0047, 0.0037, 0.0046, 0.0045,\n                      0.0049, 0.0043, 0.0048, 0.0042, 0.0049, 0.0060, 0.0044, 0.0039, 0.0035,\n                      0.0043, 0.0045, 0.0043, 0.0044, 0.0042, 0.0041, 0.0046, 0.0055, 0.0044,\n                      0.0046, 0.0048, 0.0045, 0.0047, 0.0041, 0.0041, 0.0038, 0.0038, 0.0037,\n                      0.0045, 0.0045, 0.0046, 0.0042, 0.0047, 0.0039, 0.0039, 0.0051, 0.0040,\n                      0.0040, 0.0044, 0.0047, 0.0045, 0.0039, 0.0038, 0.0043, 0.0038, 0.0043,\n                      0.0026, 0.0050, 0.0044, 0.0042, 0.0052, 0.0037, 0.0036, 0.0056, 0.0048,\n                      0.0041, 0.0039, 0.0040, 0.0043, 0.0055, 0.0039, 0.0048, 0.0041, 0.0040,\n                      0.0039, 0.0048, 0.0045, 0.0048, 0.0037, 0.0043, 0.0044, 0.0047, 0.0049,\n                      0.0048, 0.0046, 0.0049, 0.0043, 0.0043, 0.0040, 0.0040, 0.0045, 0.0044,\n                      0.0036, 0.0044, 0.0042, 0.0043, 0.0046, 0.0037, 0.0039, 0.0043, 0.0038,\n                      0.0046, 0.0035, 0.0045, 0.0040, 0.0047, 0.0037, 0.0052, 0.0042, 0.0044,\n                      0.0049, 0.0037, 0.0044, 0.0044, 0.0046, 0.0048, 0.0039, 0.0040, 0.0037,\n                      0.0046, 0.0042, 0.0045, 0.0042, 0.0045, 0.0047, 0.0041, 0.0051, 0.0041,\n                      0.0045, 0.0042, 0.0048, 0.0044, 0.0037, 0.0042, 0.0042, 0.0046, 0.0040,\n                      0.0034, 0.0048, 0.0056, 0.0057, 0.0045, 0.0042, 0.0048, 0.0050, 0.0057,\n                      0.0051, 0.0043, 0.0041, 0.0044, 0.0042, 0.0044, 0.0039, 0.0051, 0.0044,\n                      0.0051, 0.0047, 0.0046, 0.0048, 0.0040, 0.0043, 0.0040, 0.0052, 0.0049,\n                      0.0043, 0.0041, 0.0040, 0.0036, 0.0055, 0.0039, 0.0042, 0.0037, 0.0050,\n                      0.0037, 0.0044, 0.0039, 0.0051, 0.0032, 0.0045, 0.0041, 0.0033, 0.0037,\n                      0.0041, 0.0049, 0.0042, 0.0048, 0.0042, 0.0041, 0.0046, 0.0042, 0.0041,\n                      0.0041, 0.0053, 0.0042, 0.0043, 0.0042, 0.0049, 0.0043, 0.0038, 0.0046,\n                      0.0043, 0.0044, 0.0049, 0.0053, 0.0036, 0.0040, 0.0042, 0.0053, 0.0042,\n                      0.0038, 0.0036, 0.0043, 0.0043, 0.0059, 0.0047, 0.0037, 0.0040, 0.0046,\n                      0.0042, 0.0044, 0.0045, 0.0042, 0.0040, 0.0042, 0.0049, 0.0049, 0.0037,\n                      0.0039, 0.0052, 0.0046, 0.0045, 0.0033, 0.0044, 0.0046, 0.0040, 0.0048,\n                      0.0035, 0.0049, 0.0045, 0.0047, 0.0050, 0.0049, 0.0044, 0.0037, 0.0046,\n                      0.0038, 0.0043, 0.0046, 0.0045, 0.0046, 0.0051, 0.0039, 0.0042, 0.0049,\n                      0.0056, 0.0058, 0.0049, 0.0045, 0.0045, 0.0044, 0.0059, 0.0035, 0.0045,\n                      0.0063, 0.0047, 0.0045, 0.0050, 0.0042, 0.0046, 0.0039, 0.0047, 0.0051,\n                      0.0057, 0.0040, 0.0053, 0.0037, 0.0042, 0.0043, 0.0042, 0.0044, 0.0062,\n                      0.0039, 0.0039, 0.0054, 0.0045, 0.0049, 0.0043, 0.0046, 0.0043, 0.0033,\n                      0.0047, 0.0043, 0.0037, 0.0046, 0.0049, 0.0035, 0.0042, 0.0039, 0.0041,\n                      0.0048, 0.0036, 0.0043, 0.0044, 0.0043], dtype=torch.float64),\n                     zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0]),\n                     axis=0)),\n             ('mnet.features.17.conv.2.bias',\n              Parameter containing:\n              tensor([ 0.0957,  0.0927,  0.1009, -0.2094,  0.1783,  0.1982,  0.4200, -0.4191,\n                       0.5846,  0.0322,  0.4789,  0.0192, -0.4891,  0.3636, -0.3457, -0.1708,\n                       0.3909,  0.2087, -0.1783,  0.7258,  0.1022, -0.7595,  0.1857, -0.0699,\n                      -0.0631,  0.3866, -0.7139,  0.4138,  0.7473,  0.2384, -0.7144, -0.1099,\n                       0.1397,  0.1659, -0.1669, -0.5714,  0.2500, -0.4493, -0.1131,  0.2862,\n                       0.7438, -0.4087,  0.2922,  0.1075,  0.0907,  0.7541,  0.0171, -0.3274,\n                       0.2569,  0.7544, -0.4608,  0.2674, -0.2481, -0.2136,  0.0105,  0.2107,\n                      -0.2873, -0.2408,  0.0256,  0.4530,  0.0195, -0.0235,  0.0193, -0.0322,\n                       0.3298,  0.1369,  1.0668, -0.1752,  0.0967,  0.1501,  0.3713, -0.2264,\n                       0.5279, -0.6227, -0.1832,  0.6254, -0.0966, -0.0732,  0.4421,  0.3743,\n                      -0.1391, -0.4448, -0.1950,  0.6602, -0.4463, -0.5108,  0.3344,  1.3539,\n                       0.5613, -0.4728, -0.1703,  0.4080, -0.1623, -0.6122,  0.6219,  0.9054,\n                       0.0217,  0.1531, -0.8480,  0.3758,  0.6484,  0.2465,  0.0606,  0.0144,\n                      -0.6668, -0.4910,  1.1556,  0.1008, -0.6443,  0.7222, -0.2229, -0.0542,\n                       0.3088,  0.6761,  0.1850, -0.6812,  0.2443,  0.6570, -0.5758,  0.2960,\n                       0.3363,  0.5173, -0.2922, -0.2802,  0.0077, -0.0526,  0.1364,  0.0607,\n                      -0.2635,  0.4690,  0.8360,  0.3209, -0.2678,  0.1153, -0.4923, -0.0556,\n                      -0.1109, -0.0584,  0.4889,  0.3263,  0.4460, -0.4364,  0.1334, -0.4730,\n                      -0.7322, -0.3283,  0.3482,  0.0039, -0.4356,  0.1159,  0.1790,  0.6063,\n                       0.2322, -0.2727, -0.0339, -0.3960, -0.6133,  0.1063,  0.4353,  1.3586,\n                      -0.3826, -0.2583, -0.4303,  0.7367,  0.0340,  0.4463,  0.5215, -0.1455,\n                       0.3346, -0.4602,  0.4536,  0.3348, -0.8095, -0.3729,  0.5189, -0.4313,\n                       0.0696, -0.0429,  0.6646, -0.1776, -0.1333, -0.6695, -0.0611, -0.7256,\n                      -0.5962,  0.1287,  0.4126,  0.4635, -0.4368, -0.0753, -0.0884,  0.3607,\n                      -0.1771, -0.5487,  0.1969, -0.4580,  0.5146,  0.1203, -0.5623, -0.7303,\n                       0.4627, -0.1856,  0.6894, -0.0264,  0.2620, -0.2480,  0.5956,  0.5760,\n                       0.9855, -0.2339,  0.6608, -0.6830,  0.3123,  0.0243,  0.2728, -0.0848,\n                      -0.0327,  0.2426,  0.3071, -0.0548, -0.0885, -0.0290, -0.3028, -0.2493,\n                      -0.2566,  0.7833,  0.2040,  0.2663,  0.0173,  0.9121,  0.5428, -0.2541,\n                       0.1004,  0.4561,  0.6389, -0.3425,  0.4492,  0.3779, -0.1508, -0.0403,\n                      -0.3988, -0.4057, -0.4204, -0.0581, -0.5164, -0.1470,  1.2090, -0.1078,\n                      -0.3716, -0.1092, -0.2427, -0.5021, -0.2604,  0.3305,  0.0115, -0.4419,\n                       0.4686,  0.1693, -0.2337,  0.1754, -0.3478,  0.1497, -0.0934,  0.2262,\n                       0.3665,  0.2142,  0.4624,  0.1937,  0.2577, -0.4255,  0.3805, -0.4623,\n                      -0.7613,  0.3331,  0.0862, -0.0288,  0.2479, -0.4351,  1.3042,  0.5546,\n                      -0.2779,  0.3124,  0.3297,  0.4123, -0.3907, -0.0904, -0.0529, -0.3379,\n                      -0.1128,  0.3849,  0.9351, -1.3346, -0.2780,  0.0033, -0.2932, -0.3937,\n                       0.3389, -0.1757, -0.1788, -0.6576, -0.4916,  0.3844, -0.2420, -0.1929,\n                      -0.2320,  0.0677, -0.5961,  0.8103,  0.1099,  0.1021,  0.6442,  0.0196,\n                      -0.3213, -0.1093, -0.3063, -0.5251,  0.0611, -0.5503,  0.5237, -0.3717],\n                     requires_grad=True)),\n             ('mnet.features.17.conv.2.scale', tensor(0.0187)),\n             ('mnet.features.17.conv.2.zero_point', tensor(64)),\n             ('mnet.features.18.0.weight',\n              tensor([[[[-0.1346]],\n              \n                       [[ 0.2296]],\n              \n                       [[-0.0079]],\n              \n                       ...,\n              \n                       [[-0.1108]],\n              \n                       [[ 0.9104]],\n              \n                       [[-0.0633]]],\n              \n              \n                      [[[-0.3861]],\n              \n                       [[-0.2527]],\n              \n                       [[-0.1966]],\n              \n                       ...,\n              \n                       [[ 0.1334]],\n              \n                       [[ 0.4142]],\n              \n                       [[-0.3019]]],\n              \n              \n                      [[[-0.6092]],\n              \n                       [[ 0.0896]],\n              \n                       [[ 0.1702]],\n              \n                       ...,\n              \n                       [[-0.1971]],\n              \n                       [[-0.1613]],\n              \n                       [[-0.1075]]],\n              \n              \n                      ...,\n              \n              \n                      [[[ 0.2878]],\n              \n                       [[ 0.5436]],\n              \n                       [[-0.3624]],\n              \n                       ...,\n              \n                       [[-0.7247]],\n              \n                       [[-0.2345]],\n              \n                       [[-0.3517]]],\n              \n              \n                      [[[-0.1045]],\n              \n                       [[-0.3618]],\n              \n                       [[ 0.0724]],\n              \n                       ...,\n              \n                       [[ 0.3538]],\n              \n                       [[-0.5870]],\n              \n                       [[ 0.1689]]],\n              \n              \n                      [[[ 0.4492]],\n              \n                       [[-0.4647]],\n              \n                       [[-0.1626]],\n              \n                       ...,\n              \n                       [[ 0.0542]],\n              \n                       [[-0.2169]],\n              \n                       [[-0.3408]]]], size=(1280, 320, 1, 1), dtype=torch.qint8,\n                     quantization_scheme=torch.per_channel_affine,\n                     scale=tensor([0.0079, 0.0070, 0.0090,  ..., 0.0107, 0.0080, 0.0077],\n                     dtype=torch.float64),\n                     zero_point=tensor([0, 0, 0,  ..., 0, 0, 0]), axis=0)),\n             ('mnet.features.18.0.bias',\n              Parameter containing:\n              tensor([-0.1547, -0.1335, -0.0604,  ..., -0.1559, -0.2386, -0.1603],\n                     requires_grad=True)),\n             ('mnet.features.18.0.scale', tensor(0.1281)),\n             ('mnet.features.18.0.zero_point', tensor(57)),\n             ('mnet.classifier.0.scale', tensor(0.1073)),\n             ('mnet.classifier.0.zero_point', tensor(0)),\n             ('mnet.classifier.0._packed_params.dtype', torch.qint8),\n             ('mnet.classifier.0._packed_params._packed_params',\n              (tensor([[ 0.0237, -0.0057, -0.0028,  ...,  0.0503,  0.0807, -0.0066],\n                       [-0.0478,  0.0063, -0.0099,  ..., -0.0036,  0.0000, -0.0054],\n                       [-0.0426, -0.0091,  0.0744,  ..., -0.0317, -0.0109, -0.0381],\n                       ...,\n                       [ 0.0109,  0.0023, -0.0195,  ...,  0.0033,  0.0069,  0.0228],\n                       [ 0.0167, -0.0167,  0.0013,  ..., -0.0067, -0.0114,  0.0074],\n                       [-0.0016, -0.0341, -0.0103,  ...,  0.0151, -0.0405, -0.0317]],\n                      size=(1024, 1280), dtype=torch.qint8,\n                      quantization_scheme=torch.per_channel_affine,\n                      scale=tensor([0.0009, 0.0009, 0.0009,  ..., 0.0003, 0.0007, 0.0008],\n                      dtype=torch.float64),\n                      zero_point=tensor([0, 0, 0,  ..., 0, 0, 0]), axis=0),\n               Parameter containing:\n               tensor([ 2.8276e-02,  2.1374e-02,  1.1268e-02,  ..., -3.5557e-02,\n                       -3.2298e-03, -2.1157e-05], requires_grad=True))),\n             ('mnet.classifier.2.weight',\n              tensor([0.9637, 1.0197, 1.0060,  ..., 0.9705, 0.9770, 0.8599])),\n             ('mnet.classifier.2.bias',\n              tensor([-0.0734,  0.0456, -0.0928,  ..., -0.0896,  0.0464,  0.0948])),\n             ('mnet.classifier.2.running_mean',\n              tensor([2.8598e+00, 2.6836e+00, 3.2238e+00,  ..., 5.6052e-45, 3.8028e-37,\n                      1.6644e-12])),\n             ('mnet.classifier.2.running_var',\n              tensor([4.0063e+00, 1.4499e+00, 3.5974e+00,  ..., 5.6052e-45, 1.3073e-38,\n                      1.3425e-12])),\n             ('mnet.classifier.2.num_batches_tracked', tensor(1890)),\n             ('mnet.classifier.3.scale', tensor(0.2884)),\n             ('mnet.classifier.3.zero_point', tensor(0)),\n             ('mnet.classifier.3._packed_params.dtype', torch.qint8),\n             ('mnet.classifier.3._packed_params._packed_params',\n              (tensor([[ 0.0262, -0.0181, -0.0298,  ...,  0.0036,  0.0099, -0.0135],\n                       [-0.0479, -0.0866,  0.0204,  ...,  0.0061,  0.0051, -0.0275],\n                       [-0.0909, -0.0244, -0.0283,  ..., -0.0229, -0.0084,  0.0138],\n                       ...,\n                       [-0.0196, -0.0245, -0.0372,  ...,  0.0039,  0.0461,  0.0020],\n                       [ 0.0036,  0.0407, -0.0015,  ...,  0.0087,  0.0348, -0.0211],\n                       [ 0.0099,  0.0563,  0.0546,  ..., -0.0116,  0.0306,  0.0017]],\n                      size=(512, 1024), dtype=torch.qint8,\n                      quantization_scheme=torch.per_channel_affine,\n                      scale=tensor([0.0009, 0.0010, 0.0008, 0.0009, 0.0010, 0.0011, 0.0008, 0.0013, 0.0008,\n                       0.0009, 0.0010, 0.0008, 0.0010, 0.0009, 0.0008, 0.0011, 0.0010, 0.0010,\n                       0.0007, 0.0009, 0.0008, 0.0008, 0.0011, 0.0009, 0.0009, 0.0010, 0.0009,\n                       0.0012, 0.0009, 0.0012, 0.0010, 0.0009, 0.0010, 0.0007, 0.0009, 0.0011,\n                       0.0007, 0.0009, 0.0009, 0.0009, 0.0012, 0.0013, 0.0010, 0.0008, 0.0009,\n                       0.0010, 0.0008, 0.0009, 0.0008, 0.0009, 0.0009, 0.0010, 0.0007, 0.0008,\n                       0.0009, 0.0008, 0.0009, 0.0010, 0.0010, 0.0008, 0.0007, 0.0007, 0.0008,\n                       0.0007, 0.0008, 0.0009, 0.0011, 0.0009, 0.0008, 0.0007, 0.0009, 0.0008,\n                       0.0009, 0.0010, 0.0009, 0.0011, 0.0011, 0.0006, 0.0008, 0.0010, 0.0008,\n                       0.0007, 0.0011, 0.0008, 0.0013, 0.0009, 0.0008, 0.0009, 0.0009, 0.0010,\n                       0.0009, 0.0009, 0.0008, 0.0008, 0.0011, 0.0007, 0.0013, 0.0008, 0.0009,\n                       0.0009, 0.0009, 0.0013, 0.0013, 0.0009, 0.0008, 0.0008, 0.0010, 0.0007,\n                       0.0009, 0.0008, 0.0012, 0.0008, 0.0009, 0.0007, 0.0007, 0.0009, 0.0009,\n                       0.0011, 0.0008, 0.0008, 0.0008, 0.0012, 0.0011, 0.0009, 0.0008, 0.0011,\n                       0.0009, 0.0011, 0.0008, 0.0009, 0.0008, 0.0010, 0.0009, 0.0009, 0.0010,\n                       0.0010, 0.0011, 0.0009, 0.0009, 0.0008, 0.0010, 0.0009, 0.0009, 0.0008,\n                       0.0009, 0.0009, 0.0009, 0.0014, 0.0009, 0.0008, 0.0009, 0.0009, 0.0008,\n                       0.0010, 0.0009, 0.0007, 0.0009, 0.0008, 0.0012, 0.0006, 0.0009, 0.0009,\n                       0.0008, 0.0020, 0.0007, 0.0011, 0.0009, 0.0010, 0.0009, 0.0008, 0.0009,\n                       0.0011, 0.0014, 0.0008, 0.0010, 0.0008, 0.0009, 0.0007, 0.0007, 0.0009,\n                       0.0007, 0.0008, 0.0009, 0.0010, 0.0008, 0.0010, 0.0008, 0.0008, 0.0008,\n                       0.0008, 0.0008, 0.0008, 0.0010, 0.0012, 0.0010, 0.0010, 0.0009, 0.0008,\n                       0.0011, 0.0010, 0.0010, 0.0009, 0.0008, 0.0010, 0.0008, 0.0008, 0.0008,\n                       0.0010, 0.0010, 0.0010, 0.0009, 0.0008, 0.0007, 0.0009, 0.0008, 0.0009,\n                       0.0011, 0.0008, 0.0011, 0.0007, 0.0009, 0.0011, 0.0010, 0.0008, 0.0012,\n                       0.0011, 0.0009, 0.0009, 0.0007, 0.0010, 0.0009, 0.0010, 0.0008, 0.0009,\n                       0.0009, 0.0008, 0.0009, 0.0008, 0.0009, 0.0009, 0.0010, 0.0008, 0.0009,\n                       0.0008, 0.0010, 0.0010, 0.0013, 0.0010, 0.0009, 0.0008, 0.0008, 0.0010,\n                       0.0009, 0.0009, 0.0009, 0.0008, 0.0011, 0.0008, 0.0008, 0.0010, 0.0008,\n                       0.0012, 0.0009, 0.0008, 0.0008, 0.0007, 0.0008, 0.0007, 0.0008, 0.0009,\n                       0.0011, 0.0010, 0.0010, 0.0010, 0.0007, 0.0010, 0.0010, 0.0008, 0.0008,\n                       0.0007, 0.0009, 0.0008, 0.0010, 0.0011, 0.0008, 0.0011, 0.0011, 0.0008,\n                       0.0008, 0.0016, 0.0009, 0.0011, 0.0007, 0.0007, 0.0013, 0.0010, 0.0009,\n                       0.0008, 0.0009, 0.0009, 0.0011, 0.0013, 0.0008, 0.0009, 0.0010, 0.0008,\n                       0.0008, 0.0008, 0.0009, 0.0009, 0.0010, 0.0009, 0.0008, 0.0010, 0.0009,\n                       0.0009, 0.0009, 0.0011, 0.0009, 0.0007, 0.0010, 0.0008, 0.0010, 0.0011,\n                       0.0009, 0.0008, 0.0008, 0.0008, 0.0011, 0.0010, 0.0009, 0.0008, 0.0008,\n                       0.0009, 0.0009, 0.0008, 0.0011, 0.0011, 0.0008, 0.0009, 0.0009, 0.0010,\n                       0.0009, 0.0016, 0.0009, 0.0008, 0.0008, 0.0009, 0.0010, 0.0009, 0.0009,\n                       0.0009, 0.0010, 0.0009, 0.0009, 0.0011, 0.0009, 0.0009, 0.0013, 0.0011,\n                       0.0009, 0.0008, 0.0008, 0.0015, 0.0014, 0.0009, 0.0008, 0.0008, 0.0009,\n                       0.0009, 0.0014, 0.0009, 0.0008, 0.0009, 0.0010, 0.0010, 0.0009, 0.0010,\n                       0.0010, 0.0010, 0.0010, 0.0011, 0.0008, 0.0010, 0.0010, 0.0010, 0.0008,\n                       0.0009, 0.0008, 0.0009, 0.0008, 0.0009, 0.0007, 0.0010, 0.0010, 0.0008,\n                       0.0012, 0.0007, 0.0008, 0.0010, 0.0009, 0.0008, 0.0008, 0.0008, 0.0011,\n                       0.0007, 0.0011, 0.0009, 0.0007, 0.0008, 0.0008, 0.0010, 0.0008, 0.0010,\n                       0.0008, 0.0009, 0.0009, 0.0010, 0.0008, 0.0010, 0.0014, 0.0009, 0.0008,\n                       0.0007, 0.0009, 0.0009, 0.0010, 0.0011, 0.0007, 0.0008, 0.0011, 0.0010,\n                       0.0009, 0.0009, 0.0009, 0.0010, 0.0014, 0.0011, 0.0008, 0.0010, 0.0015,\n                       0.0008, 0.0006, 0.0010, 0.0008, 0.0009, 0.0008, 0.0008, 0.0010, 0.0009,\n                       0.0009, 0.0011, 0.0008, 0.0007, 0.0010, 0.0010, 0.0010, 0.0009, 0.0008,\n                       0.0009, 0.0013, 0.0009, 0.0009, 0.0009, 0.0010, 0.0009, 0.0009, 0.0009,\n                       0.0009, 0.0008, 0.0008, 0.0008, 0.0011, 0.0009, 0.0010, 0.0010, 0.0007,\n                       0.0011, 0.0008, 0.0007, 0.0009, 0.0007, 0.0012, 0.0008, 0.0008, 0.0007,\n                       0.0008, 0.0008, 0.0009, 0.0014, 0.0010, 0.0010, 0.0009, 0.0009, 0.0009,\n                       0.0009, 0.0008, 0.0007, 0.0008, 0.0008, 0.0009, 0.0009, 0.0009, 0.0009,\n                       0.0008, 0.0011, 0.0009, 0.0016, 0.0008, 0.0010, 0.0007, 0.0008],\n                      dtype=torch.float64),\n                      zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0]),\n                      axis=0),\n               Parameter containing:\n               tensor([-1.3243e-02, -4.3690e-03, -5.7523e-03,  3.9070e-02, -6.2509e-02,\n                       -2.4974e-02, -1.6290e-02, -9.9743e-02, -7.4302e-03,  2.2444e-02,\n                       -3.9031e-02,  1.3392e-02,  2.1660e-02, -5.9912e-03,  8.0260e-03,\n                       -3.8856e-02, -9.7976e-03, -1.1598e-02, -1.4002e-02, -1.1108e-02,\n                       -6.5832e-03,  1.8819e-02, -5.3383e-02, -4.1542e-02, -1.3171e-02,\n                       -4.1576e-02,  3.3289e-03, -2.3281e-04,  4.1729e-03, -8.9043e-02,\n                        2.6029e-02, -1.1504e-02, -2.8364e-02, -5.6985e-02,  2.7389e-02,\n                        3.5344e-02, -3.8089e-02,  7.4120e-02, -2.1795e-03, -5.0881e-02,\n                       -9.9406e-02, -5.9812e-02, -4.6038e-02, -1.4093e-02, -2.7436e-02,\n                       -3.4265e-02, -5.1337e-02,  1.7415e-02,  9.0226e-02, -7.3854e-03,\n                       -6.2634e-02,  1.3503e-02,  3.3852e-02, -2.8988e-03, -7.8418e-04,\n                       -7.9466e-02, -5.7712e-02, -4.5730e-02, -4.9150e-03, -7.6352e-02,\n                       -2.2134e-02, -6.3714e-03,  4.2170e-03,  8.1281e-03,  5.5950e-02,\n                        1.3124e-02, -9.7673e-02,  4.4926e-02, -2.1975e-02, -3.1856e-03,\n                       -2.8118e-02, -2.8300e-02, -1.3247e-02, -5.4382e-02,  4.6710e-02,\n                       -3.1345e-02,  3.9049e-02,  2.4482e-02, -1.7862e-02,  5.3778e-02,\n                        1.8846e-02,  9.8001e-03,  4.3318e-02, -2.0501e-02, -8.0394e-02,\n                       -3.9617e-02,  4.2134e-02, -2.8823e-02,  4.1644e-02, -1.0092e-01,\n                       -6.3552e-02,  4.4184e-03, -3.1850e-02, -4.9754e-02, -1.6965e-02,\n                       -5.5001e-03, -1.7537e-02,  2.5889e-02, -5.0882e-02, -7.0933e-03,\n                       -1.1509e-04, -3.9224e-02, -6.5027e-02,  1.8800e-02, -2.8622e-02,\n                        2.5877e-02,  2.2297e-02,  2.1569e-02,  2.9074e-04, -3.1338e-02,\n                       -7.1877e-02,  1.4426e-02, -4.7094e-02,  3.1547e-02, -2.4264e-02,\n                        2.6398e-02, -1.8910e-02,  7.6569e-02,  2.5726e-03,  4.3389e-02,\n                       -1.0953e-02, -5.8475e-02, -2.2810e-02, -2.0995e-02, -5.2865e-02,\n                       -3.8127e-02, -2.2975e-02,  1.4187e-02,  3.2125e-02, -4.3361e-02,\n                       -1.7683e-02, -8.1355e-02, -4.1522e-02, -8.5970e-03, -7.2422e-02,\n                        4.8993e-02, -6.3276e-02,  1.2247e-02, -2.6541e-02, -3.9347e-02,\n                        2.3868e-02,  6.6555e-02, -3.3674e-02, -5.3070e-02,  3.8926e-02,\n                       -1.6245e-03, -9.6249e-02,  4.3337e-02, -5.2747e-03, -3.5141e-02,\n                       -8.0096e-03,  7.4208e-03, -2.4775e-02, -2.3836e-02, -4.8096e-02,\n                       -1.8865e-02, -3.0210e-02,  3.5258e-02, -6.6469e-02,  4.8492e-03,\n                       -2.5505e-02, -1.1399e-02,  7.6423e-03, -1.5577e-01, -1.6163e-02,\n                       -7.5578e-02,  4.8559e-02,  1.0997e-03, -5.6444e-02, -3.4629e-03,\n                        5.5159e-03, -9.5591e-03, -1.0329e-01, -6.3681e-03,  3.9369e-02,\n                       -2.7645e-02, -5.4654e-02, -4.2288e-02, -1.3300e-02,  5.7302e-02,\n                        4.1301e-02, -5.4767e-02,  2.5853e-02, -1.8093e-02, -3.2753e-02,\n                       -9.1903e-02,  1.9941e-02,  3.6382e-02, -1.3847e-03,  5.5291e-02,\n                       -1.9410e-02, -2.9208e-02,  1.7734e-02, -4.9598e-02, -3.0914e-02,\n                        2.6805e-02, -1.2375e-02, -1.2282e-02, -1.0626e-01,  2.0102e-02,\n                       -1.5798e-02,  2.3594e-02,  6.9228e-03, -4.2914e-02, -1.5994e-02,\n                        3.7325e-02,  1.6565e-03,  2.5743e-02,  8.2941e-02,  1.4128e-02,\n                        1.6951e-02,  2.6580e-02,  2.4285e-03, -5.8584e-02, -4.5280e-02,\n                        7.2316e-03, -3.3576e-02, -9.4800e-03, -6.2636e-02, -3.8384e-02,\n                       -6.8046e-03, -6.6534e-02, -3.9120e-02, -6.1531e-03,  4.5014e-02,\n                        2.9557e-02,  7.7443e-03,  3.2027e-02, -4.1873e-02, -1.5805e-02,\n                       -3.3927e-02, -4.2708e-02, -3.1178e-02, -1.7125e-02, -7.7327e-02,\n                       -2.9307e-02, -4.5157e-02, -3.5094e-03, -5.0429e-02, -2.1773e-02,\n                        4.0565e-03, -3.0976e-02, -3.3380e-02,  5.0082e-02, -2.9056e-02,\n                       -8.0043e-02, -5.2094e-02, -4.6371e-02, -6.2184e-02,  3.5206e-02,\n                        4.5912e-03,  5.7597e-02,  7.1520e-04, -4.6203e-02,  7.5120e-02,\n                       -1.0480e-02,  1.6138e-02,  4.4285e-04, -4.8341e-02,  7.6036e-03,\n                       -2.2210e-02,  6.1518e-03, -1.3354e-02,  2.6253e-02, -1.3357e-02,\n                       -3.6255e-02,  3.9838e-03, -3.8410e-02,  7.7124e-02,  3.4702e-02,\n                       -6.7721e-03, -3.1678e-02,  1.6090e-02, -2.8160e-02,  1.1610e-02,\n                       -2.5810e-02, -5.8311e-02,  4.0729e-03,  4.9223e-02,  2.6153e-02,\n                        2.1298e-02,  4.5924e-02, -2.0809e-02,  2.7160e-02,  1.0506e-02,\n                        6.6226e-02, -6.1190e-02,  1.1480e-02, -5.6370e-02, -1.3093e-01,\n                        1.3344e-02, -3.6890e-02, -2.9415e-02,  2.5548e-02, -6.6851e-02,\n                        3.2417e-02, -3.6013e-02,  4.7647e-02, -1.5263e-02, -1.5319e-02,\n                       -1.6145e-02, -7.4248e-02, -3.0082e-02,  6.3266e-02, -5.0084e-02,\n                       -1.3161e-02, -1.4357e-02,  3.7982e-02, -3.0813e-02, -4.6520e-02,\n                        3.8050e-02, -1.4147e-03,  3.2287e-02, -1.8815e-02, -1.4323e-02,\n                        2.3901e-02, -1.1263e-02, -8.4068e-02,  2.2698e-02, -1.1656e-02,\n                       -5.1869e-02,  2.1311e-02, -1.5532e-02, -2.1069e-02, -6.8214e-02,\n                       -5.6596e-02, -4.0931e-02,  1.5950e-02,  2.4073e-02, -4.0847e-02,\n                       -1.2211e-02,  2.0888e-02,  9.6587e-04,  2.4702e-02, -3.1771e-02,\n                       -3.6100e-02, -6.0078e-02,  4.5367e-02, -4.1994e-02, -2.4057e-02,\n                       -2.8302e-02, -2.8173e-02, -4.0167e-02, -1.0163e-01,  2.4546e-02,\n                        1.0327e-02,  2.4375e-03, -7.4571e-02,  2.5395e-02, -7.1105e-02,\n                       -2.1885e-02, -2.0863e-02, -2.5297e-02, -4.9125e-02, -4.6028e-02,\n                       -1.4745e-03, -2.1262e-02, -9.3856e-02, -5.5824e-02, -7.5868e-02,\n                       -3.2168e-02, -1.9720e-02, -1.5205e-02, -8.5520e-02, -9.8256e-02,\n                        3.8419e-03,  1.4819e-02,  5.2987e-02, -1.2702e-02, -3.2173e-02,\n                       -1.2188e-01,  2.6744e-02, -1.7455e-02,  1.6636e-02,  2.0575e-02,\n                        2.2625e-02, -7.6748e-03, -7.1639e-02,  3.9196e-02, -5.2403e-03,\n                       -1.7689e-04, -7.4150e-02, -1.1470e-02,  6.4691e-03, -4.6724e-02,\n                        3.2299e-04, -2.6109e-02,  2.3419e-02,  4.6957e-02,  5.2272e-02,\n                        2.1436e-02,  9.3042e-03, -6.2233e-03, -1.3672e-02, -9.1403e-02,\n                       -1.2741e-03, -9.2830e-02,  1.1414e-02, -5.9528e-02, -4.2111e-02,\n                       -3.1595e-02, -5.0565e-02,  2.5355e-02, -2.2377e-02, -6.9549e-02,\n                       -3.3616e-03,  3.6958e-03,  2.6251e-02, -2.2530e-02, -1.4281e-02,\n                       -2.8365e-02, -3.5151e-02, -2.4806e-02, -5.9336e-02, -5.6807e-02,\n                        5.3990e-02,  1.0113e-03, -7.6081e-03, -1.6977e-02, -6.8569e-02,\n                       -7.9606e-02, -8.0876e-02, -1.6092e-02,  3.9850e-02, -1.4651e-02,\n                        1.0806e-02, -1.9328e-02, -1.7819e-02, -8.6540e-03, -6.3488e-02,\n                       -2.9738e-03, -6.4398e-02, -2.1635e-02, -1.4959e-02, -4.6729e-02,\n                       -4.2997e-02,  5.5577e-02, -9.8319e-02,  6.6081e-02, -1.4951e-02,\n                       -6.0687e-03, -1.0098e-02, -2.4551e-02, -2.2756e-02, -1.7917e-02,\n                       -5.6258e-02, -4.1391e-02, -3.9473e-02, -4.9808e-02, -8.6435e-03,\n                       -1.5155e-02, -3.0805e-02, -9.0798e-03,  6.8586e-02, -6.2373e-02,\n                       -5.0970e-02, -3.0673e-02, -1.5432e-02,  5.4531e-02, -5.8919e-02,\n                        1.6657e-03, -7.2297e-02,  4.9458e-02,  1.9596e-02, -1.0405e-02,\n                       -2.7196e-03, -5.6808e-02,  2.3080e-02,  1.9166e-02,  2.9778e-02,\n                        4.4308e-02,  6.2430e-02, -6.4074e-02,  3.4616e-02, -1.9691e-03,\n                       -4.2040e-02,  3.6574e-02, -2.4536e-02, -5.3921e-02,  3.4318e-02,\n                        1.4049e-02, -2.6237e-02, -1.9377e-02,  2.1508e-02, -7.1143e-03,\n                       -2.6234e-02, -4.9648e-02, -4.9914e-02,  5.2753e-03, -9.0475e-02,\n                        2.6019e-02, -4.9629e-02, -5.3812e-02, -7.6594e-03, -9.7794e-03,\n                       -7.1255e-02, -6.0896e-03, -4.5954e-02, -1.3395e-02, -2.3211e-02,\n                        5.0581e-02, -2.5721e-02,  3.6280e-02,  2.2031e-03,  1.3101e-02,\n                        2.4892e-02,  4.2932e-02, -1.1388e-01, -2.4345e-02, -6.9531e-03,\n                       -3.3022e-02, -2.7088e-02], requires_grad=True))),\n             ('mnet.classifier.5.weight',\n              tensor([0.8668, 0.9247, 0.9316, 0.9593, 1.0190, 0.9471, 0.9046, 0.8783, 0.9287,\n                      0.9286, 0.8980, 0.8973, 0.9287, 0.9112, 0.9277, 0.8967, 0.9330, 0.9514,\n                      0.8677, 0.9758, 0.9147, 0.9744, 0.9661, 0.9618, 0.9225, 0.9947, 0.9764,\n                      0.9375, 0.9309, 0.9565, 0.9483, 0.8390, 0.9265, 0.9030, 0.8989, 0.8817,\n                      0.9067, 0.9387, 0.9393, 0.8952, 0.9557, 0.8949, 0.9185, 0.9575, 0.9341,\n                      0.9721, 0.9385, 0.9755, 0.9221, 0.9514, 0.9170, 0.9636, 0.9223, 0.9292,\n                      0.9018, 0.9181, 0.9395, 0.9227, 0.9045, 0.9318, 0.9372, 0.9176, 0.9175,\n                      0.9329, 0.9528, 0.9334, 0.9452, 0.9550, 0.8902, 0.8885, 0.8648, 0.9535,\n                      0.9718, 0.9262, 0.8185, 0.9341, 0.9512, 0.8267, 0.8756, 0.9536, 0.8931,\n                      0.8942, 0.9036, 0.9206, 0.8884, 0.8906, 0.8731, 0.8968, 0.9444, 0.9427,\n                      0.9163, 0.9129, 0.9190, 0.9620, 0.9338, 0.8946, 0.9437, 0.8945, 0.8406,\n                      0.9186, 0.9093, 0.9266, 0.9242, 0.9156, 0.9435, 0.9408, 0.9285, 0.8734,\n                      0.8963, 0.8875, 0.8891, 0.9200, 0.9411, 0.8834, 0.8920, 0.8909, 0.9428,\n                      0.9024, 0.9417, 0.8877, 0.8971, 0.9122, 0.9283, 0.9178, 0.9326, 0.9053,\n                      0.8383, 0.9243, 0.8798, 0.9032, 0.9553, 0.9022, 0.9213, 0.9176, 0.9539,\n                      0.9029, 0.8540, 0.9113, 0.9412, 0.8807, 0.9563, 0.9391, 0.9337, 0.9326,\n                      0.9568, 0.9204, 0.9420, 0.9052, 0.9454, 0.9120, 0.9251, 0.9664, 0.9058,\n                      0.9589, 0.9136, 0.9120, 0.8976, 0.9136, 0.8831, 0.8606, 0.9470, 0.9152,\n                      0.9380, 0.9653, 0.8655, 0.7948, 0.9104, 0.8741, 0.9266, 0.9092, 0.8919,\n                      0.8546, 0.9340, 0.9374, 0.9134, 0.8899, 0.9531, 0.9143, 0.9358, 0.8869,\n                      0.9359, 0.9414, 0.9358, 0.9599, 0.9152, 0.9691, 0.9096, 0.8305, 0.8869,\n                      0.9115, 0.9201, 0.9604, 0.9443, 0.9739, 0.9344, 0.9161, 0.9159, 0.9136,\n                      0.9388, 0.9452, 0.9563, 0.9026, 0.9282, 0.9282, 0.9172, 0.9119, 0.9230,\n                      0.8734, 0.9103, 0.9300, 0.8981, 0.9257, 0.9438, 0.9110, 0.9174, 0.8900,\n                      0.8884, 0.9135, 0.9153, 0.9351, 0.9162, 0.9623, 0.9123, 0.9240, 0.9298,\n                      0.9320, 0.9169, 0.8544, 0.9238, 0.9257, 0.9101, 0.9073, 0.9521, 0.9263,\n                      0.9353, 0.8978, 0.9076, 0.9563, 0.9380, 0.8966, 0.9537, 0.9100, 0.8968,\n                      0.8913, 0.9284, 0.8407, 0.9510, 0.8755, 0.9347, 0.8913, 0.9193, 0.8834,\n                      0.8785, 0.9557, 0.9321, 0.9218, 0.9241, 0.9454, 0.9263, 0.9024, 0.9249,\n                      0.8992, 0.8961, 0.9316, 0.9270, 0.8378, 0.9079, 0.9102, 0.8903, 0.9059,\n                      0.9652, 0.9168, 0.8845, 0.9024, 0.8521, 0.8454, 0.9894, 0.8853, 0.9082,\n                      0.9130, 0.9097, 0.9409, 0.9334, 0.8982, 0.8966, 0.9473, 0.9492, 0.8950,\n                      0.8804, 0.9685, 0.9176, 0.8356, 0.9227, 0.9063, 0.9208, 0.8990, 0.9115,\n                      0.9294, 0.8926, 0.9146, 0.9107, 0.9711, 0.9042, 0.9163, 0.8628, 0.8929,\n                      0.8982, 0.9360, 0.9264, 0.9310, 0.8829, 0.8950, 0.8821, 0.9311, 0.9735,\n                      0.9475, 0.9535, 0.9838, 0.9395, 0.9275, 0.9321, 0.9212, 0.9400, 0.8615,\n                      0.8938, 0.9617, 0.9258, 0.9362, 0.9444, 0.9191, 0.9039, 0.8322, 0.9280,\n                      0.9223, 0.8765, 0.9249, 0.9270, 0.9228, 0.8281, 0.9373, 0.9116, 0.9296,\n                      0.9291, 0.9414, 0.9507, 0.9211, 0.9416, 0.9425, 0.8845, 0.9472, 0.9219,\n                      0.9249, 0.9548, 0.9387, 0.8756, 0.8875, 0.9225, 0.9115, 0.9676, 0.9347,\n                      0.9333, 0.9106, 0.9049, 0.8986, 0.8769, 0.9458, 0.9188, 0.9119, 0.9310,\n                      0.9428, 0.9440, 0.9118, 0.9297, 0.9283, 0.9128, 0.9055, 0.9096, 0.9486,\n                      0.9495, 0.9290, 0.9514, 0.9796, 0.9174, 0.9114, 0.9466, 0.9489, 0.9075,\n                      0.9092, 0.9075, 0.9098, 0.8633, 0.9450, 0.9090, 0.9081, 0.9203, 0.9275,\n                      0.9480, 0.9290, 0.8779, 0.9354, 0.9937, 0.9452, 0.8455, 0.9210, 0.8965,\n                      0.9028, 0.8765, 0.9057, 0.9111, 0.9092, 0.9327, 0.9617, 0.9275, 0.9078,\n                      0.9190, 0.8949, 0.9114, 0.9009, 0.8922, 0.9896, 1.0013, 0.9361, 0.9212,\n                      0.9395, 0.9542, 0.8781, 0.9413, 0.8927, 0.9102, 0.9094, 0.9047, 0.8752,\n                      0.9681, 0.9339, 0.9292, 0.9407, 0.9243, 0.9349, 0.9080, 0.9076, 0.9174,\n                      0.9191, 0.8673, 0.9848, 0.8882, 0.9103, 0.9208, 0.9413, 0.9158, 0.8476,\n                      0.8964, 0.9200, 0.8904, 0.9290, 0.9203, 0.9501, 0.8914, 0.8954, 0.9325,\n                      0.9455, 0.9050, 0.9577, 0.9061, 0.9174, 0.9180, 0.9145, 0.9705, 0.8866,\n                      0.9406, 0.8983, 0.9004, 0.9209, 0.9390, 0.9464, 0.9295, 0.9410, 0.9457,\n                      0.9022, 0.9044, 0.8905, 0.9621, 0.8887, 1.0104, 0.9090, 0.8913, 0.8929,\n                      0.9431, 0.9373, 0.9194, 0.9687, 0.9470, 0.9541, 0.9205, 0.9096, 0.9512,\n                      0.9096, 0.9380, 0.9126, 0.9271, 0.7703, 0.9218, 0.9302, 0.9034, 0.9117,\n                      0.9142, 0.8554, 0.8790, 0.9894, 0.9475, 0.9002, 0.9236, 0.8634])),\n             ('mnet.classifier.5.bias',\n              tensor([-8.7838e-03, -5.2742e-02, -2.8794e-03,  2.4644e-02, -5.8685e-02,\n                       9.5890e-02, -5.1127e-02, -6.0708e-03,  3.0013e-02, -3.5594e-02,\n                      -5.4559e-03,  2.4435e-02,  1.0114e-02,  5.7557e-02,  5.6508e-02,\n                      -2.5088e-02, -5.3854e-02,  6.8533e-02,  1.8807e-02,  6.3211e-02,\n                      -4.0799e-02,  3.5538e-02,  1.7984e-02, -4.7677e-02,  3.4034e-02,\n                       5.7981e-02, -2.5636e-02,  1.9948e-02, -4.0535e-03, -6.1500e-02,\n                       3.4360e-03,  1.8522e-02, -5.1127e-02, -5.4960e-02, -3.5946e-02,\n                       7.2367e-02,  3.9354e-02,  6.6458e-02, -3.0040e-04, -3.6159e-02,\n                      -6.4688e-02,  5.6752e-02, -5.1875e-02,  6.1220e-02,  1.0639e-01,\n                      -4.4552e-02, -8.2294e-02,  4.2086e-02, -8.1370e-03,  1.1356e-01,\n                       3.8109e-02,  5.2648e-02, -6.8197e-03,  3.9124e-02,  2.6938e-02,\n                      -9.3570e-03, -2.7080e-02, -1.8257e-02,  5.5112e-02,  2.4246e-02,\n                       6.2869e-02,  2.1245e-02,  5.5893e-02, -2.3375e-02,  5.8719e-02,\n                      -1.7328e-02, -2.8139e-02, -5.3234e-02,  1.3560e-02,  9.1338e-03,\n                      -1.9133e-02, -8.1694e-02,  3.0193e-02,  7.6361e-02,  6.8714e-02,\n                      -6.5081e-02, -4.9647e-03,  4.0862e-02, -2.1544e-02,  3.0264e-02,\n                      -1.3192e-03,  1.5285e-02, -2.3928e-03,  7.4453e-02,  5.4000e-02,\n                      -1.0278e-02,  9.6392e-03,  3.4554e-02,  2.9425e-02, -5.3628e-02,\n                       8.2771e-02,  7.0596e-02,  8.7528e-03, -1.7924e-02, -7.8074e-03,\n                      -1.7731e-02, -7.5041e-02,  7.7205e-04,  6.0469e-02,  8.4650e-02,\n                      -4.0509e-02, -7.6159e-03,  5.4232e-02, -1.4712e-02, -4.7936e-02,\n                       2.2639e-02, -4.9350e-02, -3.8937e-02, -5.2154e-02, -3.0575e-02,\n                       4.7671e-02,  4.4800e-02, -3.4255e-02,  6.0661e-02, -1.3234e-02,\n                       3.3818e-02,  4.0897e-03,  5.9151e-02, -2.1344e-02,  2.6983e-02,\n                       4.1740e-02, -3.5385e-02,  4.9551e-02,  4.4075e-02,  1.8776e-02,\n                       4.5749e-02,  2.2008e-02,  5.8477e-03,  4.7894e-02, -3.7170e-02,\n                      -1.9665e-02, -2.8715e-02, -2.9515e-02,  1.7116e-02, -3.6012e-03,\n                       2.5538e-02, -5.0614e-02, -3.5090e-02, -8.2529e-04, -1.1163e-03,\n                       4.8953e-02,  7.4778e-02,  3.3453e-02, -3.3375e-02,  4.7077e-03,\n                      -1.5722e-03,  1.5925e-02, -3.0323e-02,  5.5120e-02,  6.8218e-02,\n                       4.6597e-02,  5.8176e-02, -3.1919e-02, -4.8670e-02,  7.1459e-02,\n                      -4.2058e-02,  3.2856e-02,  9.6567e-02,  2.1751e-02, -7.9270e-03,\n                      -7.7624e-02,  9.3503e-02,  2.0185e-02, -2.1125e-02,  1.7226e-02,\n                      -3.6691e-02, -3.4449e-02,  6.8916e-02, -1.4977e-03,  3.3117e-02,\n                       2.8871e-02, -4.7488e-03,  2.1288e-02,  7.8548e-02,  2.5211e-02,\n                       2.5446e-03, -4.0901e-02,  2.7146e-02, -2.1689e-02,  3.7168e-02,\n                      -2.0218e-02, -1.9649e-02,  3.1241e-02,  4.5865e-02, -2.3920e-02,\n                       3.0722e-02,  5.2829e-02,  6.6955e-02,  6.0978e-02,  9.1153e-03,\n                       3.4528e-02,  7.4956e-02, -2.4038e-02,  4.3183e-02,  6.7485e-02,\n                      -1.5109e-02,  9.4254e-02,  1.0044e-02,  8.8892e-03,  5.6914e-02,\n                       7.2227e-03,  3.6959e-02,  2.1514e-02,  2.9897e-03, -4.8973e-02,\n                       2.5992e-02,  4.0056e-02, -1.9900e-02, -1.8495e-02, -3.4403e-02,\n                      -7.1602e-02, -5.2248e-02,  1.0533e-01,  6.7320e-02, -7.5825e-03,\n                       9.1746e-02,  3.9782e-02, -2.9119e-03, -3.5710e-02, -3.1715e-02,\n                      -4.2589e-02, -3.2866e-02, -7.3156e-02,  1.9275e-02,  1.9377e-03,\n                      -4.5707e-02, -9.2497e-03,  2.0749e-02, -5.2155e-02, -3.0181e-02,\n                       1.8199e-02, -8.9459e-03, -8.0625e-02, -8.9463e-02,  2.6782e-02,\n                      -1.0262e-01, -2.3582e-02,  6.0722e-02, -1.2506e-02,  1.7510e-03,\n                       1.0075e-01, -3.7780e-02,  4.7185e-02, -1.2026e-02, -3.3098e-02,\n                       1.1832e-01,  4.8323e-03, -4.4227e-02,  2.3500e-02, -1.0456e-02,\n                       5.4883e-02,  6.2711e-02, -3.5063e-03,  1.4436e-02,  3.6042e-02,\n                       4.3010e-02,  4.8947e-02,  1.0212e-01, -5.4772e-02,  5.8506e-02,\n                       4.1010e-02, -4.5124e-02, -2.3776e-02,  9.6631e-02,  3.1139e-02,\n                       4.4846e-02,  5.0066e-02,  2.9710e-02,  1.8223e-03,  3.9205e-02,\n                       7.2714e-02,  5.2201e-02, -2.0290e-02,  4.8492e-02, -6.4555e-03,\n                       9.4065e-02, -1.4684e-03, -4.8090e-02, -5.3984e-02, -1.0661e-02,\n                       3.2348e-02, -3.5589e-02,  2.4676e-02,  1.0464e-02,  7.1835e-03,\n                       9.8961e-03, -1.9897e-02, -3.9851e-02, -3.3581e-03, -9.0593e-02,\n                      -9.4178e-02,  7.9729e-02, -1.1878e-02,  1.1091e-02, -3.1076e-02,\n                       1.7988e-02,  3.7781e-02,  1.1916e-05, -5.8059e-02,  1.4457e-02,\n                       1.0283e-02, -7.2905e-02,  1.6150e-02,  9.1284e-02,  3.4065e-02,\n                       1.8226e-02,  9.3635e-04,  1.3227e-02, -7.7630e-04,  1.7136e-02,\n                       1.4334e-02,  6.2340e-02, -1.4472e-02, -5.6601e-02,  1.8521e-02,\n                       5.8148e-02,  1.2224e-02,  1.0582e-01, -7.4172e-03,  2.2109e-02,\n                      -8.1831e-02,  9.6454e-03,  3.3114e-03, -2.0664e-02,  4.8711e-02,\n                      -5.5907e-02, -2.8141e-02, -9.0274e-02,  4.9294e-02,  2.9735e-02,\n                       1.5033e-02,  6.0961e-03, -3.5813e-02,  1.4470e-02, -2.6152e-02,\n                      -3.7893e-02,  8.7899e-03,  1.0947e-02, -7.8914e-03,  9.1435e-03,\n                      -2.9506e-02, -3.4923e-03, -6.5662e-02,  2.4183e-02,  6.1062e-02,\n                      -9.2902e-03,  7.8784e-02,  1.2856e-01,  2.9835e-02,  6.0433e-02,\n                       9.4148e-03,  3.0853e-02,  4.6050e-02,  6.5851e-02,  1.0072e-02,\n                       3.0127e-02,  6.1857e-03, -1.5429e-02, -7.2945e-02,  2.2653e-02,\n                       4.7391e-02, -4.4356e-02,  1.6737e-03, -1.3283e-02,  8.5898e-02,\n                       8.8126e-02,  5.2081e-02, -5.0090e-02, -4.7345e-03,  1.6245e-02,\n                       3.0045e-02, -3.8723e-02, -9.0539e-03,  2.8625e-02,  5.3309e-02,\n                       3.2633e-02, -1.8511e-03, -3.1641e-02,  7.3105e-02, -3.1967e-02,\n                       2.5219e-02, -1.6640e-02, -4.0782e-02, -6.1360e-02,  1.7561e-02,\n                      -2.7039e-03,  6.3651e-02, -1.5458e-02,  6.0219e-02,  1.6241e-02,\n                      -6.2586e-02,  4.2246e-02, -1.8383e-02,  1.8310e-02, -4.5109e-02,\n                       3.8903e-02,  5.6906e-02, -8.0907e-02, -7.0986e-03,  4.8078e-02,\n                       2.4803e-02,  3.7460e-02,  3.6609e-02,  2.5837e-02, -6.6459e-02,\n                       2.7786e-02, -4.5030e-02,  5.3068e-02, -1.5042e-02, -2.6421e-03,\n                      -3.6207e-03,  7.8253e-02, -7.1836e-02,  7.9691e-02,  5.7505e-02,\n                       1.0726e-03,  2.1813e-02,  1.4098e-02,  5.3002e-02,  6.1047e-02,\n                       8.4898e-02, -4.4867e-03, -4.2710e-02, -3.8668e-02,  7.2561e-04,\n                      -3.9426e-02,  4.9986e-02, -1.6219e-02,  1.1605e-02, -2.0877e-02,\n                       4.4510e-02,  1.9579e-02,  3.0135e-02,  3.0132e-02, -2.1181e-02,\n                       1.7130e-02, -3.8891e-03, -1.6616e-02, -6.1070e-02,  3.6369e-02,\n                      -8.9148e-02,  3.0556e-02,  4.4777e-02,  1.9054e-03,  1.7361e-03,\n                       3.0679e-02,  3.2653e-03, -8.1128e-02,  4.3419e-03,  7.8688e-02,\n                       2.7036e-02,  1.1153e-02, -2.5156e-02,  1.0845e-01, -5.2344e-02,\n                       4.1487e-02, -4.9379e-02, -2.2809e-02, -9.4824e-07,  6.2299e-02,\n                       3.6489e-02,  3.4962e-02,  7.4431e-02, -3.5390e-02,  7.2926e-02,\n                       5.8870e-02, -3.8965e-03, -1.7318e-02,  3.9262e-02, -2.7825e-03,\n                       1.2793e-02, -3.9344e-02,  1.4326e-02, -3.7579e-02, -5.1754e-02,\n                      -5.8556e-02,  9.1836e-02,  5.6772e-02,  4.1220e-02,  5.1571e-02,\n                      -8.6306e-03,  1.2947e-02, -5.3115e-02,  4.4824e-02, -1.8481e-02,\n                       9.8266e-03, -1.9695e-02,  2.0333e-02, -4.3590e-02,  3.0779e-02,\n                      -9.3821e-03, -9.1422e-02, -4.0459e-02,  4.2825e-02,  2.4992e-02,\n                       2.4784e-02, -1.7203e-02, -5.2772e-02,  2.8280e-02,  1.0813e-01,\n                      -1.6302e-02,  8.8948e-03,  8.9644e-02,  7.2065e-04,  1.6349e-02,\n                       2.4814e-02,  1.2436e-02,  5.6688e-02, -2.9702e-02, -2.5289e-03,\n                       5.4003e-02,  7.0747e-02])),\n             ('mnet.classifier.5.running_mean',\n              tensor([1.3878, 1.2410, 1.1299, 1.6530, 0.9777, 1.2636, 1.6185, 0.9958, 1.4265,\n                      0.9331, 0.9675, 1.5570, 1.4480, 1.1139, 1.4427, 0.8524, 1.2068, 1.0687,\n                      1.4417, 1.2367, 1.4494, 1.1688, 1.0597, 0.9956, 1.1590, 1.4857, 1.4617,\n                      1.4143, 1.4896, 1.0146, 1.2700, 1.1371, 0.8447, 1.2015, 1.3010, 1.4858,\n                      1.0084, 1.8400, 1.5146, 1.1152, 0.7932, 1.0220, 0.9420, 1.3645, 1.2722,\n                      1.0689, 0.9658, 1.6222, 1.2655, 1.6469, 1.0357, 1.4417, 1.4622, 1.1730,\n                      1.2983, 0.9531, 0.9863, 0.7929, 1.1298, 0.9653, 1.0244, 1.1129, 1.2154,\n                      1.5051, 1.7358, 1.4841, 0.7423, 1.4254, 0.9328, 0.9487, 0.9087, 1.0228,\n                      1.1477, 1.2629, 1.2864, 0.8559, 1.0114, 0.7627, 0.7676, 1.3465, 1.0081,\n                      1.0350, 1.3784, 1.3023, 1.1022, 0.9602, 1.0886, 1.1986, 1.6516, 1.2413,\n                      1.0391, 1.0138, 1.2936, 1.1175, 1.5583, 1.2685, 1.1438, 1.5997, 0.9614,\n                      1.2233, 1.1421, 1.5534, 0.9538, 1.2573, 1.5435, 1.3218, 1.5204, 1.1812,\n                      1.0703, 0.8614, 1.1536, 1.5277, 0.9788, 1.5004, 1.1557, 1.1465, 1.0451,\n                      2.1585, 1.7903, 1.2590, 1.2798, 0.8552, 1.5457, 1.1715, 1.2030, 1.4226,\n                      1.3898, 1.5284, 1.2721, 0.7490, 1.3317, 0.9828, 1.0519, 0.9640, 0.8315,\n                      1.9881, 0.7661, 1.4878, 1.3934, 1.3954, 1.4255, 1.3268, 1.1794, 0.9267,\n                      1.4648, 1.5117, 1.1481, 1.7263, 1.4044, 1.2354, 0.9159, 1.4327, 1.1729,\n                      1.4490, 1.0704, 1.3883, 1.3809, 1.3363, 0.9823, 0.8986, 1.1556, 1.1252,\n                      1.2332, 0.8963, 1.2811, 0.9152, 1.6840, 0.9720, 1.0800, 1.0924, 1.1361,\n                      1.1841, 0.7031, 1.4406, 1.5307, 1.6254, 1.1767, 0.8801, 0.9405, 1.5773,\n                      1.1887, 0.7686, 1.4682, 1.1734, 1.2684, 1.0459, 1.2784, 1.2701, 1.1209,\n                      1.5165, 1.1394, 0.9047, 1.3946, 1.1447, 1.1161, 1.2824, 1.3335, 1.0142,\n                      1.2211, 1.3714, 1.3585, 1.1255, 1.2475, 0.9358, 1.1940, 1.2291, 1.4091,\n                      1.0550, 1.7226, 0.9676, 1.0114, 1.6107, 1.4592, 1.1379, 1.2162, 1.2022,\n                      1.3063, 1.6605, 0.9058, 0.9428, 0.9747, 0.9857, 0.8422, 0.8398, 1.6563,\n                      1.1785, 1.3264, 1.1956, 0.9327, 0.8023, 1.1863, 1.3780, 1.1217, 1.1085,\n                      1.0237, 0.9852, 1.1854, 0.9298, 0.6873, 1.2539, 1.4274, 1.1793, 0.9571,\n                      1.5225, 0.8274, 1.1286, 0.8950, 0.9333, 1.0895, 1.3486, 1.2443, 1.5327,\n                      1.2728, 0.8429, 1.6571, 1.1183, 1.5076, 1.1577, 1.1521, 1.0526, 1.0764,\n                      1.1304, 1.2648, 1.4466, 1.4072, 0.9979, 1.0886, 1.4673, 1.4012, 1.1098,\n                      0.8864, 0.8185, 1.1308, 1.3787, 1.0129, 1.1243, 1.0202, 0.9754, 1.3034,\n                      1.3591, 1.4629, 1.5596, 1.3439, 1.2900, 1.3481, 1.4640, 1.2695, 1.2166,\n                      0.6562, 0.5614, 0.7833, 1.0619, 1.3934, 1.2300, 0.7646, 1.3958, 1.4767,\n                      1.6782, 0.7462, 0.9516, 1.4635, 1.0753, 1.2945, 1.5667, 1.0222, 1.1804,\n                      1.2054, 1.7226, 1.3212, 0.8563, 1.3243, 1.4338, 1.1066, 1.5050, 1.1000,\n                      1.3061, 1.4143, 1.0604, 1.4379, 1.3039, 1.0078, 0.9555, 1.3821, 0.8645,\n                      0.9970, 0.9138, 1.3139, 1.2846, 1.3883, 1.3148, 1.3211, 1.2187, 1.1142,\n                      1.2863, 0.9904, 0.7835, 0.8522, 1.3199, 0.9636, 1.2888, 1.2373, 1.0574,\n                      0.8328, 0.6650, 1.2345, 1.2643, 1.3422, 1.0207, 1.3590, 1.1380, 1.0143,\n                      1.6174, 1.3391, 1.1255, 1.1809, 1.0260, 1.1674, 0.8194, 0.9294, 1.1301,\n                      1.2185, 1.2288, 1.2192, 0.4489, 0.9960, 1.3077, 1.2839, 1.3117, 1.2622,\n                      1.4137, 0.8057, 1.0321, 1.2816, 1.2047, 1.4284, 1.5003, 1.3451, 0.7179,\n                      1.4358, 1.1557, 1.0627, 0.8151, 1.0112, 1.3221, 1.3626, 1.1964, 1.1483,\n                      1.2162, 1.4627, 1.3408, 1.1529, 1.5003, 1.1280, 1.2164, 0.7765, 1.1338,\n                      0.7928, 0.9107, 1.0263, 0.8919, 1.1546, 1.2344, 1.4513, 1.1904, 1.0624,\n                      1.0937, 1.0604, 1.5261, 1.1261, 1.3081, 1.0321, 1.1645, 1.1481, 1.0154,\n                      0.9205, 1.4287, 1.2397, 1.1682, 1.6756, 0.9941, 0.9708, 1.0416, 1.0349,\n                      1.4481, 1.1364, 1.6159, 1.2258, 0.9061, 1.2633, 0.8757, 1.0846, 1.0098,\n                      1.2714, 0.7685, 0.9752, 1.1900, 1.6075, 1.1011, 1.6499, 1.3737, 1.5367,\n                      1.2309, 1.0720, 1.0540, 0.8364, 0.7460, 0.9872, 1.1281, 1.3717, 1.1076,\n                      1.1700, 1.1443, 1.2398, 1.5411, 0.9065, 0.8676, 1.3716, 1.0634, 1.3794,\n                      1.3724, 1.4875, 1.0203, 1.3828, 0.9114, 1.1105, 1.1912, 1.2570, 1.3390,\n                      1.2432, 1.3916, 1.7440, 1.7288, 1.1055, 0.9131, 1.4370, 1.2125, 1.3214,\n                      1.1151, 0.7999, 1.0345, 1.4433, 1.0996, 1.0107, 1.2317, 1.4310, 0.7272,\n                      1.2133, 1.2036, 1.3977, 0.7340, 1.8757, 1.0043, 0.9428, 1.0107, 1.1140,\n                      0.9884, 1.1759, 0.8305, 1.1601, 0.6646, 1.3008, 0.8225, 1.5333, 1.3140,\n                      1.3212, 1.1437, 1.2701, 0.6522, 1.3283, 1.2808, 1.2684, 1.0027])),\n             ('mnet.classifier.5.running_var',\n              tensor([3.1638, 3.6171, 2.9332, 4.6322, 2.9096, 3.3464, 5.0504, 2.9630, 3.2911,\n                      2.1417, 2.5731, 4.2465, 4.2174, 2.8530, 3.7750, 2.3283, 4.4566, 2.9768,\n                      5.5274, 3.8704, 4.0727, 2.6612, 3.6866, 3.0453, 3.0577, 3.9469, 3.8561,\n                      4.2866, 4.9484, 3.8253, 3.0813, 3.1247, 1.7323, 3.8140, 3.0022, 4.1067,\n                      3.4827, 4.6683, 5.5256, 3.0046, 2.1198, 3.2927, 2.8379, 4.6082, 3.7931,\n                      2.7529, 2.8357, 4.9424, 2.9806, 5.6150, 2.3904, 4.3198, 4.5374, 3.6269,\n                      4.0790, 2.7786, 2.7708, 1.8114, 2.9504, 2.5248, 2.5544, 2.9300, 3.7654,\n                      5.4397, 5.6049, 4.4870, 1.8829, 3.8572, 2.3092, 2.3085, 2.0576, 3.0456,\n                      3.0986, 3.9051, 3.0538, 2.2285, 2.6291, 1.3550, 1.4826, 2.9932, 2.4402,\n                      2.6527, 3.4474, 3.4896, 3.0326, 2.5754, 2.6357, 3.8518, 3.8560, 4.2273,\n                      3.3093, 2.3279, 3.9566, 3.1783, 4.4554, 4.9433, 3.9192, 4.6048, 3.7876,\n                      3.8834, 2.8668, 6.3604, 2.3294, 3.4819, 5.0821, 4.4077, 4.4416, 3.0046,\n                      2.6627, 1.9663, 4.1846, 3.9664, 2.6764, 5.8493, 3.6566, 2.8791, 2.9517,\n                      6.8002, 6.8032, 3.1298, 3.2782, 2.6184, 4.5778, 2.8335, 3.5533, 4.9988,\n                      4.3115, 3.9417, 3.6608, 1.6538, 3.4747, 2.3800, 3.3270, 2.5717, 1.9263,\n                      5.5944, 2.2590, 3.7390, 4.3841, 5.0390, 4.3774, 3.4440, 3.9250, 2.3439,\n                      4.6428, 5.4279, 3.2000, 4.8935, 4.3680, 3.4659, 2.1275, 4.0313, 3.9377,\n                      4.5496, 3.5555, 5.5409, 4.7046, 4.5274, 2.7700, 2.2535, 4.2622, 2.7157,\n                      4.0378, 2.2670, 3.4015, 2.5849, 5.1312, 2.4889, 3.6375, 2.2970, 2.8740,\n                      4.5722, 1.5798, 5.1846, 4.7281, 5.1222, 3.0440, 2.4413, 2.7737, 3.9323,\n                      3.6703, 1.6640, 4.3072, 3.3038, 4.2106, 3.1395, 3.0690, 3.4755, 3.4400,\n                      5.3275, 3.0986, 2.4219, 4.6283, 3.5925, 3.4249, 4.2922, 4.2332, 2.5010,\n                      3.5648, 3.3238, 4.1099, 3.2756, 3.8365, 2.1582, 3.3831, 3.1496, 4.3050,\n                      3.1884, 5.1013, 2.0400, 2.3635, 4.7336, 3.3025, 2.9277, 4.1134, 3.0525,\n                      3.5422, 4.5538, 2.2142, 2.6299, 2.5653, 3.1104, 2.6226, 1.7822, 4.7766,\n                      3.2252, 4.8673, 2.5217, 2.2646, 1.9794, 3.8618, 3.8453, 2.9002, 3.4421,\n                      3.1290, 2.5874, 3.2414, 2.1955, 1.4977, 4.1360, 3.0767, 3.8421, 2.6188,\n                      5.1950, 1.8172, 4.0166, 2.4538, 2.6778, 3.4473, 3.7311, 2.8052, 4.6572,\n                      2.4832, 1.9693, 4.7657, 3.2950, 3.6008, 2.8909, 4.0262, 2.3762, 2.9560,\n                      3.4692, 4.7763, 5.0117, 3.9576, 2.1105, 2.7404, 4.8274, 3.9983, 3.1292,\n                      2.0462, 2.2746, 2.6159, 5.3577, 2.6399, 3.2190, 2.6588, 2.4657, 4.4488,\n                      4.8263, 4.1564, 4.2688, 3.7425, 3.8747, 3.3520, 3.7066, 4.2609, 3.4723,\n                      1.6187, 1.2968, 1.6400, 3.9182, 4.8669, 2.9040, 2.0731, 4.0599, 5.3874,\n                      4.6516, 1.9677, 2.4426, 4.6433, 3.0119, 4.0060, 5.3799, 3.6700, 3.3812,\n                      3.9278, 7.1252, 4.4487, 2.6119, 3.8512, 3.4699, 2.2226, 4.8023, 2.8205,\n                      3.8513, 4.5137, 2.9477, 3.7047, 3.9455, 3.0671, 2.3410, 3.7888, 1.7442,\n                      2.4450, 2.5293, 5.8214, 4.0330, 4.0432, 3.7948, 3.4124, 2.8611, 3.4561,\n                      3.6555, 2.8710, 1.7605, 2.6521, 3.6260, 2.3884, 4.7040, 3.5915, 2.5945,\n                      1.9688, 1.4005, 3.7635, 4.5235, 3.9418, 3.2242, 3.3365, 2.8444, 2.9394,\n                      5.7722, 4.1303, 4.0635, 3.5522, 2.8613, 2.6347, 1.9011, 2.6123, 4.2270,\n                      3.7603, 3.0524, 2.8956, 1.0731, 2.3052, 4.3680, 3.7429, 3.6449, 2.6706,\n                      3.8913, 2.7306, 3.4295, 4.4450, 3.4222, 4.0560, 4.6177, 4.9357, 1.9229,\n                      3.7828, 3.1524, 2.4015, 2.1284, 2.6761, 3.6398, 3.9069, 2.4377, 3.7673,\n                      2.4819, 3.4563, 3.3038, 2.5444, 4.4388, 3.2685, 3.1613, 1.9289, 3.0231,\n                      1.8492, 2.8276, 2.9390, 2.1954, 3.4977, 4.2752, 4.6817, 4.0706, 3.0748,\n                      2.5800, 2.7972, 4.8127, 3.5336, 4.8804, 2.9080, 2.8319, 3.7822, 3.1944,\n                      2.3639, 4.4920, 3.1864, 3.9705, 5.0251, 3.0063, 2.9066, 3.3877, 2.9455,\n                      4.1785, 3.6180, 4.8563, 3.3545, 2.5399, 4.0169, 2.4001, 2.9692, 3.0963,\n                      3.8742, 1.7732, 3.1096, 3.8021, 4.0959, 3.0820, 4.7680, 3.4924, 4.6576,\n                      3.4245, 2.2906, 3.5209, 1.9286, 1.6332, 2.9383, 4.4486, 4.1891, 2.8206,\n                      3.8171, 3.3617, 3.5304, 4.4605, 2.5131, 2.2243, 4.3981, 3.3977, 4.4650,\n                      4.8162, 5.1029, 2.9226, 3.5470, 2.1799, 2.7816, 3.1847, 3.4080, 3.6681,\n                      3.3074, 3.6998, 5.2458, 4.4464, 3.6336, 2.0734, 4.7306, 3.9254, 4.7763,\n                      3.1467, 2.3404, 2.4458, 3.5683, 2.2693, 2.8191, 2.5846, 4.2273, 1.5315,\n                      4.7007, 4.3719, 3.9427, 1.6659, 6.2802, 2.6951, 2.5374, 2.6431, 3.2661,\n                      3.3672, 3.5118, 2.0399, 3.0309, 1.5876, 4.5018, 1.7122, 5.1646, 3.4030,\n                      2.6630, 3.1962, 3.2200, 1.8611, 4.6876, 4.0548, 5.1920, 2.0426])),\n             ('mnet.classifier.5.num_batches_tracked', tensor(1890)),\n             ('mnet.classifier.6.scale', tensor(0.3951)),\n             ('mnet.classifier.6.zero_point', tensor(60)),\n             ('mnet.classifier.6._packed_params.dtype', torch.qint8),\n             ('mnet.classifier.6._packed_params._packed_params',\n              (tensor([[ 0.0013, -0.0227, -0.0854,  ..., -0.0447,  0.0060, -0.0040],\n                       [ 0.0141, -0.0185, -0.0466,  ...,  0.0178,  0.0548, -0.0015],\n                       [-0.0250, -0.0807,  0.0137,  ..., -0.0218, -0.0524, -0.0186],\n                       ...,\n                       [-0.0396, -0.0202,  0.0104,  ...,  0.0202, -0.0090, -0.0537],\n                       [ 0.0008, -0.0380, -0.0171,  ...,  0.0458,  0.0016,  0.0109],\n                       [ 0.0011,  0.0372, -0.0383,  ...,  0.0394,  0.0208, -0.0415]],\n                      size=(10, 512), dtype=torch.qint8,\n                      quantization_scheme=torch.per_channel_affine,\n                      scale=tensor([0.0007, 0.0007, 0.0008, 0.0007, 0.0007, 0.0008, 0.0008, 0.0007, 0.0008,\n                       0.0011], dtype=torch.float64),\n                      zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), axis=0),\n               Parameter containing:\n               tensor([ 0.0083, -0.0883, -0.0161,  0.1247, -0.0148, -0.0776, -0.0073, -0.0525,\n                        0.0936, -0.0569], requires_grad=True)))])"},"metadata":{}}]},{"cell_type":"code","source":"model_quantized_static","metadata":{"execution":{"iopub.status.busy":"2024-04-07T12:07:51.882710Z","iopub.execute_input":"2024-04-07T12:07:51.883149Z","iopub.status.idle":"2024-04-07T12:07:51.915446Z","shell.execute_reply.started":"2024-04-07T12:07:51.883109Z","shell.execute_reply":"2024-04-07T12:07:51.914598Z"},"trusted":true},"execution_count":34,"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"GraphModule(\n  (mnet): Module(\n    (features): Module(\n      (0): Module(\n        (0): QuantizedConv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), scale=0.0719936266541481, zero_point=44, padding=(1, 1))\n        (2): ReLU6(inplace=True)\n      )\n      (1): Module(\n        (conv): Module(\n          (0): Module(\n            (0): QuantizedConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), scale=0.20127829909324646, zero_point=70, padding=(1, 1), groups=32)\n            (2): ReLU6(inplace=True)\n          )\n          (1): QuantizedConv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), scale=0.09558489918708801, zero_point=63)\n        )\n      )\n      (2): Module(\n        (conv): Module(\n          (0): Module(\n            (0): QuantizedConv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), scale=0.08706456422805786, zero_point=66)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Module(\n            (0): QuantizedConv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), scale=0.081089548766613, zero_point=76, padding=(1, 1), groups=96)\n            (2): ReLU6(inplace=True)\n          )\n          (2): QuantizedConv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), scale=0.07530419528484344, zero_point=54)\n        )\n      )\n      (3): Module(\n        (conv): Module(\n          (0): Module(\n            (0): QuantizedConv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), scale=0.04361468926072121, zero_point=83)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Module(\n            (0): QuantizedConv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), scale=0.04012293741106987, zero_point=64, padding=(1, 1), groups=144)\n            (2): ReLU6(inplace=True)\n          )\n          (2): QuantizedConv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), scale=0.0649549812078476, zero_point=60)\n        )\n      )\n      (4): Module(\n        (conv): Module(\n          (0): Module(\n            (0): QuantizedConv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), scale=0.03819998353719711, zero_point=71)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Module(\n            (0): QuantizedConv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), scale=0.04489310830831528, zero_point=69, padding=(1, 1), groups=144)\n            (2): ReLU6(inplace=True)\n          )\n          (2): QuantizedConv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), scale=0.05144772678613663, zero_point=67)\n        )\n      )\n      (5): Module(\n        (conv): Module(\n          (0): Module(\n            (0): QuantizedConv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), scale=0.015841307118535042, zero_point=60)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Module(\n            (0): QuantizedConv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), scale=0.027735481038689613, zero_point=72, padding=(1, 1), groups=192)\n            (2): ReLU6(inplace=True)\n          )\n          (2): QuantizedConv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), scale=0.04369690641760826, zero_point=62)\n        )\n      )\n      (6): Module(\n        (conv): Module(\n          (0): Module(\n            (0): QuantizedConv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), scale=0.015200091525912285, zero_point=59)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Module(\n            (0): QuantizedConv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), scale=0.026249559596180916, zero_point=70, padding=(1, 1), groups=192)\n            (2): ReLU6(inplace=True)\n          )\n          (2): QuantizedConv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), scale=0.036040402948856354, zero_point=64)\n        )\n      )\n      (7): Module(\n        (conv): Module(\n          (0): Module(\n            (0): QuantizedConv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), scale=0.02446167543530464, zero_point=63)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Module(\n            (0): QuantizedConv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), scale=0.026697395369410515, zero_point=55, padding=(1, 1), groups=192)\n            (2): ReLU6(inplace=True)\n          )\n          (2): QuantizedConv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), scale=0.04494471848011017, zero_point=66)\n        )\n      )\n      (8): Module(\n        (conv): Module(\n          (0): Module(\n            (0): QuantizedConv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), scale=0.012286062352359295, zero_point=60)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Module(\n            (0): QuantizedConv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), scale=0.02403748407959938, zero_point=75, padding=(1, 1), groups=384)\n            (2): ReLU6(inplace=True)\n          )\n          (2): QuantizedConv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), scale=0.032516613602638245, zero_point=62)\n        )\n      )\n      (9): Module(\n        (conv): Module(\n          (0): Module(\n            (0): QuantizedConv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), scale=0.010706446133553982, zero_point=65)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Module(\n            (0): QuantizedConv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), scale=0.021441331133246422, zero_point=65, padding=(1, 1), groups=384)\n            (2): ReLU6(inplace=True)\n          )\n          (2): QuantizedConv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), scale=0.022711729630827904, zero_point=66)\n        )\n      )\n      (10): Module(\n        (conv): Module(\n          (0): Module(\n            (0): QuantizedConv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), scale=0.01208992674946785, zero_point=68)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Module(\n            (0): QuantizedConv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), scale=0.039125725626945496, zero_point=51, padding=(1, 1), groups=384)\n            (2): ReLU6(inplace=True)\n          )\n          (2): QuantizedConv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), scale=0.055386580526828766, zero_point=75)\n        )\n      )\n      (11): Module(\n        (conv): Module(\n          (0): Module(\n            (0): QuantizedConv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), scale=0.018563900142908096, zero_point=59)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Module(\n            (0): QuantizedConv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), scale=0.028895558789372444, zero_point=54, padding=(1, 1), groups=384)\n            (2): ReLU6(inplace=True)\n          )\n          (2): QuantizedConv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), scale=0.03806544467806816, zero_point=63)\n        )\n      )\n      (12): Module(\n        (conv): Module(\n          (0): Module(\n            (0): QuantizedConv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), scale=0.017154961824417114, zero_point=65)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Module(\n            (0): QuantizedConv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), scale=0.036653149873018265, zero_point=50, padding=(1, 1), groups=576)\n            (2): ReLU6(inplace=True)\n          )\n          (2): QuantizedConv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), scale=0.058225855231285095, zero_point=56)\n        )\n      )\n      (13): Module(\n        (conv): Module(\n          (0): Module(\n            (0): QuantizedConv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), scale=0.02161017619073391, zero_point=57)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Module(\n            (0): QuantizedConv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), scale=0.04321153834462166, zero_point=56, padding=(1, 1), groups=576)\n            (2): ReLU6(inplace=True)\n          )\n          (2): QuantizedConv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), scale=0.09675762802362442, zero_point=66)\n        )\n      )\n      (14): Module(\n        (conv): Module(\n          (0): Module(\n            (0): QuantizedConv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), scale=0.03324085474014282, zero_point=37)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Module(\n            (0): QuantizedConv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), scale=0.06324898451566696, zero_point=35, padding=(1, 1), groups=576)\n            (2): ReLU6(inplace=True)\n          )\n          (2): QuantizedConv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), scale=0.1463896930217743, zero_point=67)\n        )\n      )\n      (15): Module(\n        (conv): Module(\n          (0): Module(\n            (0): QuantizedConv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), scale=0.049375105649232864, zero_point=57)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Module(\n            (0): QuantizedConv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), scale=0.06291472166776657, zero_point=49, padding=(1, 1), groups=960)\n            (2): ReLU6(inplace=True)\n          )\n          (2): QuantizedConv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), scale=0.1237873062491417, zero_point=58)\n        )\n      )\n      (16): Module(\n        (conv): Module(\n          (0): Module(\n            (0): QuantizedConv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), scale=0.05347367376089096, zero_point=54)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Module(\n            (0): QuantizedConv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), scale=0.07768398523330688, zero_point=50, padding=(1, 1), groups=960)\n            (2): ReLU6(inplace=True)\n          )\n          (2): QuantizedConv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), scale=0.23742060363292694, zero_point=67)\n        )\n      )\n      (17): Module(\n        (conv): Module(\n          (0): Module(\n            (0): QuantizedConv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), scale=0.08795061707496643, zero_point=91)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Module(\n            (0): QuantizedConv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), scale=0.022622691467404366, zero_point=91, padding=(1, 1), groups=960)\n            (2): ReLU6(inplace=True)\n          )\n          (2): QuantizedConv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), scale=0.018744084984064102, zero_point=64)\n        )\n      )\n      (18): Module(\n        (0): QuantizedConv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), scale=0.12807367742061615, zero_point=57)\n        (2): ReLU6(inplace=True)\n      )\n    )\n    (classifier): Module(\n      (0): QuantizedLinearReLU(in_features=1280, out_features=1024, scale=0.1073458269238472, zero_point=0, qscheme=torch.per_channel_affine)\n      (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (3): QuantizedLinearReLU(in_features=1024, out_features=512, scale=0.2884138822555542, zero_point=0, qscheme=torch.per_channel_affine)\n      (5): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (6): QuantizedLinear(in_features=512, out_features=10, scale=0.3950657248497009, zero_point=60, qscheme=torch.per_channel_affine)\n      (7): LogSoftmax(dim=1)\n    )\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"model_quantized_static.to('cpu')","metadata":{"execution":{"iopub.status.busy":"2024-04-07T12:07:51.916436Z","iopub.execute_input":"2024-04-07T12:07:51.916697Z","iopub.status.idle":"2024-04-07T12:07:51.950412Z","shell.execute_reply.started":"2024-04-07T12:07:51.916675Z","shell.execute_reply":"2024-04-07T12:07:51.949515Z"},"trusted":true},"execution_count":35,"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"GraphModule(\n  (mnet): Module(\n    (features): Module(\n      (0): Module(\n        (0): QuantizedConv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), scale=0.0719936266541481, zero_point=44, padding=(1, 1))\n        (2): ReLU6(inplace=True)\n      )\n      (1): Module(\n        (conv): Module(\n          (0): Module(\n            (0): QuantizedConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), scale=0.20127829909324646, zero_point=70, padding=(1, 1), groups=32)\n            (2): ReLU6(inplace=True)\n          )\n          (1): QuantizedConv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), scale=0.09558489918708801, zero_point=63)\n        )\n      )\n      (2): Module(\n        (conv): Module(\n          (0): Module(\n            (0): QuantizedConv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), scale=0.08706456422805786, zero_point=66)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Module(\n            (0): QuantizedConv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), scale=0.081089548766613, zero_point=76, padding=(1, 1), groups=96)\n            (2): ReLU6(inplace=True)\n          )\n          (2): QuantizedConv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), scale=0.07530419528484344, zero_point=54)\n        )\n      )\n      (3): Module(\n        (conv): Module(\n          (0): Module(\n            (0): QuantizedConv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), scale=0.04361468926072121, zero_point=83)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Module(\n            (0): QuantizedConv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), scale=0.04012293741106987, zero_point=64, padding=(1, 1), groups=144)\n            (2): ReLU6(inplace=True)\n          )\n          (2): QuantizedConv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), scale=0.0649549812078476, zero_point=60)\n        )\n      )\n      (4): Module(\n        (conv): Module(\n          (0): Module(\n            (0): QuantizedConv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), scale=0.03819998353719711, zero_point=71)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Module(\n            (0): QuantizedConv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), scale=0.04489310830831528, zero_point=69, padding=(1, 1), groups=144)\n            (2): ReLU6(inplace=True)\n          )\n          (2): QuantizedConv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), scale=0.05144772678613663, zero_point=67)\n        )\n      )\n      (5): Module(\n        (conv): Module(\n          (0): Module(\n            (0): QuantizedConv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), scale=0.015841307118535042, zero_point=60)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Module(\n            (0): QuantizedConv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), scale=0.027735481038689613, zero_point=72, padding=(1, 1), groups=192)\n            (2): ReLU6(inplace=True)\n          )\n          (2): QuantizedConv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), scale=0.04369690641760826, zero_point=62)\n        )\n      )\n      (6): Module(\n        (conv): Module(\n          (0): Module(\n            (0): QuantizedConv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), scale=0.015200091525912285, zero_point=59)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Module(\n            (0): QuantizedConv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), scale=0.026249559596180916, zero_point=70, padding=(1, 1), groups=192)\n            (2): ReLU6(inplace=True)\n          )\n          (2): QuantizedConv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), scale=0.036040402948856354, zero_point=64)\n        )\n      )\n      (7): Module(\n        (conv): Module(\n          (0): Module(\n            (0): QuantizedConv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), scale=0.02446167543530464, zero_point=63)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Module(\n            (0): QuantizedConv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), scale=0.026697395369410515, zero_point=55, padding=(1, 1), groups=192)\n            (2): ReLU6(inplace=True)\n          )\n          (2): QuantizedConv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), scale=0.04494471848011017, zero_point=66)\n        )\n      )\n      (8): Module(\n        (conv): Module(\n          (0): Module(\n            (0): QuantizedConv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), scale=0.012286062352359295, zero_point=60)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Module(\n            (0): QuantizedConv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), scale=0.02403748407959938, zero_point=75, padding=(1, 1), groups=384)\n            (2): ReLU6(inplace=True)\n          )\n          (2): QuantizedConv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), scale=0.032516613602638245, zero_point=62)\n        )\n      )\n      (9): Module(\n        (conv): Module(\n          (0): Module(\n            (0): QuantizedConv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), scale=0.010706446133553982, zero_point=65)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Module(\n            (0): QuantizedConv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), scale=0.021441331133246422, zero_point=65, padding=(1, 1), groups=384)\n            (2): ReLU6(inplace=True)\n          )\n          (2): QuantizedConv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), scale=0.022711729630827904, zero_point=66)\n        )\n      )\n      (10): Module(\n        (conv): Module(\n          (0): Module(\n            (0): QuantizedConv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), scale=0.01208992674946785, zero_point=68)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Module(\n            (0): QuantizedConv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), scale=0.039125725626945496, zero_point=51, padding=(1, 1), groups=384)\n            (2): ReLU6(inplace=True)\n          )\n          (2): QuantizedConv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), scale=0.055386580526828766, zero_point=75)\n        )\n      )\n      (11): Module(\n        (conv): Module(\n          (0): Module(\n            (0): QuantizedConv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), scale=0.018563900142908096, zero_point=59)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Module(\n            (0): QuantizedConv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), scale=0.028895558789372444, zero_point=54, padding=(1, 1), groups=384)\n            (2): ReLU6(inplace=True)\n          )\n          (2): QuantizedConv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), scale=0.03806544467806816, zero_point=63)\n        )\n      )\n      (12): Module(\n        (conv): Module(\n          (0): Module(\n            (0): QuantizedConv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), scale=0.017154961824417114, zero_point=65)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Module(\n            (0): QuantizedConv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), scale=0.036653149873018265, zero_point=50, padding=(1, 1), groups=576)\n            (2): ReLU6(inplace=True)\n          )\n          (2): QuantizedConv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), scale=0.058225855231285095, zero_point=56)\n        )\n      )\n      (13): Module(\n        (conv): Module(\n          (0): Module(\n            (0): QuantizedConv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), scale=0.02161017619073391, zero_point=57)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Module(\n            (0): QuantizedConv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), scale=0.04321153834462166, zero_point=56, padding=(1, 1), groups=576)\n            (2): ReLU6(inplace=True)\n          )\n          (2): QuantizedConv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), scale=0.09675762802362442, zero_point=66)\n        )\n      )\n      (14): Module(\n        (conv): Module(\n          (0): Module(\n            (0): QuantizedConv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), scale=0.03324085474014282, zero_point=37)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Module(\n            (0): QuantizedConv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), scale=0.06324898451566696, zero_point=35, padding=(1, 1), groups=576)\n            (2): ReLU6(inplace=True)\n          )\n          (2): QuantizedConv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), scale=0.1463896930217743, zero_point=67)\n        )\n      )\n      (15): Module(\n        (conv): Module(\n          (0): Module(\n            (0): QuantizedConv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), scale=0.049375105649232864, zero_point=57)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Module(\n            (0): QuantizedConv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), scale=0.06291472166776657, zero_point=49, padding=(1, 1), groups=960)\n            (2): ReLU6(inplace=True)\n          )\n          (2): QuantizedConv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), scale=0.1237873062491417, zero_point=58)\n        )\n      )\n      (16): Module(\n        (conv): Module(\n          (0): Module(\n            (0): QuantizedConv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), scale=0.05347367376089096, zero_point=54)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Module(\n            (0): QuantizedConv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), scale=0.07768398523330688, zero_point=50, padding=(1, 1), groups=960)\n            (2): ReLU6(inplace=True)\n          )\n          (2): QuantizedConv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), scale=0.23742060363292694, zero_point=67)\n        )\n      )\n      (17): Module(\n        (conv): Module(\n          (0): Module(\n            (0): QuantizedConv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), scale=0.08795061707496643, zero_point=91)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Module(\n            (0): QuantizedConv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), scale=0.022622691467404366, zero_point=91, padding=(1, 1), groups=960)\n            (2): ReLU6(inplace=True)\n          )\n          (2): QuantizedConv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), scale=0.018744084984064102, zero_point=64)\n        )\n      )\n      (18): Module(\n        (0): QuantizedConv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), scale=0.12807367742061615, zero_point=57)\n        (2): ReLU6(inplace=True)\n      )\n    )\n    (classifier): Module(\n      (0): QuantizedLinearReLU(in_features=1280, out_features=1024, scale=0.1073458269238472, zero_point=0, qscheme=torch.per_channel_affine)\n      (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (3): QuantizedLinearReLU(in_features=1024, out_features=512, scale=0.2884138822555542, zero_point=0, qscheme=torch.per_channel_affine)\n      (5): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (6): QuantizedLinear(in_features=512, out_features=10, scale=0.3950657248497009, zero_point=60, qscheme=torch.per_channel_affine)\n      (7): LogSoftmax(dim=1)\n    )\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"all_predictions_int8 = []\nall_labels_int8 = []\ncorrect_pred = 0\ntotal_pred = 0\nstart_time_int8 = time()\nwith torch.no_grad():\n    model_quantized_static.eval()\n    for data in testloader:\n        images, labels = data\n        all_labels_int8.extend(labels.numpy())\n        #images, labels = images.to(device), labels.to(device)\n        outputs = model_quantized_static(images.to('cpu'))\n        _, predicted = torch.max(outputs.data, 1)\n        total_pred += labels.size(0)\n        correct_pred += (predicted == labels).sum().item()\n        predicted_tensor_cpu = predicted.to('cpu')\n        all_predictions_int8.extend(predicted_tensor_cpu.numpy())\nend_time_int8 = time()\nprint(\"Time: \",end_time_int8 - start_time_int8)\nprint('Accuracy achieved by the network on test images is: %d%%' % (100 * correct_pred / total_pred))","metadata":{"execution":{"iopub.status.busy":"2024-04-07T12:07:51.951349Z","iopub.execute_input":"2024-04-07T12:07:51.951596Z","iopub.status.idle":"2024-04-07T12:08:58.432679Z","shell.execute_reply.started":"2024-04-07T12:07:51.951574Z","shell.execute_reply":"2024-04-07T12:08:58.431740Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"Time:  66.47352957725525\nAccuracy achieved by the network on test images is: 53%\n","output_type":"stream"}]},{"cell_type":"code","source":"metrics(all_predictions_int8,all_labels_int8)","metadata":{"execution":{"iopub.status.busy":"2024-04-07T12:08:58.433829Z","iopub.execute_input":"2024-04-07T12:08:58.434106Z","iopub.status.idle":"2024-04-07T12:08:59.125055Z","shell.execute_reply.started":"2024-04-07T12:08:58.434082Z","shell.execute_reply":"2024-04-07T12:08:59.123987Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"Confusion Matrix:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 800x600 with 2 Axes>","image/png":"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"},"metadata":{"image/png":{"width":793,"height":689}}},{"name":"stdout","text":"----------------------------------------------------------------\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.71      0.59      0.65       765\n           1       0.39      0.85      0.53       260\n           2       0.51      0.51      0.51       592\n           3       0.29      0.40      0.34       354\n           4       0.40      0.88      0.55       223\n           5       0.34      0.64      0.45       214\n           6       0.13      0.92      0.23        74\n           7       0.88      0.54      0.67      1000\n           8       0.69      0.40      0.51       855\n           9       0.86      0.47      0.61      1063\n\n    accuracy                           0.54      5400\n   macro avg       0.52      0.62      0.50      5400\nweighted avg       0.67      0.54      0.56      5400\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### QAT","metadata":{}},{"cell_type":"markdown","source":"Load a new vgg model which is not trained before starting qat","metadata":{}},{"cell_type":"code","source":"input_data = next(iter(trainloader))[0]\ncalibrate_data = input_data.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-04-07T12:08:59.126393Z","iopub.execute_input":"2024-04-07T12:08:59.126788Z","iopub.status.idle":"2024-04-07T12:08:59.253732Z","shell.execute_reply.started":"2024-04-07T12:08:59.126753Z","shell.execute_reply":"2024-04-07T12:08:59.252729Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"model.eval()\nmodel.qconfig = torch.ao.quantization.get_default_qat_qconfig('x86')\nmodel.train()","metadata":{"execution":{"iopub.status.busy":"2024-04-07T12:08:59.255028Z","iopub.execute_input":"2024-04-07T12:08:59.255381Z","iopub.status.idle":"2024-04-07T12:08:59.268325Z","shell.execute_reply.started":"2024-04-07T12:08:59.255338Z","shell.execute_reply":"2024-04-07T12:08:59.267470Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":39,"outputs":[{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"CustomMobileNetv2(\n  (mnet): MobileNetV2(\n    (features): Sequential(\n      (0): Conv2dNormActivation(\n        (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU6(inplace=True)\n      )\n      (1): InvertedResidual(\n        (conv): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (2): InvertedResidual(\n        (conv): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (3): InvertedResidual(\n        (conv): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (4): InvertedResidual(\n        (conv): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (2): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (5): InvertedResidual(\n        (conv): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (6): InvertedResidual(\n        (conv): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (7): InvertedResidual(\n        (conv): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (8): InvertedResidual(\n        (conv): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (9): InvertedResidual(\n        (conv): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (10): InvertedResidual(\n        (conv): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (11): InvertedResidual(\n        (conv): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (12): InvertedResidual(\n        (conv): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (13): InvertedResidual(\n        (conv): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (14): InvertedResidual(\n        (conv): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)\n            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (2): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (15): InvertedResidual(\n        (conv): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (16): InvertedResidual(\n        (conv): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (17): InvertedResidual(\n        (conv): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (2): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (18): Conv2dNormActivation(\n        (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU6(inplace=True)\n      )\n    )\n    (classifier): Sequential(\n      (0): Linear(in_features=1280, out_features=1024, bias=True)\n      (1): ReLU(inplace=True)\n      (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (3): Linear(in_features=1024, out_features=512, bias=True)\n      (4): ReLU(inplace=True)\n      (5): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (6): Linear(in_features=512, out_features=10, bias=True)\n      (7): LogSoftmax(dim=1)\n    )\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"qconfig_mapping = get_default_qat_qconfig_mapping(\"x86\")\nmodel_prepared = quantize_fx.prepare_qat_fx(model, qconfig_mapping, calibrate_data)","metadata":{"execution":{"iopub.status.busy":"2024-04-07T12:08:59.269272Z","iopub.execute_input":"2024-04-07T12:08:59.269551Z","iopub.status.idle":"2024-04-07T12:09:00.631460Z","shell.execute_reply.started":"2024-04-07T12:08:59.269529Z","shell.execute_reply":"2024-04-07T12:09:00.630465Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/ao/quantization/observer.py:214: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"model_prepared.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-04-07T12:09:00.632763Z","iopub.execute_input":"2024-04-07T12:09:00.633129Z","iopub.status.idle":"2024-04-07T12:09:01.062111Z","shell.execute_reply.started":"2024-04-07T12:09:00.633097Z","shell.execute_reply":"2024-04-07T12:09:01.061208Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":41,"outputs":[{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"GraphModule(\n  (activation_post_process_0): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (mnet): Module(\n    (features): Module(\n      (0): Module(\n        (0): ConvBn2d(\n          3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n          (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n            (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n          )\n        )\n        (2): ReLU6(inplace=True)\n      )\n      (1): Module(\n        (conv): Module(\n          (0): Module(\n            (0): ConvBn2d(\n              32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n              (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n                fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n                (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n              )\n            )\n            (2): ReLU6(inplace=True)\n          )\n          (1): ConvBn2d(\n            32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n            )\n          )\n        )\n      )\n      (2): Module(\n        (conv): Module(\n          (0): Module(\n            (0): ConvBn2d(\n              16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n              (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n                fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n                (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n              )\n            )\n            (2): ReLU6(inplace=True)\n          )\n          (1): Module(\n            (0): ConvBn2d(\n              96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False\n              (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n                fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n                (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n              )\n            )\n            (2): ReLU6(inplace=True)\n          )\n          (2): ConvBn2d(\n            96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (bn): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n            )\n          )\n        )\n      )\n      (3): Module(\n        (conv): Module(\n          (0): Module(\n            (0): ConvBn2d(\n              24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n              (bn): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n                fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n                (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n              )\n            )\n            (2): ReLU6(inplace=True)\n          )\n          (1): Module(\n            (0): ConvBn2d(\n              144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False\n              (bn): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n                fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n                (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n              )\n            )\n            (2): ReLU6(inplace=True)\n          )\n          (2): ConvBn2d(\n            144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (bn): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n            )\n          )\n        )\n      )\n      (4): Module(\n        (conv): Module(\n          (0): Module(\n            (0): ConvBn2d(\n              24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n              (bn): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n                fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n                (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n              )\n            )\n            (2): ReLU6(inplace=True)\n          )\n          (1): Module(\n            (0): ConvBn2d(\n              144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False\n              (bn): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n                fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n                (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n              )\n            )\n            (2): ReLU6(inplace=True)\n          )\n          (2): ConvBn2d(\n            144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n            )\n          )\n        )\n      )\n      (5): Module(\n        (conv): Module(\n          (0): Module(\n            (0): ConvBn2d(\n              32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n              (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n                fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n                (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n              )\n            )\n            (2): ReLU6(inplace=True)\n          )\n          (1): Module(\n            (0): ConvBn2d(\n              192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False\n              (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n                fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n                (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n              )\n            )\n            (2): ReLU6(inplace=True)\n          )\n          (2): ConvBn2d(\n            192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n            )\n          )\n        )\n      )\n      (6): Module(\n        (conv): Module(\n          (0): Module(\n            (0): ConvBn2d(\n              32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n              (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n                fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n                (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n              )\n            )\n            (2): ReLU6(inplace=True)\n          )\n          (1): Module(\n            (0): ConvBn2d(\n              192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False\n              (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n                fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n                (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n              )\n            )\n            (2): ReLU6(inplace=True)\n          )\n          (2): ConvBn2d(\n            192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n            )\n          )\n        )\n      )\n      (7): Module(\n        (conv): Module(\n          (0): Module(\n            (0): ConvBn2d(\n              32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n              (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n                fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n                (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n              )\n            )\n            (2): ReLU6(inplace=True)\n          )\n          (1): Module(\n            (0): ConvBn2d(\n              192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False\n              (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n                fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n                (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n              )\n            )\n            (2): ReLU6(inplace=True)\n          )\n          (2): ConvBn2d(\n            192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n            )\n          )\n        )\n      )\n      (8): Module(\n        (conv): Module(\n          (0): Module(\n            (0): ConvBn2d(\n              64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False\n              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n                fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n                (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n              )\n            )\n            (2): ReLU6(inplace=True)\n          )\n          (1): Module(\n            (0): ConvBn2d(\n              384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False\n              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n                fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n                (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n              )\n            )\n            (2): ReLU6(inplace=True)\n          )\n          (2): ConvBn2d(\n            384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n            )\n          )\n        )\n      )\n      (9): Module(\n        (conv): Module(\n          (0): Module(\n            (0): ConvBn2d(\n              64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False\n              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n                fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n                (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n              )\n            )\n            (2): ReLU6(inplace=True)\n          )\n          (1): Module(\n            (0): ConvBn2d(\n              384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False\n              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n                fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n                (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n              )\n            )\n            (2): ReLU6(inplace=True)\n          )\n          (2): ConvBn2d(\n            384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n            )\n          )\n        )\n      )\n      (10): Module(\n        (conv): Module(\n          (0): Module(\n            (0): ConvBn2d(\n              64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False\n              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n                fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n                (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n              )\n            )\n            (2): ReLU6(inplace=True)\n          )\n          (1): Module(\n            (0): ConvBn2d(\n              384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False\n              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n                fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n                (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n              )\n            )\n            (2): ReLU6(inplace=True)\n          )\n          (2): ConvBn2d(\n            384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n            )\n          )\n        )\n      )\n      (11): Module(\n        (conv): Module(\n          (0): Module(\n            (0): ConvBn2d(\n              64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False\n              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n                fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n                (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n              )\n            )\n            (2): ReLU6(inplace=True)\n          )\n          (1): Module(\n            (0): ConvBn2d(\n              384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False\n              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n                fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n                (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n              )\n            )\n            (2): ReLU6(inplace=True)\n          )\n          (2): ConvBn2d(\n            384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n            )\n          )\n        )\n      )\n      (12): Module(\n        (conv): Module(\n          (0): Module(\n            (0): ConvBn2d(\n              96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False\n              (bn): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n                fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n                (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n              )\n            )\n            (2): ReLU6(inplace=True)\n          )\n          (1): Module(\n            (0): ConvBn2d(\n              576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False\n              (bn): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n                fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n                (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n              )\n            )\n            (2): ReLU6(inplace=True)\n          )\n          (2): ConvBn2d(\n            576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n            )\n          )\n        )\n      )\n      (13): Module(\n        (conv): Module(\n          (0): Module(\n            (0): ConvBn2d(\n              96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False\n              (bn): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n                fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n                (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n              )\n            )\n            (2): ReLU6(inplace=True)\n          )\n          (1): Module(\n            (0): ConvBn2d(\n              576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False\n              (bn): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n                fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n                (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n              )\n            )\n            (2): ReLU6(inplace=True)\n          )\n          (2): ConvBn2d(\n            576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n            )\n          )\n        )\n      )\n      (14): Module(\n        (conv): Module(\n          (0): Module(\n            (0): ConvBn2d(\n              96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False\n              (bn): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n                fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n                (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n              )\n            )\n            (2): ReLU6(inplace=True)\n          )\n          (1): Module(\n            (0): ConvBn2d(\n              576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False\n              (bn): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n                fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n                (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n              )\n            )\n            (2): ReLU6(inplace=True)\n          )\n          (2): ConvBn2d(\n            576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n            )\n          )\n        )\n      )\n      (15): Module(\n        (conv): Module(\n          (0): Module(\n            (0): ConvBn2d(\n              160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False\n              (bn): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n                fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n                (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n              )\n            )\n            (2): ReLU6(inplace=True)\n          )\n          (1): Module(\n            (0): ConvBn2d(\n              960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False\n              (bn): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n                fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n                (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n              )\n            )\n            (2): ReLU6(inplace=True)\n          )\n          (2): ConvBn2d(\n            960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n            )\n          )\n        )\n      )\n      (16): Module(\n        (conv): Module(\n          (0): Module(\n            (0): ConvBn2d(\n              160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False\n              (bn): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n                fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n                (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n              )\n            )\n            (2): ReLU6(inplace=True)\n          )\n          (1): Module(\n            (0): ConvBn2d(\n              960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False\n              (bn): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n                fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n                (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n              )\n            )\n            (2): ReLU6(inplace=True)\n          )\n          (2): ConvBn2d(\n            960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n            )\n          )\n        )\n      )\n      (17): Module(\n        (conv): Module(\n          (0): Module(\n            (0): ConvBn2d(\n              160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False\n              (bn): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n                fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n                (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n              )\n            )\n            (2): ReLU6(inplace=True)\n          )\n          (1): Module(\n            (0): ConvBn2d(\n              960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False\n              (bn): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n                fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n                (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n              )\n            )\n            (2): ReLU6(inplace=True)\n          )\n          (2): ConvBn2d(\n            960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n            )\n          )\n        )\n      )\n      (18): Module(\n        (0): ConvBn2d(\n          320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (bn): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n            fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n            (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n          )\n        )\n        (2): ReLU6(inplace=True)\n      )\n    )\n    (classifier): Module(\n      (0): LinearReLU(\n        in_features=1280, out_features=1024, bias=True\n        (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n          fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n        )\n      )\n      (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (3): LinearReLU(\n        in_features=1024, out_features=512, bias=True\n        (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n          fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n        )\n      )\n      (5): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (6): Linear(\n        in_features=512, out_features=10, bias=True\n        (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n          fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n        )\n      )\n      (7): LogSoftmax(dim=1)\n    )\n  )\n  (activation_post_process_1): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_2): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_3): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_4): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_5): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_6): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_7): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_8): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_9): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_10): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_11): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_12): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_13): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_14): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_15): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_16): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_17): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_18): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_19): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_20): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_21): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_22): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_23): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_24): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_25): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_26): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_27): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_28): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_29): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_30): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_31): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_32): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_33): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_34): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_35): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_36): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_37): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_38): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_39): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_40): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_41): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_42): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_43): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_44): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_45): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_46): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_47): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_48): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_49): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_50): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_51): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_52): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_53): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_54): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_55): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_56): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_57): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_58): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_59): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_60): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_61): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_62): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_63): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_64): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_65): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_66): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_67): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_68): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_69): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_70): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_71): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_72): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_73): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_74): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_75): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_76): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_77): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_78): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_79): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_80): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_81): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_82): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_83): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_84): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_85): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_86): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_87): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_88): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_89): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_90): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_91): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_92): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_93): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_94): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_95): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_96): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_97): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_98): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_99): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_100): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_101): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_102): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_103): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n  (activation_post_process_104): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"model_prepared = train(model_prepared,steps,print_every,epochs)","metadata":{"execution":{"iopub.status.busy":"2024-04-07T12:09:01.063483Z","iopub.execute_input":"2024-04-07T12:09:01.063896Z","iopub.status.idle":"2024-04-07T12:31:54.041200Z","shell.execute_reply.started":"2024-04-07T12:09:01.063864Z","shell.execute_reply":"2024-04-07T12:31:54.040229Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stdout","text":"Training process initializing .....\n\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/ao/quantization/fake_quantize.py:343: UserWarning: _aminmax is deprecated as of PyTorch 1.11 and will be removed in a future release. Use aminmax instead. This warning will only appear once per process. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/native/ReduceAllOps.cpp:72.)\n  return torch.fused_moving_avg_obs_fake_quant(\n/opt/conda/lib/python3.10/site-packages/torch/ao/quantization/fake_quantize.py:343: UserWarning: _aminmax is deprecated as of PyTorch 1.11 and will be removed in a future release. Use aminmax instead. This warning will only appear once per process. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/native/TensorCompare.cpp:677.)\n  return torch.fused_moving_avg_obs_fake_quant(\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 1/5 |  Training Loss: 0.7800 |  Validation Loss: 0.9955 |  Validation Accuracy: 0.6781\nEpoch: 1/5 |  Training Loss: 0.6218 |  Validation Loss: 0.6307 |  Validation Accuracy: 0.7807\nEpoch: 1/5 |  Training Loss: 0.5700 |  Validation Loss: 0.5469 |  Validation Accuracy: 0.7993\nEpoch: 1/5 |  Training Loss: 0.6209 |  Validation Loss: 0.6126 |  Validation Accuracy: 0.7859\nEpoch: 1/5 |  Training Loss: 0.6090 |  Validation Loss: 0.5103 |  Validation Accuracy: 0.8104\nEpoch: 1/5 |  Training Loss: 0.5598 |  Validation Loss: 0.4829 |  Validation Accuracy: 0.8300\nEpoch: 1/5 |  Training Loss: 0.5931 |  Validation Loss: 0.5018 |  Validation Accuracy: 0.8244\nEpoch: 1/5 |  Training Loss: 0.5098 |  Validation Loss: 0.4932 |  Validation Accuracy: 0.8311\nEpoch: 1/5 |  Training Loss: 0.5576 |  Validation Loss: 0.5123 |  Validation Accuracy: 0.8226\nEpoch: 1/5 |  Training Loss: 0.5694 |  Validation Loss: 0.5351 |  Validation Accuracy: 0.8178\nEpoch: 1/5 |  Training Loss: 0.5041 |  Validation Loss: 0.4887 |  Validation Accuracy: 0.8311\nEpoch: 1/5 |  Training Loss: 0.5039 |  Validation Loss: 0.5011 |  Validation Accuracy: 0.8241\nEpoch: 2/5 |  Training Loss: 0.2091 |  Validation Loss: 0.5223 |  Validation Accuracy: 0.8178\nEpoch: 2/5 |  Training Loss: 0.4945 |  Validation Loss: 0.4856 |  Validation Accuracy: 0.8300\nEpoch: 2/5 |  Training Loss: 0.5340 |  Validation Loss: 0.5031 |  Validation Accuracy: 0.8170\nEpoch: 2/5 |  Training Loss: 0.5400 |  Validation Loss: 0.4793 |  Validation Accuracy: 0.8278\nEpoch: 2/5 |  Training Loss: 0.5554 |  Validation Loss: 0.4974 |  Validation Accuracy: 0.8226\nEpoch: 2/5 |  Training Loss: 0.5118 |  Validation Loss: 0.4882 |  Validation Accuracy: 0.8385\nEpoch: 2/5 |  Training Loss: 0.5451 |  Validation Loss: 0.4618 |  Validation Accuracy: 0.8363\nEpoch: 2/5 |  Training Loss: 0.5186 |  Validation Loss: 0.4902 |  Validation Accuracy: 0.8252\nEpoch: 2/5 |  Training Loss: 0.5231 |  Validation Loss: 0.4764 |  Validation Accuracy: 0.8385\nEpoch: 2/5 |  Training Loss: 0.5314 |  Validation Loss: 0.4898 |  Validation Accuracy: 0.8274\nEpoch: 2/5 |  Training Loss: 0.5193 |  Validation Loss: 0.4924 |  Validation Accuracy: 0.8311\nEpoch: 2/5 |  Training Loss: 0.5094 |  Validation Loss: 0.4516 |  Validation Accuracy: 0.8378\nEpoch: 2/5 |  Training Loss: 0.5108 |  Validation Loss: 0.5811 |  Validation Accuracy: 0.8044\nEpoch: 3/5 |  Training Loss: 0.3649 |  Validation Loss: 0.5128 |  Validation Accuracy: 0.8156\nEpoch: 3/5 |  Training Loss: 0.5027 |  Validation Loss: 0.4369 |  Validation Accuracy: 0.8526\nEpoch: 3/5 |  Training Loss: 0.5099 |  Validation Loss: 0.4740 |  Validation Accuracy: 0.8259\nEpoch: 3/5 |  Training Loss: 0.5170 |  Validation Loss: 0.5760 |  Validation Accuracy: 0.7981\nEpoch: 3/5 |  Training Loss: 0.5537 |  Validation Loss: 0.4569 |  Validation Accuracy: 0.8330\nEpoch: 3/5 |  Training Loss: 0.4762 |  Validation Loss: 0.4268 |  Validation Accuracy: 0.8485\nEpoch: 3/5 |  Training Loss: 0.5356 |  Validation Loss: 0.4662 |  Validation Accuracy: 0.8356\nEpoch: 3/5 |  Training Loss: 0.5130 |  Validation Loss: 0.4586 |  Validation Accuracy: 0.8426\nEpoch: 3/5 |  Training Loss: 0.5134 |  Validation Loss: 0.4426 |  Validation Accuracy: 0.8430\nEpoch: 3/5 |  Training Loss: 0.5151 |  Validation Loss: 0.4704 |  Validation Accuracy: 0.8437\nEpoch: 3/5 |  Training Loss: 0.4886 |  Validation Loss: 0.4825 |  Validation Accuracy: 0.8256\nEpoch: 3/5 |  Training Loss: 0.4820 |  Validation Loss: 0.4558 |  Validation Accuracy: 0.8407\nEpoch: 4/5 |  Training Loss: 0.0953 |  Validation Loss: 0.4567 |  Validation Accuracy: 0.8326\nEpoch: 4/5 |  Training Loss: 0.4855 |  Validation Loss: 0.4600 |  Validation Accuracy: 0.8389\nEpoch: 4/5 |  Training Loss: 0.5146 |  Validation Loss: 0.4686 |  Validation Accuracy: 0.8370\nEpoch: 4/5 |  Training Loss: 0.5182 |  Validation Loss: 0.4342 |  Validation Accuracy: 0.8441\nEpoch: 4/5 |  Training Loss: 0.4915 |  Validation Loss: 0.4196 |  Validation Accuracy: 0.8474\nEpoch: 4/5 |  Training Loss: 0.4772 |  Validation Loss: 0.4499 |  Validation Accuracy: 0.8437\nEpoch: 4/5 |  Training Loss: 0.4962 |  Validation Loss: 0.4334 |  Validation Accuracy: 0.8411\nEpoch: 4/5 |  Training Loss: 0.5067 |  Validation Loss: 0.4278 |  Validation Accuracy: 0.8481\nEpoch: 4/5 |  Training Loss: 0.4702 |  Validation Loss: 0.4754 |  Validation Accuracy: 0.8356\nEpoch: 4/5 |  Training Loss: 0.4848 |  Validation Loss: 0.5254 |  Validation Accuracy: 0.8130\nEpoch: 4/5 |  Training Loss: 0.5078 |  Validation Loss: 0.4190 |  Validation Accuracy: 0.8541\nEpoch: 4/5 |  Training Loss: 0.5312 |  Validation Loss: 0.4711 |  Validation Accuracy: 0.8367\nEpoch: 4/5 |  Training Loss: 0.4715 |  Validation Loss: 0.4168 |  Validation Accuracy: 0.8567\nEpoch: 5/5 |  Training Loss: 0.3034 |  Validation Loss: 0.4343 |  Validation Accuracy: 0.8570\nEpoch: 5/5 |  Training Loss: 0.4906 |  Validation Loss: 0.4610 |  Validation Accuracy: 0.8341\nEpoch: 5/5 |  Training Loss: 0.5082 |  Validation Loss: 0.4751 |  Validation Accuracy: 0.8326\nEpoch: 5/5 |  Training Loss: 0.5608 |  Validation Loss: 0.4673 |  Validation Accuracy: 0.8400\nEpoch: 5/5 |  Training Loss: 0.4690 |  Validation Loss: 0.4802 |  Validation Accuracy: 0.8370\nEpoch: 5/5 |  Training Loss: 0.4513 |  Validation Loss: 0.4253 |  Validation Accuracy: 0.8467\nEpoch: 5/5 |  Training Loss: 0.5204 |  Validation Loss: 0.4594 |  Validation Accuracy: 0.8370\nEpoch: 5/5 |  Training Loss: 0.4935 |  Validation Loss: 0.4383 |  Validation Accuracy: 0.8433\nEpoch: 5/5 |  Training Loss: 0.5174 |  Validation Loss: 0.4781 |  Validation Accuracy: 0.8352\nEpoch: 5/5 |  Training Loss: 0.4662 |  Validation Loss: 0.5507 |  Validation Accuracy: 0.8033\nEpoch: 5/5 |  Training Loss: 0.4424 |  Validation Loss: 0.4107 |  Validation Accuracy: 0.8537\nEpoch: 5/5 |  Training Loss: 0.4583 |  Validation Loss: 0.4251 |  Validation Accuracy: 0.8437\nEpoch: 5/5 |  Training Loss: 0.4574 |  Validation Loss: 0.4893 |  Validation Accuracy: 0.8278\n\nTraining process is now complete!!\n","output_type":"stream"}]},{"cell_type":"code","source":"model_prepared.to('cpu')","metadata":{"execution":{"iopub.status.busy":"2024-04-07T12:31:54.042496Z","iopub.execute_input":"2024-04-07T12:31:54.042868Z","iopub.status.idle":"2024-04-07T12:31:54.698714Z","shell.execute_reply.started":"2024-04-07T12:31:54.042834Z","shell.execute_reply":"2024-04-07T12:31:54.697680Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":43,"outputs":[{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"GraphModule(\n  (activation_post_process_0): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0347]), zero_point=tensor([51], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.764017939567566, max_val=2.6399881839752197)\n  )\n  (mnet): Module(\n    (features): Module(\n      (0): Module(\n        (0): ConvBn2d(\n          3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n          (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n            fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([9.6501e-05, 1.5725e-02, 8.1762e-03, 1.7666e-02, 5.0973e-04, 8.6396e-04,\n                    3.8022e-03, 8.3452e-04, 5.2823e-03, 1.6010e-05, 2.4263e-03, 3.6965e-04,\n                    7.7422e-03, 1.7444e-03, 1.8367e-05, 1.5490e-05, 5.5734e-05, 9.7579e-06,\n                    2.6779e-04, 1.6403e-03, 1.3884e-03, 1.2992e-03, 4.2397e-03, 1.6400e-05,\n                    2.1208e-03, 9.9158e-03, 1.0999e-03, 4.0879e-03, 1.6963e-03, 1.9876e-03,\n                    2.1174e-03, 5.2322e-04]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                    0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n            (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n              min_val=tensor([-1.2352e-02, -1.6231e+00, -7.4482e-01, -2.2613e+00, -6.5245e-02,\n                      -7.8631e-02, -2.4235e-01, -3.2806e-02, -6.0646e-01, -2.4712e-04,\n                      -3.0882e-01, -1.3943e-02, -9.7449e-01, -5.5383e-02, -1.1393e-03,\n                      -7.6488e-04, -1.7052e-03, -1.2490e-03, -1.4798e-02, -2.0996e-01,\n                      -1.7772e-01, -1.6630e-01, -4.3862e-01, -1.6392e-03, -2.7146e-01,\n                      -1.2343e+00, -1.4079e-01, -5.2326e-01, -2.1713e-01, -2.5441e-01,\n                      -1.3826e-01, -3.5384e-02]), max_val=tensor([1.0842e-02, 1.9971e+00, 1.0384e+00, 2.0789e+00, 2.5828e-02, 1.0972e-01,\n                      4.8287e-01, 1.0598e-01, 6.7085e-01, 2.0333e-03, 3.0814e-01, 4.6946e-02,\n                      9.8325e-01, 2.2153e-01, 2.3326e-03, 1.9672e-03, 7.0782e-03, 3.9566e-04,\n                      3.4009e-02, 1.8103e-01, 1.0182e-01, 1.3619e-01, 5.3845e-01, 2.0828e-03,\n                      1.4216e-01, 1.2593e+00, 5.6188e-02, 1.6884e-01, 1.3719e-01, 2.0947e-01,\n                      2.6891e-01, 6.6449e-02])\n            )\n          )\n        )\n        (2): ReLU6(inplace=True)\n      )\n      (1): Module(\n        (conv): Module(\n          (0): Module(\n            (0): ConvBn2d(\n              32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n              (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n                fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([6.5871e-03, 3.7388e-02, 7.7820e-02, 3.0290e-02, 4.2796e-02, 4.6646e-02,\n                        9.3328e-02, 5.0090e-02, 5.3481e-02, 7.9145e-05, 2.9859e-02, 3.5852e-02,\n                        4.8167e-02, 6.0949e-02, 5.3344e-05, 6.9114e-05, 8.3524e-04, 1.8189e-04,\n                        1.3516e-01, 2.7301e-02, 7.2341e-02, 2.3155e-02, 8.5279e-03, 1.1599e-04,\n                        3.5613e-02, 9.8866e-02, 1.1587e-01, 1.2196e-02, 1.9594e-02, 4.5771e-03,\n                        1.2023e-02, 4.7769e-02]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n                (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n                  min_val=tensor([-1.4943e-01, -4.7857e+00, -1.2082e+00, -1.3973e+00, -5.4779e+00,\n                          -5.9706e+00, -1.0628e+01, -5.6741e+00, -6.8456e+00, -1.0131e-02,\n                          -3.8220e+00, -4.4983e+00, -4.8837e+00, -1.2706e+00, -6.8281e-03,\n                          -8.8466e-03, -3.5314e-02, -2.3282e-02, -1.7301e+01, -2.1316e+00,\n                          -1.4959e+00, -2.4944e+00, -1.0141e+00, -1.4847e-02, -4.5584e+00,\n                          -3.6158e+00, -6.2798e+00, -1.2959e+00, -2.5080e+00, -5.8586e-01,\n                          -1.0053e+00, -5.4564e+00]), max_val=tensor([8.3657e-01, 4.5601e+00, 9.8831e+00, 3.8468e+00, 4.8629e+00, 4.0438e+00,\n                          1.1853e+01, 6.3614e+00, 1.1024e+00, 2.0954e-03, 3.4249e+00, 4.5532e+00,\n                          6.1173e+00, 7.7406e+00, 2.5984e-03, 2.0934e-03, 1.0608e-01, 1.6266e-02,\n                          1.2187e+01, 3.4672e+00, 9.1873e+00, 2.9406e+00, 1.0830e+00, 1.4521e-02,\n                          3.0407e+00, 1.2556e+01, 1.4715e+01, 1.5489e+00, 1.3539e+00, 9.4548e-02,\n                          1.5269e+00, 6.0667e+00])\n                )\n              )\n            )\n            (2): ReLU6(inplace=True)\n          )\n          (1): ConvBn2d(\n            32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0055, 0.0072, 0.0066, 0.0041, 0.0092, 0.0051, 0.0063, 0.0073, 0.0049,\n                      0.0069, 0.0083, 0.0067, 0.0051, 0.0055, 0.0106, 0.0072]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n                min_val=tensor([-0.3267, -0.9277, -0.4934, -0.5213, -1.1766, -0.6575, -0.8072, -0.9392,\n                        -0.6299, -0.3831, -1.0687, -0.8632, -0.5471, -0.7038, -1.3566, -0.6880]), max_val=tensor([0.6978, 0.6799, 0.8323, 0.5047, 0.7777, 0.5296, 0.4109, 0.8803, 0.5099,\n                        0.8725, 0.5381, 0.7644, 0.6451, 0.6606, 0.5749, 0.9134])\n              )\n            )\n          )\n        )\n      )\n      (2): Module(\n        (conv): Module(\n          (0): Module(\n            (0): ConvBn2d(\n              16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n              (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n                fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0027, 0.0020, 0.0008, 0.0018, 0.0013, 0.0015, 0.0014, 0.0011, 0.0029,\n                        0.0009, 0.0027, 0.0020, 0.0004, 0.0006, 0.0007, 0.0025, 0.0014, 0.0022,\n                        0.0026, 0.0017, 0.0019, 0.0012, 0.0006, 0.0007, 0.0013, 0.0031, 0.0010,\n                        0.0028, 0.0014, 0.0008, 0.0029, 0.0005, 0.0008, 0.0018, 0.0030, 0.0013,\n                        0.0025, 0.0020, 0.0006, 0.0021, 0.0037, 0.0016, 0.0009, 0.0007, 0.0011,\n                        0.0021, 0.0019, 0.0008, 0.0011, 0.0005, 0.0025, 0.0026, 0.0037, 0.0012,\n                        0.0036, 0.0007, 0.0031, 0.0028, 0.0008, 0.0017, 0.0038, 0.0024, 0.0008,\n                        0.0007, 0.0030, 0.0009, 0.0008, 0.0023, 0.0013, 0.0010, 0.0013, 0.0034,\n                        0.0002, 0.0022, 0.0008, 0.0014, 0.0020, 0.0023, 0.0011, 0.0021, 0.0039,\n                        0.0022, 0.0018, 0.0027, 0.0042, 0.0039, 0.0007, 0.0039, 0.0040, 0.0009,\n                        0.0009, 0.0038, 0.0033, 0.0038, 0.0015, 0.0008]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                       dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n                (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n                  min_val=tensor([-0.2586, -0.2515, -0.1076, -0.2318, -0.1683, -0.1416, -0.1770, -0.0730,\n                          -0.3741, -0.0730, -0.3418, -0.2527, -0.0287, -0.0745, -0.0952, -0.3238,\n                          -0.1265, -0.2809, -0.3366, -0.1873, -0.2394, -0.1541, -0.0445, -0.0558,\n                          -0.1671, -0.3986, -0.0884, -0.3621, -0.1542, -0.0556, -0.3665, -0.0561,\n                          -0.1066, -0.2088, -0.3319, -0.1319, -0.1775, -0.2563, -0.0823, -0.2739,\n                          -0.1981, -0.1978, -0.0875, -0.0375, -0.1276, -0.2636, -0.1878, -0.1041,\n                          -0.1446, -0.0691, -0.3143, -0.3381, -0.2624, -0.1559, -0.4604, -0.0880,\n                          -0.3969, -0.3533, -0.1065, -0.1393, -0.4866, -0.2417, -0.1066, -0.0776,\n                          -0.2938, -0.1125, -0.0859, -0.2913, -0.1656, -0.0829, -0.1603, -0.4369,\n                          -0.0258, -0.1673, -0.0964, -0.1839, -0.2391, -0.2714, -0.0714, -0.0967,\n                          -0.4949, -0.2781, -0.2284, -0.3399, -0.4918, -0.3649, -0.0928, -0.5001,\n                          -0.2869, -0.1171, -0.1092, -0.3156, -0.4162, -0.1352, -0.1392, -0.0971]), max_val=tensor([0.3449, 0.1009, 0.0433, 0.1440, 0.1090, 0.1928, 0.1706, 0.1412, 0.3018,\n                          0.1185, 0.2078, 0.2356, 0.0540, 0.0769, 0.0913, 0.1353, 0.1830, 0.1499,\n                          0.2486, 0.2133, 0.0870, 0.0635, 0.0764, 0.0866, 0.0915, 0.2977, 0.1309,\n                          0.3208, 0.1754, 0.1028, 0.3369, 0.0600, 0.1015, 0.2233, 0.3854, 0.1615,\n                          0.3219, 0.2420, 0.0783, 0.1885, 0.4701, 0.2053, 0.1158, 0.0868, 0.1364,\n                          0.2217, 0.2407, 0.0865, 0.1083, 0.0653, 0.1885, 0.2448, 0.4699, 0.1541,\n                          0.3689, 0.0903, 0.1884, 0.1481, 0.0664, 0.2152, 0.4560, 0.3054, 0.0468,\n                          0.0891, 0.3825, 0.0910, 0.1027, 0.2256, 0.1257, 0.1262, 0.1263, 0.2996,\n                          0.0212, 0.2779, 0.0671, 0.0592, 0.2529, 0.2899, 0.1376, 0.2610, 0.2411,\n                          0.2345, 0.1715, 0.1035, 0.5300, 0.4956, 0.0777, 0.3715, 0.5054, 0.0870,\n                          0.0763, 0.4826, 0.2902, 0.4871, 0.1843, 0.0767])\n                )\n              )\n            )\n            (2): ReLU6(inplace=True)\n          )\n          (1): Module(\n            (0): ConvBn2d(\n              96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False\n              (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n                fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0017, 0.0019, 0.0213, 0.0057, 0.0144, 0.0086, 0.0025, 0.0093, 0.0022,\n                        0.0252, 0.0018, 0.0047, 0.0376, 0.0320, 0.0266, 0.0017, 0.0085, 0.0019,\n                        0.0070, 0.0059, 0.0052, 0.0170, 0.0358, 0.0392, 0.0154, 0.0021, 0.0099,\n                        0.0053, 0.0091, 0.0332, 0.0028, 0.0338, 0.0143, 0.0031, 0.0014, 0.0124,\n                        0.0008, 0.0092, 0.0278, 0.0012, 0.0016, 0.0047, 0.0143, 0.0177, 0.0142,\n                        0.0012, 0.0076, 0.0238, 0.0257, 0.0331, 0.0061, 0.0014, 0.0049, 0.0083,\n                        0.0027, 0.0355, 0.0018, 0.0022, 0.0123, 0.0070, 0.0015, 0.0013, 0.0534,\n                        0.0093, 0.0022, 0.0271, 0.0171, 0.0018, 0.0090, 0.0112, 0.0017, 0.0025,\n                        0.0525, 0.0056, 0.0140, 0.0036, 0.0048, 0.0008, 0.0181, 0.0113, 0.0024,\n                        0.0018, 0.0068, 0.0016, 0.0023, 0.0018, 0.0285, 0.0015, 0.0007, 0.0224,\n                        0.0442, 0.0023, 0.0010, 0.0021, 0.0079, 0.0137]), zero_point=tensor([ 127,  127,    0,    0,    0,    0,    0,    0, -128,    0, -128,    0,\n                           0,    0,    0,  127,    0, -128,    0,    0,    0,    0,    0,    0,\n                           0, -128,    0,    0,    0,    0,  127,    0,    0,    0,  127,    0,\n                         127,    0,    0, -128,  127,    0,    0,    0,    0, -128,    0,    0,\n                           0,    0, -128,  127,  127,    0,    0,    0, -128,  127,    0,    0,\n                        -128, -128,    0,    0,  127,    0,    0,  127,    0,    0, -128,  127,\n                           0,    0,    0,    0,    0,  127,    0,    0,  127,    0,    0, -128,\n                         127, -128,    0, -128,  127,    0,    0,  127,  127,    0,    0,    0],\n                       dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n                (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n                  min_val=tensor([-4.3797e-01, -4.7953e-01, -2.5671e+00, -1.2556e-01, -1.0676e+00,\n                          -7.9324e-01, -3.1732e-01, -1.1844e+00,  1.1349e-01, -3.2313e+00,\n                           4.5037e-02, -5.9883e-01, -4.8182e+00, -3.0958e+00, -2.4166e+00,\n                          -4.3778e-01, -8.2794e-01,  3.0998e-02, -8.9086e-01, -7.5616e-01,\n                          -7.6623e-02, -1.3833e+00, -2.8078e+00, -5.0169e+00, -1.9650e+00,\n                           4.3756e-02, -2.5260e-01, -6.0439e-02, -4.2404e-01, -4.2523e+00,\n                          -7.1357e-01, -2.9291e+00, -6.4648e-01, -3.9143e-01, -3.5555e-01,\n                          -7.9737e-01, -2.0001e-01, -1.1821e+00, -3.5646e+00,  2.1309e-02,\n                          -4.1063e-01, -1.4507e-01, -1.6072e+00, -1.3052e+00, -9.5081e-01,\n                           4.7551e-02, -9.6704e-01, -2.0085e+00, -2.5112e+00, -4.2402e+00,\n                           3.7647e-01, -3.5418e-01, -1.2564e+00, -6.0463e-01, -3.4981e-01,\n                          -4.5458e+00,  3.2631e-02, -5.7252e-01, -1.5360e+00, -8.9137e-01,\n                           2.1727e-02,  6.8286e-02, -6.8313e+00, -1.1887e+00, -5.6792e-01,\n                          -2.7026e+00, -1.9168e+00, -4.5682e-01, -1.1526e+00, -1.0911e+00,\n                           1.6440e-04, -6.3713e-01, -6.7168e+00, -7.1303e-01, -1.7880e+00,\n                          -4.6571e-01, -6.1128e-01, -1.9408e-01, -2.3217e+00, -1.1571e+00,\n                          -6.0782e-01, -1.1563e-01, -8.7155e-01,  1.1121e-01, -5.9259e-01,\n                           4.9471e-02, -1.7503e+00,  4.4544e-02, -1.8895e-01, -2.2455e+00,\n                          -4.8920e+00, -5.9841e-01, -2.6716e-01, -2.5104e-02, -1.0059e+00,\n                          -1.7572e+00]), max_val=tensor([-9.0252e-02, -3.2054e-03,  2.7036e+00,  7.2727e-01,  1.8313e+00,\n                           1.0942e+00,  6.5113e-02,  1.0797e+00,  5.5682e-01,  2.0322e+00,\n                           4.6518e-01,  2.5341e-02,  3.3561e+00,  4.0637e+00,  3.3813e+00,\n                          -5.3067e-02,  1.0760e+00,  4.7271e-01,  1.3177e-01,  4.4908e-02,\n                           6.6427e-01,  2.1579e+00,  4.5513e+00,  4.3431e+00,  1.7029e+00,\n                           5.4529e-01,  1.2541e+00,  6.7780e-01,  1.1604e+00,  2.5599e+00,\n                          -1.2347e-01,  4.2910e+00,  1.8202e+00,  1.1240e-03, -9.8312e-02,\n                           1.5788e+00, -2.3018e-02,  2.9285e-01,  1.7208e+00,  3.0464e-01,\n                          -4.6460e-03,  6.0167e-01,  1.8103e+00,  2.2470e+00,  1.8051e+00,\n                           2.9630e-01,  1.4925e-01,  3.0199e+00,  3.2661e+00,  2.2033e+00,\n                           1.5654e+00, -5.9317e-02, -1.0208e-01,  1.0566e+00,  1.0438e-01,\n                           2.5283e+00,  4.6527e-01, -1.6861e-03,  1.5621e+00,  4.7172e-01,\n                           3.7436e-01,  3.1953e-01,  5.0817e+00,  3.8176e-01, -9.2483e-02,\n                           3.4456e+00,  2.1780e+00, -7.6595e-02,  4.9466e-01,  1.4228e+00,\n                           4.2802e-01, -3.8545e-02,  5.0344e+00,  1.2145e-01,  1.6681e+00,\n                           8.6517e-02,  4.1293e-01, -3.3055e-02,  1.0874e+00,  1.4313e+00,\n                          -1.4428e-01,  2.3162e-01,  2.6804e-01,  4.0976e-01, -8.3054e-02,\n                           4.7032e-01,  3.6200e+00,  3.7584e-01, -2.3252e-02,  2.8510e+00,\n                           5.6109e+00, -1.2635e-01, -6.3806e-03,  2.6946e-01,  1.8377e-01,\n                           1.7403e+00])\n                )\n              )\n            )\n            (2): ReLU6(inplace=True)\n          )\n          (2): ConvBn2d(\n            96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (bn): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0053, 0.0047, 0.0038, 0.0041, 0.0033, 0.0049, 0.0045, 0.0068, 0.0051,\n                      0.0059, 0.0040, 0.0083, 0.0067, 0.0069, 0.0077, 0.0050, 0.0060, 0.0050,\n                      0.0043, 0.0043, 0.0054, 0.0055, 0.0037, 0.0063]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                     dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n                min_val=tensor([-0.3952, -0.5710, -0.4682, -0.5188, -0.4200, -0.5204, -0.5791, -0.8697,\n                        -0.6537, -0.5576, -0.5148, -1.0588, -0.4404, -0.7509, -0.9459, -0.6390,\n                        -0.6987, -0.4113, -0.5554, -0.5553, -0.6861, -0.7019, -0.4660, -0.5481]), max_val=tensor([0.6705, 0.5928, 0.4880, 0.3726, 0.3318, 0.6235, 0.3902, 0.8382, 0.5816,\n                        0.7510, 0.4636, 0.9355, 0.8562, 0.8769, 0.9779, 0.4826, 0.7611, 0.6340,\n                        0.4985, 0.4954, 0.6273, 0.5695, 0.4697, 0.7968])\n              )\n            )\n          )\n        )\n      )\n      (3): Module(\n        (conv): Module(\n          (0): Module(\n            (0): ConvBn2d(\n              24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n              (bn): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n                fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0034, 0.0008, 0.0010, 0.0013, 0.0009, 0.0013, 0.0017, 0.0013, 0.0015,\n                        0.0016, 0.0014, 0.0015, 0.0011, 0.0013, 0.0016, 0.0017, 0.0012, 0.0012,\n                        0.0012, 0.0011, 0.0009, 0.0011, 0.0010, 0.0015, 0.0014, 0.0014, 0.0010,\n                        0.0016, 0.0014, 0.0009, 0.0009, 0.0010, 0.0015, 0.0012, 0.0017, 0.0012,\n                        0.0015, 0.0018, 0.0015, 0.0021, 0.0013, 0.0008, 0.0009, 0.0011, 0.0005,\n                        0.0013, 0.0005, 0.0011, 0.0016, 0.0025, 0.0015, 0.0014, 0.0016, 0.0012,\n                        0.0012, 0.0014, 0.0015, 0.0014, 0.0014, 0.0010, 0.0015, 0.0012, 0.0015,\n                        0.0010, 0.0010, 0.0015, 0.0025, 0.0008, 0.0015, 0.0017, 0.0010, 0.0015,\n                        0.0009, 0.0009, 0.0008, 0.0016, 0.0013, 0.0014, 0.0014, 0.0005, 0.0008,\n                        0.0007, 0.0020, 0.0007, 0.0010, 0.0015, 0.0018, 0.0004, 0.0008, 0.0011,\n                        0.0010, 0.0007, 0.0022, 0.0013, 0.0011, 0.0007, 0.0011, 0.0007, 0.0017,\n                        0.0019, 0.0016, 0.0017, 0.0013, 0.0010, 0.0019, 0.0015, 0.0005, 0.0011,\n                        0.0015, 0.0023, 0.0003, 0.0016, 0.0012, 0.0014, 0.0006, 0.0004, 0.0011,\n                        0.0018, 0.0013, 0.0013, 0.0006, 0.0011, 0.0012, 0.0021, 0.0022, 0.0012,\n                        0.0006, 0.0015, 0.0008, 0.0029, 0.0011, 0.0011, 0.0021, 0.0014, 0.0015,\n                        0.0012, 0.0013, 0.0012, 0.0005, 0.0009, 0.0012, 0.0011, 0.0009, 0.0009]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                       dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n                (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n                  min_val=tensor([-0.4380, -0.0838, -0.1143, -0.0898, -0.0849, -0.1633, -0.0973, -0.1001,\n                          -0.0829, -0.0922, -0.1791, -0.1090, -0.1361, -0.1695, -0.1199, -0.1093,\n                          -0.1147, -0.0922, -0.0871, -0.1177, -0.1187, -0.1128, -0.1263, -0.1843,\n                          -0.1089, -0.0781, -0.1130, -0.1300, -0.1785, -0.1090, -0.1168, -0.1190,\n                          -0.1884, -0.1513, -0.2146, -0.1521, -0.1945, -0.2207, -0.1636, -0.2705,\n                          -0.1121, -0.0997, -0.0741, -0.1374, -0.0679, -0.1725, -0.0626, -0.1210,\n                          -0.2031, -0.3206, -0.1899, -0.0712, -0.1229, -0.1393, -0.1526, -0.1684,\n                          -0.1598, -0.1738, -0.1760, -0.0989, -0.1355, -0.0948, -0.1514, -0.1311,\n                          -0.1270, -0.0854, -0.3185, -0.1049, -0.0786, -0.2203, -0.0884, -0.1918,\n                          -0.0977, -0.0542, -0.1022, -0.1798, -0.1327, -0.0693, -0.1807, -0.0517,\n                          -0.0903, -0.0887, -0.1027, -0.0858, -0.1040, -0.1053, -0.2023, -0.0499,\n                          -0.0929, -0.1426, -0.1267, -0.0633, -0.2812, -0.1644, -0.1367, -0.0901,\n                          -0.1366, -0.0674, -0.1309, -0.1706, -0.1034, -0.1850, -0.1630, -0.1342,\n                          -0.2465, -0.1519, -0.0633, -0.1039, -0.1975, -0.2883, -0.0316, -0.1378,\n                          -0.1053, -0.1739, -0.0574, -0.0306, -0.0432, -0.2326, -0.0893, -0.0655,\n                          -0.0319, -0.1213, -0.1330, -0.2642, -0.1926, -0.1229, -0.0765, -0.0865,\n                          -0.1008, -0.3717, -0.1372, -0.1100, -0.1828, -0.1839, -0.0849, -0.1533,\n                          -0.1153, -0.1585, -0.0597, -0.1180, -0.1572, -0.1198, -0.0864, -0.1094]), max_val=tensor([0.2936, 0.1011, 0.1331, 0.1637, 0.1099, 0.1514, 0.2191, 0.1616, 0.1921,\n                          0.2019, 0.1131, 0.1959, 0.0734, 0.1407, 0.2026, 0.2148, 0.1468, 0.1505,\n                          0.1534, 0.1366, 0.1089, 0.1354, 0.1323, 0.1868, 0.1793, 0.1750, 0.1279,\n                          0.1999, 0.1725, 0.0792, 0.0789, 0.1274, 0.1887, 0.1584, 0.0926, 0.1316,\n                          0.1564, 0.2261, 0.1912, 0.2179, 0.1675, 0.0778, 0.1115, 0.1061, 0.0520,\n                          0.1042, 0.0695, 0.1420, 0.1739, 0.1503, 0.1156, 0.1802, 0.2022, 0.1585,\n                          0.1296, 0.1759, 0.1956, 0.1612, 0.0960, 0.1259, 0.1911, 0.1527, 0.1938,\n                          0.1220, 0.0411, 0.1940, 0.1892, 0.1035, 0.1910, 0.1033, 0.1303, 0.1959,\n                          0.1148, 0.1183, 0.0976, 0.2035, 0.1662, 0.1806, 0.1454, 0.0640, 0.0985,\n                          0.0785, 0.2517, 0.0941, 0.1316, 0.1925, 0.2225, 0.0485, 0.1024, 0.1127,\n                          0.0823, 0.0946, 0.0891, 0.1535, 0.1226, 0.0871, 0.1023, 0.0878, 0.2147,\n                          0.2397, 0.1980, 0.2114, 0.1227, 0.1240, 0.1801, 0.1881, 0.0653, 0.1388,\n                          0.1268, 0.2009, 0.0359, 0.2009, 0.1583, 0.1427, 0.0813, 0.0496, 0.1412,\n                          0.1194, 0.1677, 0.1591, 0.0783, 0.1438, 0.1565, 0.0773, 0.2825, 0.1584,\n                          0.0734, 0.1851, 0.0543, 0.1745, 0.0497, 0.1441, 0.2651, 0.0818, 0.1946,\n                          0.1314, 0.1598, 0.1327, 0.0677, 0.0902, 0.1266, 0.1422, 0.1118, 0.0984])\n                )\n              )\n            )\n            (2): ReLU6(inplace=True)\n          )\n          (1): Module(\n            (0): ConvBn2d(\n              144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False\n              (bn): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n                fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0043, 0.0209, 0.0158, 0.0151, 0.0132, 0.0209, 0.0082, 0.0157, 0.0233,\n                        0.0205, 0.0169, 0.0113, 0.0109, 0.0104, 0.0078, 0.0120, 0.0118, 0.0242,\n                        0.0199, 0.0213, 0.0123, 0.0293, 0.0150, 0.0203, 0.0136, 0.0189, 0.0180,\n                        0.0086, 0.0120, 0.0278, 0.0301, 0.0122, 0.0176, 0.0108, 0.0278, 0.0229,\n                        0.0166, 0.0134, 0.0217, 0.0170, 0.0128, 0.0226, 0.0260, 0.0150, 0.0333,\n                        0.0210, 0.0305, 0.0138, 0.0070, 0.0129, 0.0150, 0.0199, 0.0171, 0.0131,\n                        0.0134, 0.0144, 0.0138, 0.0073, 0.0019, 0.0140, 0.0157, 0.0151, 0.0034,\n                        0.0107, 0.0262, 0.0142, 0.0112, 0.0282, 0.0169, 0.0180, 0.0167, 0.0205,\n                        0.0110, 0.0166, 0.0097, 0.0068, 0.0158, 0.0175, 0.0105, 0.0315, 0.0115,\n                        0.0235, 0.0069, 0.0199, 0.0247, 0.0147, 0.0167, 0.0403, 0.0204, 0.0064,\n                        0.0293, 0.0106, 0.0192, 0.0211, 0.0161, 0.0197, 0.0123, 0.0357, 0.0162,\n                        0.0058, 0.0211, 0.0028, 0.0115, 0.0093, 0.0193, 0.0187, 0.0172, 0.0108,\n                        0.0089, 0.0184, 0.0102, 0.0086, 0.0099, 0.0145, 0.0251, 0.0458, 0.0066,\n                        0.0190, 0.0194, 0.0251, 0.0241, 0.0150, 0.0180, 0.0203, 0.0113, 0.0214,\n                        0.0165, 0.0133, 0.0204, 0.0106, 0.0138, 0.0157, 0.0184, 0.0159, 0.0235,\n                        0.0161, 0.0309, 0.0114, 0.0238, 0.0143, 0.0201, 0.0059, 0.0324, 0.0197]), zero_point=tensor([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                          0,   0, 127,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                          0,   0,   0,   0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n                (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n                  min_val=tensor([-0.5490, -2.6720, -0.9213, -1.2052, -0.5531, -2.6770, -1.0459, -1.0067,\n                          -1.7180, -1.3849, -2.1695, -1.4523, -0.5137, -0.6418, -0.6189, -0.1810,\n                          -0.8460, -2.7805, -0.6660, -2.7328, -1.1555, -1.5970, -0.5582, -0.4364,\n                          -1.0825, -0.6738, -1.3643, -0.8082, -1.5325, -1.2772, -1.0546, -0.7061,\n                          -2.2492, -0.9144, -2.3962, -2.3409, -2.1297, -1.3850, -2.7805, -2.1701,\n                          -0.4237, -2.8911, -1.2751, -1.4384, -4.2688, -1.7635, -3.8405, -1.7687,\n                          -0.6903, -1.6531, -1.9150, -2.5467, -1.9350, -1.6724, -1.7134, -0.3583,\n                          -1.1742, -0.5871, -0.4957, -0.6330, -2.0077, -1.0318, -0.4368, -0.6509,\n                          -3.3561, -1.8142, -1.4363, -3.6039, -1.4764, -1.1422, -1.7335, -2.6198,\n                          -1.1600, -1.3565, -1.2359, -0.6998, -2.0190, -1.1202, -0.9180, -4.0343,\n                          -1.4729, -0.9828, -0.5851, -2.5533, -1.9658, -0.8673, -0.5898, -4.5526,\n                          -1.4364, -0.6478, -3.7441, -1.3127, -1.2873, -2.7031, -1.4994, -0.9005,\n                          -1.5763, -1.3793, -1.7361, -0.6776, -1.2911, -0.2453, -1.4744, -0.8674,\n                          -0.5992, -2.1658, -1.8677, -1.3171, -1.1413, -2.3550, -1.3111, -1.1013,\n                          -0.8408, -1.8610, -0.2847, -4.2163, -0.8168, -1.4449, -1.1965, -0.7527,\n                          -3.0796, -1.8717, -0.8840, -1.1255, -1.3202, -0.4675, -0.3473, -1.6993,\n                          -2.0003, -1.3616, -1.7699, -0.3732, -2.3542, -0.9490, -0.5492, -2.0629,\n                          -1.2955, -0.6291, -2.2238, -0.2711, -1.8025, -0.6422, -2.3586, -1.0673]), max_val=tensor([ 3.2438e-01,  1.7190e+00,  2.0026e+00,  1.9157e+00,  1.6731e+00,\n                           2.4905e+00,  4.2799e-01,  1.9906e+00,  2.9592e+00,  2.6027e+00,\n                           1.2231e+00,  1.0321e+00,  1.3803e+00,  1.3199e+00,  9.9624e-01,\n                           1.5254e+00,  1.5010e+00,  3.0739e+00,  2.5255e+00,  2.1632e+00,\n                           1.5603e+00,  3.7217e+00,  1.9055e+00,  2.5783e+00,  1.7222e+00,\n                           2.4049e+00,  2.2864e+00,  1.0910e+00,  1.0798e+00,  3.5316e+00,\n                           3.8245e+00,  1.5525e+00,  1.6617e+00,  1.3752e+00,  3.5313e+00,\n                           2.9142e+00,  1.4458e+00,  1.7024e+00,  1.1523e+00,  8.9361e-01,\n                           1.6233e+00,  1.7246e+00,  3.3021e+00,  1.9107e+00,  3.5826e+00,\n                           2.6616e+00,  3.8765e+00,  1.7545e+00,  8.8302e-01,  1.0243e+00,\n                           1.3356e+00,  3.5863e-01,  2.1695e+00,  1.1393e+00,  1.2422e+00,\n                           1.8262e+00,  1.7542e+00,  9.2461e-01, -5.4230e-03,  1.7762e+00,\n                           1.6263e+00,  1.9188e+00,  3.0830e-01,  1.3595e+00,  3.3094e+00,\n                           1.1631e+00,  1.6358e-01,  1.1774e+00,  2.1452e+00,  2.2825e+00,\n                           2.1222e+00,  1.1565e+00,  1.4005e+00,  2.1135e+00,  8.6535e-01,\n                           8.6989e-01,  1.8966e+00,  2.2212e+00,  1.3380e+00,  3.2515e+00,\n                           1.0123e+00,  2.9849e+00,  8.7627e-01,  2.3363e+00,  3.1352e+00,\n                           1.8674e+00,  2.1217e+00,  5.1126e+00,  2.5921e+00,  8.1053e-01,\n                           2.9101e+00,  1.3411e+00,  2.4350e+00,  1.4836e+00,  2.0507e+00,\n                           2.5040e+00,  1.2457e+00,  4.5397e+00,  2.0541e+00,  7.4160e-01,\n                           2.6742e+00,  3.6078e-01,  1.0066e+00,  1.1777e+00,  2.4564e+00,\n                           2.3722e+00,  2.1800e+00,  1.3702e+00,  1.0424e+00,  1.0278e+00,\n                           1.2253e+00,  5.4447e-01,  1.2616e+00,  1.6986e+00,  3.1894e+00,\n                           5.8115e+00,  8.4153e-01,  2.4135e+00,  2.4700e+00,  3.1867e+00,\n                           2.2807e+00,  1.9054e+00,  2.2871e+00,  2.5784e+00,  1.4338e+00,\n                           2.7178e+00,  2.0999e+00,  1.5516e+00,  2.5940e+00,  5.8247e-01,\n                           1.5398e+00,  1.9963e+00,  1.1950e+00,  2.0231e+00,  2.9829e+00,\n                           1.9048e+00,  3.9236e+00,  1.4504e+00,  3.0252e+00,  1.8153e+00,\n                           2.5532e+00,  7.4731e-01,  4.1164e+00,  2.5030e+00])\n                )\n              )\n            )\n            (2): ReLU6(inplace=True)\n          )\n          (2): ConvBn2d(\n            144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (bn): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0063, 0.0067, 0.0049, 0.0057, 0.0073, 0.0050, 0.0068, 0.0047, 0.0057,\n                      0.0041, 0.0065, 0.0043, 0.0053, 0.0061, 0.0064, 0.0059, 0.0037, 0.0055,\n                      0.0040, 0.0044, 0.0063, 0.0076, 0.0089, 0.0073]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                     dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n                min_val=tensor([-0.5768, -0.6947, -0.4599, -0.7281, -0.6015, -0.6417, -0.8659, -0.6034,\n                        -0.5423, -0.4725, -0.7294, -0.5103, -0.6178, -0.7801, -0.6802, -0.3957,\n                        -0.4586, -0.7100, -0.3982, -0.5629, -0.8074, -0.9041, -0.6171, -0.8929]), max_val=tensor([0.7946, 0.8525, 0.6190, 0.5473, 0.9318, 0.5529, 0.6567, 0.4858, 0.7220,\n                        0.5191, 0.8253, 0.5472, 0.6753, 0.7412, 0.8156, 0.7470, 0.4723, 0.6831,\n                        0.5049, 0.4568, 0.5603, 0.9714, 1.1269, 0.9220])\n              )\n            )\n          )\n        )\n      )\n      (4): Module(\n        (conv): Module(\n          (0): Module(\n            (0): ConvBn2d(\n              24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n              (bn): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n                fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0019, 0.0004, 0.0008, 0.0006, 0.0012, 0.0005, 0.0010, 0.0020, 0.0008,\n                        0.0010, 0.0011, 0.0013, 0.0009, 0.0012, 0.0009, 0.0009, 0.0010, 0.0015,\n                        0.0013, 0.0015, 0.0015, 0.0009, 0.0013, 0.0009, 0.0017, 0.0010, 0.0004,\n                        0.0011, 0.0009, 0.0010, 0.0008, 0.0013, 0.0008, 0.0013, 0.0013, 0.0018,\n                        0.0012, 0.0010, 0.0011, 0.0014, 0.0010, 0.0020, 0.0017, 0.0007, 0.0016,\n                        0.0010, 0.0006, 0.0014, 0.0011, 0.0011, 0.0016, 0.0012, 0.0013, 0.0023,\n                        0.0014, 0.0009, 0.0010, 0.0008, 0.0014, 0.0010, 0.0012, 0.0011, 0.0002,\n                        0.0002, 0.0013, 0.0008, 0.0011, 0.0012, 0.0017, 0.0011, 0.0005, 0.0013,\n                        0.0019, 0.0008, 0.0010, 0.0020, 0.0009, 0.0009, 0.0008, 0.0005, 0.0009,\n                        0.0004, 0.0003, 0.0011, 0.0009, 0.0007, 0.0010, 0.0009, 0.0014, 0.0025,\n                        0.0013, 0.0014, 0.0008, 0.0013, 0.0017, 0.0003, 0.0011, 0.0005, 0.0006,\n                        0.0010, 0.0007, 0.0003, 0.0012, 0.0009, 0.0012, 0.0013, 0.0004, 0.0002,\n                        0.0008, 0.0016, 0.0002, 0.0009, 0.0011, 0.0015, 0.0010, 0.0002, 0.0008,\n                        0.0009, 0.0007, 0.0011, 0.0005, 0.0016, 0.0005, 0.0012, 0.0007, 0.0012,\n                        0.0014, 0.0006, 0.0011, 0.0011, 0.0018, 0.0008, 0.0022, 0.0009, 0.0010,\n                        0.0004, 0.0009, 0.0014, 0.0022, 0.0008, 0.0008, 0.0012, 0.0008, 0.0017]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                       dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n                (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n                  min_val=tensor([-0.1404, -0.0434, -0.1035, -0.0494, -0.0959, -0.0605, -0.1131, -0.0559,\n                          -0.0713, -0.1200, -0.1174, -0.1623, -0.1137, -0.1546, -0.1043, -0.1104,\n                          -0.1294, -0.1884, -0.1702, -0.1890, -0.1937, -0.0897, -0.1484, -0.1050,\n                          -0.2080, -0.0756, -0.0393, -0.1457, -0.1148, -0.1073, -0.0765, -0.1367,\n                          -0.0976, -0.1686, -0.0726, -0.2269, -0.1523, -0.1324, -0.0813, -0.1769,\n                          -0.1092, -0.2515, -0.2177, -0.0914, -0.2007, -0.0964, -0.0796, -0.0754,\n                          -0.1098, -0.0850, -0.0781, -0.1497, -0.0943, -0.2934, -0.1731, -0.1124,\n                          -0.1342, -0.0993, -0.1741, -0.0855, -0.1580, -0.1392, -0.0219, -0.0302,\n                          -0.1642, -0.0793, -0.1255, -0.1241, -0.2162, -0.1455, -0.0260, -0.1288,\n                          -0.1112, -0.1023, -0.0936, -0.2541, -0.0943, -0.0775, -0.0720, -0.0667,\n                          -0.1162, -0.0473, -0.0237, -0.1455, -0.1209, -0.0911, -0.0812, -0.1184,\n                          -0.1774, -0.3211, -0.0759, -0.1289, -0.1024, -0.1305, -0.1855, -0.0438,\n                          -0.1268, -0.0667, -0.0768, -0.0623, -0.0955, -0.0408, -0.1495, -0.1032,\n                          -0.1532, -0.1232, -0.0434, -0.0291, -0.0617, -0.2048, -0.0271, -0.1149,\n                          -0.1392, -0.1934, -0.0858, -0.0280, -0.0545, -0.1137, -0.0833, -0.0863,\n                          -0.0574, -0.2045, -0.0579, -0.1394, -0.0911, -0.1013, -0.1762, -0.0737,\n                          -0.1401, -0.1436, -0.1058, -0.1015, -0.1734, -0.0778, -0.1321, -0.0497,\n                          -0.0791, -0.1785, -0.1332, -0.0635, -0.0957, -0.1568, -0.1042, -0.1013]), max_val=tensor([0.2356, 0.0542, 0.0975, 0.0763, 0.1479, 0.0624, 0.1252, 0.2489, 0.0987,\n                          0.1245, 0.1398, 0.1585, 0.1016, 0.0606, 0.1099, 0.0950, 0.1251, 0.1765,\n                          0.0965, 0.1503, 0.0999, 0.1116, 0.1595, 0.1133, 0.2216, 0.1227, 0.0553,\n                          0.0693, 0.1076, 0.1268, 0.1061, 0.1630, 0.0235, 0.0946, 0.1706, 0.1574,\n                          0.1063, 0.0764, 0.1403, 0.1027, 0.1244, 0.1071, 0.1283, 0.0894, 0.1222,\n                          0.1251, 0.0609, 0.1775, 0.1345, 0.1349, 0.2004, 0.1392, 0.1622, 0.2070,\n                          0.0666, 0.0679, 0.0816, 0.0660, 0.0642, 0.1263, 0.1330, 0.1023, 0.0192,\n                          0.0264, 0.0890, 0.0986, 0.1422, 0.1491, 0.0850, 0.1123, 0.0583, 0.1697,\n                          0.2427, 0.0965, 0.1313, 0.0810, 0.1098, 0.1191, 0.1030, 0.0592, 0.1040,\n                          0.0543, 0.0351, 0.1255, 0.0904, 0.0807, 0.1303, 0.1026, 0.1665, 0.0846,\n                          0.1672, 0.1787, 0.0548, 0.1628, 0.2106, 0.0354, 0.1416, 0.0624, 0.0659,\n                          0.1320, 0.0603, 0.0414, 0.0915, 0.1118, 0.1431, 0.1673, 0.0515, 0.0289,\n                          0.1030, 0.1032, 0.0290, 0.0804, 0.0929, 0.1793, 0.1225, 0.0317, 0.1045,\n                          0.0889, 0.0617, 0.1393, 0.0624, 0.1463, 0.0530, 0.1476, 0.0951, 0.1578,\n                          0.1314, 0.0765, 0.0892, 0.1280, 0.2245, 0.0281, 0.2750, 0.1125, 0.0960,\n                          0.0454, 0.1146, 0.0817, 0.2770, 0.0974, 0.0975, 0.1490, 0.0965, 0.2198])\n                )\n              )\n            )\n            (2): ReLU6(inplace=True)\n          )\n          (1): Module(\n            (0): ConvBn2d(\n              144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False\n              (bn): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n                fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0026, 0.0129, 0.0045, 0.0108, 0.0030, 0.0137, 0.0016, 0.0018, 0.0035,\n                        0.0076, 0.0026, 0.0042, 0.0059, 0.0122, 0.0036, 0.0040, 0.0027, 0.0014,\n                        0.0024, 0.0049, 0.0031, 0.0034, 0.0027, 0.0032, 0.0016, 0.0120, 0.0147,\n                        0.0018, 0.0054, 0.0028, 0.0026, 0.0033, 0.0079, 0.0023, 0.0048, 0.0029,\n                        0.0019, 0.0023, 0.0037, 0.0069, 0.0049, 0.0060, 0.0051, 0.0020, 0.0026,\n                        0.0026, 0.0028, 0.0031, 0.0103, 0.0018, 0.0019, 0.0040, 0.0048, 0.0035,\n                        0.0018, 0.0104, 0.0032, 0.0033, 0.0015, 0.0153, 0.0029, 0.0028, 0.0425,\n                        0.0284, 0.0046, 0.0044, 0.0036, 0.0024, 0.0025, 0.0035, 0.0057, 0.0022,\n                        0.0015, 0.0033, 0.0074, 0.0026, 0.0031, 0.0045, 0.0093, 0.0127, 0.0045,\n                        0.0147, 0.0141, 0.0038, 0.0037, 0.0060, 0.0025, 0.0026, 0.0030, 0.0040,\n                        0.0023, 0.0020, 0.0048, 0.0036, 0.0040, 0.0212, 0.0024, 0.0128, 0.0091,\n                        0.0020, 0.0100, 0.0158, 0.0027, 0.0028, 0.0056, 0.0022, 0.0086, 0.0252,\n                        0.0093, 0.0020, 0.0155, 0.0035, 0.0026, 0.0019, 0.0121, 0.0307, 0.0034,\n                        0.0018, 0.0101, 0.0031, 0.0153, 0.0030, 0.0125, 0.0017, 0.0029, 0.0034,\n                        0.0029, 0.0075, 0.0031, 0.0039, 0.0013, 0.0111, 0.0013, 0.0101, 0.0027,\n                        0.0150, 0.0021, 0.0063, 0.0013, 0.0056, 0.0104, 0.0026, 0.0027, 0.0026]), zero_point=tensor([ 127,    0,  127,    0,  127,    0,  127, -128,  127,    0, -128, -128,\n                           0,    0,  127, -128, -128, -128,  127, -128,  127,  127,  127, -128,\n                        -128,    0,    0,  127, -128, -128,  127,  127,    0, -128, -128, -128,\n                        -128,  127,  127,    0,  127,  127, -128,  127, -128,  127,  127,  127,\n                           0, -128, -128,  127,  127, -128, -128,    0, -128,  127, -128,    0,\n                         127, -128,    0,    0,    0,  127, -128,  127, -128, -128,  127, -128,\n                        -128, -128, -128, -128,  127,  127, -128,    0,  127,    0,    0, -128,\n                         127,    0, -128, -128, -128, -128, -128,  127,  127, -128, -128,    0,\n                         127,    0,    0,  127,    0,    0,  127, -128, -128, -128,    0,    0,\n                           0, -128,    0,  127, -128,  127,    0,    0, -128,  127,    0,  127,\n                           0,  127,    0, -128,  127, -128,  127,    0, -128,  127, -128,    0,\n                        -128,    0,  127,    0,  127,    0, -128, -128,    0, -128, -128, -128],\n                       dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n                (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n                  min_val=tensor([-6.6032e-01, -1.6470e+00, -1.1392e+00, -1.3767e+00, -7.5231e-01,\n                          -1.7554e+00, -4.0268e-01,  8.1575e-02, -8.8776e-01, -9.7865e-01,\n                           5.3423e-02,  3.0117e-01, -7.5012e-01, -4.9216e-01, -9.1478e-01,\n                           1.9354e-01,  1.3316e-01,  6.2569e-02, -6.1787e-01,  2.5902e-01,\n                          -7.8653e-01, -8.5869e-01, -6.9430e-01,  1.7521e-02,  1.0586e-01,\n                          -1.5385e+00, -1.6143e+00, -4.5073e-01,  1.9487e-01,  1.6959e-01,\n                          -6.6977e-01, -8.3495e-01, -9.8891e-01,  1.7004e-01,  1.2458e-01,\n                           9.6025e-02,  1.2210e-01, -5.9545e-01, -9.3821e-01, -4.2642e-03,\n                          -1.2450e+00, -1.5280e+00,  1.6397e-01, -5.0682e-01,  1.3688e-01,\n                          -6.5303e-01, -7.1644e-01, -7.9437e-01, -1.3233e+00,  9.1496e-02,\n                           6.1582e-02, -1.0137e+00, -1.2170e+00,  2.2904e-01,  1.1354e-01,\n                          -1.3333e+00,  1.6107e-01, -8.4457e-01,  7.0935e-02, -1.9547e+00,\n                          -7.4984e-01,  1.4376e-01, -4.9614e+00, -3.2512e+00, -6.4973e-02,\n                          -1.1114e+00,  1.7751e-01, -6.2451e-01,  9.5818e-02,  2.1850e-01,\n                          -1.4496e+00,  6.8583e-02,  7.9912e-02,  9.9885e-02,  6.8784e-01,\n                           1.3772e-01, -7.8782e-01, -1.1549e+00,  4.9730e-01, -1.6281e+00,\n                          -1.1550e+00, -1.7405e+00, -1.7741e+00,  2.4522e-01, -9.5472e-01,\n                          -5.3450e-02,  1.2004e-01,  6.8300e-02,  1.9996e-01,  2.3009e-01,\n                           1.1176e-01, -5.2164e-01, -1.2235e+00,  2.5785e-01,  2.4173e-01,\n                          -2.7193e+00, -6.1688e-01, -1.4440e+00, -9.4624e-01, -4.9903e-01,\n                          -9.5967e-01, -2.0275e+00, -6.9487e-01,  1.3084e-01,  3.4854e-01,\n                           5.5820e-02, -1.0949e+00, -3.2281e+00, -1.1967e+00,  1.0913e-01,\n                          -1.9873e+00, -8.8545e-01,  1.4446e-01, -4.9098e-01, -1.4322e+00,\n                          -2.7531e+00,  1.3949e-01, -4.4997e-01, -1.2919e+00, -7.9653e-01,\n                          -1.9577e+00, -7.5241e-01, -1.2322e+00,  9.8558e-02, -7.3482e-01,\n                           6.0037e-02, -7.5054e-01, -9.6152e-01,  1.1774e-01, -9.9828e-01,\n                           5.8430e-02, -1.1151e+00,  6.3815e-02, -1.1496e+00, -6.9416e-01,\n                          -1.2716e+00, -5.3677e-01, -1.0137e-01,  2.5558e-03,  4.3915e-01,\n                          -1.3335e+00,  7.3375e-02,  3.2934e-02,  1.6695e-01]), max_val=tensor([-2.0620e-01,  1.4346e+00, -4.5419e-01,  3.7947e-02, -1.8735e-01,\n                           1.1404e+00, -1.1711e-01,  4.5882e-01, -2.1121e-01,  6.4431e-02,\n                           6.5458e-01,  1.0668e+00,  3.0552e-01,  1.5458e+00, -2.0035e-01,\n                           1.0283e+00,  6.9120e-01,  3.5645e-01, -2.1982e-01,  1.2607e+00,\n                          -2.1301e-01, -1.7709e-01, -1.7764e-01,  8.1925e-01,  3.9547e-01,\n                           8.5904e-01,  1.8678e+00, -1.0393e-01,  1.3736e+00,  7.0295e-01,\n                          -9.4689e-02, -1.5148e-01,  1.0077e+00,  5.7951e-01,  1.2175e+00,\n                           7.3360e-01,  4.9109e-01, -1.2120e-01, -1.5187e-01,  8.7923e-01,\n                          -2.5203e-01, -3.4488e-01,  1.2953e+00, -1.4988e-03,  6.6967e-01,\n                          -1.6855e-01, -9.3371e-02, -1.4942e-01,  7.9158e-01,  4.6824e-01,\n                           4.9703e-01, -1.9723e-01, -3.6980e-01,  8.8597e-01,  4.6836e-01,\n                           9.0719e-01,  8.0861e-01, -2.1751e-01,  3.8310e-01,  1.3266e+00,\n                          -8.1675e-02,  7.1716e-01,  5.3997e+00,  3.6009e+00,  5.7892e-01,\n                          -2.3086e-01,  9.0984e-01, -1.5065e-02,  6.4106e-01,  8.9146e-01,\n                          -4.6105e-02,  5.6617e-01,  3.8152e-01,  8.4461e-01,  1.8854e+00,\n                           6.6097e-01, -9.6750e-02, -4.1142e-01,  2.3752e+00,  7.5980e-01,\n                          -2.3401e-01,  1.8700e+00,  1.7953e+00,  9.8112e-01, -2.6801e-01,\n                           7.6123e-01,  6.3273e-01,  6.6558e-01,  7.6247e-01,  1.0139e+00,\n                           5.8890e-01, -1.0951e-01, -1.6669e-01,  9.1085e-01,  1.0265e+00,\n                           2.4915e+00, -2.3712e-01,  1.6251e+00,  1.1494e+00, -1.2974e-01,\n                           1.2729e+00,  1.8976e+00, -2.4265e-01,  7.0212e-01,  1.4213e+00,\n                           5.6379e-01,  1.0635e+00,  2.4772e+00,  1.1296e+00,  4.9857e-01,\n                           1.8693e+00, -2.3838e-01,  6.5525e-01, -1.3115e-01,  1.5370e+00,\n                           3.8998e+00,  8.5745e-01, -1.1012e-01,  9.3711e-01, -1.8840e-01,\n                           1.2809e+00, -3.1741e-01,  1.5836e+00,  4.3477e-01, -2.0472e-01,\n                           8.6811e-01, -3.1386e-01,  7.6811e-01,  7.8352e-01, -1.7668e-01,\n                           3.4394e-01,  1.4150e+00,  3.3408e-01,  1.2823e+00, -2.2302e-01,\n                           1.9010e+00, -9.9810e-02,  7.9684e-01,  3.2018e-01,  1.4266e+00,\n                           7.8900e-01,  6.5783e-01,  6.9282e-01,  6.5803e-01])\n                )\n              )\n            )\n            (2): ReLU6(inplace=True)\n          )\n          (2): ConvBn2d(\n            144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0055, 0.0040, 0.0043, 0.0040, 0.0038, 0.0063, 0.0056, 0.0053, 0.0043,\n                      0.0050, 0.0038, 0.0049, 0.0048, 0.0048, 0.0043, 0.0041, 0.0037, 0.0048,\n                      0.0038, 0.0050, 0.0040, 0.0040, 0.0042, 0.0057, 0.0045, 0.0043, 0.0042,\n                      0.0047, 0.0042, 0.0042, 0.0057, 0.0042]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n                min_val=tensor([-0.7055, -0.5057, -0.5444, -0.5117, -0.4886, -0.3465, -0.5380, -0.3869,\n                        -0.5105, -0.3397, -0.4519, -0.6264, -0.6204, -0.4861, -0.5547, -0.4250,\n                        -0.4795, -0.6119, -0.2917, -0.4348, -0.4876, -0.4750, -0.5397, -0.7281,\n                        -0.4297, -0.5555, -0.4934, -0.5977, -0.5429, -0.3876, -0.5745, -0.4686]), max_val=tensor([0.3160, 0.5113, 0.5359, 0.3869, 0.4183, 0.7985, 0.7101, 0.6757, 0.5467,\n                        0.6354, 0.4846, 0.3747, 0.5959, 0.6035, 0.3572, 0.5240, 0.3542, 0.4344,\n                        0.4822, 0.6370, 0.5098, 0.5107, 0.3685, 0.5767, 0.5701, 0.4664, 0.5375,\n                        0.5159, 0.5329, 0.5345, 0.7238, 0.5298])\n              )\n            )\n          )\n        )\n      )\n      (5): Module(\n        (conv): Module(\n          (0): Module(\n            (0): ConvBn2d(\n              32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n              (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n                fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0009, 0.0007, 0.0004, 0.0008, 0.0014, 0.0004, 0.0006, 0.0005, 0.0009,\n                        0.0011, 0.0004, 0.0020, 0.0005, 0.0005, 0.0011, 0.0010, 0.0009, 0.0006,\n                        0.0005, 0.0008, 0.0004, 0.0013, 0.0007, 0.0007, 0.0007, 0.0003, 0.0006,\n                        0.0006, 0.0008, 0.0009, 0.0007, 0.0006, 0.0010, 0.0008, 0.0008, 0.0004,\n                        0.0004, 0.0005, 0.0003, 0.0005, 0.0009, 0.0007, 0.0003, 0.0006, 0.0007,\n                        0.0006, 0.0003, 0.0004, 0.0003, 0.0007, 0.0006, 0.0005, 0.0008, 0.0007,\n                        0.0004, 0.0004, 0.0004, 0.0009, 0.0004, 0.0008, 0.0004, 0.0009, 0.0006,\n                        0.0004, 0.0012, 0.0004, 0.0005, 0.0004, 0.0012, 0.0010, 0.0005, 0.0008,\n                        0.0012, 0.0010, 0.0011, 0.0006, 0.0004, 0.0007, 0.0005, 0.0006, 0.0011,\n                        0.0009, 0.0007, 0.0013, 0.0011, 0.0005, 0.0005, 0.0010, 0.0010, 0.0006,\n                        0.0008, 0.0004, 0.0006, 0.0008, 0.0012, 0.0007, 0.0010, 0.0004, 0.0005,\n                        0.0005, 0.0010, 0.0005, 0.0005, 0.0008, 0.0006, 0.0008, 0.0007, 0.0009,\n                        0.0007, 0.0005, 0.0005, 0.0004, 0.0012, 0.0004, 0.0005, 0.0006, 0.0010,\n                        0.0004, 0.0005, 0.0008, 0.0008, 0.0006, 0.0007, 0.0008, 0.0010, 0.0005,\n                        0.0007, 0.0006, 0.0005, 0.0007, 0.0006, 0.0007, 0.0008, 0.0003, 0.0004,\n                        0.0008, 0.0004, 0.0006, 0.0011, 0.0008, 0.0010, 0.0008, 0.0004, 0.0010,\n                        0.0005, 0.0011, 0.0006, 0.0004, 0.0005, 0.0004, 0.0004, 0.0006, 0.0009,\n                        0.0008, 0.0005, 0.0012, 0.0008, 0.0007, 0.0011, 0.0006, 0.0006, 0.0005,\n                        0.0007, 0.0009, 0.0004, 0.0006, 0.0009, 0.0010, 0.0009, 0.0007, 0.0010,\n                        0.0006, 0.0006, 0.0005, 0.0007, 0.0007, 0.0005, 0.0007, 0.0010, 0.0006,\n                        0.0007, 0.0008, 0.0004, 0.0006, 0.0006, 0.0008, 0.0002, 0.0003, 0.0003,\n                        0.0009, 0.0007, 0.0009]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                       dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n                (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n                  min_val=tensor([-0.0643, -0.0598, -0.0543, -0.0976, -0.1737, -0.0448, -0.0595, -0.0644,\n                          -0.1000, -0.1236, -0.0468, -0.1225, -0.0539, -0.0348, -0.0899, -0.0794,\n                          -0.0917, -0.0505, -0.0529, -0.0921, -0.0303, -0.1469, -0.0862, -0.0897,\n                          -0.0683, -0.0396, -0.0767, -0.0675, -0.1088, -0.0770, -0.0518, -0.0785,\n                          -0.0849, -0.0817, -0.0463, -0.0573, -0.0469, -0.0677, -0.0388, -0.0566,\n                          -0.0850, -0.0929, -0.0418, -0.0599, -0.0845, -0.0828, -0.0440, -0.0473,\n                          -0.0307, -0.0887, -0.0609, -0.0590, -0.0992, -0.0850, -0.0452, -0.0399,\n                          -0.0516, -0.1202, -0.0472, -0.0846, -0.0567, -0.0727, -0.0500, -0.0470,\n                          -0.1573, -0.0428, -0.0272, -0.0371, -0.1080, -0.1331, -0.0323, -0.1021,\n                          -0.1552, -0.1019, -0.1344, -0.0333, -0.0487, -0.0845, -0.0701, -0.0560,\n                          -0.0766, -0.1171, -0.0825, -0.1715, -0.0554, -0.0492, -0.0634, -0.0643,\n                          -0.0917, -0.0730, -0.1044, -0.0370, -0.0556, -0.1004, -0.0897, -0.0793,\n                          -0.1258, -0.0485, -0.0612, -0.0515, -0.0779, -0.0520, -0.0539, -0.1016,\n                          -0.0798, -0.0975, -0.0538, -0.0671, -0.0846, -0.0585, -0.0386, -0.0504,\n                          -0.1163, -0.0455, -0.0583, -0.0632, -0.1242, -0.0575, -0.0595, -0.1054,\n                          -0.0713, -0.0471, -0.0954, -0.1008, -0.1250, -0.0652, -0.0864, -0.0782,\n                          -0.0607, -0.0629, -0.0826, -0.0783, -0.0436, -0.0428, -0.0195, -0.0657,\n                          -0.0464, -0.0812, -0.1360, -0.1035, -0.1041, -0.1014, -0.0458, -0.1273,\n                          -0.0636, -0.1230, -0.0493, -0.0497, -0.0526, -0.0528, -0.0576, -0.0622,\n                          -0.1195, -0.0981, -0.0704, -0.0987, -0.0746, -0.0802, -0.1388, -0.0755,\n                          -0.0785, -0.0699, -0.0620, -0.1206, -0.0525, -0.0538, -0.0985, -0.0974,\n                          -0.1210, -0.0910, -0.0736, -0.0762, -0.0786, -0.0617, -0.0877, -0.0778,\n                          -0.0533, -0.0765, -0.0753, -0.0819, -0.0686, -0.0601, -0.0504, -0.0816,\n                          -0.0819, -0.0747, -0.0236, -0.0291, -0.0186, -0.1198, -0.0341, -0.0899]), max_val=tensor([0.1123, 0.0878, 0.0377, 0.0869, 0.0707, 0.0385, 0.0801, 0.0600, 0.1165,\n                          0.1353, 0.0482, 0.2503, 0.0576, 0.0602, 0.1375, 0.1265, 0.1167, 0.0728,\n                          0.0646, 0.1004, 0.0563, 0.1632, 0.0853, 0.0635, 0.0826, 0.0340, 0.0821,\n                          0.0704, 0.0788, 0.1123, 0.0841, 0.0799, 0.1246, 0.1047, 0.0955, 0.0264,\n                          0.0533, 0.0592, 0.0400, 0.0613, 0.1149, 0.0572, 0.0386, 0.0716, 0.0665,\n                          0.0375, 0.0358, 0.0498, 0.0349, 0.0699, 0.0736, 0.0656, 0.0434, 0.0872,\n                          0.0236, 0.0465, 0.0565, 0.0787, 0.0460, 0.1036, 0.0418, 0.1200, 0.0786,\n                          0.0292, 0.1234, 0.0451, 0.0670, 0.0497, 0.1502, 0.0841, 0.0657, 0.0736,\n                          0.1094, 0.1272, 0.1228, 0.0718, 0.0304, 0.0455, 0.0453, 0.0779, 0.1408,\n                          0.0451, 0.0892, 0.0927, 0.1432, 0.0582, 0.0458, 0.1222, 0.1251, 0.0593,\n                          0.0653, 0.0563, 0.0732, 0.1023, 0.1503, 0.0868, 0.0797, 0.0393, 0.0664,\n                          0.0640, 0.1208, 0.0662, 0.0600, 0.0525, 0.0806, 0.0742, 0.0896, 0.1139,\n                          0.0860, 0.0578, 0.0623, 0.0536, 0.1506, 0.0423, 0.0552, 0.0768, 0.1240,\n                          0.0561, 0.0497, 0.0401, 0.1008, 0.0704, 0.0732, 0.1052, 0.1016, 0.0493,\n                          0.0321, 0.0580, 0.0600, 0.0903, 0.0622, 0.0839, 0.0966, 0.0439, 0.0476,\n                          0.1047, 0.0402, 0.0554, 0.1003, 0.0405, 0.1209, 0.0665, 0.0365, 0.0769,\n                          0.0539, 0.1447, 0.0791, 0.0192, 0.0667, 0.0321, 0.0569, 0.0758, 0.0513,\n                          0.0682, 0.0461, 0.1571, 0.1019, 0.0890, 0.0788, 0.0581, 0.0509, 0.0384,\n                          0.0910, 0.1141, 0.0484, 0.0821, 0.1180, 0.1325, 0.0722, 0.0601, 0.1209,\n                          0.0397, 0.0690, 0.0166, 0.0539, 0.0888, 0.0610, 0.0939, 0.1240, 0.0473,\n                          0.0930, 0.0980, 0.0557, 0.0679, 0.0549, 0.0966, 0.0308, 0.0323, 0.0435,\n                          0.0675, 0.0878, 0.1164])\n                )\n              )\n            )\n            (2): ReLU6(inplace=True)\n          )\n          (1): Module(\n            (0): ConvBn2d(\n              192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False\n              (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n                fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0176, 0.0127, 0.0131, 0.0091, 0.0153, 0.0242, 0.0093, 0.0134, 0.0017,\n                        0.0120, 0.0253, 0.0217, 0.0236, 0.0105, 0.0142, 0.0096, 0.0211, 0.0140,\n                        0.0149, 0.0125, 0.0258, 0.0203, 0.0154, 0.0132, 0.0138, 0.0276, 0.0185,\n                        0.0112, 0.0135, 0.0092, 0.0019, 0.0113, 0.0183, 0.0248, 0.0108, 0.0228,\n                        0.0157, 0.0244, 0.0170, 0.0115, 0.0071, 0.0153, 0.0317, 0.0116, 0.0129,\n                        0.0143, 0.0292, 0.0123, 0.0316, 0.0136, 0.0130, 0.0143, 0.0116, 0.0151,\n                        0.0317, 0.0180, 0.0159, 0.0189, 0.0135, 0.0092, 0.0297, 0.0167, 0.0130,\n                        0.0224, 0.0033, 0.0211, 0.0159, 0.0125, 0.0136, 0.0148, 0.0225, 0.0339,\n                        0.0021, 0.0041, 0.0063, 0.0214, 0.0202, 0.0129, 0.0097, 0.0242, 0.0086,\n                        0.0053, 0.0115, 0.0185, 0.0122, 0.0053, 0.0101, 0.0091, 0.0136, 0.0134,\n                        0.0097, 0.0374, 0.0107, 0.0225, 0.0058, 0.0097, 0.0139, 0.0326, 0.0123,\n                        0.0144, 0.0142, 0.0220, 0.0050, 0.0122, 0.0178, 0.0114, 0.0085, 0.0104,\n                        0.0087, 0.0213, 0.0079, 0.0137, 0.0088, 0.0216, 0.0097, 0.0136, 0.0104,\n                        0.0104, 0.0299, 0.0131, 0.0134, 0.0170, 0.0134, 0.0138, 0.0201, 0.0124,\n                        0.0120, 0.0140, 0.0100, 0.0179, 0.0164, 0.0114, 0.0145, 0.0225, 0.0147,\n                        0.0316, 0.0353, 0.0174, 0.0134, 0.0097, 0.0076, 0.0081, 0.0131, 0.0103,\n                        0.0151, 0.0173, 0.0136, 0.0255, 0.0126, 0.0148, 0.0163, 0.0146, 0.0112,\n                        0.0131, 0.0156, 0.0201, 0.0132, 0.0152, 0.0116, 0.0116, 0.0235, 0.0311,\n                        0.0224, 0.0200, 0.0138, 0.0157, 0.0081, 0.0130, 0.0137, 0.0176, 0.0069,\n                        0.0161, 0.0235, 0.0295, 0.0034, 0.0095, 0.0174, 0.0091, 0.0160, 0.0165,\n                        0.0189, 0.0132, 0.0144, 0.0113, 0.0109, 0.0241, 0.0433, 0.0251, 0.0200,\n                        0.0105, 0.0103, 0.0095]), zero_point=tensor([   0,    0,    0,    0,    0,    0,    0,    0,  127,    0,    0,    0,\n                           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                           0,    0,    0,    0,    0,    0,  127,    0,    0,    0,    0,    0,\n                           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         127,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                           0,    0,    0,    0,    0,    0,    0,    0,    0, -128,    0,    0,\n                           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n                       dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n                (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n                  min_val=tensor([-2.2467, -1.6222, -1.6728, -0.1719, -0.7580, -2.6479, -1.0010, -1.0879,\n                          -0.4288, -0.1854, -2.4547, -0.8842, -1.5237, -1.0759, -1.8178, -0.5908,\n                          -0.8334, -1.5057, -1.9009, -1.4535, -2.9986, -0.9376, -0.4566, -0.6546,\n                          -1.3717, -3.1620, -0.5649, -1.4296, -1.2233, -1.1199, -0.4866, -1.3886,\n                          -2.3409, -3.1732, -0.3915, -0.8410, -1.4854, -0.8926, -1.6048, -0.6562,\n                          -0.9125, -1.9552, -3.1020, -0.9461, -1.6525, -1.3826, -1.8330, -1.4582,\n                          -4.0218, -1.3575, -1.2283, -1.8244, -1.2444, -0.3013, -4.0633, -2.3044,\n                          -0.5186, -0.7659, -0.9479, -1.1835, -3.7486, -0.3701, -1.0506, -2.4314,\n                          -0.4164, -2.6970, -1.1404, -1.6048, -0.4760, -0.5614, -2.8808, -0.1085,\n                          -0.5280, -0.5246, -0.8120, -1.8597, -2.0803, -1.3986, -1.2364, -1.3180,\n                          -1.0755, -0.6819, -0.8102, -0.8516, -0.1303, -0.6780, -0.8420, -1.1681,\n                          -0.7479, -1.5726, -1.0696, -1.9358, -1.3661, -1.1977, -0.7368, -0.7987,\n                          -1.7740, -2.6728, -1.3595, -1.4140, -0.9605, -1.1101, -0.6451, -1.2925,\n                          -0.2637, -0.5484, -0.5968, -1.2543, -0.6795, -2.7267, -1.0126, -1.4602,\n                          -1.1254, -2.7671, -1.2417, -0.7713, -0.3699, -1.3293, -1.5967, -1.4985,\n                          -1.2423, -2.1819, -1.3819, -1.1228, -1.0116, -1.4485, -1.5372, -1.2779,\n                          -1.2400, -2.2893, -1.4370, -1.2837, -1.7124, -2.5112, -1.8836, -4.0463,\n                          -4.5165, -1.0317, -1.7119, -1.2380, -0.9723, -0.7510, -1.6813, -1.3209,\n                          -1.4537, -0.3881, -0.7001, -2.8237, -1.3997, -1.6313, -1.3827, -0.4965,\n                          -0.9039, -1.6832, -1.9946, -2.5759, -1.6903, -1.1040, -1.4875, -1.4818,\n                          -1.7178, -3.9809, -1.1611, -0.4290, -1.3417, -0.0776, -0.7406, -0.0746,\n                          -1.7556, -1.2587, -0.8811, -1.0557, -0.8734, -1.6991, -0.4351, -1.0235,\n                          -0.8273,  0.0471, -2.0418, -1.4768, -2.4191, -1.3420, -0.8225, -1.4444,\n                          -1.1135, -0.9120, -5.5466, -2.6832, -2.2795, -0.5603, -1.3198, -1.1183]), max_val=tensor([ 1.0994,  1.3889,  1.5817,  1.1580,  1.9480,  3.0714,  1.1766,  1.7002,\n                          -0.0739,  1.5248,  3.2128,  2.7581,  2.9966,  1.3361,  0.5072,  1.2201,\n                           2.6837,  1.7737,  1.0674,  1.5907,  3.2712,  2.5817,  1.9585,  1.6723,\n                           1.7569,  3.5048,  2.3485,  1.3865,  1.7196,  1.1713, -0.0310,  1.4301,\n                           0.6211,  1.4787,  1.3758,  2.8925,  1.9966,  3.0932,  2.1649,  1.4638,\n                           0.4240,  0.4335,  4.0229,  1.4683,  1.0899,  1.8143,  3.7075,  1.5598,\n                           4.0078,  1.7331,  1.6494,  0.7532,  1.4719,  1.9191,  4.0301,  1.9618,\n                           2.0180,  2.4063,  1.7208,  0.8938,  3.7750,  2.1245,  1.6522,  2.8455,\n                           0.0310,  0.2636,  2.0194,  1.4779,  1.7212,  1.8831,  2.7124,  4.3113,\n                          -0.1040,  0.0152,  0.0588,  2.7199,  2.5647,  1.6407,  0.5630,  3.0676,\n                           1.0956,  0.1985,  1.4578,  2.3494,  1.5521,  0.1708,  1.2810,  1.0416,\n                           1.7229,  1.6988,  1.2298,  4.7535,  1.1291,  2.8611,  0.1566,  1.2336,\n                           1.3688,  4.1428,  1.5636,  1.8315,  1.8030,  2.7997,  0.3840,  1.5551,\n                           2.2597,  1.4478,  1.0816,  1.3197,  1.1060,  1.1930,  0.7375,  1.7398,\n                           0.9641,  0.0395,  0.8036,  1.7214,  1.3156,  1.2267,  3.7963,  1.6595,\n                           1.7026,  1.5318,  1.7005,  1.7507,  2.5549,  1.5713,  1.3324,  1.7798,\n                           1.2736,  1.2569,  2.0777,  1.4490,  1.8428,  2.8541,  1.0382,  1.8670,\n                           0.7134,  2.2101,  1.0021,  1.0997,  0.8594,  1.0325,  0.5922,  0.3674,\n                           1.9219,  2.1928,  1.7231,  3.2407,  1.6021,  1.8733,  2.0722,  1.8480,\n                           1.4246,  1.1833,  1.4098,  0.0322,  1.6177,  1.9318,  1.3538,  1.3668,\n                           2.9892,  1.9400,  2.8490,  2.5341,  1.7544,  1.9998,  1.0348,  1.6571,\n                           0.7104,  2.2386,  0.5004,  2.0441,  2.9874,  3.7519,  0.2814,  1.2019,\n                           2.2158,  2.3306,  1.7753,  2.0988,  0.9125,  1.6786,  1.8304,  1.4326,\n                           1.3828,  3.0586,  1.3235,  3.1828,  2.5446,  1.3343,  0.5121,  1.2076])\n                )\n              )\n            )\n            (2): ReLU6(inplace=True)\n          )\n          (2): ConvBn2d(\n            192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0036, 0.0035, 0.0042, 0.0033, 0.0050, 0.0030, 0.0059, 0.0042, 0.0034,\n                      0.0044, 0.0054, 0.0035, 0.0031, 0.0041, 0.0037, 0.0037, 0.0065, 0.0038,\n                      0.0032, 0.0068, 0.0044, 0.0027, 0.0036, 0.0068, 0.0039, 0.0036, 0.0044,\n                      0.0052, 0.0041, 0.0059, 0.0031, 0.0047]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n                min_val=tensor([-0.4564, -0.3981, -0.4949, -0.4191, -0.5699, -0.3890, -0.7495, -0.4518,\n                        -0.3236, -0.5647, -0.6856, -0.4426, -0.3030, -0.3492, -0.3292, -0.4433,\n                        -0.8305, -0.3795, -0.4063, -0.8689, -0.5531, -0.2980, -0.4012, -0.5910,\n                        -0.5003, -0.4583, -0.3594, -0.5322, -0.4519, -0.6038, -0.4026, -0.5523]), max_val=tensor([0.4477, 0.4490, 0.5351, 0.3567, 0.6303, 0.2452, 0.4879, 0.5368, 0.4333,\n                        0.4188, 0.6121, 0.4173, 0.3942, 0.5241, 0.4714, 0.4719, 0.6456, 0.4813,\n                        0.3126, 0.6135, 0.5538, 0.3485, 0.4616, 0.8634, 0.4889, 0.4560, 0.5551,\n                        0.6604, 0.5270, 0.7459, 0.3189, 0.5990])\n              )\n            )\n          )\n        )\n      )\n      (6): Module(\n        (conv): Module(\n          (0): Module(\n            (0): ConvBn2d(\n              32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n              (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n                fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([4.0486e-04, 2.9541e-04, 2.6363e-04, 1.6009e-04, 3.4935e-04, 8.3262e-04,\n                        2.4026e-05, 8.5237e-04, 8.3720e-04, 3.9972e-04, 6.1426e-04, 9.5525e-04,\n                        5.8180e-04, 4.6597e-04, 5.9875e-04, 5.5806e-04, 3.5880e-04, 3.6609e-04,\n                        8.5571e-04, 6.1769e-04, 3.7387e-04, 9.0252e-04, 7.2644e-04, 4.5973e-04,\n                        4.6821e-04, 5.0631e-04, 6.6098e-04, 2.4547e-04, 3.7136e-04, 4.5634e-04,\n                        7.6692e-04, 3.1010e-04, 5.8512e-04, 6.4575e-04, 7.1043e-04, 8.1428e-04,\n                        6.0603e-04, 5.2797e-04, 7.6259e-04, 5.4747e-04, 5.9300e-04, 4.8036e-04,\n                        2.9510e-04, 3.0414e-04, 7.5314e-04, 8.7673e-04, 4.2211e-04, 7.2097e-04,\n                        5.2638e-04, 6.1563e-04, 6.0189e-04, 8.3653e-04, 1.1485e-03, 6.2211e-04,\n                        4.7126e-04, 7.1432e-04, 3.4078e-04, 3.7825e-04, 7.0447e-04, 8.0335e-04,\n                        4.0379e-04, 2.2581e-04, 1.9679e-04, 4.9177e-04, 6.0602e-04, 6.2671e-04,\n                        4.8693e-04, 5.8752e-04, 1.1450e-03, 5.8971e-04, 6.7881e-04, 4.6870e-04,\n                        6.5394e-04, 7.9716e-04, 7.4872e-04, 3.4882e-04, 2.5247e-04, 6.4786e-04,\n                        5.4295e-04, 5.8150e-04, 6.0828e-04, 6.3384e-04, 5.2471e-04, 7.7632e-04,\n                        6.5427e-04, 6.4711e-04, 6.5212e-04, 4.2760e-04, 4.4588e-04, 7.1795e-04,\n                        6.3027e-04, 2.7805e-04, 7.7097e-04, 5.7492e-04, 4.5935e-04, 7.0261e-04,\n                        7.8053e-04, 5.3158e-04, 5.4596e-04, 3.2229e-04, 5.3966e-04, 3.8592e-04,\n                        5.7548e-04, 9.6790e-04, 4.5163e-04, 4.9837e-04, 8.5998e-04, 7.1492e-04,\n                        3.8181e-04, 7.9291e-04, 6.6166e-04, 7.2393e-04, 7.4600e-04, 6.4068e-04,\n                        6.1851e-04, 5.0625e-04, 3.7025e-04, 2.1379e-04, 5.1791e-04, 9.3105e-04,\n                        4.1132e-04, 8.8908e-04, 6.2979e-04, 4.5905e-04, 7.2649e-04, 5.7231e-04,\n                        6.4637e-04, 6.0711e-04, 7.2802e-04, 6.1906e-04, 6.3303e-04, 2.9787e-04,\n                        8.9986e-04, 5.8643e-04, 4.3516e-04, 4.6251e-04, 5.1291e-04, 5.9837e-04,\n                        3.7350e-04, 7.6410e-04, 7.0629e-04, 7.5177e-04, 6.0930e-04, 6.3740e-04,\n                        7.3331e-04, 5.2062e-04, 6.6746e-04, 6.6025e-04, 4.4953e-04, 6.9922e-04,\n                        5.0129e-04, 6.7803e-04, 6.0717e-04, 5.5281e-04, 7.6441e-04, 6.6996e-04,\n                        5.7128e-04, 4.7216e-04, 3.9888e-04, 5.0553e-04, 7.9822e-04, 6.0439e-04,\n                        5.0098e-04, 7.0934e-04, 8.7417e-04, 4.8498e-04, 3.1072e-04, 5.2678e-04,\n                        4.6033e-04, 4.1911e-04, 5.0170e-04, 3.9860e-04, 7.1564e-04, 9.4175e-04,\n                        6.4772e-04, 6.2805e-04, 5.2564e-04, 6.8693e-04, 3.4543e-04, 9.3772e-04,\n                        4.3393e-04, 9.7878e-04, 4.7582e-04, 7.8905e-04, 6.6498e-04, 6.5216e-04,\n                        5.3404e-04, 5.3426e-04, 4.8692e-04, 5.7443e-04, 8.5227e-04, 5.6620e-04]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                       dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n                (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n                  min_val=tensor([-0.0518, -0.0378, -0.0242, -0.0183, -0.0408, -0.1066, -0.0023, -0.1091,\n                          -0.0544, -0.0512, -0.0786, -0.0750, -0.0713, -0.0490, -0.0766, -0.0714,\n                          -0.0307, -0.0408, -0.1095, -0.0777, -0.0479, -0.1155, -0.0781, -0.0391,\n                          -0.0577, -0.0531, -0.0833, -0.0314, -0.0475, -0.0277, -0.0908, -0.0397,\n                          -0.0749, -0.0827, -0.0909, -0.1042, -0.0776, -0.0529, -0.0976, -0.0545,\n                          -0.0759, -0.0615, -0.0378, -0.0257, -0.0964, -0.0971, -0.0540, -0.0729,\n                          -0.0674, -0.0788, -0.0770, -0.1071, -0.0853, -0.0796, -0.0603, -0.0877,\n                          -0.0283, -0.0484, -0.0589, -0.1028, -0.0374, -0.0248, -0.0170, -0.0629,\n                          -0.0739, -0.0508, -0.0623, -0.0752, -0.0788, -0.0755, -0.0869, -0.0600,\n                          -0.0444, -0.1020, -0.0593, -0.0343, -0.0323, -0.0580, -0.0695, -0.0744,\n                          -0.0550, -0.0811, -0.0484, -0.0994, -0.0837, -0.0816, -0.0482, -0.0547,\n                          -0.0571, -0.0749, -0.0807, -0.0356, -0.0566, -0.0736, -0.0588, -0.0407,\n                          -0.0954, -0.0650, -0.0616, -0.0371, -0.0420, -0.0484, -0.0386, -0.0746,\n                          -0.0578, -0.0412, -0.0471, -0.0915, -0.0355, -0.0629, -0.0636, -0.0927,\n                          -0.0955, -0.0581, -0.0694, -0.0212, -0.0474, -0.0254, -0.0660, -0.1192,\n                          -0.0417, -0.0487, -0.0688, -0.0401, -0.0930, -0.0733, -0.0827, -0.0777,\n                          -0.0932, -0.0270, -0.0632, -0.0359, -0.1152, -0.0751, -0.0557, -0.0592,\n                          -0.0657, -0.0766, -0.0478, -0.0668, -0.0572, -0.0962, -0.0518, -0.0750,\n                          -0.0686, -0.0511, -0.0854, -0.0845, -0.0575, -0.0895, -0.0495, -0.0868,\n                          -0.0777, -0.0653, -0.0851, -0.0858, -0.0731, -0.0604, -0.0511, -0.0290,\n                          -0.0541, -0.0737, -0.0552, -0.0464, -0.0729, -0.0621, -0.0391, -0.0573,\n                          -0.0589, -0.0536, -0.0573, -0.0330, -0.0850, -0.0631, -0.0829, -0.0529,\n                          -0.0673, -0.0879, -0.0442, -0.1200, -0.0555, -0.1108, -0.0609, -0.0839,\n                          -0.0851, -0.0682, -0.0510, -0.0684, -0.0623, -0.0735, -0.0911, -0.0725]), max_val=tensor([0.0455, 0.0343, 0.0335, 0.0203, 0.0444, 0.0614, 0.0031, 0.0929, 0.1063,\n                          0.0479, 0.0608, 0.1213, 0.0739, 0.0592, 0.0555, 0.0492, 0.0456, 0.0465,\n                          0.0574, 0.0784, 0.0460, 0.0603, 0.0923, 0.0584, 0.0595, 0.0643, 0.0839,\n                          0.0263, 0.0354, 0.0580, 0.0974, 0.0344, 0.0680, 0.0735, 0.0665, 0.0633,\n                          0.0756, 0.0671, 0.0547, 0.0695, 0.0644, 0.0506, 0.0341, 0.0386, 0.0649,\n                          0.1113, 0.0446, 0.0916, 0.0591, 0.0676, 0.0561, 0.0491, 0.1459, 0.0387,\n                          0.0363, 0.0907, 0.0433, 0.0369, 0.0895, 0.0590, 0.0513, 0.0287, 0.0250,\n                          0.0619, 0.0770, 0.0796, 0.0586, 0.0582, 0.1454, 0.0572, 0.0617, 0.0322,\n                          0.0831, 0.0776, 0.0951, 0.0443, 0.0201, 0.0823, 0.0401, 0.0551, 0.0773,\n                          0.0343, 0.0666, 0.0599, 0.0505, 0.0822, 0.0828, 0.0495, 0.0438, 0.0912,\n                          0.0480, 0.0345, 0.0979, 0.0691, 0.0567, 0.0892, 0.0991, 0.0675, 0.0693,\n                          0.0409, 0.0685, 0.0490, 0.0731, 0.1229, 0.0342, 0.0633, 0.1092, 0.0880,\n                          0.0485, 0.1007, 0.0840, 0.0657, 0.0477, 0.0814, 0.0786, 0.0643, 0.0286,\n                          0.0272, 0.0658, 0.0904, 0.0522, 0.1129, 0.0800, 0.0583, 0.0797, 0.0666,\n                          0.0688, 0.0659, 0.0593, 0.0786, 0.0804, 0.0378, 0.0778, 0.0429, 0.0511,\n                          0.0546, 0.0534, 0.0571, 0.0441, 0.0970, 0.0897, 0.0763, 0.0774, 0.0809,\n                          0.0931, 0.0661, 0.0563, 0.0720, 0.0309, 0.0598, 0.0637, 0.0607, 0.0600,\n                          0.0702, 0.0971, 0.0728, 0.0569, 0.0494, 0.0504, 0.0642, 0.1014, 0.0768,\n                          0.0636, 0.0901, 0.1110, 0.0598, 0.0395, 0.0669, 0.0511, 0.0420, 0.0637,\n                          0.0506, 0.0909, 0.1196, 0.0633, 0.0798, 0.0553, 0.0837, 0.0419, 0.0912,\n                          0.0372, 0.1243, 0.0464, 0.1002, 0.0667, 0.0828, 0.0678, 0.0597, 0.0525,\n                          0.0472, 0.1082, 0.0453])\n                )\n              )\n            )\n            (2): ReLU6(inplace=True)\n          )\n          (1): Module(\n            (0): ConvBn2d(\n              192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False\n              (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n                fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0136, 0.0229, 0.0134, 0.0377, 0.0039, 0.0072, 0.0003, 0.0064, 0.0113,\n                        0.0131, 0.0083, 0.0092, 0.0106, 0.0127, 0.0128, 0.0161, 0.0264, 0.0177,\n                        0.0075, 0.0090, 0.0161, 0.0170, 0.0088, 0.0199, 0.0141, 0.0139, 0.0028,\n                        0.0186, 0.0175, 0.0079, 0.0044, 0.0180, 0.0123, 0.0021, 0.0112, 0.0153,\n                        0.0150, 0.0103, 0.0047, 0.0129, 0.0244, 0.0107, 0.0165, 0.0146, 0.0127,\n                        0.0035, 0.0296, 0.0137, 0.0099, 0.0089, 0.0075, 0.0130, 0.0052, 0.0083,\n                        0.0184, 0.0098, 0.0175, 0.0177, 0.0086, 0.0107, 0.0167, 0.0386, 0.0393,\n                        0.0104, 0.0028, 0.0230, 0.0093, 0.0071, 0.0031, 0.0117, 0.0164, 0.0153,\n                        0.0074, 0.0121, 0.0122, 0.0287, 0.0266, 0.0053, 0.0101, 0.0136, 0.0223,\n                        0.0087, 0.0112, 0.0159, 0.0098, 0.0027, 0.0117, 0.0121, 0.0093, 0.0120,\n                        0.0084, 0.0236, 0.0101, 0.0133, 0.0112, 0.0261, 0.0118, 0.0096, 0.0257,\n                        0.0278, 0.0114, 0.0134, 0.0166, 0.0234, 0.0208, 0.0064, 0.0099, 0.0101,\n                        0.0100, 0.0245, 0.0164, 0.0035, 0.0158, 0.0120, 0.0129, 0.0114, 0.0236,\n                        0.0210, 0.0236, 0.0131, 0.0170, 0.0165, 0.0114, 0.0168, 0.0124, 0.0165,\n                        0.0050, 0.0127, 0.0079, 0.0192, 0.0063, 0.0129, 0.0185, 0.0135, 0.0087,\n                        0.0125, 0.0174, 0.0101, 0.0283, 0.0100, 0.0206, 0.0080, 0.0140, 0.0048,\n                        0.0090, 0.0114, 0.0132, 0.0057, 0.0194, 0.0127, 0.0085, 0.0180, 0.0081,\n                        0.0099, 0.0071, 0.0110, 0.0068, 0.0111, 0.0230, 0.0286, 0.0087, 0.0107,\n                        0.0109, 0.0145, 0.0069, 0.0149, 0.0147, 0.0100, 0.0108, 0.0132, 0.0328,\n                        0.0186, 0.0023, 0.0204, 0.0149, 0.0135, 0.0113, 0.0342, 0.0166, 0.0020,\n                        0.0210, 0.0061, 0.0121, 0.0150, 0.0027, 0.0081, 0.0072, 0.0096, 0.0158,\n                        0.0098, 0.0115, 0.0097]), zero_point=tensor([  0,   0,   0,   0, 127,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 127,   0,\n                          0,   0, 127,   0,   0, 127,   0,   0,   0,   0,   0,   0,   0,   0,\n                          0,   0,   0, 127,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                          0,   0,   0,   0,   0,   0,   0,   0, 127,   0,   0,   0, 127,   0,\n                          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 127,\n                          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                          0,   0,   0, 127,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                          0,   0,   0,   0, 127,   0,   0,   0,   0,   0,   0, 127,   0,   0,\n                          0,   0, 127,   0,   0,   0,   0,   0,   0,   0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n                (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n                  min_val=tensor([-1.1885, -1.8516, -0.3090, -4.8291, -0.9996, -0.9227, -0.0392, -0.8199,\n                          -0.5379, -1.0171, -0.3957, -0.3117, -0.6464, -1.4722, -0.7741, -1.7249,\n                          -1.3551, -1.1769, -0.9646, -0.8169, -2.0654, -1.7436, -1.1292, -1.4573,\n                          -1.8003, -0.9090, -0.7237, -2.3813, -1.1953, -1.0140, -1.1319, -1.1692,\n                          -0.8466, -0.5452, -1.3885, -1.0500, -1.9202, -1.3138, -0.6042, -1.6574,\n                          -1.3018, -1.3686, -0.6959, -1.6673, -0.2360, -0.8820, -1.0088, -1.5004,\n                          -1.0556, -0.7379, -0.9587, -1.6704, -0.6643, -0.6587, -1.7754, -0.3771,\n                          -2.2447, -1.6118, -1.0971, -0.7132, -1.4494, -4.9449, -5.0283, -1.1042,\n                          -0.7068, -0.9831, -0.8953, -0.9035, -0.7819, -0.8807, -2.0955, -1.2199,\n                          -0.9084, -1.5491, -1.5579, -1.5006, -1.9516, -0.6758, -1.0681, -0.9040,\n                          -1.0972, -1.1144, -0.9764, -2.0343, -1.2553, -0.3505, -0.6167, -1.5493,\n                          -1.1947, -1.0453, -0.8398, -3.0268, -0.0643, -0.7696, -0.7609, -0.1435,\n                          -0.3968, -1.2268, -1.1893, -1.8223, -0.3397, -1.0121, -0.8972, -0.2858,\n                          -1.7076, -0.8141, -0.9304, -0.5735, -1.2205, -1.0570, -0.4387, -0.8943,\n                          -2.0210, -1.2282, -0.7408, -1.4600, -1.1761, -2.1881, -1.0222, -0.5553,\n                          -2.1808, -1.5289, -0.2326, -2.0975, -1.5917, -1.4998, -0.6383, -0.8380,\n                          -1.0057, -1.5171, -0.8101, -1.6511, -0.2081, -0.9532, -0.6304, -1.0683,\n                          -1.5874, -0.8379, -1.1684, -0.3688, -1.6442, -0.1193, -0.9545, -1.2232,\n                          -0.8424, -0.9774, -1.6935, -0.7296, -1.1659, -1.6208, -0.7784, -1.5536,\n                          -1.0428, -1.2635, -0.9039, -0.0780, -0.8690, -1.2413, -1.0330, -2.9744,\n                          -0.3139, -1.3689, -0.9919, -0.4527, -0.8887, -1.1479, -0.8513, -0.6237,\n                          -0.4014, -1.6928, -4.1938, -2.3784, -0.5797, -0.0518, -1.1379, -1.7258,\n                          -0.9631, -0.0925, -1.2593, -0.5094, -0.6174, -0.4511, -0.7410, -0.5269,\n                          -0.6892, -0.7410, -0.7655, -1.0905, -1.0432, -1.1411, -0.0108, -1.2386]), max_val=tensor([ 1.7242e+00,  2.9047e+00,  1.7056e+00,  3.2628e+00, -8.4827e-02,\n                           1.2098e-01,  3.4811e-02,  6.9326e-01,  1.4404e+00,  1.6691e+00,\n                           1.0586e+00,  1.1665e+00,  1.3450e+00,  1.6145e+00,  1.6309e+00,\n                           2.0384e+00,  3.3588e+00,  2.2450e+00,  2.5106e-01,  1.1401e+00,\n                           1.7066e+00,  2.1643e+00,  7.5076e-01,  2.5222e+00,  1.6457e+00,\n                           1.7635e+00, -2.8803e-01,  2.2623e+00,  2.2209e+00,  6.2351e-01,\n                          -3.3577e-01,  2.2901e+00,  1.5584e+00, -7.8342e-02,  1.4163e+00,\n                           1.9457e+00,  6.2979e-01,  1.2426e+00,  1.5723e-01,  1.3921e+00,\n                           3.0935e+00,  1.1061e+00,  2.1018e+00,  1.8577e+00,  1.6119e+00,\n                          -1.3352e-01,  3.7542e+00,  1.7355e+00,  1.2599e+00,  1.1352e+00,\n                           7.4098e-01,  1.2986e+00,  5.2627e-01,  1.0501e+00,  2.3429e+00,\n                           1.2450e+00,  4.1275e-02,  2.2498e+00,  9.4275e-01,  1.3589e+00,\n                           2.1261e+00,  4.6046e-01,  7.7235e-01,  1.3166e+00, -1.4352e-01,\n                           2.9246e+00,  1.1863e+00,  2.3581e-01, -4.8469e-01,  1.4850e+00,\n                           1.2213e+00,  1.9489e+00,  9.3621e-01,  1.0044e+00,  1.3338e+00,\n                           3.6500e+00,  3.3742e+00,  2.8161e-03,  1.2830e+00,  1.7225e+00,\n                           2.8371e+00,  9.9369e-01,  1.4228e+00,  1.5113e+00,  1.0678e+00,\n                           2.9429e-02,  1.4858e+00,  1.4256e+00,  1.0904e+00,  1.5295e+00,\n                           1.0690e+00,  1.9816e+00,  1.2794e+00,  1.6883e+00,  1.4282e+00,\n                           3.3188e+00,  1.4952e+00,  1.0546e+00,  3.2656e+00,  3.5247e+00,\n                           1.4451e+00,  1.7002e+00,  2.1134e+00,  2.9678e+00,  2.6401e+00,\n                           7.9623e-01,  1.2601e+00,  1.2787e+00,  1.2685e+00,  3.1096e+00,\n                           2.0768e+00, -2.0103e-01,  1.3619e+00,  1.5298e+00,  1.6342e+00,\n                           1.3505e+00,  2.9995e+00,  2.6690e+00,  2.9989e+00,  1.6584e+00,\n                           1.7350e+00,  2.0986e+00,  1.4467e+00,  2.1274e+00,  8.7684e-01,\n                           2.0966e+00,  4.2106e-02,  1.6118e+00,  1.4422e-01,  2.4327e+00,\n                           7.3861e-01,  1.0573e+00,  2.3459e+00,  1.7172e+00,  1.1091e+00,\n                           1.5846e+00,  2.2161e+00,  1.2859e+00,  3.5936e+00,  1.2659e+00,\n                           2.6199e+00,  1.0126e+00,  1.7748e+00, -2.1484e-01,  1.1435e+00,\n                           1.4493e+00,  1.0181e+00,  2.0061e-01,  2.4672e+00,  1.1569e+00,\n                           1.0745e+00,  2.2810e+00,  2.4260e-01,  8.0791e-01,  8.6407e-01,\n                           1.4016e+00,  1.8541e-01,  1.4096e+00,  2.9214e+00,  3.6288e+00,\n                           1.0995e+00,  8.7751e-01,  1.3830e+00,  1.8435e+00,  9.3245e-02,\n                           1.8875e+00,  1.8723e+00,  1.2725e+00,  1.3765e+00,  1.1357e+00,\n                           2.7069e+00,  1.1846e+00, -1.1490e-02,  2.5858e+00,  1.8941e+00,\n                           1.0491e+00,  1.4362e+00,  4.3406e+00,  2.1084e+00, -8.5224e-02,\n                           2.6642e+00,  7.7655e-01,  1.5307e+00,  1.8988e+00, -1.0411e-01,\n                           1.0330e+00,  9.1423e-01,  1.2200e+00,  2.0089e+00,  1.2483e+00,\n                           1.4586e+00,  1.1114e+00])\n                )\n              )\n            )\n            (2): ReLU6(inplace=True)\n          )\n          (2): ConvBn2d(\n            192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0047, 0.0049, 0.0045, 0.0037, 0.0048, 0.0060, 0.0049, 0.0048, 0.0039,\n                      0.0056, 0.0074, 0.0029, 0.0031, 0.0030, 0.0029, 0.0049, 0.0070, 0.0053,\n                      0.0032, 0.0065, 0.0049, 0.0054, 0.0040, 0.0048, 0.0053, 0.0070, 0.0056,\n                      0.0045, 0.0047, 0.0060, 0.0058, 0.0067]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n                min_val=tensor([-0.5593, -0.3970, -0.5718, -0.4146, -0.6130, -0.5540, -0.6241, -0.5626,\n                        -0.5005, -0.6366, -0.9454, -0.3602, -0.3818, -0.3410, -0.3674, -0.5325,\n                        -0.5899, -0.5694, -0.4144, -0.8379, -0.6212, -0.6929, -0.4169, -0.6109,\n                        -0.4872, -0.5330, -0.6432, -0.5505, -0.3190, -0.5329, -0.7474, -0.8557]), max_val=tensor([0.5958, 0.6258, 0.5446, 0.4668, 0.5735, 0.7602, 0.5163, 0.6055, 0.3869,\n                        0.7169, 0.8640, 0.3675, 0.3934, 0.3748, 0.3328, 0.6206, 0.8887, 0.6727,\n                        0.3273, 0.7001, 0.5109, 0.5071, 0.5131, 0.5972, 0.6712, 0.8832, 0.7163,\n                        0.5763, 0.6019, 0.7597, 0.7126, 0.5696])\n              )\n            )\n          )\n        )\n      )\n      (7): Module(\n        (conv): Module(\n          (0): Module(\n            (0): ConvBn2d(\n              32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n              (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n                fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0011, 0.0007, 0.0006, 0.0009, 0.0005, 0.0005, 0.0006, 0.0007, 0.0012,\n                        0.0011, 0.0007, 0.0009, 0.0006, 0.0004, 0.0004, 0.0008, 0.0006, 0.0009,\n                        0.0005, 0.0013, 0.0008, 0.0006, 0.0002, 0.0013, 0.0008, 0.0005, 0.0008,\n                        0.0010, 0.0009, 0.0010, 0.0008, 0.0009, 0.0007, 0.0012, 0.0008, 0.0006,\n                        0.0009, 0.0008, 0.0012, 0.0007, 0.0006, 0.0005, 0.0008, 0.0012, 0.0009,\n                        0.0006, 0.0010, 0.0010, 0.0007, 0.0005, 0.0010, 0.0008, 0.0006, 0.0006,\n                        0.0009, 0.0005, 0.0006, 0.0015, 0.0007, 0.0007, 0.0006, 0.0003, 0.0010,\n                        0.0009, 0.0006, 0.0008, 0.0006, 0.0006, 0.0009, 0.0006, 0.0013, 0.0012,\n                        0.0010, 0.0006, 0.0011, 0.0003, 0.0002, 0.0008, 0.0004, 0.0011, 0.0008,\n                        0.0010, 0.0010, 0.0004, 0.0008, 0.0003, 0.0006, 0.0009, 0.0011, 0.0010,\n                        0.0006, 0.0006, 0.0009, 0.0009, 0.0005, 0.0004, 0.0011, 0.0010, 0.0010,\n                        0.0012, 0.0015, 0.0005, 0.0010, 0.0007, 0.0007, 0.0014, 0.0006, 0.0004,\n                        0.0009, 0.0005, 0.0004, 0.0005, 0.0007, 0.0006, 0.0008, 0.0008, 0.0008,\n                        0.0008, 0.0003, 0.0009, 0.0007, 0.0010, 0.0008, 0.0006, 0.0007, 0.0011,\n                        0.0004, 0.0007, 0.0008, 0.0008, 0.0011, 0.0007, 0.0008, 0.0006, 0.0010,\n                        0.0007, 0.0009, 0.0008, 0.0010, 0.0005, 0.0006, 0.0005, 0.0004, 0.0012,\n                        0.0010, 0.0007, 0.0013, 0.0008, 0.0008, 0.0005, 0.0014, 0.0011, 0.0008,\n                        0.0008, 0.0006, 0.0006, 0.0005, 0.0011, 0.0003, 0.0007, 0.0009, 0.0009,\n                        0.0006, 0.0007, 0.0006, 0.0013, 0.0010, 0.0008, 0.0007, 0.0007, 0.0012,\n                        0.0004, 0.0005, 0.0013, 0.0012, 0.0010, 0.0014, 0.0009, 0.0011, 0.0008,\n                        0.0005, 0.0019, 0.0008, 0.0009, 0.0005, 0.0006, 0.0007, 0.0005, 0.0008,\n                        0.0008, 0.0010, 0.0010]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                       dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n                (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n                  min_val=tensor([-0.0839, -0.0863, -0.0720, -0.0588, -0.0491, -0.0581, -0.0777, -0.0950,\n                          -0.1537, -0.1350, -0.0528, -0.1156, -0.0525, -0.0460, -0.0457, -0.0788,\n                          -0.0799, -0.1110, -0.0514, -0.0884, -0.0809, -0.0770, -0.0215, -0.1688,\n                          -0.1062, -0.0594, -0.0481, -0.1243, -0.1094, -0.0798, -0.0870, -0.1167,\n                          -0.0889, -0.0992, -0.0748, -0.0783, -0.1102, -0.0553, -0.1074, -0.0619,\n                          -0.0723, -0.0626, -0.1035, -0.1510, -0.0978, -0.0809, -0.1182, -0.1027,\n                          -0.0917, -0.0648, -0.0794, -0.0968, -0.0788, -0.0809, -0.0340, -0.0618,\n                          -0.0738, -0.1888, -0.0705, -0.0844, -0.0625, -0.0420, -0.1282, -0.1033,\n                          -0.0747, -0.1031, -0.0730, -0.0671, -0.1106, -0.0830, -0.0808, -0.0779,\n                          -0.0904, -0.0829, -0.0972, -0.0383, -0.0317, -0.1072, -0.0254, -0.1452,\n                          -0.0964, -0.1251, -0.1288, -0.0524, -0.0879, -0.0342, -0.0815, -0.1092,\n                          -0.1116, -0.0948, -0.0635, -0.0766, -0.1115, -0.0933, -0.0685, -0.0547,\n                          -0.1268, -0.0718, -0.1237, -0.1530, -0.1936, -0.0586, -0.1230, -0.0844,\n                          -0.0863, -0.1004, -0.0703, -0.0558, -0.0803, -0.0690, -0.0500, -0.0357,\n                          -0.0847, -0.0755, -0.0823, -0.0692, -0.1047, -0.0996, -0.0386, -0.0544,\n                          -0.0643, -0.1016, -0.1072, -0.0751, -0.0789, -0.1368, -0.0551, -0.0891,\n                          -0.1065, -0.1087, -0.1000, -0.0550, -0.1082, -0.0514, -0.0928, -0.0838,\n                          -0.1134, -0.0943, -0.1288, -0.0299, -0.0713, -0.0653, -0.0423, -0.1130,\n                          -0.0889, -0.0875, -0.1179, -0.1046, -0.1070, -0.0616, -0.0969, -0.0660,\n                          -0.0952, -0.1084, -0.0790, -0.0812, -0.0657, -0.1381, -0.0363, -0.0839,\n                          -0.1177, -0.1182, -0.0698, -0.0843, -0.0639, -0.1657, -0.1298, -0.1061,\n                          -0.0506, -0.0756, -0.0640, -0.0489, -0.0697, -0.0840, -0.1496, -0.1249,\n                          -0.1827, -0.1094, -0.1288, -0.1054, -0.0700, -0.1007, -0.0689, -0.1097,\n                          -0.0681, -0.0601, -0.0904, -0.0565, -0.0937, -0.0860, -0.1137, -0.0617]), max_val=tensor([0.1453, 0.0549, 0.0496, 0.1170, 0.0572, 0.0383, 0.0682, 0.0592, 0.0980,\n                          0.0827, 0.0881, 0.0756, 0.0755, 0.0323, 0.0422, 0.1076, 0.0801, 0.0983,\n                          0.0609, 0.1702, 0.1010, 0.0509, 0.0244, 0.1189, 0.0647, 0.0428, 0.0969,\n                          0.0833, 0.0661, 0.1324, 0.1031, 0.0950, 0.0691, 0.1517, 0.1046, 0.0279,\n                          0.1052, 0.0964, 0.1489, 0.0907, 0.0705, 0.0447, 0.0731, 0.0745, 0.1141,\n                          0.0422, 0.1225, 0.1265, 0.0596, 0.0664, 0.1289, 0.1043, 0.0562, 0.0541,\n                          0.1190, 0.0691, 0.0737, 0.0949, 0.0910, 0.0569, 0.0753, 0.0336, 0.0545,\n                          0.1114, 0.0643, 0.0693, 0.0815, 0.0762, 0.1079, 0.0795, 0.1604, 0.1502,\n                          0.1214, 0.0734, 0.1460, 0.0413, 0.0191, 0.0435, 0.0471, 0.0708, 0.0766,\n                          0.1181, 0.1251, 0.0532, 0.1053, 0.0252, 0.0800, 0.0986, 0.1338, 0.1264,\n                          0.0798, 0.0652, 0.0916, 0.1199, 0.0616, 0.0205, 0.1453, 0.1276, 0.0869,\n                          0.0948, 0.0568, 0.0351, 0.0814, 0.0749, 0.0641, 0.1733, 0.0823, 0.0419,\n                          0.1091, 0.0595, 0.0391, 0.0689, 0.0785, 0.0281, 0.1004, 0.0982, 0.0674,\n                          0.0859, 0.0279, 0.1176, 0.0914, 0.1327, 0.0694, 0.0798, 0.0908, 0.0553,\n                          0.0402, 0.0700, 0.0344, 0.0764, 0.1386, 0.0868, 0.0986, 0.0722, 0.1286,\n                          0.0406, 0.0961, 0.0955, 0.1138, 0.0670, 0.0489, 0.0487, 0.0561, 0.1530,\n                          0.1262, 0.0550, 0.1617, 0.0998, 0.0591, 0.0468, 0.1740, 0.1443, 0.0981,\n                          0.0953, 0.0706, 0.0663, 0.0595, 0.0731, 0.0420, 0.0800, 0.0719, 0.0980,\n                          0.0780, 0.0947, 0.0783, 0.1252, 0.1218, 0.0578, 0.0911, 0.0883, 0.1560,\n                          0.0221, 0.0412, 0.1609, 0.0467, 0.0877, 0.0653, 0.1111, 0.1423, 0.0934,\n                          0.0638, 0.2403, 0.1048, 0.0835, 0.0559, 0.0713, 0.0745, 0.0591, 0.0957,\n                          0.1076, 0.1265, 0.1286])\n                )\n              )\n            )\n            (2): ReLU6(inplace=True)\n          )\n          (1): Module(\n            (0): ConvBn2d(\n              192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False\n              (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n                fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0011, 0.0019, 0.0053, 0.0030, 0.0021, 0.0032, 0.0032, 0.0036, 0.0016,\n                        0.0038, 0.0031, 0.0031, 0.0024, 0.0040, 0.0035, 0.0028, 0.0029, 0.0032,\n                        0.0090, 0.0020, 0.0031, 0.0017, 0.0175, 0.0038, 0.0028, 0.0114, 0.0031,\n                        0.0018, 0.0022, 0.0026, 0.0022, 0.0027, 0.0032, 0.0031, 0.0021, 0.0065,\n                        0.0023, 0.0034, 0.0030, 0.0025, 0.0017, 0.0103, 0.0064, 0.0022, 0.0027,\n                        0.0113, 0.0041, 0.0031, 0.0068, 0.0027, 0.0014, 0.0032, 0.0078, 0.0061,\n                        0.0015, 0.0029, 0.0031, 0.0027, 0.0024, 0.0025, 0.0027, 0.0173, 0.0016,\n                        0.0041, 0.0023, 0.0025, 0.0030, 0.0038, 0.0031, 0.0014, 0.0030, 0.0020,\n                        0.0027, 0.0026, 0.0029, 0.0037, 0.0190, 0.0067, 0.0096, 0.0036, 0.0034,\n                        0.0033, 0.0024, 0.0128, 0.0023, 0.0136, 0.0033, 0.0026, 0.0026, 0.0027,\n                        0.0021, 0.0027, 0.0014, 0.0033, 0.0066, 0.0149, 0.0027, 0.0015, 0.0049,\n                        0.0022, 0.0015, 0.0036, 0.0047, 0.0022, 0.0039, 0.0020, 0.0029, 0.0031,\n                        0.0036, 0.0042, 0.0071, 0.0027, 0.0024, 0.0038, 0.0023, 0.0020, 0.0013,\n                        0.0034, 0.0120, 0.0032, 0.0025, 0.0030, 0.0026, 0.0021, 0.0031, 0.0017,\n                        0.0020, 0.0021, 0.0017, 0.0028, 0.0032, 0.0064, 0.0022, 0.0026, 0.0035,\n                        0.0062, 0.0017, 0.0018, 0.0025, 0.0145, 0.0023, 0.0148, 0.0125, 0.0028,\n                        0.0037, 0.0028, 0.0016, 0.0018, 0.0041, 0.0119, 0.0036, 0.0020, 0.0032,\n                        0.0033, 0.0044, 0.0035, 0.0082, 0.0032, 0.0030, 0.0016, 0.0032, 0.0033,\n                        0.0036, 0.0026, 0.0027, 0.0035, 0.0051, 0.0027, 0.0015, 0.0024, 0.0024,\n                        0.0057, 0.0066, 0.0020, 0.0014, 0.0032, 0.0020, 0.0030, 0.0025, 0.0021,\n                        0.0027, 0.0020, 0.0027, 0.0032, 0.0018, 0.0016, 0.0029, 0.0158, 0.0033,\n                        0.0028, 0.0030, 0.0040]), zero_point=tensor([-128,  127,  127, -128,  127,  127,  127,  127,  127,  127,  127,  127,\n                         127,  127,  127, -128,  127, -128,    0, -128, -128,  127,    0, -128,\n                         127,    0,  127,  127,  127, -128,  127, -128, -128, -128, -128,    0,\n                         127, -128,  127,  127,  127,    0, -128, -128, -128,    0, -128, -128,\n                           0,  127, -128,  127,    0,    0,  127,  127,  127, -128,  127,  127,\n                        -128,    0,  127, -128,  127, -128,  127, -128, -128,  127, -128, -128,\n                        -128,  127, -128,  127,    0,    0,    0, -128,  127, -128, -128,    0,\n                        -128,    0,  127, -128,  127, -128,  127,  127, -128, -128,    0,    0,\n                        -128, -128, -128, -128, -128, -128, -128,  127, -128, -128,  127,  127,\n                        -128,  127,    0,  127,  127,  127, -128,  127,  127, -128,    0, -128,\n                         127, -128,  127,  127,  127,  127,  127,  127,  127, -128, -128, -128,\n                        -128,  127, -128,    0, -128,  127, -128,    0,  127,    0,    0, -128,\n                        -128,  127, -128, -128, -128,    0, -128, -128,  127, -128,  127,  127,\n                           0, -128,  127,  127,  127, -128, -128, -128, -128, -128, -128,  127,\n                         127,  127,  127,  127, -128,  127,  127, -128, -128, -128, -128,  127,\n                         127, -128,  127, -128,  127,  127, -128,    0,  127, -128,  127, -128],\n                       dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n                (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n                  min_val=tensor([ 0.0688, -0.4915, -1.3505,  0.1936, -0.5365, -0.8061, -0.8140, -0.9276,\n                          -0.4179, -0.9635, -0.8022, -0.7886, -0.6113, -1.0147, -0.9031,  0.1134,\n                          -0.7448,  0.1050, -1.1462,  0.0961,  0.1179, -0.4328, -2.2386,  0.0848,\n                          -0.7048, -0.9789, -0.7799, -0.4511, -0.5696,  0.1175, -0.5656,  0.1167,\n                           0.1086,  0.2076,  0.1267, -0.8334, -0.5818,  0.1603, -0.7593, -0.6425,\n                          -0.4243, -0.7452,  0.3505,  0.1382,  0.1000, -1.4446,  0.2401,  0.1524,\n                          -0.4055, -0.6887,  0.1143, -0.8039, -0.4678, -0.6916, -0.3863, -0.7406,\n                          -0.7939,  0.1243, -0.5998, -0.6401,  0.1394, -1.7067, -0.4136,  0.2232,\n                          -0.5842,  0.0926, -0.7756,  0.1603,  0.0154, -0.3604,  0.1333,  0.1469,\n                           0.0898, -0.6598,  0.1953, -0.9510, -2.4381, -0.5865, -1.2028,  0.2443,\n                          -0.8589,  0.1947,  0.1558, -1.6173,  0.1656, -1.6659, -0.8489,  0.1772,\n                          -0.6646,  0.0271, -0.5354, -0.6937,  0.0988,  0.2040, -0.5805, -1.1399,\n                           0.1569,  0.0354,  0.3323,  0.1158,  0.0759,  0.1228,  0.1877, -0.5707,\n                           0.2302,  0.0749, -0.7487, -0.8028,  0.1485, -1.0758, -0.9082, -0.6768,\n                          -0.6028, -0.9766,  0.0951, -0.5013, -0.3395,  0.1888, -1.5388,  0.1912,\n                          -0.6386,  0.1485, -0.6748, -0.5258, -0.7791, -0.4424, -0.5195, -0.5481,\n                          -0.4215,  0.1754,  0.2007,  0.3148,  0.1436, -0.6717,  0.1369, -0.4087,\n                           0.0164, -0.4542,  0.0923, -1.8624, -0.5815, -1.8904, -1.5225,  0.0976,\n                           0.1599, -0.7179,  0.0756,  0.1220,  0.2230, -0.7307,  0.2243,  0.1429,\n                          -0.8114,  0.2926, -1.1210, -0.9001, -1.0448,  0.2059, -0.7546, -0.4127,\n                          -0.8196,  0.2290,  0.1096,  0.1289,  0.1262,  0.1793,  0.3412, -0.6925,\n                          -0.3843, -0.6176, -0.6158, -1.4488,  0.4520, -0.5148, -0.3689,  0.2020,\n                           0.0980,  0.1088,  0.1390, -0.5408, -0.6859,  0.0835, -0.6942,  0.2821,\n                          -0.4583, -0.4141,  0.1603, -1.6871, -0.8472,  0.1730, -0.7630,  0.2607]), max_val=tensor([ 0.2848, -0.1479, -0.2896,  0.7776, -0.0615, -0.1756, -0.1532, -0.2373,\n                          -0.1690, -0.1893, -0.1815, -0.1435, -0.1142, -0.0142, -0.1341,  0.7047,\n                          -0.1502,  0.8277,  0.0139,  0.5150,  0.7829, -0.1167,  0.4674,  0.9597,\n                          -0.0770,  1.4444, -0.0287, -0.1070, -0.0928,  0.6582, -0.1244,  0.6890,\n                           0.8145,  0.7840,  0.5237,  0.8151, -0.1796,  0.8690, -0.1495, -0.1551,\n                          -0.1741,  1.3125,  1.6331,  0.5603,  0.6966,  0.9055,  1.0529,  0.7780,\n                           0.8679, -0.1047,  0.3614, -0.2146,  0.9969,  0.7714, -0.0518, -0.2496,\n                          -0.2561,  0.6882, -0.1035, -0.2253,  0.6764,  2.2000, -0.1435,  1.0582,\n                          -0.0613,  0.6403, -0.1602,  0.9597,  0.7887, -0.1088,  0.7529,  0.5214,\n                           0.6884, -0.0802,  0.7490, -0.0854,  1.8652,  0.8533,  1.2156,  0.9190,\n                          -0.1813,  0.8316,  0.6146,  1.6221,  0.5918,  1.7256, -0.2109,  0.6561,\n                          -0.1423,  0.6790, -0.1527, -0.1829,  0.3628,  0.8350,  0.8373,  1.8968,\n                           0.6933,  0.3702,  1.2598,  0.5611,  0.3858,  0.9097,  1.2058, -0.1720,\n                           1.0069,  0.4975, -0.2804, -0.0774,  0.9285, -0.3245,  0.0114, -0.1042,\n                          -0.0706, -0.1894,  0.5886, -0.2559, -0.1175,  0.8564,  0.1470,  0.8088,\n                          -0.1607,  0.7720, -0.1294, -0.0928, -0.2691, -0.1611, -0.1226, -0.1284,\n                          -0.1627,  0.7178,  0.8063,  1.6232,  0.5619, -0.1717,  0.8997,  0.7924,\n                           0.4387, -0.1614,  0.6443,  1.5301, -0.1878,  1.2583,  1.5907,  0.7052,\n                           0.9553, -0.0472,  0.3989,  0.4624,  1.0518,  1.5161,  0.9179,  0.5162,\n                          -0.2556,  0.8488, -0.2174, -0.2314,  0.8391,  0.8265, -0.1141, -0.0772,\n                          -0.1427,  0.8353,  0.9097,  0.6550,  0.7001,  0.8914,  1.3022, -0.1180,\n                          -0.0432, -0.1371, -0.0912, -0.1010,  1.6872, -0.1682, -0.0497,  0.8160,\n                           0.4996,  0.7723,  0.6337, -0.1342, -0.1841,  0.5008, -0.0872,  0.8166,\n                          -0.1279, -0.1940,  0.7490,  2.0127, -0.1812,  0.7135, -0.1271,  1.0205])\n                )\n              )\n            )\n            (2): ReLU6(inplace=True)\n          )\n          (2): ConvBn2d(\n            192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0029, 0.0044, 0.0027, 0.0034, 0.0030, 0.0044, 0.0025, 0.0037, 0.0029,\n                      0.0037, 0.0028, 0.0032, 0.0036, 0.0036, 0.0033, 0.0033, 0.0032, 0.0027,\n                      0.0034, 0.0042, 0.0046, 0.0048, 0.0033, 0.0032, 0.0031, 0.0032, 0.0029,\n                      0.0040, 0.0032, 0.0028, 0.0027, 0.0032, 0.0030, 0.0030, 0.0030, 0.0044,\n                      0.0037, 0.0038, 0.0032, 0.0027, 0.0035, 0.0047, 0.0031, 0.0038, 0.0031,\n                      0.0036, 0.0034, 0.0052, 0.0034, 0.0028, 0.0033, 0.0027, 0.0040, 0.0034,\n                      0.0032, 0.0030, 0.0024, 0.0025, 0.0032, 0.0030, 0.0037, 0.0037, 0.0032,\n                      0.0032]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n                min_val=tensor([-0.3590, -0.5695, -0.3344, -0.3157, -0.3555, -0.3672, -0.3232, -0.2599,\n                        -0.3762, -0.4722, -0.3591, -0.4081, -0.4429, -0.4597, -0.2811, -0.3715,\n                        -0.4119, -0.3286, -0.3319, -0.4287, -0.5400, -0.4204, -0.4269, -0.4099,\n                        -0.3250, -0.3237, -0.3771, -0.4534, -0.3486, -0.3601, -0.3517, -0.4070,\n                        -0.3862, -0.3895, -0.3325, -0.5663, -0.4173, -0.4922, -0.2768, -0.3516,\n                        -0.3637, -0.6046, -0.4001, -0.3609, -0.3143, -0.3962, -0.4370, -0.5375,\n                        -0.4314, -0.2885, -0.4209, -0.3494, -0.5141, -0.4408, -0.4065, -0.3418,\n                        -0.3127, -0.3043, -0.4141, -0.2848, -0.4686, -0.4701, -0.4082, -0.3020]), max_val=tensor([0.3651, 0.3585, 0.3386, 0.4327, 0.3810, 0.5596, 0.3058, 0.4646, 0.3543,\n                        0.3711, 0.3373, 0.3250, 0.4520, 0.3950, 0.4163, 0.4221, 0.3650, 0.3490,\n                        0.4359, 0.5316, 0.5832, 0.6103, 0.3691, 0.3574, 0.3993, 0.4035, 0.2638,\n                        0.5062, 0.4120, 0.3026, 0.3048, 0.3777, 0.3613, 0.3865, 0.3749, 0.3453,\n                        0.4652, 0.4122, 0.4066, 0.3148, 0.4450, 0.4464, 0.3239, 0.4781, 0.3896,\n                        0.4572, 0.3939, 0.6664, 0.3885, 0.3564, 0.3493, 0.2467, 0.2705, 0.3671,\n                        0.3861, 0.3771, 0.2617, 0.3142, 0.3580, 0.3801, 0.3419, 0.4434, 0.4069,\n                        0.4038])\n              )\n            )\n          )\n        )\n      )\n      (8): Module(\n        (conv): Module(\n          (0): Module(\n            (0): ConvBn2d(\n              64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False\n              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n                fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0003, 0.0004, 0.0006, 0.0005, 0.0009, 0.0005, 0.0005, 0.0008, 0.0004,\n                        0.0006, 0.0006, 0.0003, 0.0005, 0.0007, 0.0009, 0.0005, 0.0005, 0.0006,\n                        0.0004, 0.0005, 0.0004, 0.0007, 0.0005, 0.0006, 0.0004, 0.0004, 0.0006,\n                        0.0009, 0.0004, 0.0004, 0.0004, 0.0004, 0.0005, 0.0007, 0.0007, 0.0010,\n                        0.0006, 0.0003, 0.0005, 0.0008, 0.0008, 0.0003, 0.0004, 0.0003, 0.0005,\n                        0.0005, 0.0006, 0.0004, 0.0006, 0.0006, 0.0006, 0.0003, 0.0004, 0.0009,\n                        0.0004, 0.0005, 0.0006, 0.0003, 0.0003, 0.0004, 0.0004, 0.0004, 0.0006,\n                        0.0005, 0.0005, 0.0009, 0.0004, 0.0005, 0.0006, 0.0005, 0.0005, 0.0005,\n                        0.0010, 0.0004, 0.0003, 0.0002, 0.0004, 0.0005, 0.0006, 0.0004, 0.0003,\n                        0.0006, 0.0005, 0.0004, 0.0003, 0.0006, 0.0006, 0.0011, 0.0006, 0.0003,\n                        0.0005, 0.0004, 0.0007, 0.0004, 0.0003, 0.0007, 0.0004, 0.0005, 0.0006,\n                        0.0007, 0.0005, 0.0005, 0.0004, 0.0004, 0.0011, 0.0005, 0.0005, 0.0005,\n                        0.0007, 0.0009, 0.0004, 0.0006, 0.0008, 0.0003, 0.0004, 0.0005, 0.0003,\n                        0.0006, 0.0004, 0.0006, 0.0005, 0.0004, 0.0005, 0.0007, 0.0004, 0.0007,\n                        0.0005, 0.0008, 0.0004, 0.0005, 0.0004, 0.0002, 0.0002, 0.0005, 0.0007,\n                        0.0004, 0.0003, 0.0005, 0.0006, 0.0008, 0.0005, 0.0007, 0.0005, 0.0002,\n                        0.0005, 0.0005, 0.0004, 0.0004, 0.0007, 0.0004, 0.0005, 0.0005, 0.0007,\n                        0.0008, 0.0005, 0.0005, 0.0005, 0.0006, 0.0006, 0.0005, 0.0006, 0.0006,\n                        0.0004, 0.0005, 0.0004, 0.0005, 0.0005, 0.0004, 0.0004, 0.0005, 0.0005,\n                        0.0006, 0.0009, 0.0008, 0.0002, 0.0007, 0.0005, 0.0006, 0.0007, 0.0005,\n                        0.0004, 0.0009, 0.0004, 0.0003, 0.0005, 0.0005, 0.0003, 0.0005, 0.0004,\n                        0.0004, 0.0004, 0.0004, 0.0004, 0.0003, 0.0005, 0.0004, 0.0004, 0.0005,\n                        0.0006, 0.0006, 0.0004, 0.0005, 0.0004, 0.0004, 0.0005, 0.0003, 0.0003,\n                        0.0003, 0.0005, 0.0004, 0.0004, 0.0004, 0.0007, 0.0006, 0.0004, 0.0004,\n                        0.0008, 0.0005, 0.0007, 0.0003, 0.0008, 0.0003, 0.0004, 0.0009, 0.0005,\n                        0.0005, 0.0003, 0.0008, 0.0004, 0.0006, 0.0010, 0.0006, 0.0005, 0.0005,\n                        0.0008, 0.0007, 0.0005, 0.0003, 0.0007, 0.0005, 0.0005, 0.0006, 0.0005,\n                        0.0005, 0.0003, 0.0008, 0.0006, 0.0004, 0.0003, 0.0005, 0.0004, 0.0006,\n                        0.0004, 0.0006, 0.0008, 0.0006, 0.0006, 0.0007, 0.0004, 0.0008, 0.0005,\n                        0.0004, 0.0008, 0.0004, 0.0007, 0.0003, 0.0004, 0.0003, 0.0008, 0.0004,\n                        0.0008, 0.0003, 0.0005, 0.0007, 0.0004, 0.0006, 0.0003, 0.0004, 0.0003,\n                        0.0004, 0.0005, 0.0005, 0.0003, 0.0004, 0.0006, 0.0007, 0.0003, 0.0005,\n                        0.0004, 0.0003, 0.0010, 0.0005, 0.0004, 0.0004, 0.0003, 0.0005, 0.0004,\n                        0.0003, 0.0006, 0.0004, 0.0004, 0.0004, 0.0004, 0.0006, 0.0008, 0.0003,\n                        0.0005, 0.0003, 0.0004, 0.0005, 0.0004, 0.0007, 0.0003, 0.0003, 0.0008,\n                        0.0006, 0.0010, 0.0005, 0.0005, 0.0003, 0.0006, 0.0005, 0.0004, 0.0004,\n                        0.0008, 0.0004, 0.0006, 0.0003, 0.0004, 0.0005, 0.0003, 0.0007, 0.0006,\n                        0.0005, 0.0004, 0.0005, 0.0004, 0.0005, 0.0005, 0.0005, 0.0004, 0.0003,\n                        0.0007, 0.0007, 0.0006, 0.0005, 0.0006, 0.0011, 0.0005, 0.0004, 0.0003,\n                        0.0006, 0.0006, 0.0008, 0.0004, 0.0007, 0.0005, 0.0004, 0.0008, 0.0009,\n                        0.0009, 0.0008, 0.0004, 0.0005, 0.0003, 0.0004, 0.0004, 0.0005, 0.0007,\n                        0.0002, 0.0004, 0.0007, 0.0006, 0.0005, 0.0007, 0.0003, 0.0006, 0.0007,\n                        0.0006, 0.0006, 0.0006, 0.0005, 0.0006, 0.0005]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                       dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n                (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n                  min_val=tensor([-0.0308, -0.0436, -0.0810, -0.0558, -0.0690, -0.0448, -0.0625, -0.0847,\n                          -0.0474, -0.0543, -0.0739, -0.0359, -0.0586, -0.0851, -0.0774, -0.0620,\n                          -0.0677, -0.0818, -0.0487, -0.0636, -0.0550, -0.0664, -0.0448, -0.0804,\n                          -0.0480, -0.0478, -0.0760, -0.1118, -0.0574, -0.0477, -0.0513, -0.0484,\n                          -0.0532, -0.0854, -0.0907, -0.1321, -0.0370, -0.0343, -0.0523, -0.0944,\n                          -0.0868, -0.0334, -0.0540, -0.0429, -0.0431, -0.0369, -0.0521, -0.0435,\n                          -0.0808, -0.0775, -0.0779, -0.0401, -0.0525, -0.0721, -0.0422, -0.0639,\n                          -0.0739, -0.0368, -0.0435, -0.0533, -0.0409, -0.0486, -0.0746, -0.0595,\n                          -0.0515, -0.1190, -0.0481, -0.0337, -0.0807, -0.0477, -0.0685, -0.0498,\n                          -0.0832, -0.0489, -0.0435, -0.0318, -0.0497, -0.0595, -0.0624, -0.0539,\n                          -0.0405, -0.0809, -0.0522, -0.0523, -0.0252, -0.0754, -0.0814, -0.1352,\n                          -0.0716, -0.0412, -0.0578, -0.0353, -0.0699, -0.0564, -0.0364, -0.0582,\n                          -0.0556, -0.0586, -0.0765, -0.0665, -0.0643, -0.0660, -0.0472, -0.0484,\n                          -0.1361, -0.0671, -0.0593, -0.0386, -0.0945, -0.1015, -0.0456, -0.0603,\n                          -0.0851, -0.0399, -0.0509, -0.0598, -0.0358, -0.0498, -0.0484, -0.0593,\n                          -0.0687, -0.0474, -0.0703, -0.0775, -0.0485, -0.0619, -0.0584, -0.0961,\n                          -0.0393, -0.0592, -0.0463, -0.0234, -0.0262, -0.0618, -0.0837, -0.0480,\n                          -0.0163, -0.0650, -0.0521, -0.1022, -0.0579, -0.0943, -0.0617, -0.0263,\n                          -0.0536, -0.0515, -0.0556, -0.0568, -0.0867, -0.0460, -0.0342, -0.0663,\n                          -0.0850, -0.0975, -0.0585, -0.0683, -0.0404, -0.0554, -0.0563, -0.0429,\n                          -0.0785, -0.0783, -0.0355, -0.0379, -0.0481, -0.0612, -0.0443, -0.0455,\n                          -0.0381, -0.0673, -0.0664, -0.0805, -0.0670, -0.0995, -0.0290, -0.0627,\n                          -0.0640, -0.0739, -0.0585, -0.0468, -0.0516, -0.1139, -0.0494, -0.0407,\n                          -0.0455, -0.0597, -0.0370, -0.0537, -0.0563, -0.0332, -0.0389, -0.0451,\n                          -0.0563, -0.0344, -0.0639, -0.0483, -0.0412, -0.0699, -0.0733, -0.0549,\n                          -0.0490, -0.0622, -0.0550, -0.0509, -0.0396, -0.0339, -0.0445, -0.0423,\n                          -0.0622, -0.0381, -0.0424, -0.0528, -0.0921, -0.0826, -0.0478, -0.0538,\n                          -0.0901, -0.0589, -0.0781, -0.0238, -0.0461, -0.0371, -0.0404, -0.0610,\n                          -0.0565, -0.0610, -0.0321, -0.0996, -0.0496, -0.0722, -0.0668, -0.0756,\n                          -0.0556, -0.0601, -0.0962, -0.0941, -0.0405, -0.0419, -0.0592, -0.0680,\n                          -0.0618, -0.0348, -0.0644, -0.0639, -0.0318, -0.1011, -0.0638, -0.0467,\n                          -0.0393, -0.0609, -0.0428, -0.0641, -0.0461, -0.0769, -0.0973, -0.0727,\n                          -0.0666, -0.0781, -0.0459, -0.1047, -0.0490, -0.0558, -0.1081, -0.0534,\n                          -0.0652, -0.0430, -0.0493, -0.0342, -0.0744, -0.0297, -0.0861, -0.0357,\n                          -0.0631, -0.0868, -0.0359, -0.0708, -0.0391, -0.0476, -0.0290, -0.0398,\n                          -0.0597, -0.0519, -0.0422, -0.0440, -0.0581, -0.0926, -0.0292, -0.0526,\n                          -0.0527, -0.0408, -0.0967, -0.0550, -0.0540, -0.0335, -0.0444, -0.0661,\n                          -0.0398, -0.0446, -0.0626, -0.0421, -0.0508, -0.0499, -0.0383, -0.0783,\n                          -0.0609, -0.0369, -0.0535, -0.0333, -0.0575, -0.0636, -0.0566, -0.0748,\n                          -0.0417, -0.0436, -0.0842, -0.0446, -0.1181, -0.0584, -0.0497, -0.0444,\n                          -0.0806, -0.0523, -0.0551, -0.0507, -0.1033, -0.0403, -0.0621, -0.0329,\n                          -0.0470, -0.0581, -0.0347, -0.0819, -0.0755, -0.0637, -0.0532, -0.0473,\n                          -0.0444, -0.0525, -0.0558, -0.0616, -0.0490, -0.0336, -0.0851, -0.0733,\n                          -0.0748, -0.0702, -0.0796, -0.1386, -0.0588, -0.0403, -0.0377, -0.0462,\n                          -0.0811, -0.0897, -0.0334, -0.0578, -0.0571, -0.0526, -0.0978, -0.1004,\n                          -0.1131, -0.0970, -0.0477, -0.0602, -0.0393, -0.0483, -0.0453, -0.0604,\n                          -0.0528, -0.0252, -0.0507, -0.0864, -0.0488, -0.0539, -0.0629, -0.0384,\n                          -0.0757, -0.0919, -0.0717, -0.0757, -0.0748, -0.0458, -0.0749, -0.0609]), max_val=tensor([0.0437, 0.0567, 0.0449, 0.0696, 0.1151, 0.0679, 0.0451, 0.1027, 0.0378,\n                          0.0781, 0.0657, 0.0406, 0.0349, 0.0661, 0.1111, 0.0546, 0.0634, 0.0597,\n                          0.0432, 0.0584, 0.0345, 0.0916, 0.0590, 0.0679, 0.0477, 0.0510, 0.0814,\n                          0.0880, 0.0420, 0.0484, 0.0322, 0.0474, 0.0612, 0.0637, 0.0622, 0.1080,\n                          0.0702, 0.0372, 0.0611, 0.0985, 0.1013, 0.0402, 0.0510, 0.0346, 0.0652,\n                          0.0636, 0.0794, 0.0498, 0.0640, 0.0361, 0.0719, 0.0203, 0.0352, 0.1160,\n                          0.0571, 0.0610, 0.0475, 0.0417, 0.0291, 0.0508, 0.0489, 0.0407, 0.0598,\n                          0.0684, 0.0574, 0.0965, 0.0455, 0.0616, 0.0451, 0.0684, 0.0465, 0.0590,\n                          0.1293, 0.0435, 0.0325, 0.0281, 0.0514, 0.0456, 0.0731, 0.0490, 0.0396,\n                          0.0546, 0.0595, 0.0538, 0.0344, 0.0695, 0.0701, 0.0799, 0.0599, 0.0442,\n                          0.0317, 0.0494, 0.0902, 0.0463, 0.0438, 0.0926, 0.0326, 0.0473, 0.0674,\n                          0.0880, 0.0500, 0.0572, 0.0431, 0.0379, 0.0727, 0.0466, 0.0633, 0.0593,\n                          0.0706, 0.1190, 0.0285, 0.0706, 0.0982, 0.0366, 0.0442, 0.0618, 0.0255,\n                          0.0706, 0.0428, 0.0742, 0.0659, 0.0461, 0.0432, 0.0916, 0.0522, 0.0843,\n                          0.0526, 0.0835, 0.0451, 0.0574, 0.0408, 0.0212, 0.0283, 0.0564, 0.0687,\n                          0.0338, 0.0403, 0.0528, 0.0764, 0.1007, 0.0499, 0.0433, 0.0660, 0.0313,\n                          0.0669, 0.0648, 0.0547, 0.0542, 0.0809, 0.0484, 0.0574, 0.0540, 0.0567,\n                          0.0655, 0.0530, 0.0637, 0.0637, 0.0810, 0.0712, 0.0580, 0.0670, 0.0703,\n                          0.0455, 0.0638, 0.0463, 0.0423, 0.0692, 0.0318, 0.0512, 0.0560, 0.0611,\n                          0.0650, 0.1122, 0.0403, 0.0270, 0.0878, 0.0587, 0.0692, 0.0875, 0.0600,\n                          0.0304, 0.0751, 0.0535, 0.0337, 0.0674, 0.0449, 0.0343, 0.0667, 0.0439,\n                          0.0522, 0.0449, 0.0565, 0.0464, 0.0336, 0.0446, 0.0456, 0.0485, 0.0679,\n                          0.0801, 0.0741, 0.0467, 0.0613, 0.0508, 0.0504, 0.0673, 0.0422, 0.0311,\n                          0.0381, 0.0574, 0.0525, 0.0452, 0.0549, 0.0907, 0.0662, 0.0458, 0.0516,\n                          0.1039, 0.0691, 0.0949, 0.0332, 0.1003, 0.0412, 0.0543, 0.1177, 0.0657,\n                          0.0332, 0.0364, 0.0798, 0.0560, 0.0584, 0.1266, 0.0469, 0.0677, 0.0600,\n                          0.0685, 0.0586, 0.0670, 0.0437, 0.0926, 0.0526, 0.0444, 0.0708, 0.0630,\n                          0.0603, 0.0363, 0.0968, 0.0791, 0.0461, 0.0354, 0.0452, 0.0458, 0.0766,\n                          0.0449, 0.0453, 0.0600, 0.0695, 0.0760, 0.0892, 0.0346, 0.0621, 0.0628,\n                          0.0497, 0.0554, 0.0452, 0.0854, 0.0364, 0.0321, 0.0438, 0.1053, 0.0497,\n                          0.0955, 0.0414, 0.0441, 0.0874, 0.0459, 0.0680, 0.0311, 0.0459, 0.0343,\n                          0.0448, 0.0518, 0.0671, 0.0378, 0.0453, 0.0718, 0.0817, 0.0388, 0.0660,\n                          0.0527, 0.0378, 0.1232, 0.0587, 0.0516, 0.0542, 0.0419, 0.0434, 0.0468,\n                          0.0411, 0.0804, 0.0566, 0.0563, 0.0528, 0.0485, 0.0640, 0.1028, 0.0274,\n                          0.0666, 0.0285, 0.0452, 0.0677, 0.0541, 0.0904, 0.0360, 0.0412, 0.1049,\n                          0.0784, 0.1296, 0.0405, 0.0613, 0.0380, 0.0604, 0.0611, 0.0522, 0.0419,\n                          0.0571, 0.0501, 0.0763, 0.0414, 0.0369, 0.0511, 0.0378, 0.0881, 0.0528,\n                          0.0658, 0.0355, 0.0592, 0.0557, 0.0627, 0.0634, 0.0594, 0.0513, 0.0280,\n                          0.0714, 0.0905, 0.0574, 0.0691, 0.0582, 0.0888, 0.0450, 0.0448, 0.0322,\n                          0.0739, 0.0667, 0.1062, 0.0561, 0.0835, 0.0579, 0.0501, 0.0715, 0.1102,\n                          0.0911, 0.0503, 0.0386, 0.0643, 0.0419, 0.0505, 0.0464, 0.0662, 0.0827,\n                          0.0312, 0.0433, 0.0941, 0.0711, 0.0654, 0.0898, 0.0284, 0.0495, 0.0806,\n                          0.0586, 0.0551, 0.0647, 0.0675, 0.0625, 0.0547])\n                )\n              )\n            )\n            (2): ReLU6(inplace=True)\n          )\n          (1): Module(\n            (0): ConvBn2d(\n              384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False\n              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n                fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0234, 0.0112, 0.0141, 0.0202, 0.0112, 0.0119, 0.0135, 0.0023, 0.0118,\n                        0.0263, 0.0111, 0.0242, 0.0141, 0.0141, 0.0073, 0.0090, 0.0086, 0.0124,\n                        0.0107, 0.0130, 0.0066, 0.0040, 0.0127, 0.0028, 0.0092, 0.0146, 0.0121,\n                        0.0099, 0.0105, 0.0100, 0.0194, 0.0138, 0.0106, 0.0048, 0.0113, 0.0106,\n                        0.0039, 0.0121, 0.0142, 0.0024, 0.0108, 0.0209, 0.0272, 0.0156, 0.0110,\n                        0.0111, 0.0148, 0.0236, 0.0211, 0.0127, 0.0287, 0.0176, 0.0147, 0.0175,\n                        0.0109, 0.0098, 0.0029, 0.0249, 0.0092, 0.0114, 0.0127, 0.0236, 0.0140,\n                        0.0165, 0.0102, 0.0046, 0.0120, 0.0057, 0.0127, 0.0140, 0.0176, 0.0202,\n                        0.0013, 0.0109, 0.0201, 0.0215, 0.0107, 0.0101, 0.0198, 0.0122, 0.0298,\n                        0.0183, 0.0178, 0.0139, 0.0120, 0.0034, 0.0072, 0.0071, 0.0070, 0.0188,\n                        0.0138, 0.0178, 0.0020, 0.0024, 0.0098, 0.0023, 0.0140, 0.0099, 0.0046,\n                        0.0173, 0.0147, 0.0142, 0.0156, 0.0178, 0.0125, 0.0137, 0.0185, 0.0078,\n                        0.0131, 0.0214, 0.0107, 0.0136, 0.0019, 0.0181, 0.0070, 0.0093, 0.0259,\n                        0.0182, 0.0149, 0.0220, 0.0111, 0.0146, 0.0031, 0.0231, 0.0146, 0.0025,\n                        0.0119, 0.0018, 0.0157, 0.0074, 0.0144, 0.0064, 0.0260, 0.0207, 0.0206,\n                        0.0147, 0.0243, 0.0101, 0.0148, 0.0020, 0.0130, 0.0088, 0.0112, 0.0157,\n                        0.0089, 0.0206, 0.0170, 0.0107, 0.0106, 0.0414, 0.0040, 0.0111, 0.0122,\n                        0.0017, 0.0109, 0.0105, 0.0113, 0.0163, 0.0056, 0.0051, 0.0044, 0.0124,\n                        0.0207, 0.0107, 0.0108, 0.0184, 0.0153, 0.0272, 0.0180, 0.0317, 0.0046,\n                        0.0076, 0.0104, 0.0131, 0.0111, 0.0031, 0.0132, 0.0145, 0.0071, 0.0078,\n                        0.0146, 0.0117, 0.0149, 0.0058, 0.0143, 0.0172, 0.0137, 0.0245, 0.0112,\n                        0.0098, 0.0084, 0.0098, 0.0201, 0.0177, 0.0205, 0.0168, 0.0198, 0.0138,\n                        0.0040, 0.0033, 0.0172, 0.0123, 0.0114, 0.0155, 0.0150, 0.0125, 0.0184,\n                        0.0141, 0.0162, 0.0129, 0.0134, 0.0111, 0.0018, 0.0040, 0.0151, 0.0133,\n                        0.0125, 0.0065, 0.0100, 0.0162, 0.0193, 0.0144, 0.0119, 0.0109, 0.0128,\n                        0.0134, 0.0039, 0.0196, 0.0056, 0.0151, 0.0213, 0.0185, 0.0256, 0.0107,\n                        0.0188, 0.0111, 0.0094, 0.0156, 0.0219, 0.0097, 0.0112, 0.0159, 0.0132,\n                        0.0030, 0.0147, 0.0130, 0.0165, 0.0104, 0.0179, 0.0187, 0.0113, 0.0107,\n                        0.0114, 0.0088, 0.0199, 0.0021, 0.0087, 0.0036, 0.0108, 0.0027, 0.0173,\n                        0.0095, 0.0129, 0.0030, 0.0136, 0.0198, 0.0234, 0.0165, 0.0145, 0.0177,\n                        0.0202, 0.0127, 0.0193, 0.0069, 0.0153, 0.0121, 0.0094, 0.0137, 0.0171,\n                        0.0101, 0.0247, 0.0149, 0.0129, 0.0231, 0.0108, 0.0011, 0.0094, 0.0136,\n                        0.0165, 0.0169, 0.0032, 0.0144, 0.0131, 0.0129, 0.0157, 0.0140, 0.0223,\n                        0.0235, 0.0055, 0.0098, 0.0094, 0.0024, 0.0080, 0.0127, 0.0227, 0.0274,\n                        0.0121, 0.0219, 0.0205, 0.0128, 0.0089, 0.0027, 0.0103, 0.0112, 0.0020,\n                        0.0142, 0.0028, 0.0125, 0.0108, 0.0134, 0.0116, 0.0069, 0.0019, 0.0094,\n                        0.0306, 0.0104, 0.0103, 0.0225, 0.0143, 0.0080, 0.0114, 0.0194, 0.0238,\n                        0.0102, 0.0092, 0.0155, 0.0080, 0.0108, 0.0140, 0.0154, 0.0098, 0.0218,\n                        0.0108, 0.0020, 0.0109, 0.0184, 0.0102, 0.0088, 0.0287, 0.0155, 0.0249,\n                        0.0193, 0.0101, 0.0170, 0.0185, 0.0140, 0.0264, 0.0044, 0.0091, 0.0110,\n                        0.0024, 0.0184, 0.0076, 0.0063, 0.0071, 0.0106, 0.0190, 0.0015, 0.0095,\n                        0.0180, 0.0156, 0.0014, 0.0213, 0.0069, 0.0110, 0.0203, 0.0132, 0.0156,\n                        0.0120, 0.0120, 0.0117, 0.0082, 0.0112, 0.0013]), zero_point=tensor([   0,    0,    0,    0,    0,    0,    0,  127,    0,    0,    0,    0,\n                           0,    0, -128,    0,    0,    0,    0,    0,    0,    0,    0,  127,\n                        -128,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                           0,    0,    0,  127,    0,    0,    0,    0,    0,    0,    0,    0,\n                           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         127,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                           0,    0,    0, -128,    0,    0,    0,    0,  127,  127,    0,  127,\n                           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                           0,    0,    0,    0,  127,    0, -128,    0,    0,    0,    0,    0,\n                           0,    0,    0,    0,    0,  127,    0,  127,    0, -128,    0, -128,\n                           0,    0,    0,    0,    0,    0,    0,  127,    0,    0,    0,    0,\n                           0,    0,    0,    0,    0,    0,  127,    0,    0,  127,    0,    0,\n                           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                           0,    0,    0,    0,    0,    0,    0,  127,    0,    0,    0,    0,\n                           0,    0,    0,  127,    0,    0,    0,    0,    0,    0,    0,    0,\n                           0,    0,    0,    0,    0,    0,    0,  127,    0,    0,    0,    0,\n                           0,    0,    0,    0,    0,    0,    0,    0,  127,    0,    0,    0,\n                           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,  127,    0,\n                        -128,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                           0,    0,    0,  127,    0,    0,    0,    0,    0,    0,    0,    0,\n                           0,    0,    0, -128,    0,    0,    0,  127,    0,    0,    0,  127,\n                           0,    0,    0,    0,    0,    0,    0,    0,    0, -128,    0,    0,\n                           0,    0,    0,    0,    0,    0,    0,    0,    0,  127,    0,    0,\n                           0,    0,    0,    0, -128,    0,    0,    0,    0,    0,    0,    0,\n                           0,  127,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                           0,    0,  127,    0,  127,    0,    0,    0,    0,    0,  127,    0,\n                           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                           0,    0,    0,    0,    0,    0,    0,  127,    0,    0,    0, -128,\n                           0,    0,    0,    0,    0,    0,    0,    0,    0,    0, -128,    0,\n                         127,    0,    0,    0,    0,    0,    0,  127,    0,    0,    0,  127,\n                           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,  127],\n                       dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n                (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n                  min_val=tensor([-1.2345, -0.9570, -1.3488, -2.0084, -1.4383, -1.5177, -1.3498, -0.5766,\n                          -1.2239, -1.0085, -1.2229, -3.1024, -1.5755, -1.0162,  0.0227, -0.5963,\n                          -0.8485, -0.1281, -1.2197, -1.2777, -0.8442, -0.5158, -1.4763, -0.7058,\n                           0.0609, -0.5596, -0.9875, -1.2170, -1.0007, -1.2836, -1.0702, -1.4762,\n                          -1.3554, -0.6126, -1.4459, -0.2009, -0.5046, -1.3645, -1.4414, -0.6118,\n                          -1.3817, -1.2707, -0.9436, -1.0414, -1.1109, -1.4166, -0.7091, -1.1926,\n                          -0.7407, -1.6285, -1.1696, -1.9613, -0.8199, -0.9199, -1.3988, -1.1625,\n                          -0.3675, -2.0704, -0.9667, -1.0825, -1.6242, -0.9119, -1.7971, -1.2368,\n                          -1.2526, -0.5838, -1.4530, -0.7309, -0.8709, -1.2428, -1.4723, -0.5510,\n                          -0.3282, -1.1280, -0.1366, -1.8352, -1.3222, -1.1629, -1.2216, -1.5657,\n                          -1.1776, -2.3406, -0.7173, -1.5655, -1.5405, -0.4330, -0.9202,  0.1304,\n                          -0.7289, -2.4070, -1.0758, -1.3058, -0.5227, -0.6114, -0.0472, -0.5769,\n                          -1.3308, -1.2682, -0.5894, -2.2101, -1.0376, -0.6784, -1.2028, -1.5760,\n                          -1.4719, -1.5190, -0.9708, -0.8772, -0.5845, -0.4699, -0.9629, -1.7366,\n                          -0.4911, -0.5163,  0.0746, -0.9670, -3.3143, -2.3299, -1.6564, -0.1987,\n                          -1.4164, -1.5692, -0.3913, -2.9619, -0.3941, -0.6268, -1.2650, -0.4688,\n                          -1.3532,  0.0220, -1.8408,  0.2842, -0.4323, -0.7010, -0.6787, -1.4477,\n                          -3.1131, -0.6830, -1.1916, -0.5132, -1.2402, -0.9141, -1.4055, -2.0084,\n                          -1.1370, -0.6775, -1.5253, -1.3174, -0.6728, -0.5104, -1.0180, -1.0462,\n                          -1.3019, -0.4458, -1.1331, -1.1261, -1.3605, -0.2860, -0.7206, -0.6484,\n                          -0.5672, -1.3268, -2.6485, -1.3686, -1.0252, -1.2876, -0.9993, -3.3888,\n                          -1.1676, -1.2316, -0.5919, -0.8385, -1.2223, -1.3810, -1.4268, -0.7920,\n                          -1.6845, -0.4526, -0.9076, -0.8006, -1.8657, -0.7103, -1.8399, -1.4908,\n                          -1.4430, -2.2019, -1.0004, -2.5286, -1.3606, -0.7773, -1.0041, -0.5357,\n                          -2.1384, -2.2596, -1.8038, -1.8080, -2.1939, -0.4457, -0.5088, -0.8413,\n                          -1.0732, -1.5722, -1.0563, -1.9864, -1.7479, -1.4910, -2.2547, -1.1212,\n                          -1.1035, -1.3010, -0.9846, -1.1147, -0.4681, -0.5133, -0.6063, -1.3112,\n                          -1.4089, -0.8278, -0.2991, -1.8989, -0.5965, -1.0276, -1.5293, -0.9993,\n                          -1.1218, -1.4285, -1.0047, -0.7131,  0.1024, -0.8963, -0.7151, -0.1152,\n                          -0.9357, -1.3148, -0.8690, -1.4254, -1.1442, -1.2061, -0.6508, -0.7986,\n                          -0.5035, -0.9273, -1.3748, -0.7697, -1.3628, -1.5845, -1.0497, -1.3370,\n                          -1.5499, -2.0108, -1.4035, -0.8954, -1.3115, -1.0160, -1.9069,  0.1436,\n                          -0.5939, -0.4647, -1.1253, -0.6810, -0.9697, -1.2131, -1.3598, -0.7567,\n                          -0.9810, -1.6449, -1.4625, -1.5866, -0.4970, -1.7121, -0.3190, -1.1483,\n                          -0.5222,  0.2372, -1.2997, -0.9843, -1.1376, -1.6703, -2.1161, -1.2916,\n                          -3.1659, -0.8823, -1.3572, -2.9506, -0.6480, -0.2820, -0.8000, -1.6107,\n                          -0.0153, -1.2551, -0.4048, -1.8125,  0.0106, -0.9913, -1.5879, -1.7981,\n                          -2.5131, -2.3533, -0.6861, -1.2130, -1.1973, -0.6176, -0.7873, -1.4135,\n                          -2.9080, -2.4811, -0.4537, -2.2625, -2.1940, -1.6357, -1.0153, -0.3420,\n                          -1.3225, -1.3516, -0.5083, -0.8083, -0.7141, -0.8211, -1.2929, -1.3951,\n                          -1.1898, -0.8815, -0.4951, -1.1858, -0.4606, -1.3375, -0.2166, -1.0486,\n                          -1.0531, -1.0269, -1.4581, -1.2710, -0.8374, -1.3022, -1.1817, -1.4825,\n                          -0.9200, -1.3535, -0.9555, -1.1320, -1.2511, -0.1987, -1.1845, -0.5041,\n                          -1.2449, -0.9128, -1.1321,  0.0129, -0.4390, -0.6620, -0.0330, -0.7765,\n                          -1.0852, -2.1720, -1.6062, -1.3962, -1.7387, -0.5616,  0.0890, -0.3582,\n                          -0.6052, -2.3514, -0.7006, -0.8092, -0.8734, -1.3558, -0.7039, -0.3846,\n                          -1.2159, -1.4310, -1.9955, -0.3536, -1.0160, -0.8862, -1.0487, -2.2467,\n                          -0.2407, -0.0367, -0.8424, -1.1226, -1.1031, -0.8513, -0.3644, -0.3212]), max_val=tensor([ 2.9777,  1.4232,  1.7948,  2.5630,  1.2365,  1.2855,  1.7180, -0.0413,\n                           1.5043,  3.3349,  1.4082,  2.0498,  1.7930,  1.7844,  1.8608,  1.1484,\n                           1.0873,  1.5785,  1.3586,  1.6556,  0.2058,  0.2311,  1.6151, -0.1903,\n                           2.3471,  1.8517,  1.5414,  1.2599,  1.3275,  0.9022,  2.4608,  1.7537,\n                           1.3188,  0.3738,  1.3404,  1.3505,  0.0093,  1.5367,  1.8061, -0.1029,\n                           1.3562,  2.6485,  3.4506,  1.9804,  1.4021,  1.0821,  1.8783,  2.9909,\n                           2.6761,  1.5627,  3.6429,  2.2313,  1.8682,  2.2207,  1.2564,  1.2398,\n                           0.0223,  3.1661,  1.1725,  1.4457,  0.2613,  2.9980,  1.2896,  2.0930,\n                           1.2949,  0.2417,  1.5193,  0.1708,  1.6185,  1.7804,  2.2409,  2.5698,\n                          -0.1911,  1.3824,  2.5509,  2.7263,  1.3551,  1.2880,  2.5154,  1.3414,\n                           3.7804,  1.3912,  2.2568,  1.7609,  1.2391,  0.4191,  0.2685,  1.8227,\n                           0.8863,  2.0221,  1.7480,  2.2544, -0.0733, -0.0128,  1.2388, -0.0742,\n                           1.7817,  0.2591,  0.0117,  0.7286,  1.8662,  1.8010,  1.9828,  2.2646,\n                           1.5919,  1.7381,  2.3535,  0.9937,  1.6593,  2.7237,  1.3538,  1.6568,\n                          -0.0854,  2.2931,  1.7931,  1.1774,  3.2781,  1.6551,  1.8921,  2.7908,\n                           1.2531,  1.8506,  0.0313,  0.6564,  1.8601, -0.0317,  1.5064, -0.2421,\n                           1.9969,  1.8967,  1.4396,  1.6279,  3.3038,  2.6237,  2.6108,  1.8720,\n                           2.0527,  1.2768,  1.8788, -0.2306,  1.6534,  1.1126,  1.4234,  1.9191,\n                           0.9152,  2.6202,  2.1606,  1.3593,  1.3429,  5.2523, -0.1098,  1.4044,\n                           1.5522, -0.1073,  1.3887,  1.3322,  1.4408,  2.0649,  0.5411,  0.1521,\n                           0.1887,  1.5798,  1.9213,  1.2299,  1.3670,  2.3400,  1.9434,  3.4597,\n                           2.2865,  4.0276,  0.3820,  0.9639,  1.3160,  1.6579,  1.1197, -0.0245,\n                           1.6390,  1.8458,  0.0378,  0.9844,  1.2357,  1.4823,  1.8886, -0.0736,\n                           1.8166,  1.3663,  1.7390,  3.1130,  1.4166,  1.2408,  1.0730,  1.2500,\n                           2.5584,  0.9013,  2.6092,  2.1374,  2.5147,  1.7533,  0.0453, -0.0899,\n                           2.1828,  1.4291,  1.4501,  1.6453,  1.9047,  1.5826,  2.3402,  1.7908,\n                           2.0594,  1.6345,  1.6975,  1.4066, -0.1545,  0.3129,  1.9136,  1.6873,\n                           1.5925,  0.5567,  1.2692,  2.0635,  2.4564,  1.8300,  1.0105,  1.3861,\n                           1.6295,  1.6965, -0.0159,  2.4842,  1.4313,  1.9146,  2.7111,  2.3442,\n                           3.2493,  1.3586,  2.3933,  1.1044,  1.1900,  1.9852,  2.7761,  1.2301,\n                           1.4203,  2.0207,  1.6750, -0.2039,  1.8712,  1.6526,  2.0937,  1.2005,\n                           2.2729,  2.3728,  1.4374,  1.3537,  1.4439,  1.1228,  2.5254,  0.5424,\n                           1.1026,  0.0427,  1.3664, -0.0881,  2.1998,  1.0174,  1.6427, -0.2086,\n                           1.7316,  2.5202,  2.9776,  2.0932,  1.8375,  2.2462,  2.5676,  1.6090,\n                           2.4455,  1.7495,  1.9394,  1.5412,  1.1990,  1.7353,  2.1659,  1.1917,\n                           1.2943,  1.8880,  1.6369,  0.9491,  1.3739, -0.1211,  1.1917,  1.7252,\n                           2.0982,  2.1503,  0.3855,  1.8239,  3.3299,  1.6331,  1.9991,  1.4112,\n                           2.8334,  2.9874,  0.7034,  1.2496,  1.0194, -0.0460,  1.0129,  1.6092,\n                           1.5965,  3.4815,  1.5383,  2.7844,  2.5979,  1.0567,  1.1308,  0.2370,\n                           0.9050,  1.4213, -0.0173,  1.8060, -0.0056,  1.5844,  1.3721,  1.7057,\n                           1.4694,  0.3812, -0.0448,  1.1922,  3.8829,  1.2267,  1.3041,  2.8584,\n                           1.8127,  0.9181,  1.4023,  2.4696,  3.0186,  1.1427,  1.1460,  1.9745,\n                           1.0203,  1.3679,  1.7819,  1.9585,  1.1387,  2.7692,  1.3740, -0.2059,\n                           1.3871,  2.3393,  1.2914,  2.2557,  3.6404,  1.9654,  3.1648,  2.4524,\n                           1.2772,  0.5545,  2.3441,  1.7808,  3.3550,  0.0598,  2.3252,  1.3916,\n                          -0.1691,  1.6902,  0.9692,  0.1105,  0.9021,  1.2255,  2.4120, -0.1352,\n                           1.0475,  2.2898,  1.7485, -0.0452,  2.7079,  0.0711,  1.4007,  2.5786,\n                           1.6777,  1.9810,  1.5199,  1.5229,  1.4880,  1.0393,  1.4258, -0.0542])\n                )\n              )\n            )\n            (2): ReLU6(inplace=True)\n          )\n          (2): ConvBn2d(\n            384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0031, 0.0025, 0.0031, 0.0032, 0.0026, 0.0029, 0.0031, 0.0046, 0.0033,\n                      0.0025, 0.0024, 0.0030, 0.0025, 0.0032, 0.0031, 0.0026, 0.0031, 0.0020,\n                      0.0035, 0.0025, 0.0023, 0.0029, 0.0032, 0.0046, 0.0033, 0.0034, 0.0031,\n                      0.0022, 0.0042, 0.0033, 0.0039, 0.0026, 0.0021, 0.0021, 0.0042, 0.0021,\n                      0.0039, 0.0030, 0.0023, 0.0018, 0.0028, 0.0025, 0.0027, 0.0028, 0.0035,\n                      0.0023, 0.0033, 0.0041, 0.0034, 0.0026, 0.0032, 0.0059, 0.0045, 0.0031,\n                      0.0031, 0.0029, 0.0039, 0.0026, 0.0029, 0.0049, 0.0036, 0.0033, 0.0023,\n                      0.0026]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n                min_val=tensor([-0.3882, -0.3158, -0.3957, -0.4056, -0.2634, -0.3707, -0.4004, -0.5846,\n                        -0.3466, -0.3145, -0.2324, -0.3812, -0.2645, -0.2868, -0.3760, -0.3345,\n                        -0.3946, -0.2104, -0.4309, -0.3256, -0.2892, -0.2869, -0.4108, -0.5913,\n                        -0.4110, -0.2778, -0.4003, -0.2829, -0.4610, -0.3760, -0.5026, -0.3304,\n                        -0.2657, -0.2588, -0.3173, -0.2184, -0.5039, -0.3300, -0.2939, -0.2052,\n                        -0.3625, -0.3211, -0.3097, -0.3591, -0.4475, -0.2939, -0.3776, -0.5233,\n                        -0.4316, -0.2731, -0.3979, -0.5341, -0.3594, -0.3963, -0.3537, -0.2980,\n                        -0.4977, -0.3078, -0.3705, -0.6245, -0.4583, -0.2931, -0.2984, -0.3002]), max_val=tensor([0.3900, 0.2939, 0.3101, 0.3334, 0.3318, 0.3392, 0.3948, 0.4339, 0.4211,\n                        0.3074, 0.3000, 0.3010, 0.3184, 0.4002, 0.3957, 0.3045, 0.2952, 0.2544,\n                        0.4410, 0.2250, 0.2353, 0.3708, 0.3011, 0.5155, 0.4135, 0.4259, 0.3995,\n                        0.2107, 0.5323, 0.4173, 0.3681, 0.3094, 0.2492, 0.2711, 0.5396, 0.2725,\n                        0.3767, 0.3810, 0.2729, 0.2318, 0.3618, 0.2981, 0.3437, 0.3581, 0.3814,\n                        0.2896, 0.4190, 0.3520, 0.2826, 0.3355, 0.4092, 0.7478, 0.5661, 0.3413,\n                        0.3931, 0.3707, 0.3590, 0.3365, 0.3420, 0.3628, 0.3877, 0.4185, 0.2617,\n                        0.3344])\n              )\n            )\n          )\n        )\n      )\n      (9): Module(\n        (conv): Module(\n          (0): Module(\n            (0): ConvBn2d(\n              64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False\n              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n                fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([6.5916e-04, 3.2751e-04, 5.1156e-04, 5.3774e-04, 7.0849e-04, 3.9665e-04,\n                        4.3289e-04, 5.1433e-04, 3.8093e-04, 5.2343e-04, 5.9698e-04, 2.9683e-04,\n                        3.5421e-04, 4.3356e-04, 3.4311e-04, 3.1863e-04, 4.0309e-04, 8.5940e-04,\n                        5.2001e-04, 5.1516e-04, 3.2666e-04, 3.7332e-04, 4.0697e-04, 3.9499e-04,\n                        7.5793e-04, 6.9202e-04, 2.6428e-04, 5.6932e-04, 7.6952e-04, 5.4891e-04,\n                        4.6454e-04, 4.5815e-04, 2.9765e-04, 4.4295e-04, 4.5641e-04, 2.8949e-04,\n                        7.5200e-04, 4.1946e-04, 6.1910e-04, 3.6491e-04, 4.4968e-04, 5.5359e-04,\n                        4.2650e-04, 6.4844e-04, 3.8565e-04, 4.7555e-04, 5.4561e-04, 3.6379e-04,\n                        3.4211e-04, 5.9019e-04, 4.9368e-04, 5.7716e-04, 5.4393e-04, 3.5490e-04,\n                        2.3871e-04, 3.1770e-04, 3.4598e-04, 4.2376e-04, 3.6991e-04, 5.7176e-04,\n                        4.8643e-04, 4.1410e-04, 3.1940e-04, 4.0338e-04, 4.4455e-04, 3.4039e-04,\n                        4.9614e-04, 3.7103e-04, 5.2967e-04, 2.3148e-04, 4.0561e-04, 2.9442e-04,\n                        4.0053e-04, 3.5081e-04, 6.5457e-04, 6.1209e-04, 3.1575e-04, 3.8548e-04,\n                        4.2034e-04, 3.9395e-04, 1.0446e-04, 5.2356e-04, 4.1748e-04, 7.8406e-04,\n                        4.2623e-04, 6.1201e-04, 4.1392e-04, 2.9548e-04, 5.0555e-04, 3.7384e-04,\n                        4.9026e-04, 3.1047e-04, 1.8841e-05, 3.2397e-04, 5.2159e-04, 3.2498e-04,\n                        5.9160e-04, 1.2984e-04, 3.9943e-04, 5.5189e-04, 5.0285e-04, 3.7998e-04,\n                        7.1843e-04, 4.9783e-04, 2.5762e-04, 8.2837e-04, 5.4278e-04, 5.2491e-04,\n                        5.4834e-04, 4.8028e-04, 2.5915e-04, 2.3551e-04, 5.4374e-04, 4.6269e-04,\n                        8.9085e-05, 8.6915e-04, 4.4379e-04, 6.9267e-04, 1.8083e-04, 5.8695e-04,\n                        4.5599e-04, 4.7095e-04, 2.9314e-04, 5.0933e-04, 6.4462e-04, 3.2417e-04,\n                        3.4715e-04, 3.0220e-04, 4.2684e-04, 4.9495e-04, 4.0726e-04, 3.2865e-04,\n                        4.6519e-04, 4.3604e-04, 4.8169e-04, 7.3616e-04, 5.7521e-04, 5.0856e-04,\n                        3.5223e-04, 3.3590e-04, 6.3620e-04, 5.6715e-04, 5.4959e-04, 5.1272e-04,\n                        4.5536e-04, 5.1036e-04, 3.4274e-04, 4.8930e-04, 1.9072e-04, 4.0353e-04,\n                        6.9132e-04, 3.6197e-04, 3.7937e-04, 5.2222e-04, 4.3659e-04, 4.8255e-04,\n                        5.3584e-04, 2.9958e-04, 3.4012e-04, 4.3824e-04, 3.2078e-04, 5.4713e-04,\n                        5.6811e-04, 3.8293e-04, 4.8234e-04, 4.6260e-04, 3.3645e-04, 6.5639e-04,\n                        2.8913e-04, 2.1045e-04, 7.1776e-04, 4.8291e-04, 5.5998e-04, 6.4644e-04,\n                        6.3596e-04, 8.7381e-04, 4.1851e-04, 3.4286e-04, 3.0942e-04, 4.7672e-04,\n                        4.6150e-04, 4.3101e-04, 3.3450e-04, 5.1965e-04, 6.6036e-04, 3.5934e-04,\n                        3.8442e-04, 5.7452e-04, 4.6296e-04, 6.4694e-04, 3.0174e-04, 3.7930e-04,\n                        6.1834e-04, 4.2469e-04, 2.7424e-04, 4.8412e-04, 4.3574e-04, 2.2281e-04,\n                        7.0473e-04, 5.9276e-04, 5.5660e-04, 2.9014e-04, 2.8293e-04, 3.9468e-04,\n                        6.3576e-04, 2.2229e-04, 4.4710e-04, 5.0560e-04, 6.0927e-04, 4.8845e-04,\n                        5.0004e-04, 4.1664e-04, 3.0588e-04, 7.6831e-04, 2.9339e-04, 3.6599e-04,\n                        2.3850e-04, 5.9598e-04, 7.9962e-05, 5.1505e-04, 3.8557e-04, 3.2763e-04,\n                        3.7671e-04, 5.2197e-04, 5.2998e-04, 4.1122e-04, 4.8884e-04, 5.1690e-04,\n                        4.6933e-04, 5.1692e-04, 3.9305e-04, 7.4153e-04, 4.3649e-04, 4.8735e-04,\n                        5.9638e-04, 2.8150e-04, 4.1224e-04, 4.6438e-04, 3.0952e-04, 3.6341e-04,\n                        3.3444e-04, 3.9736e-04, 6.4399e-04, 4.1414e-04, 5.1293e-04, 4.2417e-04,\n                        3.3099e-04, 4.4328e-04, 6.2944e-04, 3.5080e-04, 3.2709e-04, 6.0822e-04,\n                        6.2988e-04, 3.0546e-04, 3.4894e-04, 3.8361e-04, 7.0989e-04, 4.0361e-04,\n                        4.9008e-04, 5.5196e-04, 5.1144e-04, 4.5067e-04, 4.4298e-04, 3.5576e-04,\n                        6.1084e-04, 5.5034e-04, 8.8224e-06, 4.9829e-04, 4.3575e-04, 4.5347e-04,\n                        6.4190e-04, 3.7482e-04, 3.5736e-04, 3.4659e-04, 4.0208e-04, 4.7719e-04,\n                        4.7011e-04, 4.5334e-04, 5.2886e-04, 6.2145e-04, 4.3636e-04, 2.3067e-04,\n                        5.4122e-04, 5.2159e-04, 3.3168e-04, 7.3452e-04, 2.5771e-04, 4.7265e-04,\n                        2.6087e-04, 3.9852e-04, 4.2142e-04, 5.8358e-04, 3.8037e-04, 4.3809e-04,\n                        3.5298e-04, 3.7050e-04, 3.6300e-04, 2.1449e-04, 3.0231e-04, 5.5011e-04,\n                        5.6102e-04, 2.8221e-04, 5.5241e-04, 4.7367e-04, 4.1988e-04, 5.6817e-04,\n                        4.7645e-04, 4.9721e-04, 5.2157e-04, 6.5842e-04, 6.5348e-04, 3.0352e-04,\n                        4.2458e-04, 4.5791e-04, 5.1911e-04, 3.6525e-04, 1.8617e-05, 5.3104e-04,\n                        7.8687e-04, 4.0709e-04, 4.3381e-04, 6.1611e-04, 3.3453e-04, 3.4063e-04,\n                        4.8519e-04, 5.2466e-04, 5.8373e-04, 2.9113e-04, 6.9466e-04, 3.5514e-04,\n                        5.8794e-04, 3.4303e-04, 5.2896e-04, 3.2902e-04, 2.7044e-04, 3.7795e-04,\n                        2.7043e-04, 4.0187e-04, 6.0813e-04, 3.5393e-04, 3.4712e-04, 4.5987e-04,\n                        4.8864e-04, 3.7423e-04, 2.3611e-04, 7.1654e-04, 2.7944e-04, 5.9730e-04,\n                        3.0134e-04, 3.8840e-04, 4.8369e-04, 8.7440e-04, 5.1831e-04, 2.3348e-04,\n                        3.1793e-04, 5.6970e-04, 4.2514e-04, 3.0053e-04, 5.1336e-04, 5.7952e-04,\n                        4.3232e-04, 6.8322e-04, 6.2770e-04, 4.4233e-04, 4.8882e-04, 2.5736e-04,\n                        4.9482e-04, 4.8727e-04, 4.9491e-04, 4.3260e-04, 5.8876e-04, 5.0246e-04,\n                        6.2050e-04, 5.7493e-04, 5.7516e-04, 4.6505e-04, 4.4394e-04, 4.2470e-04,\n                        5.8017e-04, 5.0606e-04, 4.5507e-04, 6.1006e-04, 3.2764e-04, 5.5644e-04]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                       dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n                (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n                  min_val=tensor([-0.0476, -0.0419, -0.0655, -0.0688, -0.0885, -0.0350, -0.0498, -0.0658,\n                          -0.0488, -0.0540, -0.0764, -0.0350, -0.0426, -0.0555, -0.0320, -0.0313,\n                          -0.0500, -0.0592, -0.0598, -0.0606, -0.0265, -0.0335, -0.0490, -0.0476,\n                          -0.0533, -0.0774, -0.0338, -0.0729, -0.0674, -0.0703, -0.0523, -0.0343,\n                          -0.0381, -0.0567, -0.0584, -0.0362, -0.0665, -0.0537, -0.0792, -0.0467,\n                          -0.0576, -0.0707, -0.0506, -0.0801, -0.0494, -0.0609, -0.0682, -0.0466,\n                          -0.0438, -0.0522, -0.0450, -0.0739, -0.0640, -0.0454, -0.0306, -0.0407,\n                          -0.0433, -0.0406, -0.0473, -0.0694, -0.0531, -0.0516, -0.0409, -0.0516,\n                          -0.0357, -0.0306, -0.0635, -0.0475, -0.0447, -0.0296, -0.0359, -0.0377,\n                          -0.0491, -0.0392, -0.0473, -0.0783, -0.0404, -0.0420, -0.0538, -0.0504,\n                          -0.0134, -0.0529, -0.0375, -0.0904, -0.0546, -0.0783, -0.0530, -0.0263,\n                          -0.0647, -0.0458, -0.0456, -0.0314, -0.0024, -0.0375, -0.0533, -0.0416,\n                          -0.0500, -0.0158, -0.0511, -0.0706, -0.0644, -0.0486, -0.0523, -0.0589,\n                          -0.0243, -0.0703, -0.0436, -0.0547, -0.0702, -0.0615, -0.0332, -0.0301,\n                          -0.0696, -0.0592, -0.0114, -0.0794, -0.0502, -0.0887, -0.0231, -0.0751,\n                          -0.0584, -0.0603, -0.0375, -0.0486, -0.0825, -0.0415, -0.0423, -0.0387,\n                          -0.0546, -0.0634, -0.0419, -0.0421, -0.0595, -0.0558, -0.0617, -0.0942,\n                          -0.0736, -0.0606, -0.0415, -0.0430, -0.0676, -0.0726, -0.0703, -0.0656,\n                          -0.0537, -0.0653, -0.0368, -0.0626, -0.0239, -0.0483, -0.0885, -0.0463,\n                          -0.0486, -0.0535, -0.0557, -0.0509, -0.0622, -0.0383, -0.0334, -0.0346,\n                          -0.0324, -0.0441, -0.0552, -0.0490, -0.0542, -0.0396, -0.0431, -0.0530,\n                          -0.0370, -0.0269, -0.0919, -0.0618, -0.0717, -0.0504, -0.0814, -0.0716,\n                          -0.0536, -0.0370, -0.0320, -0.0501, -0.0378, -0.0552, -0.0359, -0.0665,\n                          -0.0845, -0.0460, -0.0463, -0.0735, -0.0593, -0.0452, -0.0386, -0.0474,\n                          -0.0791, -0.0478, -0.0269, -0.0574, -0.0409, -0.0285, -0.0902, -0.0759,\n                          -0.0548, -0.0371, -0.0314, -0.0389, -0.0814, -0.0285, -0.0354, -0.0536,\n                          -0.0780, -0.0625, -0.0640, -0.0484, -0.0348, -0.0983, -0.0364, -0.0358,\n                          -0.0305, -0.0494, -0.0102, -0.0478, -0.0494, -0.0419, -0.0482, -0.0668,\n                          -0.0678, -0.0526, -0.0536, -0.0662, -0.0322, -0.0662, -0.0438, -0.0949,\n                          -0.0498, -0.0624, -0.0686, -0.0360, -0.0528, -0.0594, -0.0368, -0.0465,\n                          -0.0428, -0.0509, -0.0685, -0.0530, -0.0523, -0.0414, -0.0424, -0.0469,\n                          -0.0532, -0.0449, -0.0381, -0.0582, -0.0650, -0.0383, -0.0383, -0.0491,\n                          -0.0623, -0.0517, -0.0627, -0.0707, -0.0655, -0.0425, -0.0505, -0.0338,\n                          -0.0782, -0.0571, -0.0011, -0.0299, -0.0544, -0.0580, -0.0482, -0.0453,\n                          -0.0395, -0.0444, -0.0515, -0.0611, -0.0602, -0.0580, -0.0576, -0.0471,\n                          -0.0423, -0.0295, -0.0499, -0.0668, -0.0425, -0.0460, -0.0330, -0.0605,\n                          -0.0320, -0.0510, -0.0539, -0.0747, -0.0462, -0.0539, -0.0295, -0.0474,\n                          -0.0424, -0.0257, -0.0387, -0.0704, -0.0718, -0.0361, -0.0707, -0.0606,\n                          -0.0522, -0.0401, -0.0534, -0.0636, -0.0668, -0.0843, -0.0690, -0.0389,\n                          -0.0543, -0.0580, -0.0664, -0.0468, -0.0024, -0.0680, -0.0475, -0.0521,\n                          -0.0555, -0.0619, -0.0428, -0.0414, -0.0621, -0.0672, -0.0596, -0.0373,\n                          -0.0889, -0.0447, -0.0753, -0.0428, -0.0522, -0.0421, -0.0324, -0.0419,\n                          -0.0303, -0.0514, -0.0685, -0.0453, -0.0444, -0.0509, -0.0625, -0.0479,\n                          -0.0302, -0.0917, -0.0358, -0.0765, -0.0386, -0.0497, -0.0606, -0.0917,\n                          -0.0586, -0.0299, -0.0375, -0.0729, -0.0329, -0.0385, -0.0657, -0.0742,\n                          -0.0487, -0.0439, -0.0595, -0.0453, -0.0626, -0.0329, -0.0617, -0.0624,\n                          -0.0481, -0.0554, -0.0633, -0.0643, -0.0794, -0.0736, -0.0509, -0.0491,\n                          -0.0362, -0.0354, -0.0699, -0.0648, -0.0582, -0.0781, -0.0380, -0.0712]), max_val=tensor([0.0837, 0.0364, 0.0348, 0.0619, 0.0900, 0.0504, 0.0550, 0.0566, 0.0350,\n                          0.0665, 0.0487, 0.0377, 0.0450, 0.0466, 0.0436, 0.0405, 0.0512, 0.1091,\n                          0.0660, 0.0654, 0.0415, 0.0474, 0.0517, 0.0502, 0.0963, 0.0879, 0.0331,\n                          0.0589, 0.0977, 0.0553, 0.0590, 0.0582, 0.0374, 0.0514, 0.0452, 0.0368,\n                          0.0955, 0.0435, 0.0596, 0.0317, 0.0532, 0.0703, 0.0542, 0.0824, 0.0286,\n                          0.0573, 0.0693, 0.0405, 0.0292, 0.0750, 0.0627, 0.0691, 0.0691, 0.0413,\n                          0.0284, 0.0319, 0.0439, 0.0538, 0.0360, 0.0726, 0.0618, 0.0526, 0.0306,\n                          0.0510, 0.0565, 0.0432, 0.0491, 0.0416, 0.0673, 0.0243, 0.0515, 0.0364,\n                          0.0509, 0.0446, 0.0831, 0.0678, 0.0361, 0.0490, 0.0481, 0.0424, 0.0120,\n                          0.0665, 0.0530, 0.0996, 0.0406, 0.0461, 0.0506, 0.0375, 0.0520, 0.0475,\n                          0.0623, 0.0394, 0.0019, 0.0411, 0.0662, 0.0373, 0.0751, 0.0165, 0.0439,\n                          0.0509, 0.0631, 0.0422, 0.0912, 0.0632, 0.0327, 0.1052, 0.0689, 0.0667,\n                          0.0599, 0.0379, 0.0283, 0.0284, 0.0441, 0.0461, 0.0078, 0.1104, 0.0564,\n                          0.0792, 0.0171, 0.0590, 0.0569, 0.0371, 0.0335, 0.0647, 0.0609, 0.0366,\n                          0.0441, 0.0365, 0.0526, 0.0498, 0.0517, 0.0324, 0.0539, 0.0403, 0.0524,\n                          0.0605, 0.0573, 0.0646, 0.0447, 0.0359, 0.0808, 0.0631, 0.0603, 0.0578,\n                          0.0578, 0.0625, 0.0435, 0.0397, 0.0242, 0.0512, 0.0576, 0.0421, 0.0419,\n                          0.0663, 0.0554, 0.0613, 0.0681, 0.0294, 0.0432, 0.0557, 0.0407, 0.0695,\n                          0.0722, 0.0386, 0.0613, 0.0587, 0.0422, 0.0834, 0.0346, 0.0204, 0.0585,\n                          0.0603, 0.0637, 0.0821, 0.0669, 0.1110, 0.0494, 0.0435, 0.0393, 0.0605,\n                          0.0586, 0.0437, 0.0425, 0.0610, 0.0725, 0.0363, 0.0488, 0.0399, 0.0386,\n                          0.0822, 0.0285, 0.0482, 0.0645, 0.0539, 0.0348, 0.0615, 0.0553, 0.0248,\n                          0.0367, 0.0593, 0.0707, 0.0299, 0.0359, 0.0501, 0.0585, 0.0272, 0.0568,\n                          0.0642, 0.0554, 0.0620, 0.0548, 0.0529, 0.0388, 0.0446, 0.0373, 0.0465,\n                          0.0271, 0.0757, 0.0083, 0.0654, 0.0310, 0.0399, 0.0396, 0.0630, 0.0611,\n                          0.0416, 0.0621, 0.0459, 0.0596, 0.0472, 0.0499, 0.0881, 0.0554, 0.0581,\n                          0.0757, 0.0318, 0.0471, 0.0489, 0.0393, 0.0315, 0.0391, 0.0387, 0.0818,\n                          0.0460, 0.0651, 0.0539, 0.0235, 0.0563, 0.0799, 0.0262, 0.0415, 0.0772,\n                          0.0800, 0.0388, 0.0443, 0.0467, 0.0902, 0.0500, 0.0557, 0.0509, 0.0488,\n                          0.0572, 0.0563, 0.0452, 0.0741, 0.0699, 0.0009, 0.0633, 0.0553, 0.0502,\n                          0.0815, 0.0476, 0.0454, 0.0239, 0.0509, 0.0443, 0.0504, 0.0313, 0.0672,\n                          0.0789, 0.0554, 0.0287, 0.0687, 0.0389, 0.0400, 0.0933, 0.0187, 0.0579,\n                          0.0331, 0.0321, 0.0400, 0.0572, 0.0483, 0.0556, 0.0448, 0.0334, 0.0461,\n                          0.0272, 0.0371, 0.0427, 0.0602, 0.0344, 0.0568, 0.0342, 0.0533, 0.0722,\n                          0.0605, 0.0462, 0.0605, 0.0629, 0.0830, 0.0325, 0.0461, 0.0582, 0.0391,\n                          0.0257, 0.0018, 0.0673, 0.0999, 0.0468, 0.0408, 0.0782, 0.0417, 0.0433,\n                          0.0433, 0.0542, 0.0741, 0.0312, 0.0803, 0.0451, 0.0690, 0.0436, 0.0672,\n                          0.0376, 0.0343, 0.0480, 0.0343, 0.0434, 0.0772, 0.0388, 0.0427, 0.0584,\n                          0.0471, 0.0410, 0.0209, 0.0681, 0.0266, 0.0447, 0.0370, 0.0433, 0.0614,\n                          0.1110, 0.0658, 0.0215, 0.0404, 0.0716, 0.0540, 0.0377, 0.0465, 0.0562,\n                          0.0549, 0.0868, 0.0797, 0.0562, 0.0536, 0.0317, 0.0628, 0.0517, 0.0629,\n                          0.0358, 0.0748, 0.0544, 0.0506, 0.0723, 0.0730, 0.0591, 0.0564, 0.0539,\n                          0.0737, 0.0494, 0.0572, 0.0635, 0.0416, 0.0545])\n                )\n              )\n            )\n            (2): ReLU6(inplace=True)\n          )\n          (1): Module(\n            (0): ConvBn2d(\n              384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False\n              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n                fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0109, 0.0183, 0.0118, 0.0128, 0.0205, 0.0145, 0.0201, 0.0141, 0.0113,\n                        0.0119, 0.0112, 0.0299, 0.0143, 0.0108, 0.0152, 0.0127, 0.0158, 0.0023,\n                        0.0141, 0.0117, 0.0260, 0.0124, 0.0112, 0.0184, 0.0110, 0.0145, 0.0338,\n                        0.0048, 0.0137, 0.0022, 0.0168, 0.0094, 0.0191, 0.0110, 0.0105, 0.0144,\n                        0.0153, 0.0060, 0.0145, 0.0162, 0.0213, 0.0026, 0.0094, 0.0190, 0.0137,\n                        0.0114, 0.0073, 0.0104, 0.0134, 0.0052, 0.0102, 0.0123, 0.0039, 0.0106,\n                        0.0111, 0.0115, 0.0169, 0.0135, 0.0187, 0.0209, 0.0168, 0.0078, 0.0149,\n                        0.0108, 0.0028, 0.0095, 0.0016, 0.0108, 0.0212, 0.0243, 0.0186, 0.0122,\n                        0.0134, 0.0050, 0.0062, 0.0199, 0.0121, 0.0081, 0.0112, 0.0122, 0.0485,\n                        0.0070, 0.0159, 0.0198, 0.0122, 0.0120, 0.0026, 0.0250, 0.0080, 0.0131,\n                        0.0220, 0.0137, 0.0059, 0.0245, 0.0251, 0.0140, 0.0018, 0.0152, 0.0148,\n                        0.0140, 0.0050, 0.0139, 0.0095, 0.0021, 0.0296, 0.0108, 0.0029, 0.0165,\n                        0.0044, 0.0128, 0.0104, 0.0232, 0.0093, 0.0124, 0.0858, 0.0041, 0.0147,\n                        0.0167, 0.0287, 0.0128, 0.0024, 0.0141, 0.0213, 0.0014, 0.0075, 0.0097,\n                        0.0126, 0.0150, 0.0143, 0.0095, 0.0104, 0.0197, 0.0108, 0.0036, 0.0117,\n                        0.0056, 0.0022, 0.0027, 0.0111, 0.0093, 0.0071, 0.0020, 0.0109, 0.0035,\n                        0.0077, 0.0180, 0.0078, 0.0120, 0.0237, 0.0111, 0.0030, 0.0145, 0.0096,\n                        0.0066, 0.0112, 0.0129, 0.0074, 0.0203, 0.0148, 0.0087, 0.0112, 0.0128,\n                        0.0128, 0.0074, 0.0121, 0.0142, 0.0174, 0.0144, 0.0092, 0.0119, 0.0032,\n                        0.0080, 0.0122, 0.0020, 0.0044, 0.0129, 0.0023, 0.0030, 0.0099, 0.0118,\n                        0.0126, 0.0031, 0.0102, 0.0050, 0.0124, 0.0046, 0.0044, 0.0083, 0.0134,\n                        0.0108, 0.0098, 0.0179, 0.0166, 0.0111, 0.0108, 0.0130, 0.0108, 0.0184,\n                        0.0148, 0.0140, 0.0098, 0.0178, 0.0212, 0.0039, 0.0076, 0.0181, 0.0097,\n                        0.0169, 0.0062, 0.0119, 0.0269, 0.0103, 0.0043, 0.0200, 0.0116, 0.0019,\n                        0.0170, 0.0052, 0.0665, 0.0171, 0.0120, 0.0040, 0.0144, 0.0147, 0.0075,\n                        0.0153, 0.0098, 0.0024, 0.0089, 0.0100, 0.0085, 0.0171, 0.0013, 0.0102,\n                        0.0046, 0.0146, 0.0114, 0.0200, 0.0138, 0.0091, 0.0064, 0.0138, 0.0036,\n                        0.0441, 0.0099, 0.0115, 0.0180, 0.0190, 0.0183, 0.0125, 0.0025, 0.0061,\n                        0.0089, 0.0135, 0.0087, 0.0130, 0.0037, 0.0049, 0.0115, 0.0056, 0.0090,\n                        0.0098, 0.0094, 0.0028, 0.0093, 0.0150, 0.0005, 0.0203, 0.0124, 0.0087,\n                        0.0128, 0.0061, 0.0161, 0.0214, 0.0063, 0.0016, 0.0047, 0.0112, 0.0164,\n                        0.0130, 0.0098, 0.0157, 0.0130, 0.0097, 0.0126, 0.0095, 0.0212, 0.0107,\n                        0.0212, 0.0110, 0.0242, 0.0090, 0.0146, 0.0115, 0.0102, 0.0112, 0.0235,\n                        0.0199, 0.0122, 0.0153, 0.0137, 0.0125, 0.0106, 0.0207, 0.0175, 0.0029,\n                        0.0033, 0.0113, 0.0165, 0.0212, 0.0059, 0.0130, 0.0109, 0.0069, 0.0179,\n                        0.0246, 0.0020, 0.0124, 0.0140, 0.0112, 0.0218, 0.0020, 0.0097, 0.0152,\n                        0.0098, 0.0155, 0.0098, 0.0165, 0.0079, 0.0119, 0.0112, 0.0137, 0.0173,\n                        0.0152, 0.0195, 0.0144, 0.0217, 0.0130, 0.0185, 0.0099, 0.0152, 0.0197,\n                        0.0155, 0.0198, 0.0288, 0.0056, 0.0059, 0.0113, 0.0154, 0.0022, 0.0088,\n                        0.0140, 0.0280, 0.0316, 0.0104, 0.0023, 0.0174, 0.0127, 0.0040, 0.0104,\n                        0.0116, 0.0228, 0.0209, 0.0140, 0.0086, 0.0196, 0.0195, 0.0117, 0.0105,\n                        0.0180, 0.0123, 0.0020, 0.0101, 0.0019, 0.0078, 0.0115, 0.0170, 0.0129,\n                        0.0149, 0.0019, 0.0210, 0.0059, 0.0080, 0.0176]), zero_point=tensor([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                           0,    0,    0,    0,    0,  127,    0,    0,    0,    0,    0,    0,\n                           0,    0,    0,    0,    0,  127,    0,    0,    0,    0,    0,    0,\n                           0,    0,    0,    0,    0,  127,    0,    0,    0,    0,  127,    0,\n                           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                           0,    0,    0,    0,  127,    0,  127,    0,    0,    0,    0,    0,\n                           0,  127,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                           0,    0,  127,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         127, -128,    0,    0,    0,    0,    0,  127,    0,    0,  127,    0,\n                           0,    0,    0,    0,    0,    0,    0,  127,    0,    0,    0,    0,\n                         127,    0,    0,  127,    0,    0,    0,    0,    0,    0,    0,    0,\n                           0,    0,    0,    0,  127,  127,    0,    0,    0,  127,    0,    0,\n                           0,    0,    0,    0,    0,    0,  127,    0,    0,    0,    0,    0,\n                           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                           0,    0,  127,    0,    0,  127,    0,    0,  127, -128, -128,    0,\n                           0,    0,    0,    0,    0,  127,  127, -128,    0,    0,    0,    0,\n                           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0, -128,\n                           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,  127,\n                           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,  127,\n                           0,    0,    0,    0,  127,    0,    0,    0,    0,    0,    0,    0,\n                           0,    0,  127,    0,    0,    0,    0,    0,    0,    0,  127, -128,\n                           0,    0, -128,    0,  127,    0,    0,    0,    0,    0,    0,  127,\n                           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,  127,\n                           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                           0,    0,    0,    0,    0,    0, -128,    0,    0, -128,    0,    0,\n                           0,    0,    0,    0,    0,  127,  127,    0,    0,    0,    0,    0,\n                           0,    0,    0,    0,    0,    0,    0,    0,    0,  127,    0,    0,\n                           0,    0,    0,    0,    0,    0,    0,    0,    0, -128,    0,    0,\n                           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                           0,  127, -128,    0,    0,    0,    0, -128,    0,    0,  127,    0,\n                           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,  127,\n                           0,  127,    0,    0,    0,    0,    0,  127,    0, -128,    0,    0],\n                       dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n                (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n                  min_val=tensor([-1.3953e+00, -1.7424e+00, -1.5116e+00, -4.2378e-01, -2.6178e+00,\n                          -6.4267e-01, -4.0519e-01, -1.6210e+00, -1.3217e+00, -7.5414e-01,\n                          -8.5817e-01, -1.2796e+00, -1.1410e+00, -4.9633e-01, -1.6896e+00,\n                          -1.2022e+00, -8.7485e-01, -5.7487e-01, -1.8060e+00, -9.1725e-01,\n                          -2.4642e-01, -1.1773e+00, -1.0680e+00, -1.5491e+00, -5.1376e-01,\n                          -1.2699e+00, -1.4744e+00, -6.1920e-01, -4.3736e-01, -5.6489e-01,\n                          -1.9737e-02, -8.3051e-01, -2.4470e+00, -1.4081e+00, -1.3429e+00,\n                          -1.8493e+00, -6.5209e-01, -7.6608e-01, -1.8570e+00, -2.0720e+00,\n                          -8.5251e-01, -6.5749e-01, -8.1625e-01, -7.0263e-01, -1.7498e+00,\n                          -1.4542e+00, -1.8710e+00, -1.0971e+00, -1.2109e+00, -6.6290e-01,\n                          -1.2383e+00, -1.5762e+00, -4.9387e-01, -1.1803e+00, -5.9900e-01,\n                          -6.1103e-02, -1.7379e+00, -1.4429e+00, -2.3555e+00, -2.6757e+00,\n                          -2.8332e-01, -9.3868e-01, -9.0681e-01, -1.1648e+00, -7.1568e-01,\n                          -9.0676e-01, -3.9694e-01, -1.3817e+00, -2.7107e+00, -1.9281e-01,\n                          -2.0656e+00, -1.1217e+00, -1.7193e+00, -1.2754e+00, -1.8903e-01,\n                          -6.3986e-01, -1.2587e+00, -1.0002e+00, -1.0154e+00, -1.2647e+00,\n                          -1.6710e+00, -8.9769e-01, -8.4305e-01, -2.5349e+00, -1.5357e+00,\n                          -1.3337e+00, -6.6812e-01, -1.6788e+00, -8.9144e-01, -1.1600e+00,\n                          -2.8217e+00, -1.7539e+00, -6.8515e-01, -1.3027e+00, -1.0687e+00,\n                          -1.7929e+00, -4.6217e-01,  7.3620e-01, -6.0170e-01, -1.5173e+00,\n                          -6.4058e-01, -1.7155e+00, -1.2215e+00, -5.2666e-01, -4.7509e-01,\n                          -9.9249e-01, -7.4433e-01, -8.0149e-01, -5.5760e-01, -7.5120e-01,\n                          -1.0600e+00, -2.6732e+00, -1.0707e+00, -1.0562e+00, -7.6179e+00,\n                          -1.0341e+00, -5.5854e-01, -9.5795e-02, -1.4813e+00, -1.3014e+00,\n                          -6.0149e-01, -1.6022e+00, -1.3769e+00, -3.5352e-01, -1.0135e-01,\n                          -1.2423e+00, -8.3236e-01, -6.4511e-01, -6.1252e-01, -1.2099e+00,\n                          -8.7013e-01, -1.3172e+00, -9.3731e-01, -4.5567e-01, -1.4939e+00,\n                          -6.5806e-01, -5.5065e-01, -7.0123e-01, -1.1369e+00, -1.1902e+00,\n                          -9.1108e-01, -5.0006e-01, -5.0632e-01, -4.4373e-01, -9.8421e-01,\n                          -6.7959e-01, -1.0024e+00, -8.7492e-02, -2.4543e+00, -1.3919e+00,\n                          -7.7114e-01, -1.3607e+00, -1.0715e+00, -8.4010e-01, -9.1683e-01,\n                          -7.1285e-01, -6.1893e-01, -2.6018e+00, -1.8678e+00, -1.0583e+00,\n                          -1.4364e+00, -6.5390e-01, -2.1633e-01, -9.4160e-01, -1.5499e+00,\n                          -1.2659e+00, -1.3304e+00, -9.1216e-01, -1.9739e-01, -1.4346e+00,\n                          -8.2723e-01, -6.3805e-01, -5.3509e-01, -5.0721e-01, -5.6193e-01,\n                          -8.8155e-02, -5.8701e-01,  2.1671e-01,  1.2938e-01, -1.5072e+00,\n                          -1.6162e+00, -3.9193e-01, -8.8608e-01, -6.4360e-01, -1.5863e+00,\n                          -1.1695e+00, -1.1338e+00,  6.4095e-02, -1.1237e+00, -1.2243e+00,\n                          -1.0554e+00, -1.0303e+00, -2.8612e-01, -1.4166e+00, -5.8393e-01,\n                          -8.6979e-01, -1.3773e+00, -2.3528e+00, -9.8462e-01, -9.2855e-02,\n                          -1.2415e+00, -1.2945e+00, -1.9064e+00,  8.6940e-02, -8.0540e-01,\n                          -1.3642e+00, -1.2472e+00, -2.1655e+00, -7.8844e-01, -1.4557e+00,\n                          -7.8542e-01, -6.5734e-02, -5.5249e-01, -2.2375e-01, -1.1076e+00,\n                          -4.7328e-01, -2.1487e+00, -6.7079e-01, -3.2378e+00, -1.0475e+00,\n                          -1.5422e+00, -5.1587e-01, -1.8405e+00, -7.3902e-01, -7.2890e-01,\n                          -6.5968e-01, -7.8127e-01, -6.1124e-01, -1.1334e+00, -1.2018e+00,\n                          -4.1936e-01, -5.1641e-02, -3.2999e-01, -1.0497e+00, -5.8670e-01,\n                          -1.5965e+00, -1.0639e+00, -1.1514e+00, -1.6246e+00, -1.1604e+00,\n                          -8.2225e-01, -6.5112e-01, -9.0897e-01, -1.4956e+00, -1.2199e+00,\n                          -7.0178e-01, -1.0859e+00, -5.9267e-01, -2.2765e-02, -1.2270e+00,\n                          -6.3477e-01,  1.6125e-02, -5.0003e-01, -1.3196e+00,  2.9779e-01,\n                          -1.6594e+00, -9.3707e-01, -6.3001e-01, -1.2242e+00, -7.1922e-01,\n                          -1.1117e+00, -9.0097e-01, -1.1988e+00, -7.0471e-01, -1.0293e+00,\n                          -4.1811e-02, -3.0113e-02, -7.7121e-01, -1.5862e+00, -1.0957e+00,\n                          -7.8067e-01, -4.8984e-01, -1.2763e+00, -2.5665e+00, -5.5527e-01,\n                          -4.1466e-01, -5.9792e-01, -1.3840e+00, -3.4062e-02, -1.1136e+00,\n                          -1.2553e+00, -7.4571e-01, -9.6525e-01, -9.9539e-01, -1.2529e+00,\n                          -1.2117e+00, -2.1688e+00, -1.3679e+00, -2.7138e+00, -1.4044e+00,\n                          -1.3514e+00, -2.9162e-01, -5.5911e-01, -5.1737e-01,  1.1309e-01,\n                          -1.4315e+00, -1.2377e+00,  1.9715e-01, -1.1185e+00, -1.9556e+00,\n                          -4.2807e-01, -1.3514e+00, -1.3552e+00, -1.8830e+00, -2.2371e+00,\n                          -7.4027e-01, -8.4732e-01, -1.4479e+00, -4.9668e-01, -8.0457e-01,\n                          -7.5324e-01, -1.6677e+00, -1.3971e+00, -8.6058e-01, -1.9873e+00,\n                          -2.3449e+00, -5.3944e-02, -6.2486e-01, -1.7957e+00, -1.3685e+00,\n                          -9.4871e-01, -5.1907e-01, -1.1152e+00, -1.1011e+00, -3.3619e-01,\n                          -1.0232e-01, -7.6702e-01, -1.9586e+00, -1.0121e+00, -7.0896e-01,\n                          -7.4718e-01, -9.6040e-01, -2.2098e+00,  1.2829e-01, -1.3888e+00,\n                          -1.8391e+00, -2.4302e+00, -6.9422e-01, -7.8341e-01, -7.3683e-01,\n                          -4.4150e-01, -1.1348e+00, -1.9874e+00, -3.1574e-01, -3.6836e+00,\n                          -7.1985e-01, -7.5916e-01, -2.2326e-01, -1.9704e+00, -5.6366e-01,\n                           6.0834e-02, -1.4725e-01, -7.0560e-03, -2.0446e+00, -1.3312e+00,\n                           6.7233e-02, -2.2224e+00, -1.6287e+00, -1.0275e+00, -1.3294e+00,\n                          -1.4838e+00, -2.6939e-01, -9.1323e-01, -9.7702e-01, -4.5413e-01,\n                          -1.6338e+00, -1.0436e+00, -1.3751e+00, -6.2840e-01, -1.9873e+00,\n                          -9.8140e-01, -5.0304e-01, -6.5007e-01, -4.7648e-01, -7.8492e-01,\n                          -1.4711e+00, -2.1753e+00, -1.6470e+00, -5.7444e-01, -4.7553e-01,\n                          -1.9974e-01,  1.5753e-01, -4.1554e-01, -9.0104e-01]), max_val=tensor([ 1.1136e+00,  2.3224e+00,  1.4476e+00,  1.6270e+00,  6.1998e-01,\n                           1.8414e+00,  2.5500e+00,  1.7873e+00,  1.4303e+00,  1.5060e+00,\n                           1.4214e+00,  3.7992e+00,  1.8101e+00,  1.3697e+00,  1.9300e+00,\n                           1.6152e+00,  2.0030e+00, -2.6499e-02,  1.2798e+00,  1.4878e+00,\n                           3.2958e+00,  1.5702e+00,  1.4240e+00,  2.3360e+00,  1.3993e+00,\n                           1.8369e+00,  4.2971e+00,  2.3381e-01,  1.7454e+00, -1.7379e-01,\n                           2.1312e+00,  1.1918e+00,  1.7057e+00,  7.6740e-01,  1.2008e+00,\n                           1.5124e+00,  1.9449e+00,  1.1455e-03,  1.4317e+00,  1.3197e+00,\n                           2.7054e+00, -4.2255e-02,  1.1913e+00,  2.4111e+00,  1.4273e+00,\n                           1.2668e+00, -1.7161e-01,  1.3259e+00,  1.6972e+00,  2.6092e-01,\n                           1.2989e+00,  1.3046e+00,  4.2676e-02,  1.3417e+00,  1.4127e+00,\n                           1.4581e+00,  2.1472e+00,  1.7185e+00,  2.3763e+00,  9.8756e-01,\n                           2.1397e+00,  9.9232e-01,  1.8898e+00,  1.3728e+00, -5.3007e-02,\n                           1.2063e+00, -7.2223e-02,  7.9407e-01,  7.1814e-01,  3.0824e+00,\n                           2.3601e+00,  1.5496e+00,  1.1391e+00, -1.6875e-01,  7.8236e-01,\n                           2.5270e+00,  1.5366e+00,  1.0231e+00,  1.4225e+00,  1.5488e+00,\n                           6.1593e+00,  1.5778e-01,  2.0195e+00,  7.8449e-01,  1.5496e+00,\n                           1.5213e+00, -4.1575e-02,  3.1703e+00,  1.0199e+00,  1.6612e+00,\n                           1.8799e+00,  5.1896e-02,  7.4956e-01,  3.1159e+00,  3.1884e+00,\n                           1.0881e+00, -4.8883e-02,  3.8777e+00,  1.8802e+00,  1.7721e+00,\n                           2.9694e-01,  1.7593e+00,  1.0729e+00, -7.1702e-02,  3.7628e+00,\n                           1.3664e+00, -4.9029e-02,  2.0976e+00,  2.0156e-01,  1.6230e+00,\n                           1.3147e+00,  2.9475e+00,  1.1788e+00,  1.5736e+00,  1.0897e+01,\n                          -2.4687e-01,  1.8731e+00,  2.1257e+00,  3.6481e+00,  1.6274e+00,\n                          -1.3206e-01,  1.7937e+00,  2.7008e+00, -2.1130e-01,  9.5617e-01,\n                           3.3679e-02,  1.5994e+00,  1.9065e+00,  1.8110e+00,  9.8361e-01,\n                           1.3201e+00,  2.4988e+00,  1.3667e+00,  6.8038e-02,  1.0137e+00,\n                           7.1496e-01, -9.7657e-02, -7.5056e-02,  1.4059e+00,  9.7496e-01,\n                           6.5578e-02, -2.7801e-01,  1.3884e+00,  4.5962e-02,  9.7344e-01,\n                           2.2838e+00,  6.6214e-01,  1.5283e+00,  3.0097e+00,  1.4114e+00,\n                          -1.8525e-01,  1.8428e+00,  1.2140e+00,  1.8437e-01,  1.4199e+00,\n                           1.6331e+00,  9.3826e-01,  4.7952e-01,  1.8837e+00,  1.1083e+00,\n                           1.0476e+00,  1.6296e+00,  1.6302e+00,  6.8659e-01,  1.5257e+00,\n                           1.7996e+00,  2.2135e+00,  1.8326e+00,  1.1654e+00,  1.5175e+00,\n                          -1.3887e-01,  1.0209e+00,  1.5530e+00, -1.7709e-01,  3.3925e-01,\n                           1.6352e+00, -2.8758e-01,  7.5407e-01,  2.5300e+00,  1.1203e+00,\n                           1.3543e+00,  5.8585e-02,  1.2953e+00,  4.4399e-01,  1.3263e+00,\n                          -6.3225e-04, -8.3157e-02,  2.1180e+00,  1.6979e+00,  1.3743e+00,\n                           1.2496e+00,  2.2716e+00,  2.1117e+00,  1.0820e+00,  1.3743e+00,\n                           1.6517e+00,  1.1744e+00,  1.5653e+00,  1.8803e+00,  1.7761e+00,\n                           1.2428e+00,  2.2663e+00,  2.6882e+00,  1.0028e+00,  9.7016e-01,\n                           2.2996e+00,  1.1003e+00,  1.9205e+00,  2.4860e-01,  1.5112e+00,\n                           3.4214e+00,  1.3103e+00,  8.9469e-02,  2.5456e+00,  1.4700e+00,\n                          -2.2198e-01,  2.1618e+00,  4.4693e-02,  8.4475e+00,  2.1747e+00,\n                           1.1734e+00,  1.1327e-01,  1.0326e+00,  1.8722e+00,  9.4799e-01,\n                           1.9443e+00,  1.2464e+00, -5.5063e-02,  6.8955e-01,  1.2696e+00,\n                           1.0831e+00,  2.1705e+00, -1.8756e-01,  1.2983e+00,  2.5848e-01,\n                           1.8579e+00,  1.4419e+00,  2.5345e+00,  1.7577e+00,  1.0213e+00,\n                           7.1149e-01,  1.7552e+00, -8.3840e-02,  5.5958e+00,  1.2587e+00,\n                           1.4657e+00,  2.2832e+00,  2.4137e+00,  2.3285e+00,  1.5915e+00,\n                          -1.1998e-01,  1.5578e+00,  1.1364e+00,  1.7182e+00,  2.2102e+00,\n                           1.5812e+00, -1.9035e-02,  4.0635e-01,  1.4636e+00,  3.6262e-01,\n                           1.1469e+00,  1.2383e+00,  9.9370e-01, -1.5134e-01,  1.1832e+00,\n                           1.9024e+00,  5.9870e-02,  2.5760e+00,  1.2457e+00,  1.1104e+00,\n                           1.6211e+00,  7.7805e-01,  2.0397e+00,  2.7199e+00,  7.9682e-01,\n                          -1.4699e-02,  3.0505e-02,  1.4181e+00,  2.0836e+00,  1.6537e+00,\n                           1.1308e+00,  1.9918e+00,  1.6479e+00,  1.2269e+00,  1.5982e+00,\n                           1.1683e+00,  2.6949e+00,  9.3547e-01,  2.2508e+00,  1.4029e+00,\n                           3.0775e+00,  1.1389e+00,  1.8513e+00,  1.4545e+00,  2.6004e+00,\n                           1.2239e+00,  2.9856e+00,  5.0695e+00,  1.5439e+00,  1.8131e+00,\n                           1.7412e+00,  1.5874e+00,  1.3029e+00,  2.6323e+00,  2.0001e+00,\n                          -7.1878e-02, -1.8504e-01,  7.1731e-01,  2.0970e+00,  2.6880e+00,\n                           3.7596e-01,  1.6472e+00,  9.3263e-01,  8.7865e-01,  2.2690e+00,\n                           3.1296e+00,  2.4884e-01,  1.5699e+00,  1.1964e+00,  1.4202e+00,\n                           2.7667e+00, -7.1417e-02,  1.2366e+00,  1.9262e+00,  1.2498e+00,\n                           1.9723e+00,  1.2416e+00,  2.0935e+00,  8.1327e-01,  1.5158e+00,\n                           1.4185e+00,  1.7403e+00,  9.9750e-01,  3.8783e+00,  2.4768e+00,\n                           1.7721e+00,  2.7532e+00,  1.6528e+00,  2.3462e+00,  1.2603e+00,\n                           1.9275e+00,  2.5038e+00,  1.4056e+00,  2.5106e+00,  3.5295e+00,\n                           4.2002e-01,  2.3083e-01,  1.4398e+00,  1.7239e+00, -6.9675e-03,\n                           2.2464e+00,  1.7720e+00,  3.5529e+00,  4.0146e+00,  3.6892e-01,\n                           5.9854e-01,  2.1541e+00,  4.3978e-01, -1.1222e-01,  1.2245e+00,\n                           1.3310e+00,  2.9016e+00,  2.6596e+00,  1.7843e+00,  1.0958e+00,\n                           2.4903e+00,  2.4735e+00,  1.4857e+00,  1.3345e+00,  2.2916e+00,\n                           1.5669e+00, -9.8465e-02,  1.2872e+00, -1.6967e-01,  9.9336e-01,\n                           9.9681e-01,  1.2315e+00,  1.3244e+00,  1.8951e+00, -8.4245e-02,\n                           2.6728e+00,  1.4954e+00,  1.0208e+00,  2.2324e+00])\n                )\n              )\n            )\n            (2): ReLU6(inplace=True)\n          )\n          (2): ConvBn2d(\n            384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0033, 0.0028, 0.0024, 0.0037, 0.0026, 0.0028, 0.0034, 0.0027, 0.0040,\n                      0.0030, 0.0030, 0.0030, 0.0031, 0.0024, 0.0030, 0.0022, 0.0030, 0.0021,\n                      0.0040, 0.0026, 0.0020, 0.0026, 0.0027, 0.0039, 0.0027, 0.0025, 0.0033,\n                      0.0034, 0.0035, 0.0036, 0.0057, 0.0034, 0.0026, 0.0019, 0.0048, 0.0020,\n                      0.0030, 0.0030, 0.0027, 0.0020, 0.0029, 0.0034, 0.0023, 0.0038, 0.0039,\n                      0.0026, 0.0033, 0.0034, 0.0025, 0.0020, 0.0038, 0.0052, 0.0036, 0.0029,\n                      0.0030, 0.0028, 0.0040, 0.0031, 0.0027, 0.0044, 0.0034, 0.0028, 0.0018,\n                      0.0024]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n                min_val=tensor([-0.4174, -0.2774, -0.2662, -0.3008, -0.2457, -0.3541, -0.4323, -0.3439,\n                        -0.4808, -0.2698, -0.2948, -0.3142, -0.2688, -0.2980, -0.3030, -0.2788,\n                        -0.3891, -0.2102, -0.5084, -0.2636, -0.2369, -0.2937, -0.3255, -0.4442,\n                        -0.3514, -0.3198, -0.3536, -0.4356, -0.4531, -0.4598, -0.4849, -0.4406,\n                        -0.3270, -0.2418, -0.6109, -0.2568, -0.3058, -0.3795, -0.3421, -0.2162,\n                        -0.2857, -0.4115, -0.2902, -0.4914, -0.4972, -0.2690, -0.4011, -0.4316,\n                        -0.2842, -0.2620, -0.4879, -0.6620, -0.3491, -0.3359, -0.3824, -0.3058,\n                        -0.3910, -0.3949, -0.3405, -0.4568, -0.3497, -0.3646, -0.2325, -0.3114]), max_val=tensor([0.3976, 0.3518, 0.3025, 0.4712, 0.3274, 0.3196, 0.3369, 0.3050, 0.5095,\n                        0.3833, 0.3782, 0.3821, 0.3983, 0.3018, 0.3811, 0.2405, 0.2862, 0.2625,\n                        0.3973, 0.3333, 0.2576, 0.3301, 0.3374, 0.4993, 0.3358, 0.3004, 0.4182,\n                        0.2788, 0.3798, 0.3380, 0.7236, 0.4224, 0.3216, 0.2286, 0.5653, 0.2321,\n                        0.3800, 0.3131, 0.3161, 0.2515, 0.3710, 0.4281, 0.2491, 0.4319, 0.4494,\n                        0.3256, 0.4231, 0.4045, 0.3133, 0.2475, 0.3732, 0.5474, 0.4616, 0.3702,\n                        0.3612, 0.3515, 0.5104, 0.3218, 0.3359, 0.5633, 0.4381, 0.3187, 0.2241,\n                        0.2723])\n              )\n            )\n          )\n        )\n      )\n      (10): Module(\n        (conv): Module(\n          (0): Module(\n            (0): ConvBn2d(\n              64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False\n              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n                fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.1481e-05, 7.5122e-04, 3.4353e-04, 2.9793e-04, 3.4605e-04, 3.8197e-04,\n                        5.9531e-04, 5.8245e-04, 5.1112e-04, 3.7799e-04, 4.2975e-04, 5.0517e-04,\n                        4.8523e-04, 4.6191e-04, 8.0825e-04, 4.9151e-04, 6.2039e-04, 4.7417e-04,\n                        4.8300e-04, 4.1763e-04, 3.9766e-04, 4.2057e-04, 4.6239e-04, 2.3100e-04,\n                        7.9658e-04, 6.1269e-04, 4.0188e-04, 3.9326e-04, 3.7954e-04, 7.2594e-04,\n                        6.1896e-04, 3.7930e-04, 5.5899e-04, 2.8609e-04, 6.8049e-04, 4.5388e-04,\n                        3.6893e-04, 5.8667e-04, 5.6265e-04, 5.4674e-04, 6.5046e-04, 4.4352e-04,\n                        4.8459e-04, 5.4181e-04, 4.7259e-04, 4.3826e-04, 5.0062e-04, 6.2475e-04,\n                        5.5697e-04, 5.0881e-04, 5.3149e-04, 2.6263e-04, 2.4620e-04, 8.5014e-04,\n                        5.3204e-04, 6.1510e-04, 4.3227e-04, 4.9496e-04, 6.3535e-04, 4.4218e-04,\n                        4.5240e-04, 4.8687e-04, 4.0545e-04, 3.4951e-04, 5.5012e-04, 5.5702e-04,\n                        3.1717e-04, 5.2218e-04, 4.8964e-04, 6.4984e-04, 3.6284e-04, 5.5622e-04,\n                        7.7546e-04, 4.5396e-04, 4.4514e-04, 4.2621e-04, 5.4607e-04, 4.8688e-04,\n                        5.6056e-04, 3.8327e-04, 6.4719e-04, 5.0022e-04, 4.4783e-04, 8.1109e-04,\n                        2.6960e-04, 3.3241e-04, 4.8899e-04, 3.0014e-04, 4.4620e-04, 4.8707e-04,\n                        3.8636e-04, 2.1838e-04, 4.5583e-04, 4.2133e-04, 4.0613e-04, 3.8337e-04,\n                        2.1824e-04, 4.1686e-04, 3.1508e-04, 4.6816e-04, 5.2004e-04, 6.5211e-04,\n                        3.9830e-04, 3.6434e-04, 4.7768e-04, 4.3410e-04, 4.4519e-04, 3.7999e-04,\n                        7.2664e-04, 5.0064e-04, 3.4465e-04, 3.8631e-04, 5.8851e-04, 3.8666e-04,\n                        4.3369e-04, 4.1596e-04, 4.3843e-04, 3.5284e-04, 6.6460e-04, 7.3784e-04,\n                        4.6069e-04, 3.9869e-04, 3.5820e-04, 2.8022e-04, 5.3375e-04, 3.6590e-04,\n                        7.2989e-04, 4.3224e-04, 6.4088e-04, 3.6496e-04, 6.8162e-04, 4.7030e-04,\n                        4.3032e-04, 4.9072e-04, 6.6694e-04, 5.3706e-04, 4.3513e-04, 3.4773e-04,\n                        3.7111e-04, 4.5786e-04, 3.3066e-04, 5.2131e-04, 5.1389e-04, 3.7857e-04,\n                        3.2801e-04, 7.8004e-04, 3.4517e-04, 4.2475e-04, 3.6917e-04, 4.7415e-06,\n                        6.2808e-04, 4.1299e-04, 3.7948e-04, 4.1454e-04, 4.8131e-04, 5.5094e-04,\n                        4.1345e-05, 5.5464e-04, 4.6067e-04, 3.9770e-04, 4.4915e-04, 5.8926e-04,\n                        3.0425e-04, 3.6785e-04, 4.4734e-04, 3.8563e-04, 4.9392e-04, 5.6362e-04,\n                        3.8133e-04, 4.9488e-04, 3.3270e-04, 6.2471e-04, 3.5015e-04, 3.9696e-04,\n                        8.2046e-04, 4.8109e-04, 3.8684e-04, 2.1074e-04, 2.7642e-04, 4.9479e-04,\n                        3.8109e-04, 5.4069e-04, 3.7425e-05, 3.9007e-04, 5.2651e-04, 5.5789e-04,\n                        4.5920e-04, 7.0936e-04, 4.2280e-04, 5.4427e-04, 6.8237e-04, 8.0905e-04,\n                        6.0240e-04, 2.7441e-04, 4.8811e-04, 4.0920e-04, 5.0064e-04, 4.5921e-04,\n                        6.0225e-04, 5.1472e-04, 1.9426e-04, 4.3978e-04, 7.2366e-04, 2.4532e-04,\n                        5.5895e-04, 5.6173e-04, 3.2612e-04, 3.5152e-04, 3.2141e-04, 3.9291e-04,\n                        5.6786e-04, 5.8523e-04, 4.8370e-04, 4.2353e-04, 4.7175e-04, 6.2475e-04,\n                        3.7776e-04, 4.4875e-04, 4.7736e-04, 4.2425e-04, 5.4369e-04, 4.3213e-04,\n                        4.8959e-04, 6.4958e-04, 6.7365e-04, 6.2927e-04, 3.6212e-04, 1.0035e-03,\n                        4.3658e-04, 4.5925e-04, 3.6040e-04, 3.1223e-04, 5.3147e-04, 4.5976e-04,\n                        3.8478e-04, 3.8618e-04, 4.9093e-04, 4.5690e-04, 7.2363e-04, 2.7267e-04,\n                        3.1999e-04, 4.1729e-04, 5.6691e-04, 4.2996e-04, 3.6609e-04, 4.3283e-04,\n                        3.7831e-04, 5.1606e-04, 4.2741e-04, 6.4366e-04, 4.7174e-04, 4.6085e-04,\n                        6.7555e-04, 5.9743e-04, 3.8513e-04, 5.0759e-04, 3.2604e-04, 6.0538e-04,\n                        3.9760e-04, 5.3230e-04, 3.7116e-04, 2.1094e-04, 5.2311e-04, 4.2387e-04,\n                        4.1742e-04, 6.4347e-04, 7.5534e-04, 1.6190e-04, 3.0475e-04, 5.4484e-04,\n                        3.7731e-04, 2.2678e-05, 3.1004e-04, 5.2557e-04, 5.8013e-04, 3.7012e-04,\n                        4.4359e-04, 4.8164e-04, 5.3191e-04, 4.8256e-04, 4.9676e-04, 4.6444e-04,\n                        5.2856e-04, 2.8028e-04, 3.2528e-04, 4.0501e-04, 2.9442e-04, 3.7213e-04,\n                        4.6915e-04, 3.8436e-04, 4.1947e-04, 4.8031e-04, 4.5626e-04, 3.0328e-04,\n                        5.7611e-04, 6.7298e-04, 3.4981e-04, 5.0022e-04, 4.3140e-04, 5.0080e-04,\n                        4.9115e-04, 3.0810e-04, 4.0198e-04, 4.2439e-04, 3.5971e-04, 3.8938e-04,\n                        5.3669e-04, 5.1783e-04, 4.4253e-04, 3.8774e-04, 5.0209e-04, 6.2057e-04,\n                        4.5902e-04, 5.0744e-04, 4.0205e-04, 5.9006e-04, 5.5559e-04, 5.2553e-04,\n                        4.0653e-04, 3.0064e-04, 6.0100e-04, 4.3881e-04, 5.5680e-04, 5.0225e-04,\n                        3.8071e-04, 4.6789e-04, 5.2998e-04, 4.9930e-04, 4.0208e-04, 3.7320e-04,\n                        4.6280e-04, 4.4619e-04, 5.8938e-04, 4.7395e-04, 3.3011e-04, 3.9479e-04,\n                        3.9934e-04, 3.3357e-04, 4.8659e-04, 3.2916e-04, 5.0083e-04, 3.9408e-04,\n                        5.0976e-04, 5.0177e-04, 1.9955e-04, 4.2568e-04, 6.2383e-04, 5.4183e-04,\n                        3.2867e-04, 5.7195e-04, 5.0637e-04, 4.2691e-04, 3.0403e-04, 2.4833e-04,\n                        5.8515e-04, 4.5975e-04, 3.4941e-04, 5.1246e-04, 3.2781e-04, 4.6385e-04,\n                        3.9636e-04, 2.9945e-04, 4.7516e-04, 3.8113e-04, 5.6152e-04, 6.3022e-04,\n                        7.0535e-04, 1.5904e-04, 4.5299e-04, 3.6698e-04, 5.2192e-04, 7.3904e-04,\n                        4.0158e-04, 6.6159e-04, 5.0879e-04, 4.7652e-04, 2.7165e-04, 4.0557e-04,\n                        4.4182e-04, 3.4859e-04, 3.5313e-04, 2.1820e-04, 4.4117e-04, 6.0235e-04]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                       dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n                (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n                  min_val=tensor([-0.0015, -0.0645, -0.0338, -0.0381, -0.0397, -0.0489, -0.0762, -0.0592,\n                          -0.0435, -0.0484, -0.0550, -0.0592, -0.0621, -0.0460, -0.0692, -0.0629,\n                          -0.0794, -0.0607, -0.0419, -0.0535, -0.0452, -0.0534, -0.0526, -0.0256,\n                          -0.0706, -0.0784, -0.0501, -0.0413, -0.0486, -0.0356, -0.0754, -0.0465,\n                          -0.0716, -0.0366, -0.0871, -0.0448, -0.0472, -0.0615, -0.0513, -0.0674,\n                          -0.0833, -0.0568, -0.0620, -0.0694, -0.0441, -0.0288, -0.0569, -0.0800,\n                          -0.0713, -0.0632, -0.0680, -0.0336, -0.0315, -0.0725, -0.0678, -0.0787,\n                          -0.0488, -0.0634, -0.0573, -0.0461, -0.0579, -0.0600, -0.0447, -0.0392,\n                          -0.0704, -0.0597, -0.0279, -0.0668, -0.0627, -0.0475, -0.0270, -0.0504,\n                          -0.0723, -0.0581, -0.0570, -0.0546, -0.0699, -0.0623, -0.0540, -0.0491,\n                          -0.0674, -0.0530, -0.0526, -0.1038, -0.0321, -0.0415, -0.0626, -0.0384,\n                          -0.0563, -0.0557, -0.0495, -0.0209, -0.0434, -0.0416, -0.0382, -0.0491,\n                          -0.0279, -0.0450, -0.0403, -0.0599, -0.0620, -0.0835, -0.0490, -0.0466,\n                          -0.0540, -0.0330, -0.0570, -0.0486, -0.0724, -0.0536, -0.0441, -0.0478,\n                          -0.0496, -0.0495, -0.0555, -0.0385, -0.0542, -0.0362, -0.0658, -0.0944,\n                          -0.0590, -0.0510, -0.0458, -0.0320, -0.0683, -0.0376, -0.0934, -0.0553,\n                          -0.0594, -0.0322, -0.0872, -0.0602, -0.0551, -0.0628, -0.0854, -0.0667,\n                          -0.0557, -0.0421, -0.0398, -0.0586, -0.0423, -0.0667, -0.0331, -0.0485,\n                          -0.0372, -0.0641, -0.0332, -0.0544, -0.0473, -0.0006, -0.0648, -0.0425,\n                          -0.0434, -0.0531, -0.0441, -0.0535, -0.0041, -0.0505, -0.0409, -0.0509,\n                          -0.0575, -0.0605, -0.0389, -0.0421, -0.0573, -0.0494, -0.0632, -0.0581,\n                          -0.0488, -0.0577, -0.0313, -0.0529, -0.0325, -0.0393, -0.0446, -0.0511,\n                          -0.0480, -0.0228, -0.0253, -0.0422, -0.0317, -0.0692, -0.0048, -0.0482,\n                          -0.0458, -0.0523, -0.0588, -0.0908, -0.0382, -0.0442, -0.0873, -0.1036,\n                          -0.0771, -0.0351, -0.0625, -0.0467, -0.0641, -0.0588, -0.0615, -0.0659,\n                          -0.0249, -0.0545, -0.0545, -0.0273, -0.0644, -0.0719, -0.0417, -0.0450,\n                          -0.0411, -0.0503, -0.0416, -0.0553, -0.0619, -0.0504, -0.0604, -0.0800,\n                          -0.0478, -0.0324, -0.0489, -0.0543, -0.0696, -0.0356, -0.0412, -0.0338,\n                          -0.0503, -0.0442, -0.0464, -0.0893, -0.0559, -0.0588, -0.0461, -0.0336,\n                          -0.0680, -0.0549, -0.0420, -0.0394, -0.0628, -0.0400, -0.0519, -0.0317,\n                          -0.0396, -0.0333, -0.0726, -0.0452, -0.0469, -0.0469, -0.0383, -0.0661,\n                          -0.0530, -0.0611, -0.0589, -0.0590, -0.0864, -0.0765, -0.0300, -0.0637,\n                          -0.0281, -0.0775, -0.0509, -0.0681, -0.0421, -0.0270, -0.0670, -0.0543,\n                          -0.0528, -0.0806, -0.0402, -0.0207, -0.0390, -0.0671, -0.0483, -0.0029,\n                          -0.0397, -0.0467, -0.0736, -0.0474, -0.0568, -0.0616, -0.0681, -0.0618,\n                          -0.0454, -0.0588, -0.0677, -0.0359, -0.0399, -0.0512, -0.0274, -0.0476,\n                          -0.0601, -0.0389, -0.0537, -0.0508, -0.0584, -0.0348, -0.0440, -0.0846,\n                          -0.0444, -0.0624, -0.0495, -0.0641, -0.0629, -0.0394, -0.0515, -0.0543,\n                          -0.0256, -0.0478, -0.0393, -0.0554, -0.0566, -0.0453, -0.0643, -0.0601,\n                          -0.0588, -0.0530, -0.0407, -0.0755, -0.0711, -0.0608, -0.0520, -0.0385,\n                          -0.0669, -0.0485, -0.0531, -0.0524, -0.0487, -0.0432, -0.0452, -0.0420,\n                          -0.0515, -0.0448, -0.0592, -0.0571, -0.0754, -0.0504, -0.0423, -0.0430,\n                          -0.0451, -0.0427, -0.0558, -0.0421, -0.0417, -0.0504, -0.0550, -0.0375,\n                          -0.0255, -0.0525, -0.0799, -0.0623, -0.0421, -0.0732, -0.0648, -0.0425,\n                          -0.0336, -0.0318, -0.0749, -0.0588, -0.0342, -0.0465, -0.0420, -0.0469,\n                          -0.0433, -0.0383, -0.0608, -0.0473, -0.0719, -0.0807, -0.0903, -0.0203,\n                          -0.0580, -0.0425, -0.0586, -0.0946, -0.0464, -0.0415, -0.0651, -0.0610,\n                          -0.0347, -0.0461, -0.0566, -0.0446, -0.0404, -0.0279, -0.0448, -0.0771]), max_val=tensor([0.0014, 0.0954, 0.0436, 0.0375, 0.0439, 0.0421, 0.0488, 0.0740, 0.0649,\n                          0.0313, 0.0326, 0.0642, 0.0347, 0.0587, 0.1026, 0.0461, 0.0673, 0.0364,\n                          0.0613, 0.0486, 0.0505, 0.0534, 0.0587, 0.0293, 0.1012, 0.0293, 0.0510,\n                          0.0499, 0.0396, 0.0922, 0.0786, 0.0482, 0.0442, 0.0313, 0.0542, 0.0576,\n                          0.0440, 0.0745, 0.0715, 0.0694, 0.0502, 0.0450, 0.0608, 0.0627, 0.0600,\n                          0.0557, 0.0636, 0.0754, 0.0606, 0.0646, 0.0575, 0.0307, 0.0143, 0.1080,\n                          0.0676, 0.0447, 0.0549, 0.0499, 0.0807, 0.0562, 0.0500, 0.0618, 0.0515,\n                          0.0444, 0.0503, 0.0707, 0.0403, 0.0352, 0.0474, 0.0825, 0.0461, 0.0706,\n                          0.0985, 0.0410, 0.0525, 0.0460, 0.0366, 0.0516, 0.0712, 0.0424, 0.0822,\n                          0.0635, 0.0569, 0.0724, 0.0342, 0.0422, 0.0534, 0.0353, 0.0567, 0.0619,\n                          0.0460, 0.0277, 0.0579, 0.0535, 0.0516, 0.0459, 0.0275, 0.0529, 0.0375,\n                          0.0552, 0.0660, 0.0642, 0.0506, 0.0414, 0.0607, 0.0551, 0.0563, 0.0334,\n                          0.0923, 0.0636, 0.0387, 0.0491, 0.0747, 0.0449, 0.0465, 0.0528, 0.0557,\n                          0.0448, 0.0844, 0.0778, 0.0392, 0.0490, 0.0451, 0.0356, 0.0554, 0.0465,\n                          0.0556, 0.0439, 0.0814, 0.0463, 0.0615, 0.0596, 0.0436, 0.0546, 0.0506,\n                          0.0682, 0.0486, 0.0442, 0.0471, 0.0458, 0.0304, 0.0510, 0.0653, 0.0446,\n                          0.0417, 0.0991, 0.0438, 0.0389, 0.0422, 0.0004, 0.0798, 0.0525, 0.0482,\n                          0.0480, 0.0611, 0.0700, 0.0053, 0.0704, 0.0585, 0.0477, 0.0407, 0.0748,\n                          0.0230, 0.0467, 0.0460, 0.0403, 0.0503, 0.0716, 0.0428, 0.0628, 0.0423,\n                          0.0793, 0.0445, 0.0504, 0.1042, 0.0611, 0.0491, 0.0268, 0.0351, 0.0628,\n                          0.0484, 0.0588, 0.0043, 0.0495, 0.0669, 0.0709, 0.0574, 0.0733, 0.0537,\n                          0.0691, 0.0635, 0.0610, 0.0498, 0.0338, 0.0580, 0.0520, 0.0608, 0.0439,\n                          0.0765, 0.0562, 0.0181, 0.0559, 0.0919, 0.0312, 0.0710, 0.0590, 0.0338,\n                          0.0435, 0.0393, 0.0473, 0.0721, 0.0743, 0.0484, 0.0538, 0.0516, 0.0687,\n                          0.0480, 0.0570, 0.0606, 0.0515, 0.0602, 0.0549, 0.0622, 0.0825, 0.0856,\n                          0.0799, 0.0423, 0.1274, 0.0547, 0.0455, 0.0403, 0.0397, 0.0522, 0.0584,\n                          0.0489, 0.0490, 0.0503, 0.0580, 0.0919, 0.0346, 0.0406, 0.0530, 0.0561,\n                          0.0546, 0.0434, 0.0550, 0.0480, 0.0558, 0.0543, 0.0817, 0.0599, 0.0443,\n                          0.0858, 0.0518, 0.0489, 0.0645, 0.0414, 0.0597, 0.0497, 0.0462, 0.0471,\n                          0.0250, 0.0592, 0.0530, 0.0530, 0.0817, 0.0959, 0.0178, 0.0387, 0.0692,\n                          0.0357, 0.0025, 0.0319, 0.0667, 0.0737, 0.0452, 0.0478, 0.0518, 0.0597,\n                          0.0498, 0.0631, 0.0590, 0.0421, 0.0324, 0.0413, 0.0514, 0.0374, 0.0369,\n                          0.0592, 0.0488, 0.0402, 0.0610, 0.0448, 0.0385, 0.0732, 0.0855, 0.0444,\n                          0.0635, 0.0548, 0.0592, 0.0616, 0.0361, 0.0266, 0.0475, 0.0457, 0.0495,\n                          0.0682, 0.0658, 0.0529, 0.0492, 0.0493, 0.0788, 0.0471, 0.0644, 0.0511,\n                          0.0548, 0.0571, 0.0667, 0.0486, 0.0309, 0.0763, 0.0557, 0.0707, 0.0638,\n                          0.0317, 0.0594, 0.0673, 0.0634, 0.0448, 0.0474, 0.0441, 0.0472, 0.0696,\n                          0.0602, 0.0352, 0.0501, 0.0507, 0.0310, 0.0618, 0.0317, 0.0636, 0.0428,\n                          0.0647, 0.0637, 0.0253, 0.0541, 0.0654, 0.0688, 0.0404, 0.0674, 0.0511,\n                          0.0542, 0.0386, 0.0309, 0.0550, 0.0369, 0.0444, 0.0651, 0.0367, 0.0589,\n                          0.0503, 0.0234, 0.0463, 0.0484, 0.0570, 0.0624, 0.0598, 0.0202, 0.0481,\n                          0.0466, 0.0663, 0.0811, 0.0510, 0.0840, 0.0619, 0.0587, 0.0345, 0.0515,\n                          0.0436, 0.0392, 0.0448, 0.0209, 0.0560, 0.0734])\n                )\n              )\n            )\n            (2): ReLU6(inplace=True)\n          )\n          (1): Module(\n            (0): ConvBn2d(\n              384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False\n              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n                fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0019, 0.0150, 0.0114, 0.0035, 0.0105, 0.0121, 0.0190, 0.0133, 0.0119,\n                        0.0132, 0.0111, 0.0132, 0.0186, 0.0058, 0.0013, 0.0139, 0.0104, 0.0171,\n                        0.0168, 0.0149, 0.0107, 0.0122, 0.0089, 0.0318, 0.0106, 0.0025, 0.0153,\n                        0.0117, 0.0116, 0.0030, 0.0015, 0.0180, 0.0102, 0.0354, 0.0086, 0.0024,\n                        0.0177, 0.0027, 0.0149, 0.0029, 0.0025, 0.0106, 0.0130, 0.0088, 0.0041,\n                        0.0109, 0.0118, 0.0064, 0.0078, 0.0145, 0.0131, 0.0175, 0.0187, 0.0049,\n                        0.0148, 0.0038, 0.0125, 0.0026, 0.0092, 0.0102, 0.0118, 0.0124, 0.0174,\n                        0.0092, 0.0139, 0.0175, 0.0129, 0.0027, 0.0192, 0.0112, 0.0143, 0.0112,\n                        0.0116, 0.0126, 0.0096, 0.0093, 0.0118, 0.0158, 0.0114, 0.0101, 0.0094,\n                        0.0126, 0.0137, 0.0112, 0.0105, 0.0144, 0.0122, 0.0104, 0.0184, 0.0088,\n                        0.0106, 0.0233, 0.0118, 0.0153, 0.0156, 0.0135, 0.0074, 0.0182, 0.0105,\n                        0.0024, 0.0073, 0.0107, 0.0246, 0.0140, 0.0032, 0.0190, 0.0029, 0.0266,\n                        0.0216, 0.0216, 0.0093, 0.0133, 0.0107, 0.0177, 0.0074, 0.0138, 0.0081,\n                        0.0135, 0.0086, 0.0145, 0.0157, 0.0037, 0.0057, 0.0196, 0.0104, 0.0176,\n                        0.0088, 0.0124, 0.0122, 0.0091, 0.0177, 0.0046, 0.0170, 0.0094, 0.0098,\n                        0.0027, 0.0046, 0.0190, 0.0124, 0.0127, 0.0034, 0.0160, 0.0115, 0.0064,\n                        0.0030, 0.0104, 0.0107, 0.0046, 0.0164, 0.0004, 0.0258, 0.0167, 0.0057,\n                        0.0062, 0.0114, 0.0108, 0.0086, 0.0181, 0.0117, 0.0099, 0.0133, 0.0195,\n                        0.0209, 0.0092, 0.0136, 0.0134, 0.0102, 0.0032, 0.0130, 0.0092, 0.0089,\n                        0.0072, 0.0090, 0.0161, 0.0068, 0.0145, 0.0113, 0.0181, 0.0111, 0.0120,\n                        0.0058, 0.0199, 0.0059, 0.0212, 0.0165, 0.0036, 0.0160, 0.0173, 0.0108,\n                        0.0110, 0.0043, 0.0028, 0.0039, 0.0168, 0.0048, 0.0020, 0.0138, 0.0152,\n                        0.0138, 0.0025, 0.0214, 0.0123, 0.0117, 0.0231, 0.0081, 0.0192, 0.0097,\n                        0.0069, 0.0150, 0.0108, 0.0130, 0.0349, 0.0022, 0.0177, 0.0096, 0.0108,\n                        0.0165, 0.0201, 0.0148, 0.0055, 0.0131, 0.0085, 0.0122, 0.0127, 0.0117,\n                        0.0030, 0.0073, 0.0031, 0.0028, 0.0089, 0.0159, 0.0373, 0.0148, 0.0088,\n                        0.0260, 0.0231, 0.0145, 0.0261, 0.0163, 0.0029, 0.0046, 0.0167, 0.0112,\n                        0.0206, 0.0145, 0.0102, 0.0230, 0.0207, 0.0116, 0.0111, 0.0061, 0.0122,\n                        0.0106, 0.0060, 0.0030, 0.0071, 0.0227, 0.0107, 0.0163, 0.0103, 0.0103,\n                        0.0217, 0.0118, 0.0023, 0.0114, 0.0020, 0.0096, 0.0259, 0.0130, 0.0123,\n                        0.0082, 0.0064, 0.0133, 0.0089, 0.0070, 0.0124, 0.0112, 0.0138, 0.0098,\n                        0.0146, 0.0088, 0.0036, 0.0043, 0.0157, 0.0166, 0.0147, 0.0218, 0.0139,\n                        0.0037, 0.0089, 0.0161, 0.0120, 0.0156, 0.0295, 0.0087, 0.0232, 0.0267,\n                        0.0152, 0.0200, 0.0107, 0.0128, 0.0160, 0.0057, 0.0191, 0.0177, 0.0078,\n                        0.0019, 0.0166, 0.0159, 0.0177, 0.0057, 0.0130, 0.0207, 0.0142, 0.0118,\n                        0.0104, 0.0019, 0.0257, 0.0141, 0.0258, 0.0084, 0.0031, 0.0028, 0.0208,\n                        0.0157, 0.0141, 0.0100, 0.0072, 0.0108, 0.0157, 0.0082, 0.0092, 0.0107,\n                        0.0080, 0.0027, 0.0087, 0.0165, 0.0037, 0.0120, 0.0099, 0.0106, 0.0068,\n                        0.0202, 0.0148, 0.0224, 0.0023, 0.0090, 0.0157, 0.0216, 0.0028, 0.0110,\n                        0.0139, 0.0235, 0.0167, 0.0119, 0.0207, 0.0129, 0.0029, 0.0032, 0.0091,\n                        0.0123, 0.0204, 0.0027, 0.0155, 0.0047, 0.0160, 0.0025, 0.0270, 0.0131,\n                        0.0074, 0.0162, 0.0452, 0.0031, 0.0102, 0.0114, 0.0322, 0.0211, 0.0073,\n                        0.0123, 0.0044, 0.0124, 0.0252, 0.0106, 0.0097]), zero_point=tensor([   0,    0,    0,  127,    0,    0,    0,    0,    0,    0,    0,    0,\n                           0,    0,  127,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                           0,  127,    0,    0,    0,  127,  127,    0, -128,    0,    0,  127,\n                           0,  127,    0,  127,  127,    0,    0,    0,  127,    0,    0,    0,\n                        -128,    0,    0,    0,    0,    0,    0,  127,    0,  127,    0,    0,\n                           0,    0,    0,    0,    0,    0,    0,  127,    0,    0,    0,    0,\n                           0,    0,    0,    0,    0,    0,    0, -128,    0,    0,    0,    0,\n                           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                        -128,    0,    0,  127,    0,    0,    0,    0,  127,    0,  127,    0,\n                           0,    0,    0,    0,    0,    0, -128,    0,    0,    0,    0,    0,\n                           0,  127, -128,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                           0, -128,    0,  127, -128,    0,    0,    0,    0,    0,    0,    0,\n                         127,    0, -128,  127,    0,    0,    0,    0,    0,    0,    0,    0,\n                           0,    0,    0,    0,    0,    0,    0,    0,    0, -128,    0,  127,\n                           0,    0,    0, -128,    0,    0,    0,    0,    0,    0, -128,    0,\n                        -128,    0,    0,    0,    0, -128,    0,    0,    0,    0,  127,  127,\n                           0,    0,  127,  127,    0,    0,    0,  127,    0,    0,    0,    0,\n                           0,    0,    0, -128,    0,    0,    0,    0,  127,    0,    0,    0,\n                           0,    0,    0,    0,    0,    0,    0,    0,    0,  127,    0,    0,\n                         127,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,  127,\n                           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                           0,    0,  127,    0,    0,    0,    0, -128,    0,    0,    0,  127,\n                           0,  127,    0,    0,    0,    0,    0,    0,    0,    0, -128,    0,\n                           0,    0,    0,    0,    0, -128, -128,    0,    0,    0,    0,    0,\n                         127,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                           0,    0, -128,    0,    0, -128,  127,    0,    0,    0,    0,    0,\n                           0,    0,    0,    0,  127,    0,    0,    0,    0,  127,  127,    0,\n                           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,  127,    0,\n                           0,  127,    0,    0,    0,    0,    0,    0,    0,  127,    0,    0,\n                           0,  127,    0,    0,    0,    0,    0,    0,    0,  127,  127,    0,\n                           0,    0,  127,    0, -128,    0, -128,    0,    0,    0,    0,    0,\n                         127,    0,    0,    0,    0,    0,    0,  127,    0,    0,    0,    0],\n                       dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n                (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n                  min_val=tensor([-1.4409e-01, -3.0464e-01, -1.3103e+00, -9.0471e-01, -1.1893e+00,\n                          -1.5463e+00, -2.4302e+00, -4.6863e-02, -9.1635e-01, -1.3055e+00,\n                          -1.4233e+00, -1.5389e+00, -6.6464e-01, -7.3671e-01, -3.2921e-01,\n                          -2.7776e-01, -1.3322e+00, -1.8150e-01, -1.8769e+00, -1.9087e+00,\n                          -6.1400e-01, -1.4395e+00, -2.0736e-01, -4.5999e-02, -1.3549e+00,\n                          -6.4582e-01, -2.4625e-01, -1.4988e+00, -1.2229e+00, -7.5433e-01,\n                          -3.7375e-01, -7.7524e-01,  1.5764e-01, -3.1502e-01, -1.1013e+00,\n                          -6.1992e-01, -2.2669e+00, -6.9993e-01, -1.9023e+00, -7.3372e-01,\n                          -6.3734e-01, -7.4478e-01, -1.6694e+00, -1.1103e+00, -1.0435e+00,\n                          -1.4521e-02, -8.5646e-01, -7.3668e-01,  1.4206e-01, -1.8611e+00,\n                          -1.6748e+00, -7.6185e-01, -2.3880e+00, -6.5703e-02, -1.4445e-01,\n                          -9.7434e-01, -8.3089e-01, -6.7418e-01, -2.6711e-01, -6.0285e-01,\n                          -1.5155e+00, -1.5919e+00, -7.3182e-01, -1.1806e+00, -8.1923e-01,\n                          -2.2371e+00, -1.6538e+00, -6.8775e-01, -1.5872e-01, -1.2938e+00,\n                          -5.9051e-01, -1.2150e+00, -1.4194e+00, -5.6978e-01, -1.2280e+00,\n                          -8.2484e-01, -8.4156e-01, -1.3664e+00, -9.6193e-01,  1.4833e-02,\n                          -7.0130e-01, -1.6101e+00, -1.7554e+00, -7.5604e-01, -4.6887e-01,\n                          -8.5620e-01, -9.9449e-01, -1.3309e+00, -1.2178e+00, -6.5294e-01,\n                          -1.0531e+00, -2.9882e+00, -1.6014e-01, -7.1715e-01, -2.2565e-01,\n                          -9.7402e-01,  6.3791e-01, -1.4560e+00, -7.6985e-01, -6.1603e-01,\n                          -9.3983e-01, -1.3739e+00, -1.9340e-01, -7.6006e-01, -8.0869e-01,\n                          -2.6723e-01, -7.3607e-01, -8.1816e-01, -2.7639e+00, -2.7644e+00,\n                          -1.1735e+00, -3.2534e-01, -1.3657e+00, -8.6720e-01,  7.7269e-02,\n                          -1.6346e+00, -1.0368e+00, -1.3457e+00, -1.1008e+00, -1.5659e-02,\n                          -6.1211e-01, -9.3574e-01,  4.3212e-02, -1.1108e+00, -1.3314e+00,\n                          -2.2465e+00, -5.0139e-03, -1.1367e+00, -1.5593e+00, -6.9400e-01,\n                          -6.3049e-01, -5.9460e-01, -4.5945e-01,  8.7415e-02, -9.3602e-01,\n                          -6.9903e-01,  1.3453e-01, -1.7514e+00, -1.2883e+00, -1.5472e+00,\n                          -4.2868e-01, -4.2477e-01, -1.3229e+00, -8.2281e-01, -7.6330e-01,\n                          -1.0939e-01,  8.9528e-02, -1.1748e+00, -1.2048e+00, -4.0863e-02,\n                          -3.3052e+00, -5.1829e-01, -6.4600e-01, -7.9206e-01, -1.4641e+00,\n                          -1.3813e+00, -3.3609e-01, -3.6094e-01, -1.5014e+00, -1.2704e+00,\n                          -1.1718e+00, -2.4955e+00, -1.4957e+00, -1.1791e+00, -1.2589e+00,\n                           2.1384e-01, -1.0720e+00, -8.1592e-01, -8.2498e-01, -6.7217e-01,\n                          -8.4847e-01,  1.9459e-02, -1.1575e+00, -9.8874e-02, -8.6664e-01,\n                          -6.0107e-01, -1.1496e+00, -1.7864e+00,  2.9542e-01, -1.1112e+00,\n                           1.3795e-02, -4.5997e-01, -1.2602e-01, -6.8787e-01, -1.6177e+00,\n                           4.1881e-01, -7.0823e-03, -6.8148e-01, -1.3838e+00, -1.3021e-01,\n                          -1.0838e+00, -7.0826e-01, -4.9953e-01, -1.5500e+00, -1.2190e+00,\n                          -5.2234e-01, -1.3478e+00, -1.9497e+00, -1.7717e+00, -6.2542e-01,\n                          -2.7456e+00, -1.5783e+00, -1.2590e+00, -1.2032e-01, -1.0362e+00,\n                          -1.3487e+00, -1.2372e+00,  4.8005e-02, -1.2583e+00, -1.0698e+00,\n                          -1.3849e+00, -1.7037e-01, -5.6051e-01, -5.0528e-01, -6.2776e-01,\n                          -1.4994e-01, -6.0964e-01, -5.0322e-01, -1.6545e+00, -7.0459e-01,\n                          -1.6743e+00, -1.0921e+00, -1.5625e+00, -1.6277e+00, -1.0267e+00,\n                          -7.5575e-01, -9.3793e-01, -3.8987e-01, -7.2206e-01, -3.7174e-01,\n                          -3.6183e-01, -1.2864e-01, -8.5001e-01, -1.1280e+00, -2.6900e-01,\n                          -2.9540e+00, -1.0769e+00, -1.6701e+00, -2.9955e-01, -7.5074e-01,\n                          -5.8399e-01, -1.0199e+00, -1.2087e+00, -1.2331e+00, -1.4925e+00,\n                          -9.6709e-01, -1.2745e-01, -2.6448e+00, -1.4804e+00, -7.8247e-01,\n                          -7.8290e-01, -1.5578e+00, -1.2148e+00, -7.7241e-01, -7.5330e-01,\n                          -9.0545e-01, -1.5790e+00, -1.9173e-01, -7.9138e-01,  3.9724e-02,\n                          -1.3201e+00, -1.6217e+00, -1.1387e+00, -5.9598e-01, -9.6834e-01,\n                          -5.1770e-01, -1.2318e+00, -1.2421e+00, -1.6614e+00, -1.4095e+00,\n                          -4.3085e-01, -1.0516e-01, -5.8422e-01, -1.1214e+00,  5.5665e-03,\n                          -9.0661e-01, -1.2844e+00, -1.7679e+00, -9.3048e-01, -1.2680e+00,\n                          -1.1207e+00,  1.8853e-01,  8.0708e-03, -1.6554e+00, -7.5077e-01,\n                          -7.6860e-01, -4.3918e-01, -1.3714e+00, -9.3884e-01, -1.1410e+00,\n                          -2.0602e+00, -9.0331e-01, -1.0511e+00, -2.0619e+00, -1.1199e+00,\n                          -5.7879e-01, -3.4187e+00, -1.9499e+00, -2.6790e-01, -1.3697e+00,\n                          -1.6356e+00, -1.0756e+00,  8.1065e-02, -4.3067e-02, -1.2196e+00,\n                           1.2601e-02, -4.7946e-01, -1.2560e+00, -8.1615e-01, -2.5313e-01,\n                          -7.3443e-01, -1.6686e+00, -1.5279e-01, -1.0300e+00, -1.0508e-01,\n                          -5.1457e-01, -4.7750e-01, -3.2895e+00, -9.7746e-01, -1.3187e+00,\n                          -9.5390e-01, -7.9724e-01, -7.0171e-01, -5.8594e-01, -5.4204e-01,\n                          -8.4636e-01, -1.0121e+00, -5.7958e-01, -9.6172e-01, -1.1531e+00,\n                          -1.0538e+00, -1.1748e+00, -1.2555e+00, -6.0334e-01, -6.9952e-01,\n                          -7.7503e-01, -1.6734e+00, -9.5323e-01, -9.8742e-01, -1.1552e-03,\n                          -8.5880e-01, -8.7428e-01, -2.5893e+00, -4.7256e-01, -2.8692e+00,\n                          -5.9307e-01, -1.0081e+00, -2.0070e+00, -1.6699e+00, -7.0176e-01,\n                          -1.3821e+00, -4.4601e-01, -1.5509e+00, -2.1389e+00, -1.5320e-01,\n                          -1.4859e+00, -1.6563e+00, -7.4570e-01, -8.1200e-01, -1.1642e+00,\n                          -9.8459e-01, -1.2967e-01, -6.7685e-01, -1.1474e+00,  6.1955e-02,\n                          -7.8370e-01,  4.3966e-02, -3.4538e+00, -1.6710e+00, -9.4197e-01,\n                          -2.0678e+00, -1.4098e+00, -7.8292e-01, -2.8512e-01, -1.0128e+00,\n                          -4.1255e+00, -1.3145e+00, -9.3391e-01, -1.5745e+00, -1.1275e+00,\n                          -1.2471e-01, -2.1753e+00, -7.2889e-01, -8.9157e-01]), max_val=tensor([ 2.3870e-01,  1.9096e+00,  1.4525e+00, -7.2010e-02,  1.3359e+00,\n                           1.2007e+00,  1.6456e+00,  1.6912e+00,  1.5130e+00,  1.6802e+00,\n                           4.9196e-02,  1.6726e+00,  2.3620e+00,  5.5183e-02, -1.7548e-01,\n                           1.7646e+00,  1.0870e+00,  2.1680e+00,  2.1314e+00,  1.5555e+00,\n                           1.3585e+00,  1.5495e+00,  1.1311e+00,  4.0436e+00,  1.1085e+00,\n                          -8.8375e-03,  1.9397e+00,  6.6410e-01,  1.4773e+00, -8.2852e-03,\n                          -1.5486e-01,  2.2891e+00,  2.6106e+00,  4.4972e+00,  9.9036e-01,\n                          -9.9126e-02,  7.6314e-01, -1.7286e-01,  1.0315e+00, -2.7752e-01,\n                          -1.2921e-01,  1.3444e+00,  1.2426e+00,  1.1227e+00, -7.3158e-02,\n                           1.3874e+00,  1.5004e+00,  8.1408e-01,  2.0016e+00,  1.4977e+00,\n                           1.4338e+00,  2.2190e+00,  1.8446e-01,  6.2312e-01,  1.8775e+00,\n                          -1.5422e-01,  1.5893e+00, -1.9115e-01,  1.1681e+00,  1.2976e+00,\n                           1.2355e+00,  4.9314e-01,  2.2148e+00,  1.1090e+00,  1.7677e+00,\n                           1.3744e+00,  1.1758e+00, -2.7290e-01,  2.4386e+00,  1.4165e+00,\n                           1.8161e+00,  1.4272e+00,  1.4709e+00,  1.6024e+00,  9.1536e-01,\n                           1.1854e+00,  1.5010e+00,  2.0081e+00,  1.4454e+00,  2.5786e+00,\n                           1.1924e+00,  1.4692e+00,  1.0871e+00,  1.4248e+00,  1.3370e+00,\n                           1.8343e+00,  1.5547e+00,  1.0127e+00,  2.3421e+00,  1.1187e+00,\n                           1.3447e+00,  2.1296e+00,  1.5043e+00,  1.9452e+00,  1.9834e+00,\n                           1.7115e+00,  1.8801e+00,  2.3088e+00,  1.3394e+00, -1.8223e-01,\n                           6.9406e-01,  1.2273e+00,  3.1271e+00,  1.7722e+00, -7.5450e-02,\n                           2.4136e+00, -1.5421e-01,  3.3739e+00,  1.5005e+00,  1.2103e+00,\n                           1.1772e+00,  1.6923e+00,  1.2400e+00,  2.2508e+00,  1.8806e+00,\n                           1.7503e+00,  1.0094e+00,  1.7185e+00,  1.0067e+00,  1.8472e+00,\n                           1.9926e+00, -5.9932e-03,  1.4574e+00,  2.4875e+00,  1.2447e+00,\n                           1.5753e+00,  1.1205e+00,  1.5714e+00,  1.0925e+00,  1.1520e+00,\n                           2.2456e+00,  2.4953e-01,  2.1614e+00,  2.3961e+00,  1.2493e+00,\n                          -3.7096e-02,  1.1813e+00,  2.4100e+00,  1.5709e+00,  1.6097e+00,\n                           4.2675e-01,  2.0365e+00,  1.4571e+00,  2.9359e-01, -1.7271e-01,\n                           1.3200e+00,  2.7375e+00, -1.2235e-01,  2.0819e+00,  5.1188e-02,\n                           2.4981e+00,  2.1189e+00,  7.2616e-01,  1.5877e-01,  1.3240e+00,\n                           1.5250e-01,  1.0869e+00,  2.2990e+00,  1.3499e+00,  1.0945e+00,\n                           1.6954e+00,  1.3653e+00,  2.6540e+00,  7.9910e-01,  1.7243e+00,\n                           3.4103e+00,  1.2978e+00, -1.4577e-01,  1.6563e+00,  1.1627e+00,\n                           1.1271e+00,  1.8246e+00,  1.7647e-01,  2.0426e+00,  3.7392e-02,\n                           1.8400e+00,  1.4408e+00,  2.2949e+00,  2.8312e+00,  1.5200e+00,\n                           1.4761e+00,  2.5332e+00,  7.5155e-01,  2.6968e+00,  2.0968e+00,\n                           9.2776e-01,  2.0322e+00,  2.1964e+00,  2.6601e-01,  1.3966e+00,\n                          -1.0988e-01, -2.0919e-02,  2.3311e-01,  2.1388e+00, -1.1275e-01,\n                          -9.0052e-03,  1.7488e+00,  1.1588e+00,  1.1360e+00, -1.9741e-01,\n                           2.6478e+00,  1.5684e+00,  1.4822e+00,  2.9328e+00,  1.0329e+00,\n                           2.4395e+00,  5.0167e-01,  1.7576e+00,  1.9026e+00,  1.3751e+00,\n                           1.6536e+00,  4.4325e+00, -8.1528e-03,  2.2454e+00,  1.2153e+00,\n                           1.3754e+00,  2.1004e+00,  2.5581e+00,  1.8779e+00,  2.2313e-01,\n                           7.8104e-01,  4.5390e-01,  1.0692e+00,  1.5220e+00,  1.4867e+00,\n                          -2.5616e-01,  4.2170e-02,  3.9684e-01, -2.1962e-01,  1.1281e+00,\n                           2.0205e+00,  4.7351e+00,  1.8858e+00,  8.7439e-01,  3.3005e+00,\n                           1.3457e+00,  1.8458e+00,  3.3158e+00,  2.0676e+00, -7.6892e-03,\n                           1.2886e-03,  2.1204e+00,  1.4246e+00,  2.6099e+00,  1.8374e+00,\n                           1.2894e+00,  2.9263e+00,  8.5388e-01,  1.4384e+00,  1.4087e+00,\n                           8.3860e-02,  1.3789e+00,  1.3401e+00,  7.4002e-01, -3.9673e-01,\n                           8.6526e-01,  2.8891e+00,  1.3534e+00,  2.0734e+00,  2.6352e+00,\n                           2.9707e-01,  2.7541e+00,  1.5033e+00, -2.0962e-01,  1.4487e+00,\n                          -1.9070e-01,  1.1057e+00,  3.2930e+00,  1.3881e+00,  1.5641e+00,\n                           1.0363e+00,  8.1821e-01,  1.6948e+00,  1.1264e+00,  1.7765e+00,\n                           1.5729e+00,  1.4284e+00,  1.5914e+00,  1.2408e+00,  1.8536e+00,\n                           1.5494e-01,  9.1981e-01,  1.0909e+00,  1.9945e+00,  2.1040e+00,\n                           1.8644e+00,  2.7746e+00,  1.7671e+00, -1.1444e-01,  1.0362e+00,\n                           8.2741e-01,  1.5249e+00,  1.9779e+00,  3.7440e+00,  1.0246e+00,\n                           2.9428e+00,  8.8732e-01,  1.5696e+00,  2.5437e+00,  1.0761e+00,\n                           7.1206e-01,  2.0348e+00,  1.4562e+00,  2.4214e+00,  2.2422e+00,\n                           1.9923e+00, -8.2553e-02,  2.1023e+00,  2.0177e+00,  2.2506e+00,\n                           2.2812e-01,  1.6323e+00,  2.6251e+00,  1.8090e+00,  1.4928e+00,\n                           1.3235e+00, -4.1027e-02,  7.1869e-01,  1.7940e+00,  3.2761e+00,\n                           1.0623e+00, -2.1722e-02, -2.5631e-01,  2.6435e+00,  1.9936e+00,\n                           1.7909e+00,  1.2663e+00,  9.0951e-01,  1.3673e+00,  1.9880e+00,\n                           1.0018e+00,  3.3747e-01,  1.3627e+00,  1.0125e+00, -8.7565e-02,\n                           1.1085e+00,  2.1013e+00, -1.5531e-01,  1.5264e+00,  1.2635e+00,\n                           1.3418e+00,  7.1057e-02,  1.3238e+00,  1.8737e+00,  1.8869e+00,\n                          -1.9319e-01,  1.1409e+00,  1.5383e+00,  2.7445e+00, -3.2741e-01,\n                           1.3912e+00,  1.7621e+00,  2.9830e+00,  1.7066e+00,  1.5087e+00,\n                           2.6329e+00,  1.5679e+00, -7.2568e-02, -1.7171e-01,  1.1326e+00,\n                           1.5631e+00,  2.5891e+00, -3.6912e-01,  1.9741e+00,  1.2041e+00,\n                           2.0303e+00,  6.4200e-01,  2.0669e+00,  1.4134e+00,  1.6007e-01,\n                           1.7903e+00,  5.7391e+00, -1.8616e-01,  1.2918e+00,  1.4485e+00,\n                           6.6483e-01,  2.6814e+00,  9.2967e-01,  1.3875e+00, -2.0327e-01,\n                           1.5782e+00,  3.2064e+00,  1.3448e+00,  1.2376e+00])\n                )\n              )\n            )\n            (2): ReLU6(inplace=True)\n          )\n          (2): ConvBn2d(\n            384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0026, 0.0027, 0.0062, 0.0029, 0.0027, 0.0028, 0.0030, 0.0033, 0.0032,\n                      0.0040, 0.0029, 0.0025, 0.0032, 0.0025, 0.0019, 0.0024, 0.0024, 0.0020,\n                      0.0036, 0.0023, 0.0027, 0.0024, 0.0026, 0.0048, 0.0037, 0.0034, 0.0040,\n                      0.0028, 0.0035, 0.0034, 0.0042, 0.0034, 0.0047, 0.0021, 0.0032, 0.0038,\n                      0.0049, 0.0025, 0.0027, 0.0046, 0.0026, 0.0031, 0.0028, 0.0029, 0.0030,\n                      0.0024, 0.0032, 0.0034, 0.0034, 0.0020, 0.0033, 0.0031, 0.0032, 0.0031,\n                      0.0024, 0.0027, 0.0037, 0.0049, 0.0029, 0.0034, 0.0068, 0.0034, 0.0022,\n                      0.0025]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n                min_val=tensor([-0.3093, -0.3447, -0.7880, -0.3008, -0.3472, -0.3553, -0.3705, -0.4229,\n                        -0.3680, -0.5068, -0.2958, -0.3191, -0.2184, -0.2225, -0.2400, -0.2734,\n                        -0.2248, -0.2552, -0.4496, -0.2992, -0.2721, -0.2741, -0.3335, -0.4331,\n                        -0.2723, -0.3015, -0.4218, -0.2891, -0.4209, -0.4005, -0.5349, -0.2564,\n                        -0.5974, -0.2735, -0.3445, -0.4818, -0.6313, -0.2922, -0.3430, -0.3031,\n                        -0.2932, -0.3327, -0.3569, -0.3711, -0.3888, -0.3115, -0.2998, -0.4349,\n                        -0.2951, -0.2067, -0.4284, -0.4004, -0.4047, -0.3604, -0.3043, -0.3447,\n                        -0.4088, -0.6305, -0.3139, -0.3482, -0.8688, -0.1923, -0.2146, -0.2930]), max_val=tensor([0.3267, 0.3374, 0.2754, 0.3729, 0.2421, 0.3402, 0.3833, 0.3275, 0.4044,\n                        0.3059, 0.3652, 0.3173, 0.4026, 0.3193, 0.2377, 0.3005, 0.3040, 0.2294,\n                        0.4599, 0.2101, 0.3402, 0.3071, 0.3295, 0.6044, 0.4728, 0.4302, 0.5092,\n                        0.3534, 0.4407, 0.4307, 0.3748, 0.4330, 0.3379, 0.2516, 0.4012, 0.3181,\n                        0.3361, 0.3147, 0.2770, 0.5889, 0.3245, 0.3924, 0.2820, 0.3282, 0.3248,\n                        0.2933, 0.4013, 0.4252, 0.4265, 0.2520, 0.4013, 0.3523, 0.3128, 0.3878,\n                        0.2198, 0.3177, 0.4739, 0.3864, 0.3736, 0.4267, 0.5742, 0.4258, 0.2776,\n                        0.3196])\n              )\n            )\n          )\n        )\n      )\n      (11): Module(\n        (conv): Module(\n          (0): Module(\n            (0): ConvBn2d(\n              64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False\n              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n                fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0005, 0.0004, 0.0006, 0.0004, 0.0005, 0.0006, 0.0005, 0.0008, 0.0006,\n                        0.0003, 0.0002, 0.0006, 0.0007, 0.0008, 0.0009, 0.0005, 0.0006, 0.0005,\n                        0.0005, 0.0006, 0.0007, 0.0006, 0.0005, 0.0005, 0.0004, 0.0004, 0.0004,\n                        0.0005, 0.0006, 0.0005, 0.0004, 0.0004, 0.0006, 0.0003, 0.0006, 0.0004,\n                        0.0006, 0.0005, 0.0006, 0.0006, 0.0005, 0.0006, 0.0003, 0.0005, 0.0007,\n                        0.0007, 0.0005, 0.0005, 0.0006, 0.0006, 0.0003, 0.0004, 0.0004, 0.0005,\n                        0.0004, 0.0003, 0.0005, 0.0009, 0.0005, 0.0003, 0.0004, 0.0004, 0.0004,\n                        0.0007, 0.0008, 0.0004, 0.0004, 0.0005, 0.0005, 0.0003, 0.0005, 0.0006,\n                        0.0002, 0.0007, 0.0006, 0.0004, 0.0006, 0.0006, 0.0005, 0.0005, 0.0007,\n                        0.0006, 0.0007, 0.0008, 0.0007, 0.0008, 0.0009, 0.0007, 0.0007, 0.0005,\n                        0.0005, 0.0005, 0.0007, 0.0006, 0.0007, 0.0006, 0.0005, 0.0005, 0.0005,\n                        0.0008, 0.0004, 0.0004, 0.0004, 0.0006, 0.0004, 0.0007, 0.0006, 0.0007,\n                        0.0006, 0.0010, 0.0005, 0.0006, 0.0003, 0.0005, 0.0005, 0.0005, 0.0005,\n                        0.0005, 0.0003, 0.0007, 0.0008, 0.0003, 0.0004, 0.0006, 0.0004, 0.0005,\n                        0.0005, 0.0007, 0.0005, 0.0006, 0.0005, 0.0005, 0.0004, 0.0006, 0.0004,\n                        0.0005, 0.0006, 0.0005, 0.0007, 0.0006, 0.0005, 0.0006, 0.0007, 0.0007,\n                        0.0006, 0.0007, 0.0005, 0.0005, 0.0008, 0.0008, 0.0005, 0.0006, 0.0003,\n                        0.0005, 0.0004, 0.0006, 0.0006, 0.0006, 0.0005, 0.0002, 0.0005, 0.0005,\n                        0.0005, 0.0003, 0.0006, 0.0002, 0.0008, 0.0005, 0.0006, 0.0007, 0.0004,\n                        0.0004, 0.0004, 0.0004, 0.0010, 0.0006, 0.0004, 0.0004, 0.0007, 0.0006,\n                        0.0007, 0.0003, 0.0006, 0.0004, 0.0007, 0.0007, 0.0006, 0.0008, 0.0012,\n                        0.0006, 0.0005, 0.0006, 0.0006, 0.0005, 0.0006, 0.0008, 0.0007, 0.0005,\n                        0.0007, 0.0003, 0.0005, 0.0005, 0.0006, 0.0004, 0.0009, 0.0005, 0.0006,\n                        0.0004, 0.0003, 0.0006, 0.0006, 0.0008, 0.0006, 0.0009, 0.0006, 0.0005,\n                        0.0010, 0.0004, 0.0006, 0.0006, 0.0005, 0.0006, 0.0004, 0.0006, 0.0003,\n                        0.0006, 0.0007, 0.0008, 0.0007, 0.0006, 0.0005, 0.0006, 0.0003, 0.0007,\n                        0.0006, 0.0005, 0.0005, 0.0005, 0.0004, 0.0006, 0.0005, 0.0004, 0.0007,\n                        0.0008, 0.0007, 0.0007, 0.0005, 0.0007, 0.0008, 0.0009, 0.0004, 0.0005,\n                        0.0006, 0.0004, 0.0003, 0.0005, 0.0005, 0.0006, 0.0005, 0.0003, 0.0007,\n                        0.0003, 0.0004, 0.0010, 0.0006, 0.0005, 0.0007, 0.0008, 0.0003, 0.0009,\n                        0.0005, 0.0006, 0.0005, 0.0004, 0.0005, 0.0006, 0.0004, 0.0005, 0.0006,\n                        0.0005, 0.0006, 0.0007, 0.0004, 0.0005, 0.0005, 0.0006, 0.0005, 0.0005,\n                        0.0007, 0.0003, 0.0006, 0.0008, 0.0007, 0.0006, 0.0006, 0.0005, 0.0004,\n                        0.0005, 0.0007, 0.0010, 0.0011, 0.0004, 0.0009, 0.0004, 0.0006, 0.0009,\n                        0.0006, 0.0004, 0.0006, 0.0008, 0.0007, 0.0006, 0.0004, 0.0003, 0.0007,\n                        0.0006, 0.0005, 0.0003, 0.0003, 0.0007, 0.0006, 0.0006, 0.0007, 0.0006,\n                        0.0006, 0.0006, 0.0004, 0.0003, 0.0006, 0.0007, 0.0007, 0.0007, 0.0006,\n                        0.0005, 0.0005, 0.0004, 0.0003, 0.0007, 0.0003, 0.0006, 0.0005, 0.0008,\n                        0.0002, 0.0003, 0.0006, 0.0005, 0.0006, 0.0006, 0.0007, 0.0005, 0.0004,\n                        0.0007, 0.0005, 0.0003, 0.0007, 0.0006, 0.0005, 0.0007, 0.0005, 0.0005,\n                        0.0007, 0.0007, 0.0004, 0.0005, 0.0005, 0.0006, 0.0007, 0.0007, 0.0005,\n                        0.0005, 0.0007, 0.0006, 0.0007, 0.0007, 0.0007, 0.0005, 0.0004, 0.0006,\n                        0.0004, 0.0004, 0.0007, 0.0006, 0.0004, 0.0008]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                       dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n                (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n                  min_val=tensor([-0.0615, -0.0551, -0.0745, -0.0381, -0.0589, -0.0594, -0.0458, -0.0969,\n                          -0.0736, -0.0247, -0.0226, -0.0602, -0.0839, -0.1077, -0.1177, -0.0551,\n                          -0.0721, -0.0441, -0.0586, -0.0646, -0.0886, -0.0493, -0.0673, -0.0489,\n                          -0.0566, -0.0516, -0.0416, -0.0572, -0.0760, -0.0500, -0.0407, -0.0547,\n                          -0.0533, -0.0425, -0.0617, -0.0452, -0.0711, -0.0473, -0.0717, -0.0805,\n                          -0.0538, -0.0793, -0.0385, -0.0614, -0.0859, -0.0920, -0.0575, -0.0449,\n                          -0.0666, -0.0565, -0.0306, -0.0553, -0.0381, -0.0434, -0.0472, -0.0385,\n                          -0.0589, -0.0749, -0.0428, -0.0371, -0.0539, -0.0502, -0.0330, -0.0653,\n                          -0.0775, -0.0459, -0.0401, -0.0499, -0.0426, -0.0385, -0.0655, -0.0567,\n                          -0.0239, -0.0392, -0.0785, -0.0460, -0.0758, -0.0816, -0.0532, -0.0565,\n                          -0.0892, -0.0746, -0.0681, -0.0755, -0.0622, -0.0783, -0.1161, -0.0567,\n                          -0.0727, -0.0633, -0.0483, -0.0471, -0.0855, -0.0555, -0.0682, -0.0558,\n                          -0.0699, -0.0554, -0.0568, -0.0966, -0.0452, -0.0463, -0.0569, -0.0603,\n                          -0.0304, -0.0880, -0.0777, -0.0942, -0.0597, -0.0879, -0.0634, -0.0769,\n                          -0.0350, -0.0622, -0.0403, -0.0691, -0.0593, -0.0488, -0.0353, -0.0627,\n                          -0.1021, -0.0447, -0.0555, -0.0759, -0.0440, -0.0533, -0.0645, -0.0626,\n                          -0.0682, -0.0679, -0.0371, -0.0552, -0.0534, -0.0740, -0.0522, -0.0699,\n                          -0.0574, -0.0525, -0.0904, -0.0568, -0.0664, -0.0798, -0.0942, -0.0663,\n                          -0.0704, -0.0791, -0.0594, -0.0662, -0.0382, -0.0777, -0.0634, -0.0715,\n                          -0.0356, -0.0620, -0.0486, -0.0676, -0.0507, -0.0641, -0.0614, -0.0226,\n                          -0.0317, -0.0688, -0.0685, -0.0320, -0.0678, -0.0249, -0.0978, -0.0639,\n                          -0.0777, -0.0950, -0.0543, -0.0554, -0.0492, -0.0381, -0.0750, -0.0498,\n                          -0.0429, -0.0516, -0.0712, -0.0731, -0.0585, -0.0334, -0.0740, -0.0545,\n                          -0.0954, -0.0882, -0.0740, -0.0891, -0.1501, -0.0609, -0.0646, -0.0752,\n                          -0.0810, -0.0667, -0.0781, -0.0961, -0.0607, -0.0535, -0.0927, -0.0368,\n                          -0.0561, -0.0671, -0.0806, -0.0564, -0.1214, -0.0620, -0.0821, -0.0496,\n                          -0.0375, -0.0786, -0.0789, -0.0984, -0.0798, -0.0496, -0.0550, -0.0560,\n                          -0.0561, -0.0551, -0.0706, -0.0711, -0.0667, -0.0755, -0.0500, -0.0679,\n                          -0.0394, -0.0610, -0.0950, -0.1062, -0.0849, -0.0738, -0.0662, -0.0819,\n                          -0.0284, -0.0913, -0.0628, -0.0580, -0.0654, -0.0601, -0.0522, -0.0621,\n                          -0.0679, -0.0466, -0.0698, -0.0788, -0.0884, -0.0524, -0.0480, -0.0898,\n                          -0.0962, -0.1163, -0.0467, -0.0686, -0.0827, -0.0552, -0.0391, -0.0585,\n                          -0.0686, -0.0710, -0.0654, -0.0264, -0.0712, -0.0444, -0.0497, -0.1004,\n                          -0.0712, -0.0667, -0.0744, -0.0915, -0.0339, -0.1098, -0.0591, -0.0505,\n                          -0.0702, -0.0344, -0.0575, -0.0643, -0.0508, -0.0592, -0.0611, -0.0663,\n                          -0.0709, -0.0929, -0.0493, -0.0654, -0.0588, -0.0558, -0.0678, -0.0519,\n                          -0.0775, -0.0410, -0.0742, -0.1022, -0.0948, -0.0705, -0.0459, -0.0534,\n                          -0.0491, -0.0491, -0.0625, -0.1300, -0.0529, -0.0532, -0.1201, -0.0526,\n                          -0.0757, -0.1120, -0.0828, -0.0514, -0.0791, -0.0485, -0.0739, -0.0777,\n                          -0.0542, -0.0225, -0.0857, -0.0720, -0.0632, -0.0366, -0.0351, -0.0907,\n                          -0.0557, -0.0730, -0.0839, -0.0663, -0.0412, -0.0539, -0.0498, -0.0337,\n                          -0.0528, -0.0895, -0.0847, -0.0542, -0.0574, -0.0550, -0.0585, -0.0494,\n                          -0.0335, -0.0839, -0.0369, -0.0787, -0.0693, -0.1063, -0.0239, -0.0423,\n                          -0.0737, -0.0589, -0.0685, -0.0447, -0.0449, -0.0589, -0.0542, -0.0523,\n                          -0.0668, -0.0282, -0.0839, -0.0739, -0.0645, -0.0838, -0.0672, -0.0683,\n                          -0.0606, -0.0848, -0.0414, -0.0613, -0.0504, -0.0720, -0.0861, -0.0836,\n                          -0.0617, -0.0641, -0.0916, -0.0590, -0.0510, -0.0911, -0.0585, -0.0547,\n                          -0.0363, -0.0417, -0.0574, -0.0535, -0.0948, -0.0768, -0.0544, -0.0761]), max_val=tensor([0.0602, 0.0566, 0.0820, 0.0445, 0.0627, 0.0715, 0.0673, 0.0557, 0.0553,\n                          0.0433, 0.0302, 0.0706, 0.0666, 0.0660, 0.0767, 0.0619, 0.0717, 0.0649,\n                          0.0550, 0.0792, 0.0725, 0.0701, 0.0679, 0.0613, 0.0474, 0.0401, 0.0522,\n                          0.0646, 0.0630, 0.0646, 0.0524, 0.0432, 0.0699, 0.0395, 0.0823, 0.0353,\n                          0.0820, 0.0593, 0.0699, 0.0760, 0.0634, 0.0520, 0.0344, 0.0453, 0.0883,\n                          0.0634, 0.0575, 0.0650, 0.0749, 0.0798, 0.0369, 0.0507, 0.0541, 0.0614,\n                          0.0552, 0.0421, 0.0471, 0.1117, 0.0615, 0.0397, 0.0540, 0.0480, 0.0450,\n                          0.0851, 0.0961, 0.0533, 0.0501, 0.0583, 0.0604, 0.0363, 0.0473, 0.0802,\n                          0.0279, 0.0903, 0.0674, 0.0442, 0.0719, 0.0484, 0.0645, 0.0630, 0.0789,\n                          0.0808, 0.0859, 0.1045, 0.0937, 0.0983, 0.0851, 0.0934, 0.0880, 0.0498,\n                          0.0594, 0.0622, 0.0782, 0.0807, 0.0919, 0.0778, 0.0556, 0.0586, 0.0636,\n                          0.0855, 0.0491, 0.0473, 0.0469, 0.0718, 0.0540, 0.0780, 0.0606, 0.0580,\n                          0.0754, 0.1243, 0.0544, 0.0709, 0.0199, 0.0498, 0.0605, 0.0436, 0.0602,\n                          0.0635, 0.0379, 0.0904, 0.0511, 0.0418, 0.0515, 0.0452, 0.0452, 0.0597,\n                          0.0605, 0.0919, 0.0577, 0.0773, 0.0641, 0.0648, 0.0499, 0.0550, 0.0451,\n                          0.0615, 0.0724, 0.0583, 0.0609, 0.0773, 0.0394, 0.0738, 0.0899, 0.0942,\n                          0.0782, 0.0899, 0.0691, 0.0526, 0.0958, 0.0971, 0.0478, 0.0802, 0.0395,\n                          0.0660, 0.0541, 0.0769, 0.0779, 0.0743, 0.0678, 0.0258, 0.0580, 0.0494,\n                          0.0507, 0.0329, 0.0816, 0.0314, 0.0603, 0.0531, 0.0791, 0.0789, 0.0405,\n                          0.0489, 0.0498, 0.0507, 0.1210, 0.0705, 0.0477, 0.0529, 0.0831, 0.0753,\n                          0.0894, 0.0387, 0.0683, 0.0392, 0.0761, 0.0713, 0.0643, 0.1026, 0.0631,\n                          0.0755, 0.0541, 0.0532, 0.0511, 0.0658, 0.0691, 0.0772, 0.0871, 0.0609,\n                          0.0836, 0.0227, 0.0659, 0.0566, 0.0666, 0.0530, 0.0777, 0.0457, 0.0590,\n                          0.0563, 0.0313, 0.0604, 0.0704, 0.0750, 0.0737, 0.1108, 0.0781, 0.0697,\n                          0.1236, 0.0494, 0.0324, 0.0755, 0.0559, 0.0605, 0.0386, 0.0790, 0.0240,\n                          0.0800, 0.0892, 0.0534, 0.0749, 0.0510, 0.0603, 0.0692, 0.0330, 0.0701,\n                          0.0799, 0.0485, 0.0668, 0.0445, 0.0352, 0.0704, 0.0507, 0.0355, 0.0856,\n                          0.1009, 0.0831, 0.0907, 0.0655, 0.0601, 0.0668, 0.0586, 0.0426, 0.0542,\n                          0.0539, 0.0431, 0.0373, 0.0535, 0.0669, 0.0757, 0.0360, 0.0319, 0.0861,\n                          0.0440, 0.0534, 0.1275, 0.0654, 0.0539, 0.0851, 0.1026, 0.0356, 0.0817,\n                          0.0385, 0.0752, 0.0668, 0.0449, 0.0655, 0.0765, 0.0567, 0.0554, 0.0814,\n                          0.0637, 0.0751, 0.0736, 0.0514, 0.0548, 0.0599, 0.0751, 0.0447, 0.0625,\n                          0.0867, 0.0323, 0.0761, 0.0886, 0.0873, 0.0806, 0.0823, 0.0593, 0.0530,\n                          0.0654, 0.0853, 0.0812, 0.1368, 0.0528, 0.0503, 0.0523, 0.0553, 0.0746,\n                          0.0810, 0.0508, 0.0824, 0.1059, 0.0864, 0.0520, 0.0443, 0.0386, 0.0830,\n                          0.0528, 0.0572, 0.0382, 0.0373, 0.0642, 0.0726, 0.0669, 0.0783, 0.0790,\n                          0.0704, 0.0775, 0.0407, 0.0263, 0.0763, 0.0727, 0.0634, 0.0915, 0.0716,\n                          0.0592, 0.0562, 0.0394, 0.0413, 0.0634, 0.0409, 0.0694, 0.0643, 0.0525,\n                          0.0233, 0.0270, 0.0620, 0.0449, 0.0729, 0.0753, 0.0870, 0.0518, 0.0376,\n                          0.0857, 0.0657, 0.0441, 0.0440, 0.0660, 0.0389, 0.0766, 0.0617, 0.0432,\n                          0.0881, 0.0854, 0.0568, 0.0573, 0.0588, 0.0607, 0.0466, 0.0499, 0.0465,\n                          0.0487, 0.0737, 0.0767, 0.0887, 0.0584, 0.0848, 0.0634, 0.0538, 0.0774,\n                          0.0446, 0.0454, 0.0746, 0.0486, 0.0428, 0.0966])\n                )\n              )\n            )\n            (2): ReLU6(inplace=True)\n          )\n          (1): Module(\n            (0): ConvBn2d(\n              384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False\n              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n                fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0087, 0.0153, 0.0109, 0.0070, 0.0082, 0.0025, 0.0130, 0.0040, 0.0100,\n                        0.0101, 0.0348, 0.0047, 0.0022, 0.0178, 0.0095, 0.0198, 0.0173, 0.0150,\n                        0.0095, 0.0102, 0.0171, 0.0154, 0.0155, 0.0026, 0.0170, 0.0234, 0.0188,\n                        0.0186, 0.0033, 0.0178, 0.0139, 0.0186, 0.0167, 0.0204, 0.0158, 0.0137,\n                        0.0140, 0.0082, 0.0142, 0.0147, 0.0169, 0.0079, 0.0304, 0.0066, 0.0154,\n                        0.0089, 0.0154, 0.0093, 0.0060, 0.0066, 0.0281, 0.0119, 0.0155, 0.0177,\n                        0.0248, 0.0146, 0.0208, 0.0194, 0.0180, 0.0217, 0.0194, 0.0049, 0.0129,\n                        0.0081, 0.0089, 0.0093, 0.0117, 0.0187, 0.0158, 0.0170, 0.0116, 0.0143,\n                        0.0349, 0.0030, 0.0075, 0.0167, 0.0122, 0.0161, 0.0070, 0.0353, 0.0139,\n                        0.0166, 0.0155, 0.0071, 0.0145, 0.0195, 0.0026, 0.0034, 0.0124, 0.0190,\n                        0.0198, 0.0163, 0.0171, 0.0181, 0.0147, 0.0029, 0.0131, 0.0133, 0.0142,\n                        0.0163, 0.0228, 0.0206, 0.0027, 0.0078, 0.0149, 0.0047, 0.0024, 0.0168,\n                        0.0136, 0.0053, 0.0099, 0.0135, 0.0411, 0.0102, 0.0241, 0.0157, 0.0149,\n                        0.0023, 0.0118, 0.0086, 0.0167, 0.0037, 0.0064, 0.0186, 0.0137, 0.0128,\n                        0.0093, 0.0122, 0.0115, 0.0102, 0.0100, 0.0123, 0.0224, 0.0082, 0.0055,\n                        0.0106, 0.0098, 0.0044, 0.0149, 0.0169, 0.0033, 0.0024, 0.0142, 0.0203,\n                        0.0041, 0.0097, 0.0192, 0.0099, 0.0128, 0.0154, 0.0028, 0.0087, 0.0027,\n                        0.0171, 0.0128, 0.0186, 0.0163, 0.0131, 0.0022, 0.0196, 0.0139, 0.0168,\n                        0.0115, 0.0136, 0.0115, 0.0404, 0.0148, 0.0111, 0.0201, 0.0028, 0.0184,\n                        0.0239, 0.0209, 0.0066, 0.0023, 0.0141, 0.0132, 0.0152, 0.0153, 0.0118,\n                        0.0150, 0.0087, 0.0028, 0.0158, 0.0178, 0.0178, 0.0153, 0.0155, 0.0110,\n                        0.0200, 0.0158, 0.0186, 0.0032, 0.0093, 0.0036, 0.0147, 0.0041, 0.0025,\n                        0.0162, 0.0167, 0.0028, 0.0116, 0.0017, 0.0172, 0.0144, 0.0097, 0.0091,\n                        0.0189, 0.0303, 0.0153, 0.0173, 0.0150, 0.0099, 0.0169, 0.0092, 0.0023,\n                        0.0127, 0.0123, 0.0096, 0.0155, 0.0164, 0.0155, 0.0188, 0.0137, 0.0328,\n                        0.0065, 0.0139, 0.0027, 0.0185, 0.0180, 0.0162, 0.0185, 0.0167, 0.0107,\n                        0.0064, 0.0240, 0.0109, 0.0100, 0.0161, 0.0084, 0.0158, 0.0202, 0.0162,\n                        0.0083, 0.0074, 0.0179, 0.0095, 0.0195, 0.0046, 0.0023, 0.0267, 0.0201,\n                        0.0117, 0.0042, 0.0074, 0.0148, 0.0141, 0.0023, 0.0196, 0.0371, 0.0161,\n                        0.0163, 0.0225, 0.0060, 0.0084, 0.0095, 0.0028, 0.0095, 0.0195, 0.0066,\n                        0.0060, 0.0023, 0.0200, 0.0281, 0.0018, 0.0083, 0.0164, 0.0138, 0.0034,\n                        0.0149, 0.0153, 0.0076, 0.0162, 0.0204, 0.0196, 0.0173, 0.0212, 0.0057,\n                        0.0161, 0.0200, 0.0032, 0.0091, 0.0148, 0.0103, 0.0070, 0.0120, 0.0215,\n                        0.0146, 0.0112, 0.0175, 0.0169, 0.0139, 0.0167, 0.0173, 0.0024, 0.0062,\n                        0.0106, 0.0196, 0.0100, 0.0133, 0.0138, 0.0152, 0.0236, 0.0307, 0.0183,\n                        0.0145, 0.0141, 0.0300, 0.0108, 0.0160, 0.0199, 0.0182, 0.0021, 0.0217,\n                        0.0027, 0.0203, 0.0197, 0.0223, 0.0099, 0.0128, 0.0138, 0.0105, 0.0067,\n                        0.0020, 0.0025, 0.0078, 0.0208, 0.0118, 0.0308, 0.0139, 0.0083, 0.0080,\n                        0.0498, 0.0283, 0.0209, 0.0204, 0.0037, 0.0170, 0.0030, 0.0162, 0.0167,\n                        0.0150, 0.0090, 0.0267, 0.0140, 0.0134, 0.0129, 0.0026, 0.0162, 0.0045,\n                        0.0046, 0.0150, 0.0143, 0.0152, 0.0185, 0.0087, 0.0128, 0.0261, 0.0201,\n                        0.0192, 0.0118, 0.0153, 0.0183, 0.0163, 0.0130, 0.0080, 0.0367, 0.0090,\n                        0.0032, 0.0128, 0.0181, 0.0144, 0.0018, 0.0144]), zero_point=tensor([   0,    0,    0,    0, -128,  127,    0,  127,    0,    0,    0,    0,\n                         127,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,  127,\n                           0,    0,    0,    0,  127,    0,    0,    0,    0,    0,    0,    0,\n                           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                           0, -128,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                           0,    0,    0,    0, -128,    0,    0,    0,    0,    0,    0,    0,\n                           0,  127,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                           0,    0,  127,  127,    0,    0,    0,    0,    0,    0,    0,    0,\n                           0,    0,    0,    0,    0,    0,  127,    0,    0,  127,  127,    0,\n                           0,  127,    0,    0,    0,    0,    0,    0,    0,  127,    0,    0,\n                           0,  127,  127,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                           0,    0,    0,    0,    0,  127,    0,    0,  127,  127,    0,    0,\n                           0,    0,    0,    0,    0,    0,  127,    0,  127,    0,    0,    0,\n                           0,    0,  127,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                           0,  127,    0,    0,    0,    0,  127,    0,    0,    0,    0,    0,\n                           0,    0,  127,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         127,    0,  127,    0,  127,  127,    0,    0,  127,    0,  127,    0,\n                           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,  127,\n                           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,  127,\n                           0,    0,    0,    0,    0,    0,    0,    0,    0, -128,    0, -128,\n                           0,    0,    0,    0,    0,    0,    0,    0,    0,  127,    0,    0,\n                           0,  127,    0,    0,    0,  127,    0,    0,    0,    0,    0, -128,\n                           0,    0,  127,    0,    0,    0,    0,  127,    0,    0,  127,    0,\n                           0,    0,  127,    0,    0, -128,    0,    0,    0,    0,    0,    0,\n                           0,    0,  127,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                           0,    0,    0,    0,  127,    0,    0,    0,    0,    0,    0,    0,\n                           0,    0,    0,    0,    0,    0,  127,    0,    0,    0,  127,    0,\n                         127,    0,    0,    0,    0,    0,    0,    0, -128,  127,  127,  127,\n                           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,  127,    0,\n                         127,    0,    0,    0,    0,    0,    0,    0,    0,  127,    0,  127,\n                         127,    0,    0,    0,    0, -128,    0,    0,    0,    0,    0,    0,\n                           0,    0,    0,    0,    0,    0,  127,    0,    0,    0,  127,    0],\n                       dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n                (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n                  min_val=tensor([-1.1101e+00, -1.5358e+00, -1.2612e-01, -8.9860e-01,  1.4113e-01,\n                          -6.3027e-01, -1.1774e+00, -1.0080e+00, -1.2776e+00, -1.2883e+00,\n                          -4.4571e+00, -6.0695e-01, -5.6934e-01, -1.1414e+00, -8.6375e-01,\n                          -5.9583e-01, -2.5648e-01, -3.7306e-01, -1.2198e+00, -1.0758e+00,\n                          -1.9279e-02, -1.7278e-01, -1.7714e+00, -6.5158e-01, -7.9332e-01,\n                          -3.1757e-01, -5.7711e-01, -1.3206e-01, -8.4219e-01, -6.3187e-01,\n                          -1.7856e+00, -6.9702e-01, -6.8253e-02, -2.6149e+00, -2.9480e-01,\n                          -1.7497e+00, -2.4227e-01, -1.0528e+00, -8.3721e-02, -1.2740e-01,\n                          -2.0715e-01, -1.0093e+00, -2.3865e+00, -8.5052e-01, -5.7433e-01,\n                          -1.1396e+00, -1.6484e-01, -1.1892e+00, -7.7151e-01,  2.2778e-02,\n                          -9.1401e-01, -6.8561e-01, -2.7652e-01, -3.6898e-02, -3.5983e-01,\n                          -1.8694e+00, -1.1787e+00, -2.8848e-01, -6.0866e-01, -2.7814e+00,\n                          -1.7890e-01, -6.2956e-01, -1.6466e+00, -6.2292e-01,  6.1510e-02,\n                          -1.1638e+00, -2.6806e-01, -7.7268e-01, -1.2855e-01, -2.1796e+00,\n                          -1.2448e-01, -1.6377e-01, -4.4669e+00, -7.6545e-01, -9.5702e-01,\n                          -2.7967e-01, -2.4206e-01, -1.5174e-01, -8.9854e-01, -6.3075e-01,\n                          -1.1020e-01, -8.0347e-02, -1.9882e+00, -9.1505e-01, -8.4683e-01,\n                          -2.4133e-01, -6.5730e-01, -8.5789e-01, -6.7875e-02, -2.7214e-01,\n                          -2.2524e-01, -1.4581e-01, -2.3773e-01, -6.8780e-01, -2.5798e-01,\n                          -3.6635e-01, -1.3620e+00, -4.8123e-01, -3.5464e-01, -2.4363e-01,\n                          -2.7989e-01, -1.8155e+00, -6.7875e-01, -9.9742e-01, -1.5644e-01,\n                          -1.1935e+00, -6.2135e-01, -6.3827e-02, -9.9133e-02, -1.3494e+00,\n                          -1.2709e+00, -3.6810e-01, -5.2586e+00, -1.3109e+00, -6.1509e-02,\n                          -1.1930e+00, -2.0208e-01, -5.9068e-01, -1.5084e+00, -1.0511e+00,\n                          -1.1490e+00, -9.5043e-01, -1.6293e+00, -2.7186e-01, -1.0982e+00,\n                          -1.6405e+00, -1.1896e+00, -1.0922e+00, -1.2259e+00, -1.3116e+00,\n                          -1.2767e+00, -2.5210e-01, -3.8750e-01, -1.0445e+00, -6.9898e-01,\n                          -1.3549e+00, -1.2583e+00, -1.1294e+00, -2.1097e-01, -1.5164e-01,\n                          -8.3591e-01, -6.0680e-01, -2.5590e-01, -1.0226e-01, -5.3049e-01,\n                          -6.8580e-01, -2.9259e-01, -3.7103e-01, -3.9496e-01, -3.0859e-01,\n                          -7.0768e-01, -6.0297e-01, -6.7974e-01, -1.7720e-01, -5.2839e-03,\n                          -3.3041e-01, -2.0920e+00, -1.6741e+00, -5.7339e-01, -2.5124e+00,\n                          -5.4374e-01, -2.1468e+00, -4.9533e-01, -1.7441e+00, -9.9711e-01,\n                          -9.3109e-01, -3.2037e-01, -1.4262e+00, -1.6317e-01, -7.0643e-01,\n                          -5.4748e-01, -4.3964e-01, -3.9158e-01, -8.4846e-01, -5.7948e-01,\n                          -4.9758e-01, -1.6844e+00, -8.6409e-01, -1.1862e-01, -1.0125e+00,\n                          -2.3415e-01, -7.7556e-01, -7.1986e-01, -2.0215e+00, -1.4669e-01,\n                          -3.5755e-01, -7.5558e-01, -3.9836e-01, -3.0193e-01, -2.7213e-01,\n                          -3.3378e-01, -6.1864e-01, -8.1352e-01, -3.7871e-02, -9.1540e-01,\n                          -3.0626e-01, -1.0496e+00, -6.3766e-01, -1.4610e-01, -2.1339e+00,\n                          -7.1017e-01, -8.5995e-01, -4.4206e-01, -2.1957e+00, -9.1472e-02,\n                          -1.2447e+00, -1.1647e+00, -2.4187e+00, -5.9659e-01, -3.0775e-01,\n                          -2.8870e-01, -1.0968e-01, -6.6840e-01, -2.5289e-01, -1.1724e+00,\n                          -5.8050e-01, -7.0496e-01, -6.2157e-01, -1.2326e+00, -9.0861e-02,\n                          -7.7711e-01, -1.8102e-01, -2.3681e-01, -5.9839e-01, -1.2172e+00,\n                          -8.2914e-01, -1.6271e-01, -6.9540e-01, -1.9854e-01, -2.6323e-01,\n                          -1.5039e-01, -5.9566e-01, -2.1327e+00, -6.3454e-02, -7.9704e-01,\n                          -9.7177e-01, -5.3812e-01,  1.9035e-01, -2.0645e+00,  1.3247e-03,\n                          -2.3467e-01, -5.7657e-01, -2.7431e-01, -1.0675e+00, -9.4777e-01,\n                          -8.9468e-01, -1.2177e+00, -5.2996e-01, -5.8964e-01, -5.9893e-01,\n                          -1.1660e+00, -7.6419e-02, -9.7570e-01, -1.0768e+00, -9.5207e-01,\n                          -2.1800e-01, -3.0379e-01, -5.8529e-01, -1.1299e+00, -1.1437e+00,\n                          -4.0542e-01, -2.5997e-01, -7.3681e-01,  1.5058e-02, -9.6515e-01,\n                          -2.7161e-01, -7.1067e-01, -6.0497e-01, -2.4922e+00, -8.4603e-01,\n                          -7.7361e-01, -5.8976e-01, -1.6496e-01, -8.4305e-01, -4.7113e-01,\n                          -5.2091e-01, -2.0098e-01, -1.1441e+00, -8.5823e-01, -3.2698e-01,\n                          -1.8326e-01,  5.9342e-02, -4.9518e-01, -4.2027e-04, -2.4156e-01,\n                          -1.7303e-01, -5.0595e-01, -7.2494e-01, -2.9023e-01, -1.5277e-01,\n                          -8.2048e-01, -1.1625e+00, -3.3015e-01, -7.9958e-01, -5.9908e-01,\n                          -1.5406e+00, -7.3666e-02, -1.8412e+00, -1.4276e+00, -2.5088e-01,\n                          -3.9121e-01, -1.1308e+00, -1.7768e-01, -1.0582e-01, -6.0234e-01,\n                          -7.5896e-01, -2.6986e-01, -5.8357e-01, -3.3304e-02, -5.6113e-03,\n                          -1.0986e-01, -1.2224e+00, -1.0445e+00, -3.9307e+00, -4.7115e-01,\n                          -1.9000e-01, -5.1913e-01, -1.1244e+00, -2.7628e+00, -2.8108e-01,\n                          -2.4340e-01, -2.2291e-01, -5.3703e-01, -2.6631e-01, -6.7725e-01,\n                          -6.9232e-01, -6.2958e-01, -3.5240e-01, -1.2670e+00, -1.3199e-01,\n                          -5.7844e-01, -1.2460e+00,  2.7655e-02, -5.0903e-01, -6.3781e-01,\n                          -1.9805e+00, -1.6078e+00, -3.5127e-01, -8.0618e-01, -6.1023e-01,\n                          -1.0630e+00, -1.0258e+00, -6.3707e+00, -1.1901e+00, -2.4625e-01,\n                          -3.0028e-01, -9.4212e-01, -2.1723e+00, -7.7261e-01, -6.0797e-01,\n                          -1.1256e+00, -1.0937e-01, -1.1527e+00, -1.2693e+00, -1.7936e+00,\n                          -3.0139e-01, -3.7019e-01, -6.7321e-01, -5.5682e-01, -1.1600e+00,\n                          -1.1740e+00, -1.7192e-01, -1.1216e+00, -4.2314e-01, -2.3692e+00,\n                           1.0626e-02, -1.2201e+00, -4.3285e-01, -6.1181e-01, -2.0312e-01,\n                          -4.2301e-01, -1.0454e-01, -2.1461e-01, -2.0603e-01, -6.3160e-01,\n                          -9.0516e-01, -1.1766e+00, -1.1491e+00, -8.0661e-01, -1.4775e+00,\n                          -1.9253e-01, -2.0795e-01, -4.5837e-01, -1.2189e+00]), max_val=tensor([ 6.6753e-01,  1.9464e+00,  1.3813e+00,  6.5624e-02,  2.0914e+00,\n                          -1.7133e-01,  1.6521e+00, -1.5623e-02,  3.8842e-01,  8.4059e-01,\n                           3.7794e-01,  1.1784e-01, -1.7773e-01,  2.2634e+00,  1.2004e+00,\n                           2.5128e+00,  2.1986e+00,  1.9052e+00,  4.9972e-01,  1.2965e+00,\n                           2.1672e+00,  1.9619e+00,  1.9705e+00, -2.9366e-01,  2.1603e+00,\n                           2.9742e+00,  2.3855e+00,  2.3673e+00, -1.9918e-01,  2.2631e+00,\n                           7.6708e-01,  2.3559e+00,  2.1218e+00,  8.8808e-01,  2.0043e+00,\n                           6.3798e-02,  1.7754e+00,  8.0287e-01,  1.8031e+00,  1.8710e+00,\n                           2.1451e+00,  7.5675e-01,  3.8589e+00,  5.2275e-02,  1.9495e+00,\n                           4.6033e-01,  1.9582e+00,  8.5992e-01,  1.0515e-01,  1.6896e+00,\n                           3.5633e+00,  1.5096e+00,  1.9638e+00,  2.2428e+00,  3.1555e+00,\n                           4.8216e-01,  2.6386e+00,  2.4665e+00,  2.2837e+00,  5.5090e-01,\n                           2.4619e+00,  7.8002e-03,  1.1697e-01,  1.0259e+00,  2.2630e+00,\n                           1.1809e+00,  1.4882e+00,  2.3719e+00,  2.0039e+00,  1.6148e-01,\n                           1.4741e+00,  1.8132e+00,  1.8104e-01, -1.0549e-01,  8.3467e-02,\n                           2.1160e+00,  1.5551e+00,  2.0504e+00,  3.1372e-01,  4.4842e+00,\n                           1.7608e+00,  2.1089e+00,  1.9572e+00,  8.3377e-01,  1.8419e+00,\n                           2.4762e+00, -4.3791e-02, -3.2779e-01,  1.5734e+00,  2.4122e+00,\n                           2.5132e+00,  2.0700e+00,  2.1707e+00,  2.2959e+00,  1.8624e+00,\n                           8.7193e-02,  1.6582e+00,  1.6944e+00,  1.7983e+00,  2.0736e+00,\n                           2.8896e+00,  2.6161e+00, -5.6437e-02,  4.3485e-03,  1.8948e+00,\n                          -1.8440e-01, -4.5100e-02,  2.1312e+00,  1.7234e+00, -1.1635e-01,\n                           3.4332e-01,  1.7103e+00,  6.7563e-01,  8.0720e-01,  3.0567e+00,\n                           1.9978e+00,  1.8963e+00, -3.0075e-02,  1.0636e+00,  1.0946e+00,\n                           2.1185e+00, -7.2078e-02, -3.7587e-02,  2.3644e+00,  1.7393e+00,\n                           1.4552e+00,  1.0437e+00,  1.5432e+00,  1.4584e+00,  7.5330e-02,\n                           7.4975e-01,  1.5676e+00,  2.8458e+00,  7.9660e-01,  2.8861e-02,\n                           1.1982e+00,  8.8419e-01, -1.5077e-02,  1.8929e+00,  2.1495e+00,\n                          -3.4856e-01, -3.0503e-01,  1.8068e+00,  2.5837e+00,  1.8467e-01,\n                           1.2361e+00,  2.4401e+00,  1.2584e+00,  1.6209e+00,  1.9523e+00,\n                          -9.7331e-02,  1.1045e+00, -2.1773e-01,  2.1672e+00,  1.6231e+00,\n                           2.3663e+00,  9.7875e-01,  8.3170e-02, -3.0675e-02,  1.7236e-01,\n                           1.7626e+00,  7.8869e-02,  1.4649e+00,  1.0075e-01,  1.4575e+00,\n                           5.1328e+00,  1.8782e+00,  8.6476e-01,  2.5501e+00, -2.0647e-01,\n                           2.3332e+00,  3.0404e+00,  2.6584e+00,  6.6093e-01, -2.5575e-01,\n                           1.7845e+00,  6.8141e-01,  1.9292e+00,  1.9474e+00,  1.4972e+00,\n                           1.9058e+00,  1.1003e+00, -2.4037e-01,  4.5371e-02,  2.2584e+00,\n                           2.2647e+00,  1.9476e+00,  1.9691e+00,  1.3946e+00,  2.5419e+00,\n                           2.0023e+00,  2.3615e+00, -6.2338e-02,  1.1792e+00, -4.3841e-02,\n                           1.8683e+00, -2.6091e-02, -1.8139e-01,  2.0567e+00,  1.6564e-01,\n                          -1.7228e-01,  1.4715e+00, -6.9062e-02,  1.5918e+00,  1.8266e+00,\n                           1.1876e+00,  8.3940e-01,  7.8524e-01,  3.8527e+00,  1.9468e+00,\n                           2.1961e+00,  1.9023e+00,  1.2582e+00,  2.1422e+00,  6.0428e-02,\n                          -4.6163e-02,  1.6167e+00,  1.5655e+00,  1.1871e+00,  1.9667e+00,\n                           2.0784e+00,  1.9729e+00,  2.3892e+00,  1.7339e+00,  4.1598e+00,\n                           4.5599e-02,  1.7646e+00, -1.0561e-01,  2.3545e+00,  2.2921e+00,\n                           2.0593e+00,  2.3468e+00,  5.6447e-02,  1.3607e+00,  8.1251e-01,\n                           3.0478e+00,  1.3854e+00,  2.5558e+00,  5.0939e-01,  2.1306e+00,\n                           2.0097e+00,  2.5605e+00,  2.0567e+00,  9.7437e-01,  5.1202e-02,\n                           2.2757e+00,  4.5293e-01,  2.4704e+00,  1.2681e-01, -1.9207e-01,\n                           3.3915e+00,  2.5492e+00,  1.4900e+00, -1.2420e-01,  1.1287e-01,\n                           1.8767e+00,  1.7931e+00, -1.6904e-01,  2.4938e+00,  4.7146e+00,\n                           2.0420e+00,  2.0694e+00,  2.8567e+00,  1.5278e+00,  1.0711e+00,\n                           1.2025e+00, -2.2310e-02,  1.2087e+00,  3.5901e-01,  1.6372e-01,\n                           1.2317e-02, -2.7156e-01,  2.5460e+00,  3.5643e+00, -1.1717e-01,\n                           1.0531e+00,  2.0891e+00,  1.7502e+00, -3.4803e-01,  1.8868e+00,\n                           1.9432e+00,  1.9415e+00,  2.0617e+00,  2.5936e+00,  2.4862e+00,\n                           2.1946e+00,  2.6924e+00,  2.9068e-01,  2.0409e+00,  2.5346e+00,\n                          -8.6395e-02,  3.7030e-01,  1.8784e+00,  1.3094e+00,  8.8564e-01,\n                           9.4661e-01,  2.7303e+00,  1.8501e+00,  9.1722e-01,  2.2191e+00,\n                           2.1476e+00,  1.7630e+00,  2.1233e+00,  2.2024e+00, -5.6067e-02,\n                           7.8204e-01,  1.3476e+00,  2.4912e+00,  1.2706e+00,  1.6903e+00,\n                           1.7562e+00,  1.9266e+00,  3.0020e+00,  4.9921e-01,  2.3205e+00,\n                           1.8379e+00,  1.7948e+00,  3.8126e+00, -7.7526e-02,  2.0344e+00,\n                           2.5262e+00,  2.3171e+00, -2.1692e-01,  2.7566e+00, -7.1473e-02,\n                           2.5844e+00,  2.5075e+00,  2.8297e+00,  8.9272e-03,  1.6302e+00,\n                           1.7589e+00,  1.3390e+00,  1.7123e+00, -1.0356e-01, -3.5240e-01,\n                          -5.8528e-02,  2.6394e+00,  1.5046e+00,  3.9176e+00,  1.7666e+00,\n                           5.9734e-02,  3.2856e-03,  1.4951e+00,  3.5895e+00,  2.6534e+00,\n                           2.5931e+00, -9.2200e-02,  1.4415e+00, -1.1250e-01,  2.0585e+00,\n                           2.1194e+00,  1.9011e+00,  1.1605e-01,  3.3851e+00,  1.2184e+00,\n                           1.7040e+00,  1.6421e+00, -1.1866e-01,  2.0628e+00, -3.0764e-01,\n                          -3.8825e-02,  1.9028e+00,  1.8177e+00,  1.9363e+00,  7.1873e-01,\n                           2.2284e+00,  1.6204e+00,  3.3199e+00,  2.5521e+00,  2.4376e+00,\n                           1.5046e+00,  1.9440e+00,  2.3195e+00,  2.0640e+00,  1.6461e+00,\n                           1.0207e+00,  4.6562e+00,  7.1326e-02, -2.9701e-01,  1.6242e+00,\n                           2.2973e+00,  1.8257e+00, -4.8426e-03,  1.8228e+00])\n                )\n              )\n            )\n            (2): ReLU6(inplace=True)\n          )\n          (2): ConvBn2d(\n            384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0024, 0.0020, 0.0020, 0.0022, 0.0022, 0.0020, 0.0021, 0.0020, 0.0021,\n                      0.0021, 0.0023, 0.0023, 0.0023, 0.0027, 0.0025, 0.0028, 0.0022, 0.0016,\n                      0.0021, 0.0029, 0.0026, 0.0020, 0.0029, 0.0027, 0.0019, 0.0021, 0.0022,\n                      0.0017, 0.0023, 0.0027, 0.0022, 0.0017, 0.0026, 0.0029, 0.0023, 0.0020,\n                      0.0025, 0.0025, 0.0022, 0.0030, 0.0027, 0.0025, 0.0025, 0.0021, 0.0027,\n                      0.0020, 0.0023, 0.0020, 0.0024, 0.0025, 0.0017, 0.0022, 0.0021, 0.0020,\n                      0.0028, 0.0020, 0.0028, 0.0028, 0.0018, 0.0024, 0.0024, 0.0027, 0.0029,\n                      0.0031, 0.0029, 0.0022, 0.0024, 0.0032, 0.0019, 0.0020, 0.0028, 0.0019,\n                      0.0020, 0.0021, 0.0024, 0.0028, 0.0023, 0.0033, 0.0022, 0.0024, 0.0021,\n                      0.0027, 0.0020, 0.0023, 0.0030, 0.0024, 0.0051, 0.0031, 0.0038, 0.0029,\n                      0.0027, 0.0021, 0.0023, 0.0021, 0.0024, 0.0025]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                     dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n                min_val=tensor([-0.3071, -0.2492, -0.2609, -0.2587, -0.2817, -0.2585, -0.2675, -0.2338,\n                        -0.2236, -0.2464, -0.2977, -0.2936, -0.2381, -0.3478, -0.2582, -0.3603,\n                        -0.1979, -0.2099, -0.2651, -0.2435, -0.2911, -0.2509, -0.3271, -0.3493,\n                        -0.2479, -0.2700, -0.2843, -0.2181, -0.2478, -0.3410, -0.2872, -0.2173,\n                        -0.3343, -0.3688, -0.2940, -0.2544, -0.3176, -0.3234, -0.2468, -0.3782,\n                        -0.3507, -0.3253, -0.2677, -0.2695, -0.3407, -0.2544, -0.2370, -0.2312,\n                        -0.3066, -0.3228, -0.2192, -0.2792, -0.2682, -0.1987, -0.3133, -0.2534,\n                        -0.2480, -0.2719, -0.2266, -0.2560, -0.3062, -0.3430, -0.2561, -0.3212,\n                        -0.3000, -0.2829, -0.2407, -0.3337, -0.2453, -0.2394, -0.3120, -0.2301,\n                        -0.2480, -0.2682, -0.2140, -0.3600, -0.2553, -0.4202, -0.2284, -0.2856,\n                        -0.2410, -0.2677, -0.2286, -0.2893, -0.3849, -0.2656, -0.2738, -0.3091,\n                        -0.4927, -0.3029, -0.3461, -0.2438, -0.2902, -0.2188, -0.3058, -0.3257]), max_val=tensor([0.2173, 0.2494, 0.2353, 0.2851, 0.2695, 0.2243, 0.2609, 0.2524, 0.2656,\n                        0.2682, 0.2500, 0.2812, 0.2955, 0.2981, 0.3165, 0.2707, 0.2759, 0.1935,\n                        0.2506, 0.3740, 0.3253, 0.2173, 0.3670, 0.2978, 0.2081, 0.2342, 0.2633,\n                        0.1928, 0.2913, 0.2402, 0.2464, 0.2154, 0.2563, 0.3132, 0.2284, 0.2241,\n                        0.2656, 0.3056, 0.2779, 0.2147, 0.2528, 0.2658, 0.3236, 0.2658, 0.2578,\n                        0.2480, 0.2886, 0.2501, 0.2619, 0.2877, 0.2055, 0.2684, 0.2672, 0.2538,\n                        0.3546, 0.2188, 0.3544, 0.3551, 0.2160, 0.3098, 0.3003, 0.3217, 0.3670,\n                        0.3909, 0.3710, 0.2759, 0.3004, 0.4010, 0.2458, 0.2517, 0.3509, 0.2424,\n                        0.2527, 0.2555, 0.2993, 0.2832, 0.2956, 0.2426, 0.2746, 0.2989, 0.2656,\n                        0.3462, 0.2597, 0.2887, 0.2798, 0.3030, 0.6466, 0.3883, 0.3386, 0.3728,\n                        0.2543, 0.2659, 0.2796, 0.2658, 0.2896, 0.1692])\n              )\n            )\n          )\n        )\n      )\n      (12): Module(\n        (conv): Module(\n          (0): Module(\n            (0): ConvBn2d(\n              96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False\n              (bn): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n                fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([6.6774e-04, 8.7039e-04, 7.8234e-04, 6.2546e-04, 4.7328e-04, 6.1587e-04,\n                        6.1558e-04, 6.5141e-04, 7.1993e-04, 7.0128e-04, 6.4105e-04, 3.7671e-04,\n                        4.7078e-04, 6.6940e-04, 6.8318e-04, 5.7234e-04, 5.3068e-04, 5.8290e-04,\n                        8.7321e-04, 9.8027e-04, 5.7604e-04, 7.3814e-04, 6.6750e-04, 6.2719e-04,\n                        5.2803e-04, 6.8050e-04, 6.8522e-04, 5.4874e-04, 8.5283e-04, 5.5135e-04,\n                        4.9546e-04, 6.1196e-04, 5.1208e-04, 3.5349e-04, 4.4656e-04, 5.6626e-04,\n                        4.6804e-04, 6.0564e-04, 7.7986e-04, 6.9608e-04, 6.8382e-04, 6.1015e-04,\n                        9.4076e-04, 3.9742e-04, 8.1904e-04, 8.1779e-04, 6.6929e-04, 8.4159e-04,\n                        9.8847e-04, 4.3794e-04, 6.2161e-04, 5.7717e-04, 6.3501e-04, 4.8422e-04,\n                        5.5712e-04, 1.0296e-03, 5.9382e-04, 5.8691e-04, 5.5257e-04, 6.6258e-04,\n                        5.2017e-04, 1.0183e-03, 6.9485e-04, 4.2635e-04, 5.6876e-04, 5.7480e-04,\n                        6.8890e-04, 5.1380e-04, 6.3553e-04, 5.7761e-04, 5.4715e-04, 7.9240e-04,\n                        6.4600e-04, 6.4092e-04, 6.7775e-04, 8.0614e-04, 6.0386e-04, 5.6973e-04,\n                        4.3161e-04, 7.8479e-04, 3.7817e-04, 7.3028e-04, 6.8818e-04, 7.1425e-04,\n                        8.4002e-04, 8.8681e-04, 7.4500e-04, 7.8159e-04, 4.5850e-04, 1.2979e-05,\n                        1.2268e-05, 7.6939e-04, 9.4053e-04, 6.2729e-04, 8.6272e-04, 8.2386e-04,\n                        5.0425e-04, 5.5863e-04, 7.6962e-04, 4.5266e-04, 5.8335e-04, 6.4390e-04,\n                        5.3342e-04, 8.3276e-04, 4.9834e-04, 5.8260e-04, 6.5013e-04, 6.9386e-04,\n                        4.7545e-04, 6.0418e-04, 4.4908e-04, 9.0150e-04, 8.2597e-04, 5.6432e-04,\n                        5.7340e-04, 5.7052e-04, 7.9605e-04, 1.7782e-05, 7.0913e-04, 6.2388e-04,\n                        6.5966e-04, 5.7907e-04, 8.1414e-04, 5.0413e-04, 4.8245e-04, 5.0030e-04,\n                        6.8490e-04, 4.9778e-04, 7.1678e-04, 7.0216e-04, 5.3258e-04, 1.3331e-03,\n                        5.2067e-04, 7.3900e-04, 5.5419e-04, 9.3547e-04, 7.5311e-04, 7.6698e-04,\n                        3.2249e-04, 4.8451e-04, 5.6256e-04, 7.6551e-04, 8.0442e-04, 5.4615e-04,\n                        8.6208e-04, 7.3710e-04, 6.3546e-04, 8.8162e-04, 6.6710e-04, 6.6747e-04,\n                        6.0303e-04, 4.7112e-04, 5.1270e-04, 6.3703e-04, 5.4581e-04, 5.7833e-04,\n                        6.2285e-04, 5.4019e-04, 6.1371e-04, 5.3079e-04, 8.2504e-04, 4.0038e-04,\n                        6.3201e-04, 1.4942e-05, 5.0794e-04, 6.3447e-04, 7.2783e-04, 9.5261e-04,\n                        5.4369e-04, 8.5184e-04, 7.7646e-04, 5.5045e-04, 5.4857e-04, 3.2240e-04,\n                        5.9238e-04, 7.8803e-04, 6.7628e-04, 7.6359e-04, 1.0024e-03, 6.0912e-04,\n                        4.3524e-04, 4.6481e-04, 7.1410e-04, 1.1172e-03, 7.6571e-04, 6.4748e-04,\n                        5.9253e-04, 4.6251e-04, 5.7192e-04, 9.6828e-04, 6.5165e-04, 7.9267e-04,\n                        7.5157e-04, 9.5283e-04, 5.9137e-04, 7.0681e-04, 7.9021e-04, 6.6777e-04,\n                        7.2468e-04, 6.6103e-04, 5.2490e-04, 5.3519e-04, 5.8157e-04, 7.0238e-04,\n                        6.5357e-04, 7.3848e-04, 6.4095e-04, 7.8159e-04, 4.4326e-04, 9.7290e-04,\n                        6.2201e-04, 5.5766e-04, 8.3966e-04, 5.4077e-04, 6.5409e-04, 7.2286e-04,\n                        6.3040e-04, 8.4402e-04, 6.2324e-04, 5.5376e-04, 8.6421e-04, 9.7129e-04,\n                        7.2640e-04, 5.8414e-04, 6.4953e-04, 4.9817e-04, 7.4531e-04, 9.6431e-04,\n                        7.3475e-04, 4.9680e-04, 5.5720e-04, 7.4274e-04, 1.4277e-05, 3.3715e-04,\n                        8.6841e-04, 1.0798e-05, 5.3192e-04, 5.4072e-04, 4.6160e-04, 7.7710e-04,\n                        5.2637e-04, 6.2745e-04, 8.4536e-04, 8.8180e-04, 6.1662e-04, 4.6050e-04,\n                        7.9961e-04, 7.6686e-04, 8.9100e-04, 6.6126e-04, 1.4123e-03, 7.8575e-04,\n                        5.5765e-04, 6.6706e-04, 5.5016e-04, 5.8057e-04, 8.4009e-04, 5.2007e-04,\n                        4.9006e-04, 6.5426e-04, 6.3432e-04, 7.1332e-04, 9.4523e-04, 3.8719e-04,\n                        5.6891e-04, 5.5079e-04, 5.6097e-04, 5.7508e-04, 4.8503e-04, 6.4093e-04,\n                        7.4889e-04, 6.0623e-04, 9.4642e-04, 6.0345e-04, 8.9620e-04, 7.2174e-04,\n                        6.8121e-04, 6.3266e-04, 6.1779e-04, 7.4270e-04, 6.6551e-04, 4.6485e-04,\n                        4.2745e-04, 1.1607e-03, 5.7283e-04, 5.7167e-04, 6.8291e-04, 3.6670e-04,\n                        8.0424e-04, 6.0193e-04, 5.3902e-04, 6.3097e-04, 8.4122e-04, 8.3295e-04,\n                        1.0413e-03, 5.0631e-04, 4.7783e-04, 5.0576e-04, 3.7325e-04, 8.1409e-04,\n                        6.7046e-04, 3.5119e-04, 6.4812e-04, 7.0303e-04, 4.8740e-04, 4.5246e-04,\n                        6.2667e-04, 8.4816e-04, 5.8133e-04, 5.6528e-04, 7.4309e-04, 7.5149e-04,\n                        3.6275e-04, 6.2542e-04, 8.7985e-04, 6.9338e-04, 7.9573e-04, 6.3382e-04,\n                        5.4991e-04, 6.0903e-04, 5.1662e-04, 5.9226e-04, 6.7147e-04, 6.7024e-04,\n                        4.1093e-04, 5.4431e-04, 7.9208e-04, 5.9988e-04, 5.2300e-04, 7.3607e-04,\n                        5.1221e-04, 8.8744e-04, 5.7735e-04, 7.0826e-04, 5.3166e-04, 6.9535e-04,\n                        7.9604e-04, 4.0707e-04, 9.5687e-04, 8.5485e-04, 6.1795e-04, 8.8477e-04,\n                        7.6461e-04, 6.0666e-04, 5.3492e-04, 3.5691e-04, 9.6724e-04, 8.1494e-04,\n                        7.0161e-04, 9.5884e-04, 4.7557e-04, 5.3118e-04, 5.2074e-04, 8.7849e-04,\n                        6.0806e-04, 6.2304e-04, 6.3563e-04, 5.6230e-04, 5.6442e-04, 6.2973e-04,\n                        7.9682e-04, 7.2249e-04, 7.9152e-04, 3.6417e-04, 6.9691e-04, 7.8020e-04,\n                        6.3698e-04, 7.7388e-04, 1.2226e-03, 1.9975e-05, 5.8148e-04, 4.3707e-04,\n                        9.6983e-04, 5.0839e-04, 7.5325e-04, 7.6358e-04, 8.2459e-04, 3.3746e-04,\n                        7.8233e-04, 4.5338e-04, 5.9568e-04, 5.1331e-04, 7.8748e-04, 1.0117e-03,\n                        5.0470e-04, 7.3260e-04, 2.9257e-04, 5.9764e-04, 7.7548e-04, 7.2485e-04,\n                        8.8645e-04, 6.4056e-04, 7.7922e-04, 1.0228e-03, 5.4139e-04, 7.9342e-04,\n                        6.8394e-04, 6.3354e-04, 7.3189e-04, 6.2421e-04, 6.2487e-04, 6.7589e-04,\n                        7.8261e-04, 4.1825e-04, 5.7926e-04, 5.0261e-04, 5.5378e-04, 9.6706e-04,\n                        6.0186e-04, 6.6605e-04, 6.1135e-04, 5.7950e-04, 8.8867e-04, 3.7510e-04,\n                        5.0459e-04, 9.8184e-04, 3.8247e-04, 6.0540e-04, 5.2539e-04, 5.2134e-04,\n                        4.2487e-04, 5.1930e-04, 9.1043e-04, 6.6629e-04, 7.2372e-04, 5.7340e-04,\n                        7.7865e-04, 5.0649e-04, 5.1493e-04, 6.1032e-04, 4.2810e-04, 8.4733e-04,\n                        5.7908e-04, 7.4171e-04, 5.9697e-04, 6.6655e-04, 6.2346e-04, 5.8815e-04,\n                        1.0175e-03, 7.5760e-04, 1.1170e-03, 8.0244e-04, 6.1344e-04, 8.0121e-04,\n                        6.6371e-04, 6.1794e-04, 7.4608e-04, 7.8559e-04, 6.0720e-04, 1.1791e-03,\n                        4.2623e-04, 6.5772e-04, 6.4451e-04, 4.4084e-04, 6.0959e-04, 7.3593e-04,\n                        7.9060e-04, 8.2443e-04, 5.8332e-04, 4.8990e-04, 5.7012e-04, 2.8421e-04,\n                        3.7323e-04, 9.1258e-04, 2.1998e-04, 7.3344e-04, 4.2540e-04, 5.4514e-04,\n                        8.7646e-04, 3.9216e-04, 5.8190e-04, 7.8261e-04, 5.2689e-04, 1.0021e-03,\n                        7.7265e-04, 6.0531e-04, 6.2200e-04, 1.1694e-03, 6.4001e-04, 6.4716e-04,\n                        8.5033e-04, 6.7959e-04, 6.0048e-04, 6.4482e-04, 9.2706e-04, 5.8419e-04,\n                        5.9424e-04, 8.2988e-04, 1.1670e-03, 6.0049e-04, 4.1530e-04, 6.9305e-04,\n                        8.5116e-04, 6.9841e-04, 7.4503e-04, 5.6097e-04, 6.9606e-04, 5.9891e-04,\n                        6.6832e-04, 5.6639e-04, 7.2657e-04, 6.2937e-04, 9.9643e-04, 4.6199e-04,\n                        2.7260e-04, 5.9615e-04, 4.5177e-04, 6.9215e-04, 5.8127e-04, 6.5366e-04,\n                        6.8208e-04, 7.1860e-04, 5.8390e-04, 5.8835e-04, 5.7935e-04, 4.7034e-04,\n                        5.4058e-04, 6.7499e-04, 5.6881e-04, 6.8804e-04, 8.1936e-04, 5.9933e-04,\n                        7.4882e-04, 4.8808e-04, 8.7514e-04, 1.0119e-03, 3.8517e-04, 9.2199e-04,\n                        5.7739e-04, 4.5123e-04, 8.4068e-04, 6.8943e-04, 4.8302e-04, 7.9408e-04,\n                        1.0700e-03, 4.1894e-04, 7.0827e-04, 5.5035e-04, 4.6596e-04, 8.7583e-04,\n                        8.3309e-04, 3.4709e-04, 7.8245e-04, 7.0483e-04, 8.3234e-04, 6.4371e-04,\n                        1.0539e-03, 4.2000e-04, 6.3651e-04, 7.7232e-04, 3.9257e-04, 5.3312e-04,\n                        7.0930e-04, 5.9211e-04, 3.8005e-04, 4.5313e-04, 7.9719e-04, 1.0492e-03,\n                        5.8662e-04, 6.1694e-04, 7.3670e-04, 7.9997e-04, 6.3022e-04, 8.1767e-04,\n                        6.6863e-04, 6.1778e-04, 8.5306e-04, 7.3167e-04, 6.5240e-04, 4.7707e-04,\n                        9.2409e-04, 7.3207e-04, 8.6796e-04, 8.2907e-04, 7.2447e-04, 6.0213e-04]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                       dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n                (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n                  min_val=tensor([-0.0853, -0.1112, -0.0747, -0.0801, -0.0471, -0.0740, -0.0788, -0.0834,\n                          -0.0922, -0.0827, -0.0661, -0.0467, -0.0535, -0.0721, -0.0874, -0.0733,\n                          -0.0584, -0.0697, -0.1118, -0.1255, -0.0737, -0.0707, -0.0727, -0.0803,\n                          -0.0676, -0.0871, -0.0877, -0.0702, -0.0842, -0.0454, -0.0552, -0.0783,\n                          -0.0655, -0.0452, -0.0572, -0.0725, -0.0599, -0.0575, -0.0885, -0.0891,\n                          -0.0875, -0.0781, -0.0625, -0.0482, -0.1048, -0.1047, -0.0857, -0.1077,\n                          -0.0997, -0.0561, -0.0796, -0.0739, -0.0685, -0.0553, -0.0639, -0.0972,\n                          -0.0530, -0.0751, -0.0584, -0.0848, -0.0666, -0.1090, -0.0554, -0.0535,\n                          -0.0614, -0.0657, -0.0882, -0.0508, -0.0667, -0.0561, -0.0700, -0.1014,\n                          -0.0671, -0.0820, -0.0868, -0.0938, -0.0704, -0.0729, -0.0487, -0.0719,\n                          -0.0459, -0.0662, -0.0693, -0.0914, -0.0877, -0.1135, -0.0954, -0.0826,\n                          -0.0549, -0.0011, -0.0015, -0.0985, -0.1204, -0.0803, -0.1104, -0.0878,\n                          -0.0645, -0.0715, -0.0561, -0.0455, -0.0571, -0.0824, -0.0683, -0.0867,\n                          -0.0591, -0.0668, -0.0832, -0.0800, -0.0589, -0.0773, -0.0575, -0.0880,\n                          -0.1057, -0.0722, -0.0734, -0.0519, -0.0686, -0.0023, -0.0908, -0.0799,\n                          -0.0698, -0.0741, -0.1042, -0.0552, -0.0491, -0.0525, -0.0562, -0.0611,\n                          -0.0917, -0.0899, -0.0682, -0.1428, -0.0418, -0.0652, -0.0639, -0.0776,\n                          -0.0964, -0.0940, -0.0413, -0.0620, -0.0720, -0.0980, -0.1030, -0.0699,\n                          -0.1096, -0.0831, -0.0666, -0.1128, -0.0791, -0.0736, -0.0772, -0.0603,\n                          -0.0656, -0.0815, -0.0589, -0.0654, -0.0747, -0.0598, -0.0786, -0.0668,\n                          -0.1056, -0.0512, -0.0809, -0.0015, -0.0650, -0.0702, -0.0740, -0.0902,\n                          -0.0512, -0.0784, -0.0994, -0.0705, -0.0702, -0.0413, -0.0620, -0.1009,\n                          -0.0580, -0.0977, -0.1283, -0.0780, -0.0536, -0.0494, -0.0857, -0.0878,\n                          -0.0980, -0.0821, -0.0758, -0.0575, -0.0732, -0.1239, -0.0661, -0.1004,\n                          -0.0609, -0.1220, -0.0573, -0.0905, -0.0900, -0.0855, -0.0793, -0.0696,\n                          -0.0502, -0.0685, -0.0667, -0.0899, -0.0783, -0.0595, -0.0820, -0.0767,\n                          -0.0567, -0.1245, -0.0651, -0.0683, -0.1075, -0.0692, -0.0737, -0.0925,\n                          -0.0745, -0.0820, -0.0798, -0.0709, -0.1106, -0.1243, -0.0930, -0.0745,\n                          -0.0831, -0.0570, -0.0835, -0.1234, -0.0940, -0.0593, -0.0491, -0.0951,\n                          -0.0018, -0.0432, -0.0749, -0.0013, -0.0537, -0.0692, -0.0482, -0.0685,\n                          -0.0674, -0.0788, -0.0840, -0.0766, -0.0789, -0.0589, -0.1023, -0.0982,\n                          -0.1140, -0.0846, -0.1808, -0.0856, -0.0514, -0.0854, -0.0704, -0.0709,\n                          -0.0597, -0.0641, -0.0627, -0.0835, -0.0812, -0.0857, -0.1210, -0.0480,\n                          -0.0632, -0.0692, -0.0718, -0.0736, -0.0526, -0.0813, -0.0959, -0.0700,\n                          -0.1163, -0.0772, -0.0955, -0.0852, -0.0771, -0.0810, -0.0791, -0.0661,\n                          -0.0626, -0.0595, -0.0540, -0.1486, -0.0648, -0.0732, -0.0874, -0.0469,\n                          -0.0948, -0.0770, -0.0471, -0.0808, -0.1077, -0.0726, -0.1333, -0.0641,\n                          -0.0372, -0.0532, -0.0324, -0.0993, -0.0858, -0.0450, -0.0715, -0.0900,\n                          -0.0624, -0.0579, -0.0802, -0.0976, -0.0543, -0.0724, -0.0753, -0.0962,\n                          -0.0331, -0.0801, -0.0961, -0.0555, -0.0901, -0.0811, -0.0551, -0.0720,\n                          -0.0652, -0.0724, -0.0859, -0.0858, -0.0418, -0.0697, -0.1000, -0.0636,\n                          -0.0669, -0.0770, -0.0626, -0.1136, -0.0739, -0.0907, -0.0681, -0.0890,\n                          -0.0831, -0.0370, -0.0830, -0.1094, -0.0589, -0.1133, -0.0979, -0.0740,\n                          -0.0685, -0.0398, -0.0935, -0.0624, -0.0612, -0.1227, -0.0609, -0.0411,\n                          -0.0666, -0.1124, -0.0778, -0.0650, -0.0814, -0.0579, -0.0615, -0.0806,\n                          -0.0863, -0.0925, -0.1013, -0.0464, -0.0884, -0.0757, -0.0815, -0.0991,\n                          -0.0674, -0.0026, -0.0744, -0.0559, -0.1241, -0.0634, -0.0964, -0.0977,\n                          -0.1055, -0.0432, -0.1001, -0.0512, -0.0661, -0.0629, -0.0612, -0.1295,\n                          -0.0557, -0.0938, -0.0310, -0.0621, -0.0988, -0.0928, -0.0700, -0.0755,\n                          -0.0997, -0.0889, -0.0693, -0.1016, -0.0576, -0.0811, -0.0618, -0.0568,\n                          -0.0800, -0.0832, -0.0790, -0.0535, -0.0590, -0.0643, -0.0709, -0.1238,\n                          -0.0770, -0.0812, -0.0672, -0.0742, -0.0690, -0.0417, -0.0646, -0.0989,\n                          -0.0490, -0.0775, -0.0625, -0.0667, -0.0514, -0.0665, -0.1165, -0.0853,\n                          -0.0873, -0.0612, -0.0992, -0.0648, -0.0504, -0.0568, -0.0548, -0.0879,\n                          -0.0741, -0.0722, -0.0764, -0.0853, -0.0656, -0.0672, -0.1143, -0.0731,\n                          -0.0956, -0.1027, -0.0738, -0.0725, -0.0850, -0.0791, -0.0955, -0.1006,\n                          -0.0777, -0.1509, -0.0546, -0.0842, -0.0585, -0.0547, -0.0778, -0.0942,\n                          -0.0833, -0.1009, -0.0686, -0.0522, -0.0730, -0.0249, -0.0349, -0.1168,\n                          -0.0282, -0.0939, -0.0509, -0.0698, -0.1122, -0.0408, -0.0681, -0.1002,\n                          -0.0674, -0.1283, -0.0989, -0.0775, -0.0796, -0.1497, -0.0818, -0.0798,\n                          -0.0702, -0.0742, -0.0769, -0.0825, -0.1187, -0.0728, -0.0715, -0.1062,\n                          -0.0908, -0.0643, -0.0532, -0.0887, -0.0634, -0.0894, -0.0909, -0.0712,\n                          -0.0739, -0.0755, -0.0855, -0.0700, -0.0930, -0.0765, -0.1275, -0.0591,\n                          -0.0349, -0.0763, -0.0399, -0.0804, -0.0744, -0.0837, -0.0873, -0.0920,\n                          -0.0669, -0.0656, -0.0742, -0.0554, -0.0685, -0.0864, -0.0717, -0.0881,\n                          -0.1049, -0.0561, -0.0864, -0.0621, -0.1120, -0.0650, -0.0444, -0.1180,\n                          -0.0739, -0.0578, -0.0743, -0.0882, -0.0618, -0.0705, -0.1078, -0.0527,\n                          -0.0599, -0.0704, -0.0485, -0.0655, -0.1066, -0.0419, -0.1002, -0.0840,\n                          -0.1065, -0.0760, -0.0952, -0.0401, -0.0667, -0.0989, -0.0502, -0.0682,\n                          -0.0788, -0.0711, -0.0444, -0.0535, -0.0798, -0.0874, -0.0553, -0.0790,\n                          -0.0523, -0.0800, -0.0803, -0.0700, -0.0856, -0.0674, -0.1092, -0.0937,\n                          -0.0835, -0.0565, -0.1183, -0.0937, -0.1111, -0.0715, -0.0927, -0.0771]), max_val=tensor([0.0848, 0.1105, 0.0994, 0.0463, 0.0601, 0.0782, 0.0771, 0.0825, 0.0737,\n                          0.0891, 0.0814, 0.0478, 0.0598, 0.0850, 0.0679, 0.0614, 0.0674, 0.0740,\n                          0.0575, 0.1203, 0.0673, 0.0937, 0.0848, 0.0649, 0.0615, 0.0651, 0.0625,\n                          0.0695, 0.1083, 0.0700, 0.0629, 0.0657, 0.0446, 0.0306, 0.0339, 0.0614,\n                          0.0471, 0.0769, 0.0990, 0.0823, 0.0546, 0.0652, 0.1195, 0.0505, 0.0790,\n                          0.0999, 0.0730, 0.0622, 0.1255, 0.0473, 0.0527, 0.0674, 0.0806, 0.0615,\n                          0.0708, 0.1308, 0.0754, 0.0724, 0.0702, 0.0708, 0.0582, 0.1293, 0.0882,\n                          0.0541, 0.0722, 0.0730, 0.0721, 0.0653, 0.0807, 0.0734, 0.0587, 0.0657,\n                          0.0820, 0.0654, 0.0778, 0.1024, 0.0767, 0.0652, 0.0548, 0.0997, 0.0480,\n                          0.0927, 0.0874, 0.0865, 0.1067, 0.0662, 0.0787, 0.0993, 0.0582, 0.0016,\n                          0.0016, 0.0811, 0.0833, 0.0555, 0.0750, 0.1046, 0.0614, 0.0626, 0.0977,\n                          0.0575, 0.0741, 0.0716, 0.0644, 0.1058, 0.0633, 0.0740, 0.0648, 0.0881,\n                          0.0604, 0.0555, 0.0486, 0.1145, 0.0822, 0.0479, 0.0714, 0.0725, 0.1011,\n                          0.0022, 0.0715, 0.0789, 0.0838, 0.0682, 0.0787, 0.0640, 0.0613, 0.0635,\n                          0.0870, 0.0632, 0.0819, 0.0649, 0.0468, 0.1693, 0.0661, 0.0939, 0.0704,\n                          0.1188, 0.0853, 0.0974, 0.0304, 0.0570, 0.0586, 0.0653, 0.0794, 0.0561,\n                          0.1095, 0.0936, 0.0807, 0.0957, 0.0847, 0.0848, 0.0585, 0.0525, 0.0594,\n                          0.0716, 0.0693, 0.0734, 0.0791, 0.0686, 0.0682, 0.0674, 0.0955, 0.0415,\n                          0.0725, 0.0019, 0.0559, 0.0806, 0.0924, 0.1210, 0.0690, 0.1082, 0.0740,\n                          0.0661, 0.0640, 0.0349, 0.0752, 0.0708, 0.0859, 0.0583, 0.1062, 0.0496,\n                          0.0553, 0.0590, 0.0907, 0.1419, 0.0787, 0.0822, 0.0583, 0.0587, 0.0650,\n                          0.0815, 0.0828, 0.1007, 0.0954, 0.1000, 0.0751, 0.0843, 0.1004, 0.0817,\n                          0.0920, 0.0840, 0.0667, 0.0673, 0.0739, 0.0812, 0.0830, 0.0938, 0.0644,\n                          0.0993, 0.0516, 0.0800, 0.0790, 0.0708, 0.0999, 0.0660, 0.0831, 0.0769,\n                          0.0801, 0.1072, 0.0743, 0.0694, 0.0713, 0.0815, 0.0585, 0.0742, 0.0818,\n                          0.0633, 0.0947, 0.0700, 0.0694, 0.0631, 0.0708, 0.0703, 0.0017, 0.0404,\n                          0.1103, 0.0014, 0.0676, 0.0672, 0.0586, 0.0987, 0.0412, 0.0797, 0.1074,\n                          0.1120, 0.0660, 0.0536, 0.0595, 0.0893, 0.0894, 0.0579, 0.1232, 0.0998,\n                          0.0708, 0.0817, 0.0516, 0.0737, 0.1067, 0.0660, 0.0465, 0.0831, 0.0592,\n                          0.0906, 0.0905, 0.0492, 0.0723, 0.0700, 0.0691, 0.0714, 0.0616, 0.0814,\n                          0.0830, 0.0770, 0.1202, 0.0569, 0.1138, 0.0917, 0.0865, 0.0523, 0.0522,\n                          0.0943, 0.0845, 0.0561, 0.0543, 0.0974, 0.0727, 0.0477, 0.0656, 0.0374,\n                          0.1021, 0.0565, 0.0685, 0.0795, 0.1057, 0.1058, 0.0911, 0.0643, 0.0607,\n                          0.0642, 0.0474, 0.1034, 0.0800, 0.0439, 0.0823, 0.0518, 0.0614, 0.0568,\n                          0.0454, 0.1077, 0.0738, 0.0540, 0.0944, 0.0928, 0.0461, 0.0786, 0.1117,\n                          0.0881, 0.1011, 0.0623, 0.0698, 0.0773, 0.0656, 0.0752, 0.0629, 0.0700,\n                          0.0522, 0.0675, 0.1006, 0.0762, 0.0551, 0.0935, 0.0651, 0.0756, 0.0599,\n                          0.0749, 0.0544, 0.0795, 0.1011, 0.0517, 0.1215, 0.0979, 0.0785, 0.1079,\n                          0.0769, 0.0770, 0.0617, 0.0453, 0.1228, 0.1035, 0.0891, 0.0764, 0.0567,\n                          0.0675, 0.0661, 0.0777, 0.0697, 0.0791, 0.0693, 0.0714, 0.0717, 0.0670,\n                          0.1012, 0.0781, 0.0731, 0.0462, 0.0885, 0.0991, 0.0670, 0.0726, 0.1553,\n                          0.0016, 0.0670, 0.0516, 0.0802, 0.0646, 0.0570, 0.0851, 0.0940, 0.0418,\n                          0.0696, 0.0576, 0.0757, 0.0652, 0.1000, 0.0798, 0.0641, 0.0770, 0.0372,\n                          0.0759, 0.0985, 0.0776, 0.1126, 0.0814, 0.0648, 0.1299, 0.0578, 0.0772,\n                          0.0869, 0.0572, 0.0929, 0.0793, 0.0586, 0.0858, 0.0994, 0.0405, 0.0736,\n                          0.0556, 0.0620, 0.0879, 0.0626, 0.0846, 0.0776, 0.0684, 0.1129, 0.0476,\n                          0.0556, 0.1247, 0.0323, 0.0631, 0.0667, 0.0615, 0.0540, 0.0424, 0.0831,\n                          0.0614, 0.0919, 0.0728, 0.0989, 0.0575, 0.0654, 0.0775, 0.0428, 0.1076,\n                          0.0664, 0.0942, 0.0660, 0.0788, 0.0792, 0.0747, 0.1292, 0.0962, 0.1419,\n                          0.0878, 0.0779, 0.1018, 0.0650, 0.0509, 0.0636, 0.0900, 0.0596, 0.1122,\n                          0.0304, 0.0641, 0.0819, 0.0560, 0.0774, 0.0835, 0.1004, 0.1047, 0.0741,\n                          0.0622, 0.0629, 0.0361, 0.0474, 0.0851, 0.0256, 0.0833, 0.0540, 0.0646,\n                          0.0761, 0.0498, 0.0739, 0.0640, 0.0605, 0.0939, 0.0869, 0.0721, 0.0572,\n                          0.1073, 0.0813, 0.0822, 0.1080, 0.0863, 0.0666, 0.0595, 0.0745, 0.0742,\n                          0.0755, 0.0929, 0.1482, 0.0763, 0.0490, 0.0580, 0.1081, 0.0851, 0.0946,\n                          0.0712, 0.0884, 0.0761, 0.0581, 0.0719, 0.0851, 0.0799, 0.1257, 0.0525,\n                          0.0338, 0.0756, 0.0574, 0.0879, 0.0713, 0.0596, 0.0690, 0.0831, 0.0742,\n                          0.0747, 0.0538, 0.0597, 0.0687, 0.0605, 0.0722, 0.0733, 0.0832, 0.0761,\n                          0.0951, 0.0620, 0.0873, 0.1285, 0.0489, 0.0657, 0.0675, 0.0434, 0.1068,\n                          0.0706, 0.0566, 0.1008, 0.1359, 0.0532, 0.0900, 0.0549, 0.0592, 0.1112,\n                          0.0776, 0.0441, 0.0965, 0.0895, 0.0945, 0.0818, 0.1338, 0.0533, 0.0808,\n                          0.0601, 0.0468, 0.0652, 0.0901, 0.0752, 0.0483, 0.0575, 0.1012, 0.1332,\n                          0.0745, 0.0533, 0.0936, 0.1016, 0.0800, 0.1038, 0.0660, 0.0785, 0.0803,\n                          0.0649, 0.0736, 0.0606, 0.1169, 0.0900, 0.0833, 0.1053, 0.0714, 0.0536])\n                )\n              )\n            )\n            (2): ReLU6(inplace=True)\n          )\n          (1): Module(\n            (0): ConvBn2d(\n              576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False\n              (bn): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n                fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0106, 0.0082, 0.0133, 0.0029, 0.0106, 0.0114, 0.0022, 0.0116, 0.0079,\n                        0.0175, 0.0196, 0.0190, 0.0180, 0.0153, 0.0034, 0.0154, 0.0111, 0.0021,\n                        0.0120, 0.0201, 0.0041, 0.0072, 0.0105, 0.0104, 0.0208, 0.0371, 0.0017,\n                        0.0465, 0.0025, 0.0144, 0.0185, 0.0015, 0.0190, 0.0156, 0.0284, 0.0103,\n                        0.0153, 0.0237, 0.0095, 0.0167, 0.0144, 0.0167, 0.0131, 0.0158, 0.0021,\n                        0.0140, 0.0439, 0.0073, 0.0158, 0.0212, 0.0070, 0.0106, 0.0054, 0.0025,\n                        0.0040, 0.0151, 0.0185, 0.0021, 0.0089, 0.0146, 0.0095, 0.0232, 0.0147,\n                        0.0174, 0.0287, 0.0190, 0.0177, 0.0161, 0.0059, 0.0147, 0.0153, 0.0027,\n                        0.0170, 0.0124, 0.0099, 0.0171, 0.0112, 0.0187, 0.0189, 0.0126, 0.0151,\n                        0.0200, 0.0018, 0.0032, 0.0020, 0.0087, 0.0228, 0.0153, 0.0031, 0.0010,\n                        0.0019, 0.0108, 0.0037, 0.0124, 0.0226, 0.0084, 0.0024, 0.0086, 0.0121,\n                        0.0495, 0.0031, 0.0022, 0.0234, 0.0051, 0.0139, 0.0172, 0.0161, 0.0115,\n                        0.0169, 0.0031, 0.0038, 0.0096, 0.0020, 0.0126, 0.0159, 0.0230, 0.0243,\n                        0.0010, 0.0179, 0.0060, 0.0153, 0.0071, 0.0156, 0.0130, 0.0134, 0.0082,\n                        0.0182, 0.0152, 0.0134, 0.0188, 0.0196, 0.0729, 0.0034, 0.0456, 0.0154,\n                        0.0020, 0.0181, 0.0261, 0.0360, 0.0120, 0.0033, 0.0033, 0.0165, 0.0259,\n                        0.0046, 0.0149, 0.0194, 0.0056, 0.0210, 0.0083, 0.0047, 0.0164, 0.0168,\n                        0.0020, 0.0149, 0.0117, 0.0105, 0.0144, 0.0047, 0.0095, 0.0151, 0.0112,\n                        0.0033, 0.0009, 0.0078, 0.0252, 0.0091, 0.0129, 0.0133, 0.0113, 0.0108,\n                        0.0179, 0.0061, 0.0209, 0.0034, 0.0027, 0.0242, 0.0107, 0.0023, 0.0150,\n                        0.0132, 0.0026, 0.0020, 0.0206, 0.0034, 0.0133, 0.0030, 0.0264, 0.0026,\n                        0.0163, 0.0129, 0.0226, 0.0054, 0.0122, 0.0158, 0.0139, 0.0142, 0.0204,\n                        0.0143, 0.0171, 0.0105, 0.0163, 0.0245, 0.0026, 0.0035, 0.0168, 0.0138,\n                        0.0189, 0.0194, 0.0127, 0.0123, 0.0236, 0.0024, 0.0076, 0.0032, 0.0113,\n                        0.0024, 0.0150, 0.0031, 0.0082, 0.0151, 0.0030, 0.0192, 0.0039, 0.0028,\n                        0.0135, 0.0173, 0.0114, 0.0134, 0.0159, 0.0171, 0.0162, 0.0009, 0.0173,\n                        0.0020, 0.0004, 0.0031, 0.0084, 0.0277, 0.0105, 0.0113, 0.0064, 0.0056,\n                        0.0216, 0.0124, 0.0084, 0.0021, 0.0144, 0.0173, 0.0161, 0.0133, 0.0026,\n                        0.0143, 0.0076, 0.0171, 0.0075, 0.0081, 0.0264, 0.0220, 0.0035, 0.0044,\n                        0.0146, 0.0120, 0.0110, 0.0177, 0.0022, 0.0129, 0.0068, 0.0143, 0.0028,\n                        0.0020, 0.0146, 0.0028, 0.0237, 0.0127, 0.0030, 0.0034, 0.0077, 0.0028,\n                        0.0166, 0.0118, 0.0089, 0.0123, 0.0201, 0.0021, 0.0066, 0.0097, 0.0142,\n                        0.0216, 0.0112, 0.0081, 0.0142, 0.0132, 0.0251, 0.0107, 0.0157, 0.0211,\n                        0.0178, 0.0326, 0.0226, 0.0046, 0.0114, 0.0127, 0.0120, 0.0033, 0.0147,\n                        0.0064, 0.0036, 0.0120, 0.0148, 0.0111, 0.0069, 0.0238, 0.0020, 0.0268,\n                        0.0022, 0.0243, 0.0169, 0.0189, 0.0102, 0.0076, 0.0113, 0.0115, 0.0152,\n                        0.0167, 0.0082, 0.0049, 0.0071, 0.0220, 0.0041, 0.0231, 0.0121, 0.0027,\n                        0.0127, 0.0169, 0.0023, 0.0193, 0.0159, 0.0156, 0.0025, 0.0161, 0.0248,\n                        0.0175, 0.0094, 0.0138, 0.0157, 0.0133, 0.0197, 0.0163, 0.0252, 0.0030,\n                        0.0117, 0.0140, 0.0108, 0.0038, 0.0096, 0.0110, 0.0146, 0.0043, 0.0130,\n                        0.0021, 0.0227, 0.0019, 0.0172, 0.0019, 0.0138, 0.0164, 0.0027, 0.0029,\n                        0.0016, 0.0104, 0.0138, 0.0108, 0.0357, 0.0180, 0.0144, 0.0155, 0.0199,\n                        0.0033, 0.0128, 0.0030, 0.0110, 0.0084, 0.0199, 0.0165, 0.0130, 0.0083,\n                        0.0062, 0.0156, 0.0049, 0.0030, 0.0259, 0.0166, 0.0106, 0.0173, 0.0031,\n                        0.0220, 0.0109, 0.0140, 0.0024, 0.0208, 0.0077, 0.0055, 0.0420, 0.0086,\n                        0.0152, 0.0145, 0.0185, 0.0156, 0.0189, 0.0136, 0.0145, 0.0068, 0.0135,\n                        0.0221, 0.0038, 0.0227, 0.0190, 0.0025, 0.0023, 0.0258, 0.0111, 0.0095,\n                        0.0201, 0.0025, 0.0123, 0.0019, 0.0095, 0.0107, 0.0200, 0.0106, 0.0019,\n                        0.0115, 0.0022, 0.0090, 0.0119, 0.0033, 0.0206, 0.0049, 0.0179, 0.0112,\n                        0.0041, 0.0105, 0.0071, 0.0022, 0.0018, 0.0125, 0.0024, 0.0182, 0.0129,\n                        0.0179, 0.0182, 0.0251, 0.0097, 0.0038, 0.0055, 0.0026, 0.0086, 0.0030,\n                        0.0231, 0.0030, 0.0188, 0.0111, 0.0176, 0.0292, 0.0029, 0.0307, 0.0060,\n                        0.0167, 0.0280, 0.0033, 0.0178, 0.0027, 0.0111, 0.0104, 0.0026, 0.0096,\n                        0.0043, 0.0033, 0.0098, 0.0231, 0.0157, 0.0095, 0.0022, 0.0020, 0.0243,\n                        0.0017, 0.0036, 0.0142, 0.0107, 0.0185, 0.0098, 0.0028, 0.0262, 0.0105,\n                        0.0081, 0.0029, 0.0026, 0.0163, 0.0026, 0.0157, 0.0023, 0.0120, 0.0117,\n                        0.0160, 0.0172, 0.0179, 0.0025, 0.0093, 0.0163, 0.0026, 0.0020, 0.0298,\n                        0.0026, 0.0131, 0.0164, 0.0026, 0.0087, 0.0145, 0.0019, 0.0022, 0.0177,\n                        0.0149, 0.0049, 0.0129, 0.0049, 0.0285, 0.0128, 0.0227, 0.0170, 0.0166,\n                        0.0059, 0.0302, 0.0053, 0.0100, 0.0271, 0.0128, 0.0154, 0.0182, 0.0114,\n                        0.0154, 0.0086, 0.0147, 0.0023, 0.0192, 0.0227, 0.0088, 0.0107, 0.0286,\n                        0.0106, 0.0264, 0.0107, 0.0035, 0.0222, 0.0242, 0.0175, 0.0140, 0.0015,\n                        0.0097, 0.0207, 0.0065, 0.0214, 0.0052, 0.0104, 0.0144, 0.0214, 0.0185,\n                        0.0113, 0.0023, 0.0080, 0.0107, 0.0076, 0.0026, 0.0146, 0.0028, 0.0053]), zero_point=tensor([   0, -128,    0,  127,    0, -128,  127,    0,    0,    0,    0,    0,\n                           0,    0,  127,    0,    0,  127,    0,    0,  127, -128,    0,    0,\n                           0,    0,  127,    0,  127,    0,    0,  127,    0,    0,    0,    0,\n                           0,    0,    0,    0,    0,    0,    0,    0,  127,    0,    0, -128,\n                        -128,    0, -128,    0, -128,  127,  127,    0,    0,  127,    0,    0,\n                           0,    0,    0,    0,    0,    0,    0,    0, -128,    0,    0,  127,\n                           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,  127,  127,\n                         127, -128,    0,    0,  127,    0,    0,    0,  127,    0,    0,    0,\n                         127,    0,    0,    0,  127,  127,    0, -128,    0,    0,    0,    0,\n                           0,  127,  127, -128,  127,    0,    0,    0,    0,    0,    0,    0,\n                           0,  127,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         127,    0,    0,  127,    0,    0,    0,    0,  127,  127,    0,    0,\n                         127,    0,    0, -128,    0, -128,  127,    0,    0,  127,    0, -128,\n                           0,    0,    0,    0,    0,    0,  127,    0, -128,    0, -128,    0,\n                           0,    0,    0,    0,    0,    0,  127,  127,    0,    0,  127,    0,\n                           0,  127,  127,    0,    0,    0,  127,    0,  127,    0, -128,    0,\n                        -128,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,  127,\n                         127,    0,    0,    0,    0,    0,    0,    0,  127,    0,  127,    0,\n                         127,    0,  127, -128,    0,  127,    0,  127,  127,    0,    0,    0,\n                           0,    0,    0,    0,    0,    0,  127,    0,  127, -128,    0,    0,\n                           0,    0,    0,    0,    0,    0,  127,    0,    0,    0,    0,  127,\n                           0,    0,    0,    0,    0,    0,    0, -128,  127,    0,    0, -128,\n                           0,  127,    0,    0,    0,  127,  127,    0,  127,    0,    0,  127,\n                        -128, -128,  127,    0,    0, -128,    0,    0,  127,    0,    0,    0,\n                           0,    0, -128,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         127,    0,    0,    0,  127,    0, -128,  127,    0,    0,    0, -128,\n                           0,  127,    0,  127,    0,    0,    0,    0,    0,    0,    0,    0,\n                           0, -128, -128, -128,    0,    0,    0,    0,  127,    0,    0,  127,\n                           0,    0,    0,  127,    0,    0,    0,    0,    0,    0,    0,    0,\n                           0,    0,  127, -128,    0,    0,  127,    0,    0,    0,  127,    0,\n                         127,    0,  127,    0,  127,    0,    0,  127,  127,    0,    0,    0,\n                           0,    0,    0,    0,    0,    0,  127,    0,  127,    0, -128,    0,\n                           0,    0, -128,    0,    0,    0,  127,    0,    0,    0,    0,  127,\n                           0, -128,    0,  127,    0, -128,    0,    0, -128,    0,    0,    0,\n                           0,    0,    0,    0, -128,    0,    0,  127,    0,    0,  127,  127,\n                           0,    0, -128,    0,  127, -128,  127, -128,    0,    0, -128,  127,\n                           0,  127,    0,    0,  127,    0, -128,    0,    0,  127,    0, -128,\n                         127,  127,    0,  127,    0,    0,    0,    0,    0,    0, -128,    0,\n                         127, -128,  127,    0,  127, -128,    0,    0,    0,  127,    0, -128,\n                           0,    0,  127,    0,  127,    0,    0,  127, -128,    0,  127,    0,\n                           0,    0,    0,  127,  127,    0,  127,    0,    0, -128,    0,    0,\n                         127,    0,    0,    0, -128,  127,    0,  127,    0,  127,    0,    0,\n                           0,    0,    0,  127,    0,    0,  127,  127,    0,  127,    0,    0,\n                         127,    0,    0,  127,  127,    0,    0, -128,    0, -128,    0,    0,\n                           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                           0,  127,    0,  127,    0,    0,    0,  127,    0,    0,    0,    0,\n                         127,    0,    0,    0,    0,  127, -128,    0,    0,    0,    0,    0,\n                           0,    0,    0,    0,  127, -128,    0,    0,  127,    0, -128, -128],\n                       dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n                (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n                  min_val=tensor([-9.0761e-01,  2.4125e-01, -1.6989e+00, -7.4799e-01, -1.3238e-01,\n                           6.8685e-03, -5.4840e-01, -1.0779e+00, -9.7146e-01, -1.0771e+00,\n                          -6.3141e-01, -1.2585e+00, -1.2180e-01, -1.2725e-01, -8.6272e-01,\n                          -1.9671e+00, -1.4173e+00, -5.2607e-01, -1.5403e+00, -7.6729e-01,\n                          -1.0506e+00,  1.1646e-01, -1.3450e+00, -7.2275e-01, -2.2750e-01,\n                          -1.7985e+00, -4.2913e-01, -8.8748e-01, -6.3303e-01, -3.5823e-01,\n                          -1.3884e+00, -3.7766e-01, -1.8271e-01, -1.9979e+00, -2.1795e+00,\n                          -5.2489e-01, -1.7181e+00, -7.9852e-01, -8.4103e-01, -2.1423e+00,\n                          -6.8952e-01, -6.6590e-01, -8.1478e-02, -5.9014e-01, -5.3782e-01,\n                          -8.1737e-01, -2.2120e+00,  6.9170e-02,  2.5936e-01, -1.5072e+00,\n                           1.5078e-02, -1.3606e+00,  6.8365e-02, -6.3776e-01, -1.0199e+00,\n                          -6.4467e-02, -1.0991e+00, -5.3798e-01, -1.0236e+00, -5.0196e-01,\n                          -1.2129e+00, -2.9734e+00, -5.0806e-01, -1.4896e+00, -1.9018e-01,\n                          -1.3938e-01, -4.8296e-02, -1.3785e+00,  1.7169e-01, -1.8847e+00,\n                          -1.3430e+00, -6.8941e-01, -2.1778e+00, -1.5836e+00, -8.9546e-01,\n                          -6.3354e-01, -9.0285e-01, -5.6324e-02, -1.4617e+00, -1.2645e+00,\n                          -1.9285e+00, -4.1442e-01, -4.6922e-01, -8.1434e-01, -5.1573e-01,\n                           1.5168e-01, -1.2103e+00, -2.4643e-01, -7.8152e-01, -7.4329e-03,\n                          -2.2028e-01, -4.0090e-01, -9.5409e-01, -9.1379e-01, -1.0957e+00,\n                          -8.8447e-01, -6.0683e-01, -1.0099e+00, -1.5478e+00, -2.2249e+00,\n                          -7.9522e-01, -5.5610e-01, -2.9958e+00,  7.9045e-02, -1.1588e+00,\n                          -3.8891e-01, -1.4525e+00, -1.3556e+00, -1.0486e+00, -7.7877e-01,\n                          -9.6626e-01,  5.9047e-02, -5.0824e-01, -1.2962e+00, -2.0322e+00,\n                          -1.9897e+00, -5.0222e-01, -9.5861e-02, -2.2860e+00, -7.7138e-01,\n                          -5.8953e-01, -1.8107e+00, -2.0002e+00, -1.6594e+00, -1.0817e+00,\n                          -1.0504e+00, -1.6729e+00, -9.9207e-01, -1.7118e+00, -3.1330e-02,\n                          -1.1560e+00, -9.3363e+00, -8.6635e-01, -3.2805e+00, -1.4721e+00,\n                          -5.0595e-01, -7.5473e-01, -1.0070e+00, -3.8529e-01, -1.5098e+00,\n                          -8.4900e-01, -8.4450e-01, -7.5952e-01, -9.4809e-02, -1.1638e+00,\n                          -7.3695e-01, -1.3866e+00,  2.7764e-01, -1.2590e+00,  1.1382e-01,\n                          -1.2042e+00, -3.5094e-01, -1.1200e+00, -4.9795e-01, -1.6342e+00,\n                           1.6949e-01, -7.4960e-01, -1.7872e-01, -1.7098e-02, -2.0030e-01,\n                          -1.2244e+00, -1.4393e+00, -8.2900e-01, -4.3033e-02,  1.6576e-01,\n                          -7.2687e-01,  5.6658e-02, -1.6569e+00, -1.3344e+00, -6.6338e-01,\n                          -6.7758e-01, -2.2879e+00, -7.1728e-01, -2.4807e+00, -8.7406e-01,\n                          -7.0006e-01, -1.1920e+00, -9.7000e-01, -5.8343e-01, -1.3208e+00,\n                          -1.1299e+00, -6.5991e-01, -5.1171e-01, -1.1722e-01, -4.3631e-01,\n                          -4.2933e-01, -7.7550e-01, -1.0433e+00, -6.6267e-01, -1.2992e-01,\n                           3.0425e-02, -8.9462e-01,  5.3156e-02, -1.5586e+00, -7.0255e-01,\n                          -6.7795e-01, -4.9610e-01, -7.5047e-01, -9.4336e-01, -1.5798e+00,\n                          -1.0021e+00, -1.3566e+00, -2.0067e+00, -6.5938e-01, -8.8141e-01,\n                          -1.0512e+00, -1.0739e+00, -4.5321e-02, -6.9006e-01, -1.2704e+00,\n                          -1.5744e+00, -3.0240e+00, -6.0655e-01, -8.2091e-01, -8.0754e-01,\n                          -6.8969e-01, -6.0233e-01, -1.8720e+00, -8.0003e-01,  1.1559e-01,\n                          -1.1972e+00, -7.7085e-01, -9.4560e-01, -9.9978e-01, -7.1115e-01,\n                          -1.1128e+00, -1.1826e+00, -1.4640e+00, -8.1898e-01, -7.4657e-01,\n                          -1.0271e+00, -1.1891e+00, -5.5872e-02, -1.1566e+00, -5.2005e-01,\n                          -2.4770e-02, -7.8822e-01,  7.1599e-02, -9.5204e-01, -1.0465e+00,\n                          -9.8532e-01, -8.1448e-01, -7.1771e-01, -2.7665e+00, -8.1496e-01,\n                          -3.8013e-01, -5.4114e-01, -1.5522e-01, -1.0718e+00, -1.5756e+00,\n                          -1.2669e-01, -6.5844e-01, -1.5037e+00, -9.7609e-01, -9.4112e-01,\n                          -9.6162e-01, -1.0051e+00, -1.0593e+00, -6.6989e-01,  4.5412e-02,\n                          -1.1145e+00, -9.1014e-01, -1.1160e+00,  1.4833e-01, -1.2337e+00,\n                          -5.6499e-01, -1.6520e+00, -8.6772e-01, -9.0674e-01, -7.1180e-01,\n                          -5.1152e-01, -1.1511e-01, -7.1496e-01, -5.8325e-01, -6.9645e-01,\n                          -7.5549e-01,  2.3379e-01,  2.0963e-02, -7.2461e-01, -2.2626e-01,\n                          -1.5100e+00,  5.2124e-02, -9.4120e-01, -1.9009e+00, -5.3766e-01,\n                          -8.4031e-01, -1.1684e+00, -1.2321e+00, -1.2905e+00, -1.2270e+00,\n                           2.1324e-01, -9.1406e-01, -1.1303e+00, -3.7558e-01, -9.4272e-01,\n                          -9.2856e-02, -1.3946e+00, -4.0956e-01, -7.3752e-01, -9.0615e-01,\n                          -1.1857e+00, -1.4656e+00, -1.1615e+00, -7.7412e-01, -8.5364e-01,\n                          -1.5253e+00,  4.2179e-02, -9.2916e-01, -9.2190e-01, -1.8889e+00,\n                          -1.0850e+00,  4.1456e-01, -3.0415e+00, -5.0181e-01, -7.3291e-01,\n                          -5.5787e-01, -3.1072e+00, -8.9903e-01, -2.4142e+00, -7.2306e-01,\n                          -8.7629e-01, -7.1804e-01, -8.9509e-01, -1.6919e-01, -1.7469e+00,\n                           9.5731e-02,  5.3219e-02,  1.7639e-01, -9.4872e-01, -5.2477e-01,\n                          -2.2738e+00, -4.6832e-01, -6.8518e-01, -5.4862e-01, -7.1569e-01,\n                          -5.9845e-01, -2.2781e-01, -1.5030e+00, -7.3724e-01, -6.4654e-01,\n                          -1.3932e+00, -4.5472e-01, -1.0097e+00, -1.2044e+00, -1.0309e+00,\n                          -1.7829e+00, -3.6365e-01, -6.2198e-01, -1.6749e-01, -8.9379e-01,\n                          -7.7594e-01,  4.7917e-01, -4.1854e-01, -1.1582e+00, -9.7152e-01,\n                          -1.1284e-01, -9.2155e-01, -1.5810e+00, -1.0946e+00, -1.6638e+00,\n                          -5.3656e-01, -4.3504e-01, -4.9557e-01, -1.3128e+00, -4.9388e-01,\n                          -8.9265e-01, -1.3410e+00, -6.9610e-01, -7.3353e-01, -6.1070e-02,\n                          -9.5425e-01, -1.2853e+00, -4.9775e-01, -1.5187e+00, -2.3044e+00,\n                          -8.1588e-01, -5.1573e-01, -1.7756e+00, -8.3901e-01, -1.1262e+00,\n                          -7.6272e-01, -1.0747e+00,  7.3352e-02, -1.7255e+00, -7.7184e-01,\n                          -1.2722e+00,  2.2198e-01, -7.8942e-01, -9.6655e-01, -6.3319e-01,\n                          -7.7616e-01, -3.3703e-01, -1.5589e+00, -9.4091e-01, -7.1945e-02,\n                          -7.9671e-01, -7.8137e-01,  2.1806e-01, -1.2423e+00, -6.1307e-01,\n                          -1.5036e+00,  4.9133e-02, -7.0836e-01, -2.1911e+00,  8.0628e-02,\n                          -1.6275e-01, -9.6588e-01, -1.4506e+00, -1.2336e+00, -2.2270e-01,\n                          -7.4806e-01, -1.8585e+00,  9.2781e-02, -1.2360e+00, -8.5300e-01,\n                          -9.6961e-01, -2.9056e+00, -1.1134e+00, -6.3304e-01, -5.7856e-01,\n                          -2.4319e+00, -1.4271e+00,  1.1957e-01, -1.7790e-01, -6.4564e-01,\n                           8.8684e-02, -4.7375e-01,  2.0283e-01, -1.3711e+00, -1.1442e+00,\n                           7.3048e-01, -4.7802e-01, -7.1930e-01, -5.7087e-01, -1.0951e+00,\n                          -1.1995e+00, -8.4422e-01, -1.5341e-01,  3.0300e-02, -1.8068e+00,\n                          -9.1794e-01, -1.0421e+00, -1.3453e+00,  1.0481e-01, -5.6961e-01,\n                          -4.5269e-01, -1.4835e+00, -6.0090e-01, -9.4497e-01, -5.3238e-01,\n                          -2.2909e+00, -1.5617e+00, -6.6647e-01, -1.2412e+00,  1.1565e-01,\n                          -6.9995e-01, -6.6670e-01,  5.6259e-02, -7.6847e-01, -1.9942e-01,\n                          -7.5263e-01,  1.8332e-01, -7.5929e-01, -7.5859e-02, -3.7370e+00,\n                          -7.4018e-01, -7.4586e-01,  5.1877e-02, -6.8168e-01, -6.9002e-01,\n                          -8.4912e-01, -1.0588e+00, -6.9816e-01, -1.4216e+00, -1.2662e+00,\n                          -6.6305e-01,  5.7070e-02, -5.4539e-01, -8.4844e-01, -7.5163e-01,\n                          -2.0907e-01, -2.0146e+00, -6.6467e-01, -5.6402e-01, -5.0832e-01,\n                          -1.7006e-02, -4.2444e-01, -4.6715e-01, -2.2273e-01,  2.3255e-01,\n                          -2.3243e+00, -1.0583e+00, -7.1339e-01, -2.0970e+00, -2.5093e-01,\n                          -1.0325e+00,  3.3272e-01, -6.7521e-01, -7.0470e-01, -6.5435e-01,\n                          -1.8138e+00, -5.7910e-01, -1.0620e+00, -1.4995e+00, -2.0476e+00,\n                          -1.0001e+00, -2.0899e+00, -6.4903e-01, -8.7653e-01, -1.8095e+00,\n                          -6.7222e-01, -4.9948e-01, -1.3838e+00, -6.5416e-01, -1.3542e+00,\n                          -3.4792e-02, -6.7002e-01, -1.1161e+00, -1.4787e+00, -4.7912e-01,\n                          -5.5515e-01, -1.5972e+00, -5.0790e-01,  2.9028e-01, -1.6501e+00,\n                           3.5980e-02, -1.0676e+00, -9.3916e-01, -2.9092e+00, -1.1175e+00,\n                          -7.8733e-01, -7.5447e-01, -7.4845e-01, -6.7376e-01, -2.7720e-02,\n                          -1.5131e+00, -1.9848e-01, -1.2955e+00, -2.1810e+00, -1.4594e+00,\n                          -1.0850e-01, -2.1854e+00, -9.5817e-01, -5.8526e-01, -2.4533e+00,\n                          -8.4462e-01, -1.1319e+00, -2.7350e+00, -3.6639e+00, -1.3570e+00,\n                          -1.3588e+00, -1.0542e+00, -8.8680e-01, -4.4502e-02, -1.7227e+00,\n                          -1.2705e+00, -1.7961e+00, -3.9211e-01,  5.5850e-02, -1.2765e+00,\n                          -7.6627e-01, -2.6115e+00, -6.6025e-01, -7.8873e-01, -5.8893e-02,\n                          -6.7104e-01, -1.9143e+00, -1.6513e-02, -5.8864e-01,  1.4983e-01,\n                          -1.3674e+00, -2.4890e-01, -6.5962e-01, -1.1588e-01,  1.5480e-01,\n                           1.8554e-01]), max_val=tensor([ 1.3504e+00,  2.0789e+00,  1.6280e+00, -2.2942e-01,  1.3428e+00,\n                           2.8998e+00, -5.3314e-02,  1.4700e+00,  9.9801e-01,  2.2183e+00,\n                           2.4843e+00,  2.4096e+00,  2.2891e+00,  1.9461e+00, -1.8490e-01,\n                           1.6499e+00,  9.1653e-01, -1.2033e-01,  1.0867e+00,  2.5554e+00,\n                          -4.5527e-02,  1.8348e+00,  1.2598e+00,  1.3230e+00,  2.6388e+00,\n                           4.7067e+00, -2.2331e-01,  5.8995e+00, -2.6621e-01,  1.8275e+00,\n                           2.3449e+00, -1.6017e-01,  2.4150e+00,  7.4728e-02,  3.6124e+00,\n                           1.3126e+00,  1.9455e+00,  3.0123e+00,  1.2018e+00,  1.0028e+00,\n                           1.8272e+00,  2.1148e+00,  1.6640e+00,  2.0016e+00, -3.6420e-01,\n                           1.7824e+00,  5.5723e+00,  1.8632e+00,  4.0217e+00,  2.6956e+00,\n                           1.7801e+00,  1.1256e+00,  1.3706e+00, -2.5892e-01, -2.9958e-01,\n                           1.9197e+00,  2.3501e+00, -1.9718e-01,  1.1293e+00,  1.8531e+00,\n                           1.0125e+00,  2.0309e+00,  1.8711e+00,  2.2099e+00,  3.6390e+00,\n                           2.4183e+00,  2.2530e+00,  2.0398e+00,  1.5028e+00,  1.6277e+00,\n                           1.9475e+00, -2.1701e-01,  1.3340e+00,  1.0509e+00,  1.2615e+00,\n                           2.1658e+00,  1.4199e+00,  2.3806e+00,  2.4011e+00,  1.6055e+00,\n                           9.8048e-01,  2.5462e+00, -1.7477e-01, -2.3300e-01, -3.2250e-01,\n                           2.2065e+00,  2.8925e+00,  1.9387e+00, -2.0007e-01,  1.3156e-01,\n                           2.4420e-01,  1.3665e+00, -4.6207e-01,  1.5691e+00,  2.8659e+00,\n                           1.0691e+00, -2.5026e-01,  1.0908e+00,  9.7961e-01,  6.2851e+00,\n                          -1.1580e-01, -2.8047e-01,  1.6736e+00,  1.3076e+00,  1.7638e+00,\n                           2.1786e+00,  2.0394e+00,  1.4588e+00,  2.1521e+00, -3.5303e-01,\n                          -2.8297e-01,  2.4564e+00, -3.2333e-01,  1.5973e+00,  2.0126e+00,\n                           2.9162e+00,  3.0845e+00,  1.2665e-01,  1.8891e+00,  3.6859e-02,\n                           1.9379e+00, -2.7035e-01,  1.4902e+00,  8.2215e-01,  1.6991e+00,\n                           8.0702e-01,  2.3128e+00,  1.9262e+00,  9.9229e-01,  2.3851e+00,\n                           2.4892e+00,  7.5508e+00, -4.1352e-01,  5.7916e+00,  1.9512e+00,\n                          -2.3803e-01,  2.2946e+00,  3.3104e+00,  4.5764e+00,  1.5254e+00,\n                          -1.8094e-01, -2.2070e-01,  2.0906e+00,  3.2869e+00, -2.7290e-01,\n                           1.8898e+00,  2.4632e+00,  1.4185e+00,  2.6635e+00,  2.1274e+00,\n                          -4.4912e-02,  2.0816e+00,  2.1331e+00, -2.3966e-01,  1.8904e+00,\n                           2.9714e+00,  1.3362e+00,  1.8278e+00,  6.0108e-01,  1.2054e+00,\n                           1.9239e+00,  4.4934e-02, -8.3686e-02,  1.0890e-01,  1.9808e+00,\n                           3.2003e+00,  2.3245e+00,  1.2681e+00,  1.6840e+00,  1.4351e+00,\n                           1.3764e+00,  1.9409e+00,  7.7791e-01,  2.6583e+00, -2.2910e-01,\n                          -3.3392e-01,  3.0774e+00,  1.3535e+00, -2.5444e-01,  1.9080e+00,\n                           1.6789e+00, -3.0743e-01, -2.9146e-01,  2.6101e+00,  2.4294e-01,\n                           1.6843e+00, -4.7076e-01,  3.3527e+00, -1.8452e-01,  2.0754e+00,\n                           3.2971e+00,  2.8665e+00,  1.3775e+00,  1.4563e+00,  2.0019e+00,\n                           1.7665e+00,  1.7997e+00,  2.5951e+00,  1.8205e+00,  2.1701e+00,\n                           1.3388e+00,  2.0754e+00,  3.1148e+00, -2.9452e-01, -3.1171e-01,\n                           2.1280e+00,  1.7534e+00,  2.3969e+00,  2.4701e+00,  1.6100e+00,\n                           1.4450e+00,  1.1527e+00, -6.8548e-02,  9.6923e-01, -1.5001e-01,\n                           1.4388e+00, -2.4240e-01,  1.8988e+00, -2.7447e-01,  2.0882e+00,\n                           1.9192e+00, -2.9601e-01,  2.4340e+00, -2.2698e-01, -1.3282e-01,\n                           1.7087e+00,  2.1913e+00,  1.1563e+00,  1.7030e+00,  2.0203e+00,\n                           2.1759e+00,  2.0592e+00,  1.1324e-01,  2.1953e+00, -2.6154e-01,\n                           4.5911e-02, -3.4065e-01,  2.1365e+00,  3.5235e+00,  1.3393e+00,\n                           1.4308e+00,  2.0850e-02,  5.4714e-01,  2.6211e+00,  1.5721e+00,\n                           1.0659e+00, -1.2549e-01,  1.8272e+00,  2.2008e+00,  2.0510e+00,\n                           1.6925e+00, -1.0252e-01,  1.8191e+00,  2.4355e-02,  2.1765e+00,\n                           1.4348e-01,  1.0293e+00,  3.3576e+00,  2.7993e+00,  8.8332e-01,\n                          -2.3761e-01,  1.8590e+00,  1.5242e+00,  2.8017e+00,  2.2419e+00,\n                          -7.6105e-02,  1.0704e+00,  7.7075e-01,  1.8156e+00, -2.6037e-01,\n                          -1.9449e-01,  1.8491e+00, -4.6110e-01,  3.0080e+00,  1.6169e+00,\n                          -2.7189e-01,  8.5919e-01,  1.9548e+00, -4.2381e-03,  2.1122e+00,\n                           1.2433e+00,  2.2743e+00,  1.5678e+00,  2.5501e+00, -3.5528e-01,\n                           3.0575e-01,  1.2334e+00,  1.8048e+00,  2.7406e+00,  1.4230e+00,\n                           2.0692e+00,  1.8061e+00,  1.6821e+00,  3.1884e+00,  1.3562e+00,\n                           1.9982e+00,  2.6768e+00,  2.2579e+00,  4.1424e+00,  2.8688e+00,\n                          -3.3292e-01,  1.3606e-01,  1.6093e+00,  1.5276e+00, -2.1983e-01,\n                           1.8698e+00,  1.6266e+00, -3.6058e-01,  1.5292e+00,  1.2125e+00,\n                           1.4120e+00,  1.7554e+00,  2.1049e+00, -1.3425e-01,  3.4076e+00,\n                          -4.0142e-01,  1.7864e+00,  2.1473e+00,  2.2502e+00,  1.2984e+00,\n                           9.6543e-01,  1.4317e+00,  1.4590e+00,  1.9275e+00,  2.1214e+00,\n                           2.1018e+00,  1.2607e+00,  1.8090e+00,  2.7909e+00,  1.5920e-01,\n                           2.9395e+00,  1.5345e+00, -1.6800e-01,  1.6115e+00,  2.1480e+00,\n                          -2.2404e-01,  2.4535e+00,  2.0232e+00,  1.9756e+00, -1.8701e-01,\n                           2.0497e+00,  3.1548e+00,  2.2256e+00,  1.1096e+00,  1.7575e+00,\n                           1.9957e+00,  1.6889e+00,  2.4965e+00,  2.0747e+00,  3.2027e+00,\n                          -2.3157e-01,  2.9895e+00,  1.7766e+00,  1.3758e+00, -1.8775e-01,\n                           1.2151e+00,  1.3983e+00,  1.8523e+00, -1.9173e-01,  1.3765e+00,\n                          -2.4186e-01,  2.8821e+00, -2.1684e-01,  2.1887e+00, -1.9241e-01,\n                           1.7514e+00,  2.0822e+00, -3.4601e-01, -1.6744e-01,  1.9685e-01,\n                           1.3201e+00,  1.7481e+00,  1.3727e+00,  4.5389e+00,  1.2374e+00,\n                           1.8240e+00,  1.9738e+00,  2.5243e+00, -1.5376e-01,  1.6272e+00,\n                          -2.5011e-01,  1.3971e+00,  2.1466e+00,  2.5286e+00,  2.0924e+00,\n                           1.6556e+00,  2.1109e+00,  2.8607e-01,  1.9852e+00,  1.8983e-01,\n                          -4.7725e-01,  3.2888e+00,  2.1120e+00,  1.3485e+00,  2.2013e+00,\n                          -2.5258e-01,  2.8002e+00,  2.7711e+00,  1.7782e+00, -2.8992e-01,\n                           2.6463e+00,  1.9719e+00,  4.2263e-01,  5.3311e+00,  2.1960e+00,\n                           1.9293e+00,  1.8370e+00,  2.3512e+00,  1.9812e+00,  2.3986e+00,\n                           1.7213e+00,  1.5885e+00,  1.7222e+00,  1.7107e+00,  2.8023e+00,\n                          -4.3911e-01,  2.7549e+00,  2.4104e+00, -1.2298e-01, -1.5464e-01,\n                           3.2708e+00,  1.2562e+00,  2.4292e+00,  2.5590e+00, -2.4650e-01,\n                           3.1373e+00, -5.8348e-02,  2.4282e+00,  9.0226e-01,  2.5341e+00,\n                           2.7088e+00, -3.3405e-01,  1.4642e+00, -2.4301e-01,  1.1388e+00,\n                           1.5162e+00, -2.3678e-01,  2.6163e+00,  1.2446e+00,  2.2672e+00,\n                           1.4217e+00, -5.1226e-01,  7.7710e-01,  1.8062e+00, -3.2944e-01,\n                          -2.7614e-01,  1.5814e+00, -4.4620e-02,  2.3115e+00,  1.6390e+00,\n                           2.2049e+00,  2.3055e+00,  3.1844e+00,  5.7178e-02,  9.8080e-01,\n                           1.0218e-01, -3.9442e-01,  2.1860e+00, -1.9450e-01,  2.9373e+00,\n                          -2.7823e-01,  4.7970e+00,  1.4074e+00,  2.2380e+00,  1.1289e-01,\n                          -2.8489e-01,  3.9043e+00,  1.5342e+00,  2.1239e+00,  3.5527e+00,\n                          -5.4169e-01,  2.2569e+00, -1.5878e-02,  1.0286e+00,  1.3227e+00,\n                          -6.4569e-02,  2.4459e+00,  4.9972e-02, -2.7496e-01,  1.2496e+00,\n                           2.9338e+00,  1.9177e+00,  1.2097e+00, -2.0513e-01, -7.5599e-02,\n                           3.0908e+00, -2.6996e-01,  2.4655e-02,  1.8073e+00,  2.7404e+00,\n                           2.3502e+00,  1.2415e+00, -2.1382e-01,  3.3335e+00,  1.3382e+00,\n                           5.1662e-01,  7.2708e-01, -5.9918e-02,  2.0686e+00, -2.6782e-01,\n                           1.9939e+00, -1.5473e-01,  1.5241e+00,  1.4702e+00,  1.5480e-01,\n                           2.1860e+00,  2.2726e+00, -1.9719e-01,  1.1797e+00,  2.0668e+00,\n                          -7.9854e-02, -2.8304e-01,  3.7908e+00, -3.8367e-01,  1.6574e+00,\n                           2.0844e+00, -1.1881e-01,  9.8075e-01,  1.8469e+00, -3.0218e-01,\n                          -2.6032e-01,  2.2463e+00,  1.8961e+00,  1.2443e+00,  1.1874e+00,\n                           1.2398e+00,  3.6168e+00,  1.6318e+00,  2.0013e+00,  2.1627e+00,\n                           2.1063e+00,  3.7338e-01,  3.8351e+00,  5.7562e-02,  1.2707e+00,\n                           3.4427e+00,  1.6206e+00,  1.9617e+00,  2.3097e+00,  1.1380e+00,\n                           1.9599e+00, -1.3334e-01,  1.8678e+00, -7.3765e-02,  9.4237e-01,\n                           2.8811e+00,  1.0515e+00, -1.8647e-01,  1.2529e+00,  8.8346e-01,\n                           3.3484e+00,  1.3650e+00, -5.5749e-02,  2.8181e+00,  3.0687e+00,\n                           2.2167e+00,  1.0020e+00, -2.0461e-01,  2.4692e+00,  2.6322e+00,\n                           8.2037e-01,  2.7194e+00,  2.9465e-01,  1.3261e+00,  1.8234e+00,\n                           2.7117e+00,  2.3461e+00,  1.4310e+00, -3.1500e-01,  2.0275e+00,\n                           7.6977e-01,  9.6985e-01, -1.5933e-01,  1.8527e+00,  7.1059e-01,\n                           1.3598e+00])\n                )\n              )\n            )\n            (2): ReLU6(inplace=True)\n          )\n          (2): ConvBn2d(\n            576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0025, 0.0036, 0.0022, 0.0014, 0.0028, 0.0025, 0.0024, 0.0025, 0.0025,\n                      0.0038, 0.0019, 0.0026, 0.0025, 0.0019, 0.0025, 0.0035, 0.0036, 0.0024,\n                      0.0027, 0.0026, 0.0022, 0.0036, 0.0021, 0.0030, 0.0027, 0.0026, 0.0024,\n                      0.0025, 0.0019, 0.0027, 0.0025, 0.0024, 0.0021, 0.0022, 0.0022, 0.0025,\n                      0.0023, 0.0037, 0.0025, 0.0031, 0.0017, 0.0030, 0.0029, 0.0019, 0.0033,\n                      0.0022, 0.0019, 0.0027, 0.0036, 0.0028, 0.0024, 0.0022, 0.0017, 0.0024,\n                      0.0029, 0.0027, 0.0024, 0.0016, 0.0031, 0.0026, 0.0024, 0.0034, 0.0018,\n                      0.0025, 0.0022, 0.0021, 0.0030, 0.0031, 0.0018, 0.0031, 0.0027, 0.0034,\n                      0.0024, 0.0026, 0.0029, 0.0020, 0.0030, 0.0030, 0.0020, 0.0024, 0.0029,\n                      0.0032, 0.0033, 0.0026, 0.0037, 0.0030, 0.0029, 0.0032, 0.0030, 0.0020,\n                      0.0033, 0.0024, 0.0022, 0.0029, 0.0030, 0.0037]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                     dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n                min_val=tensor([-0.3232, -0.4562, -0.2783, -0.1760, -0.3551, -0.2905, -0.3076, -0.3189,\n                        -0.3160, -0.4882, -0.2384, -0.3329, -0.3141, -0.2494, -0.3151, -0.4463,\n                        -0.3789, -0.3041, -0.3506, -0.3286, -0.2694, -0.4609, -0.2568, -0.3878,\n                        -0.2901, -0.3130, -0.3069, -0.2675, -0.2382, -0.2828, -0.3256, -0.2325,\n                        -0.2678, -0.2371, -0.2562, -0.3183, -0.2986, -0.4735, -0.2680, -0.2093,\n                        -0.2135, -0.3533, -0.2945, -0.2317, -0.2475, -0.2428, -0.2357, -0.3420,\n                        -0.4663, -0.3628, -0.2766, -0.2821, -0.2131, -0.2814, -0.3241, -0.3290,\n                        -0.2510, -0.1867, -0.3436, -0.3334, -0.3109, -0.4310, -0.2197, -0.2650,\n                        -0.1953, -0.2251, -0.2839, -0.4023, -0.2247, -0.4014, -0.3509, -0.4371,\n                        -0.2986, -0.2093, -0.3748, -0.2500, -0.3398, -0.3897, -0.2515, -0.3009,\n                        -0.3709, -0.4140, -0.3945, -0.3366, -0.4728, -0.3088, -0.3238, -0.4129,\n                        -0.3291, -0.2379, -0.3329, -0.3134, -0.2818, -0.3391, -0.3844, -0.4759]), max_val=tensor([0.2695, 0.3173, 0.2285, 0.1819, 0.2751, 0.3160, 0.2545, 0.2443, 0.2267,\n                        0.3517, 0.2223, 0.2854, 0.2735, 0.2291, 0.2664, 0.3967, 0.4608, 0.2452,\n                        0.3235, 0.3216, 0.2800, 0.4179, 0.2668, 0.3795, 0.3404, 0.3248, 0.2578,\n                        0.3184, 0.2151, 0.3436, 0.3023, 0.3106, 0.2446, 0.2829, 0.2797, 0.2823,\n                        0.1938, 0.3763, 0.3115, 0.3903, 0.1905, 0.3766, 0.3673, 0.2454, 0.4193,\n                        0.2853, 0.2356, 0.2808, 0.2593, 0.3242, 0.2992, 0.2784, 0.2188, 0.3049,\n                        0.3745, 0.3487, 0.3018, 0.2026, 0.3951, 0.3166, 0.2838, 0.3202, 0.2324,\n                        0.3201, 0.2822, 0.2680, 0.3748, 0.3026, 0.2213, 0.2970, 0.3400, 0.4272,\n                        0.3073, 0.3246, 0.2666, 0.2332, 0.3764, 0.2800, 0.2278, 0.2215, 0.3032,\n                        0.2870, 0.4192, 0.3335, 0.3154, 0.3784, 0.3681, 0.3022, 0.3831, 0.2528,\n                        0.4224, 0.3093, 0.2771, 0.3691, 0.2646, 0.3574])\n              )\n            )\n          )\n        )\n      )\n      (13): Module(\n        (conv): Module(\n          (0): Module(\n            (0): ConvBn2d(\n              96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False\n              (bn): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n                fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([6.4283e-04, 6.8469e-04, 7.3125e-04, 4.4031e-04, 6.2622e-04, 5.1468e-04,\n                        2.9878e-04, 5.6491e-04, 3.5481e-04, 6.1263e-04, 5.6712e-04, 8.1554e-04,\n                        9.4148e-04, 3.0944e-04, 4.7844e-04, 4.5621e-04, 8.5861e-04, 5.1505e-04,\n                        6.0565e-04, 8.2026e-04, 6.9369e-04, 4.7790e-04, 4.5806e-04, 6.0006e-04,\n                        4.9843e-04, 8.2400e-06, 5.5615e-04, 7.3274e-04, 5.3661e-04, 3.5382e-04,\n                        6.1600e-04, 5.7235e-04, 4.3461e-04, 5.5358e-04, 5.8599e-04, 4.2070e-04,\n                        6.4086e-04, 6.1891e-04, 4.4851e-04, 5.9800e-04, 5.6860e-04, 3.1398e-04,\n                        4.8412e-04, 7.0190e-04, 6.8989e-04, 7.8204e-04, 7.2001e-04, 5.0455e-04,\n                        5.1547e-04, 7.4894e-04, 5.6010e-04, 6.0731e-04, 5.0721e-04, 6.5436e-04,\n                        5.2491e-04, 5.4664e-04, 7.0270e-04, 6.9216e-04, 5.9346e-04, 3.8640e-04,\n                        5.4309e-04, 5.8286e-04, 4.1472e-04, 5.2842e-04, 5.6432e-04, 6.1561e-04,\n                        5.9386e-04, 5.5314e-04, 6.5609e-04, 5.0191e-04, 5.8604e-04, 4.8485e-04,\n                        4.7656e-04, 5.0786e-04, 5.6222e-04, 3.5084e-04, 5.3952e-04, 3.3401e-04,\n                        6.0733e-04, 6.1118e-04, 4.3278e-04, 3.8830e-04, 6.3979e-04, 3.1898e-04,\n                        4.5540e-04, 7.3361e-04, 6.6453e-04, 6.8622e-04, 5.6179e-04, 5.1711e-04,\n                        5.6931e-04, 5.2402e-04, 5.1245e-04, 5.7819e-04, 5.0440e-04, 4.3289e-04,\n                        5.5555e-04, 7.0231e-04, 6.7006e-04, 6.1392e-04, 6.5340e-04, 5.1717e-04,\n                        6.3788e-04, 8.0982e-04, 4.6267e-04, 8.1705e-04, 3.6966e-04, 5.9881e-04,\n                        5.2602e-04, 4.9930e-04, 4.7701e-04, 8.5474e-04, 6.0554e-04, 8.2800e-04,\n                        5.1617e-04, 5.4892e-04, 4.1620e-04, 5.9809e-04, 4.1237e-04, 6.5184e-04,\n                        4.0805e-04, 6.2767e-04, 1.9703e-05, 5.4028e-04, 6.2558e-04, 4.8202e-04,\n                        4.7635e-04, 6.3999e-04, 5.7066e-04, 4.7379e-04, 4.8281e-04, 4.7056e-04,\n                        1.6962e-05, 5.5160e-04, 4.9559e-04, 4.1391e-04, 5.2610e-04, 5.3116e-04,\n                        5.6756e-04, 5.9280e-04, 7.6272e-04, 3.4273e-04, 6.5339e-04, 6.4126e-04,\n                        7.0220e-04, 6.0713e-04, 5.7535e-04, 1.8965e-04, 3.6718e-04, 5.1887e-04,\n                        5.4156e-04, 3.2891e-04, 4.3452e-04, 6.4786e-04, 4.9403e-04, 5.5353e-04,\n                        5.0118e-04, 4.3624e-04, 3.1068e-04, 6.0975e-04, 5.5395e-04, 5.6250e-04,\n                        5.0236e-04, 5.4189e-04, 2.4961e-04, 6.3289e-04, 6.2860e-04, 4.0555e-04,\n                        4.2927e-04, 6.4883e-04, 4.6448e-04, 7.0923e-04, 9.4488e-04, 5.6165e-04,\n                        5.7852e-04, 4.9871e-04, 7.0079e-04, 5.9975e-04, 5.7426e-04, 6.4152e-04,\n                        7.4805e-04, 4.7523e-04, 5.2479e-04, 9.4837e-04, 5.7843e-04, 3.6688e-04,\n                        4.6672e-04, 4.5018e-04, 4.1563e-04, 7.5232e-04, 3.7087e-04, 6.6401e-04,\n                        4.8047e-04, 6.4913e-04, 7.2668e-04, 5.1846e-04, 6.4416e-04, 1.0178e-03,\n                        4.8195e-04, 2.2493e-04, 4.5693e-04, 5.1561e-04, 5.5352e-04, 1.9401e-03,\n                        3.8296e-04, 4.6143e-04, 5.1381e-04, 7.0123e-04, 5.0074e-04, 5.1959e-04,\n                        7.1578e-04, 6.2931e-04, 6.1664e-04, 5.2176e-04, 5.6104e-04, 8.6108e-04,\n                        7.3366e-04, 5.0486e-04, 4.6272e-04, 7.3536e-04, 5.9814e-04, 3.9908e-04,\n                        6.3187e-04, 5.6023e-04, 5.1496e-04, 4.6429e-04, 5.6419e-04, 8.5745e-04,\n                        6.8382e-04, 7.0821e-04, 7.5267e-04, 3.4137e-04, 6.5894e-04, 8.3659e-04,\n                        5.5176e-04, 5.7809e-04, 4.4271e-04, 4.8323e-04, 7.7670e-04, 5.0869e-04,\n                        4.4640e-04, 5.2318e-04, 6.8373e-04, 4.4220e-04, 7.3806e-04, 5.1872e-04,\n                        5.9218e-04, 5.1016e-04, 7.6778e-04, 5.2196e-04, 6.6835e-04, 4.7466e-04,\n                        5.9055e-04, 6.4099e-04, 4.5739e-04, 5.7100e-04, 3.8188e-04, 5.5725e-04,\n                        6.3500e-04, 3.3489e-04, 4.6346e-04, 5.4323e-04, 6.2526e-04, 4.8551e-04,\n                        4.5231e-04, 5.7620e-04, 7.6691e-04, 3.9259e-04, 5.4628e-04, 5.9182e-04,\n                        6.7217e-04, 5.1384e-04, 4.7499e-04, 5.5089e-04, 6.8337e-04, 5.7261e-04,\n                        5.2038e-04, 4.5853e-04, 6.0156e-04, 3.8583e-04, 8.0896e-06, 1.4481e-05,\n                        6.5905e-04, 4.2647e-04, 5.8714e-04, 4.6357e-04, 7.7689e-04, 4.2745e-04,\n                        7.8891e-04, 5.5375e-04, 7.3089e-04, 5.3619e-04, 5.7104e-04, 5.9731e-04,\n                        4.2311e-04, 5.7452e-04, 6.1130e-04, 5.8102e-04, 3.9560e-04, 5.0202e-04,\n                        4.7508e-04, 4.6815e-04, 6.1814e-04, 6.2657e-04, 5.3712e-04, 5.5910e-04,\n                        8.7961e-04, 5.6397e-04, 3.4710e-04, 6.0647e-04, 6.5437e-04, 6.4085e-04,\n                        5.7002e-04, 5.2682e-04, 5.3449e-04, 4.6901e-04, 6.6341e-04, 4.4825e-04,\n                        4.8260e-04, 6.0119e-04, 4.7645e-04, 4.0761e-04, 5.7963e-04, 8.1326e-04,\n                        5.3737e-04, 4.8089e-04, 5.0860e-04, 5.8237e-04, 5.1067e-04, 5.5835e-04,\n                        5.9524e-04, 8.2006e-04, 6.7003e-04, 6.7647e-04, 5.1342e-04, 6.0978e-04,\n                        2.9332e-04, 6.5677e-04, 5.3903e-04, 6.4414e-04, 6.0542e-04, 5.5949e-04,\n                        7.3083e-04, 4.5202e-04, 6.1683e-04, 5.9830e-04, 5.4642e-04, 7.0157e-04,\n                        4.8874e-04, 5.0953e-04, 5.5951e-04, 5.7715e-04, 5.8036e-04, 2.2513e-04,\n                        7.9280e-04, 5.3971e-04, 5.2823e-04, 6.6162e-04, 8.7001e-04, 5.4187e-04,\n                        4.5839e-04, 5.3551e-04, 4.8896e-04, 7.5856e-04, 1.2075e-03, 6.6270e-04,\n                        4.6696e-04, 4.7494e-04, 6.6468e-04, 5.9750e-04, 5.0180e-04, 5.4156e-04,\n                        4.7058e-04, 4.5042e-04, 5.5673e-04, 4.5584e-04, 4.3981e-04, 4.9919e-04,\n                        4.9851e-04, 6.8763e-04, 6.1364e-04, 5.8036e-04, 6.5978e-04, 4.9600e-04,\n                        7.1165e-04, 5.6583e-04, 8.3824e-04, 4.7901e-04, 3.7679e-04, 6.1124e-04,\n                        4.8547e-04, 5.6110e-04, 6.0036e-04, 4.3292e-04, 4.7002e-04, 6.0808e-04,\n                        2.8952e-04, 5.5454e-04, 5.5304e-04, 4.9415e-04, 5.6992e-04, 7.0341e-04,\n                        2.5655e-04, 2.4897e-04, 3.1850e-04, 3.5106e-04, 6.0234e-04, 4.4803e-04,\n                        4.7297e-04, 4.5142e-04, 4.4049e-04, 4.9357e-04, 3.8927e-04, 4.6286e-04,\n                        8.7678e-04, 5.8695e-04, 2.9576e-04, 7.1374e-04, 5.4373e-04, 6.2855e-04,\n                        6.3006e-04, 5.7306e-04, 4.5305e-04, 8.8089e-04, 5.6188e-04, 5.9204e-04,\n                        5.5368e-04, 5.8355e-04, 3.9147e-04, 3.8402e-04, 5.8117e-04, 5.8714e-04,\n                        4.0038e-04, 5.7742e-04, 4.2805e-04, 7.8550e-04, 6.9985e-04, 4.9222e-04,\n                        5.9508e-04, 8.0287e-04, 5.0776e-04, 2.0786e-04, 5.1562e-04, 5.2117e-04,\n                        5.3945e-04, 4.0755e-04, 4.9802e-04, 3.9587e-04, 4.6640e-04, 7.5534e-04,\n                        7.5768e-04, 4.3597e-04, 6.4873e-04, 5.5035e-04, 3.3687e-04, 5.2146e-04,\n                        6.3803e-04, 4.6091e-04, 6.1723e-04, 5.6534e-04, 5.3057e-04, 5.7442e-04,\n                        5.7777e-04, 5.9389e-04, 5.3870e-04, 2.5368e-04, 5.1661e-04, 5.9303e-04,\n                        6.4297e-04, 4.3873e-04, 3.1444e-04, 8.0068e-04, 5.1105e-04, 6.7322e-04,\n                        4.2207e-04, 9.4531e-04, 5.9404e-04, 7.6801e-04, 4.3795e-04, 5.0691e-04,\n                        4.1354e-04, 8.0447e-04, 4.9576e-04, 6.2447e-04, 7.9469e-04, 7.0898e-04,\n                        7.3944e-04, 6.6965e-04, 5.7563e-04, 5.1657e-04, 5.6821e-04, 5.8899e-04,\n                        5.6566e-04, 4.5104e-04, 6.3810e-04, 3.9913e-04, 4.5452e-04, 5.7594e-04,\n                        5.8537e-04, 5.5250e-04, 4.5748e-04, 6.8710e-04, 5.1589e-04, 1.2417e-03,\n                        6.6112e-04, 5.6460e-04, 7.4438e-04, 3.3941e-04, 8.3415e-04, 3.8269e-04,\n                        3.1782e-04, 8.0267e-04, 7.3817e-04, 6.0134e-04, 6.7817e-04, 4.8249e-04,\n                        5.9679e-04, 6.5962e-04, 5.3398e-04, 7.1584e-04, 7.2786e-04, 5.0511e-04,\n                        5.4399e-04, 6.2897e-04, 5.1180e-04, 6.5503e-04, 6.8631e-04, 6.1886e-04,\n                        4.5323e-04, 3.7435e-04, 4.9679e-04, 4.7630e-04, 5.4564e-04, 6.5930e-04,\n                        8.3109e-06, 4.4873e-04, 5.0958e-04, 4.6184e-04, 5.1280e-04, 6.7348e-04,\n                        4.8659e-04, 6.4532e-04, 7.9044e-04, 5.0614e-04, 1.6205e-05, 4.6656e-04,\n                        8.5870e-04, 5.1562e-04, 3.6371e-04, 5.3490e-04, 6.2662e-04, 6.9948e-04,\n                        5.3787e-04, 4.5435e-04, 6.9255e-04, 7.7692e-04, 5.3076e-04, 6.0285e-04,\n                        6.5034e-04, 7.8927e-04, 6.9022e-04, 4.8778e-04, 4.4826e-04, 5.9296e-04,\n                        9.9028e-04, 4.7943e-04, 5.4653e-04, 5.4755e-04, 3.3849e-04, 6.0509e-04,\n                        3.9785e-04, 4.9409e-04, 6.6279e-04, 3.2729e-04, 6.9744e-04, 5.3466e-04]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                       dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n                (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n                  min_val=tensor([-0.0823, -0.0876, -0.0728, -0.0461, -0.0802, -0.0659, -0.0355, -0.0483,\n                          -0.0454, -0.0784, -0.0726, -0.0653, -0.1205, -0.0360, -0.0612, -0.0568,\n                          -0.1060, -0.0659, -0.0775, -0.0625, -0.0740, -0.0518, -0.0586, -0.0521,\n                          -0.0638, -0.0011, -0.0712, -0.0772, -0.0578, -0.0453, -0.0476, -0.0733,\n                          -0.0556, -0.0564, -0.0750, -0.0405, -0.0820, -0.0528, -0.0424, -0.0765,\n                          -0.0420, -0.0393, -0.0620, -0.0898, -0.0608, -0.1001, -0.0922, -0.0442,\n                          -0.0660, -0.0959, -0.0717, -0.0777, -0.0649, -0.0838, -0.0602, -0.0659,\n                          -0.0899, -0.0886, -0.0558, -0.0495, -0.0695, -0.0746, -0.0531, -0.0676,\n                          -0.0722, -0.0609, -0.0760, -0.0619, -0.0783, -0.0642, -0.0750, -0.0614,\n                          -0.0610, -0.0563, -0.0644, -0.0292, -0.0685, -0.0385, -0.0777, -0.0594,\n                          -0.0488, -0.0451, -0.0819, -0.0385, -0.0520, -0.0846, -0.0851, -0.0808,\n                          -0.0719, -0.0636, -0.0646, -0.0671, -0.0497, -0.0740, -0.0646, -0.0554,\n                          -0.0711, -0.0637, -0.0631, -0.0704, -0.0836, -0.0662, -0.0489, -0.1037,\n                          -0.0592, -0.1046, -0.0453, -0.0713, -0.0664, -0.0544, -0.0611, -0.0814,\n                          -0.0775, -0.0864, -0.0624, -0.0569, -0.0533, -0.0708, -0.0528, -0.0582,\n                          -0.0521, -0.0668, -0.0022, -0.0689, -0.0716, -0.0554, -0.0547, -0.0591,\n                          -0.0616, -0.0606, -0.0618, -0.0602, -0.0016, -0.0706, -0.0634, -0.0530,\n                          -0.0532, -0.0680, -0.0726, -0.0759, -0.0725, -0.0439, -0.0631, -0.0821,\n                          -0.0500, -0.0771, -0.0736, -0.0195, -0.0458, -0.0664, -0.0618, -0.0337,\n                          -0.0556, -0.0730, -0.0632, -0.0709, -0.0642, -0.0507, -0.0393, -0.0632,\n                          -0.0698, -0.0720, -0.0510, -0.0694, -0.0295, -0.0804, -0.0805, -0.0519,\n                          -0.0549, -0.0831, -0.0385, -0.0908, -0.1209, -0.0707, -0.0740, -0.0638,\n                          -0.0897, -0.0440, -0.0735, -0.0821, -0.0871, -0.0608, -0.0672, -0.0744,\n                          -0.0740, -0.0470, -0.0597, -0.0576, -0.0532, -0.0902, -0.0380, -0.0850,\n                          -0.0615, -0.0524, -0.0930, -0.0566, -0.0485, -0.1303, -0.0617, -0.0288,\n                          -0.0585, -0.0660, -0.0709, -0.1694, -0.0463, -0.0591, -0.0595, -0.0826,\n                          -0.0520, -0.0665, -0.0916, -0.0592, -0.0733, -0.0668, -0.0472, -0.0731,\n                          -0.0709, -0.0493, -0.0459, -0.0941, -0.0766, -0.0511, -0.0762, -0.0657,\n                          -0.0659, -0.0594, -0.0722, -0.1098, -0.0591, -0.0658, -0.0963, -0.0396,\n                          -0.0843, -0.1071, -0.0620, -0.0740, -0.0499, -0.0453, -0.0610, -0.0628,\n                          -0.0571, -0.0670, -0.0875, -0.0553, -0.0945, -0.0664, -0.0758, -0.0472,\n                          -0.0983, -0.0457, -0.0534, -0.0442, -0.0756, -0.0820, -0.0585, -0.0731,\n                          -0.0367, -0.0677, -0.0813, -0.0429, -0.0593, -0.0523, -0.0607, -0.0604,\n                          -0.0413, -0.0708, -0.0982, -0.0500, -0.0574, -0.0644, -0.0860, -0.0569,\n                          -0.0511, -0.0705, -0.0736, -0.0669, -0.0666, -0.0587, -0.0751, -0.0443,\n                          -0.0010, -0.0018, -0.0844, -0.0518, -0.0752, -0.0566, -0.0994, -0.0472,\n                          -0.1010, -0.0585, -0.0723, -0.0686, -0.0731, -0.0560, -0.0439, -0.0735,\n                          -0.0566, -0.0550, -0.0489, -0.0632, -0.0608, -0.0524, -0.0791, -0.0721,\n                          -0.0494, -0.0617, -0.1126, -0.0706, -0.0444, -0.0776, -0.0699, -0.0820,\n                          -0.0730, -0.0473, -0.0684, -0.0600, -0.0771, -0.0574, -0.0618, -0.0770,\n                          -0.0610, -0.0522, -0.0614, -0.1041, -0.0607, -0.0586, -0.0651, -0.0728,\n                          -0.0654, -0.0567, -0.0554, -0.1050, -0.0581, -0.0866, -0.0593, -0.0781,\n                          -0.0343, -0.0841, -0.0690, -0.0824, -0.0716, -0.0716, -0.0530, -0.0530,\n                          -0.0560, -0.0678, -0.0507, -0.0524, -0.0626, -0.0611, -0.0698, -0.0739,\n                          -0.0643, -0.0288, -0.0748, -0.0534, -0.0676, -0.0661, -0.0920, -0.0573,\n                          -0.0587, -0.0685, -0.0564, -0.0971, -0.1546, -0.0617, -0.0593, -0.0545,\n                          -0.0711, -0.0765, -0.0449, -0.0693, -0.0506, -0.0577, -0.0551, -0.0439,\n                          -0.0516, -0.0639, -0.0402, -0.0860, -0.0523, -0.0743, -0.0587, -0.0635,\n                          -0.0784, -0.0581, -0.1073, -0.0498, -0.0350, -0.0680, -0.0611, -0.0718,\n                          -0.0600, -0.0554, -0.0346, -0.0778, -0.0311, -0.0710, -0.0670, -0.0385,\n                          -0.0729, -0.0701, -0.0242, -0.0319, -0.0408, -0.0449, -0.0771, -0.0450,\n                          -0.0592, -0.0578, -0.0564, -0.0632, -0.0361, -0.0592, -0.1122, -0.0626,\n                          -0.0379, -0.0914, -0.0696, -0.0665, -0.0806, -0.0734, -0.0564, -0.0674,\n                          -0.0719, -0.0758, -0.0434, -0.0422, -0.0501, -0.0492, -0.0744, -0.0752,\n                          -0.0497, -0.0739, -0.0548, -0.0630, -0.0896, -0.0594, -0.0714, -0.0720,\n                          -0.0648, -0.0266, -0.0559, -0.0667, -0.0690, -0.0522, -0.0637, -0.0471,\n                          -0.0597, -0.0645, -0.0933, -0.0474, -0.0830, -0.0704, -0.0431, -0.0667,\n                          -0.0817, -0.0470, -0.0644, -0.0562, -0.0679, -0.0643, -0.0653, -0.0674,\n                          -0.0511, -0.0325, -0.0661, -0.0759, -0.0794, -0.0562, -0.0308, -0.1025,\n                          -0.0507, -0.0562, -0.0540, -0.1210, -0.0760, -0.0983, -0.0561, -0.0480,\n                          -0.0463, -0.0998, -0.0635, -0.0585, -0.1017, -0.0907, -0.0779, -0.0857,\n                          -0.0737, -0.0592, -0.0437, -0.0639, -0.0724, -0.0536, -0.0817, -0.0374,\n                          -0.0582, -0.0737, -0.0749, -0.0630, -0.0526, -0.0801, -0.0660, -0.1532,\n                          -0.0650, -0.0723, -0.0648, -0.0434, -0.1068, -0.0488, -0.0367, -0.1027,\n                          -0.0945, -0.0770, -0.0805, -0.0618, -0.0764, -0.0844, -0.0606, -0.0837,\n                          -0.0701, -0.0647, -0.0514, -0.0622, -0.0537, -0.0750, -0.0689, -0.0792,\n                          -0.0555, -0.0479, -0.0522, -0.0610, -0.0698, -0.0807, -0.0011, -0.0574,\n                          -0.0548, -0.0481, -0.0656, -0.0821, -0.0623, -0.0826, -0.0934, -0.0648,\n                          -0.0021, -0.0525, -0.1099, -0.0613, -0.0434, -0.0613, -0.0802, -0.0740,\n                          -0.0563, -0.0582, -0.0886, -0.0994, -0.0679, -0.0562, -0.0832, -0.0971,\n                          -0.0883, -0.0624, -0.0574, -0.0759, -0.1268, -0.0614, -0.0676, -0.0471,\n                          -0.0433, -0.0692, -0.0509, -0.0632, -0.0848, -0.0353, -0.0893, -0.0684]), max_val=tensor([0.0480, 0.0362, 0.0929, 0.0559, 0.0791, 0.0644, 0.0379, 0.0717, 0.0437,\n                          0.0772, 0.0712, 0.1036, 0.0932, 0.0393, 0.0549, 0.0579, 0.1090, 0.0477,\n                          0.0675, 0.1042, 0.0881, 0.0607, 0.0489, 0.0762, 0.0566, 0.0007, 0.0683,\n                          0.0931, 0.0681, 0.0434, 0.0782, 0.0720, 0.0501, 0.0703, 0.0704, 0.0534,\n                          0.0613, 0.0786, 0.0570, 0.0518, 0.0722, 0.0399, 0.0418, 0.0722, 0.0876,\n                          0.0620, 0.0806, 0.0641, 0.0610, 0.0838, 0.0576, 0.0678, 0.0521, 0.0685,\n                          0.0667, 0.0694, 0.0619, 0.0844, 0.0754, 0.0420, 0.0548, 0.0643, 0.0433,\n                          0.0438, 0.0547, 0.0782, 0.0653, 0.0702, 0.0833, 0.0583, 0.0704, 0.0616,\n                          0.0446, 0.0645, 0.0714, 0.0446, 0.0685, 0.0424, 0.0480, 0.0776, 0.0550,\n                          0.0493, 0.0554, 0.0405, 0.0578, 0.0932, 0.0525, 0.0872, 0.0634, 0.0657,\n                          0.0723, 0.0622, 0.0651, 0.0539, 0.0607, 0.0539, 0.0508, 0.0892, 0.0851,\n                          0.0780, 0.0552, 0.0548, 0.0810, 0.0842, 0.0491, 0.0588, 0.0469, 0.0760,\n                          0.0668, 0.0634, 0.0506, 0.1086, 0.0566, 0.1052, 0.0656, 0.0697, 0.0495,\n                          0.0760, 0.0459, 0.0828, 0.0518, 0.0797, 0.0025, 0.0686, 0.0794, 0.0612,\n                          0.0605, 0.0813, 0.0725, 0.0530, 0.0476, 0.0544, 0.0022, 0.0571, 0.0576,\n                          0.0505, 0.0668, 0.0513, 0.0514, 0.0504, 0.0969, 0.0410, 0.0830, 0.0685,\n                          0.0892, 0.0771, 0.0603, 0.0241, 0.0466, 0.0587, 0.0688, 0.0418, 0.0482,\n                          0.0823, 0.0482, 0.0652, 0.0617, 0.0554, 0.0395, 0.0774, 0.0704, 0.0675,\n                          0.0638, 0.0592, 0.0317, 0.0804, 0.0564, 0.0490, 0.0494, 0.0475, 0.0590,\n                          0.0629, 0.1171, 0.0713, 0.0683, 0.0561, 0.0590, 0.0762, 0.0664, 0.0566,\n                          0.0950, 0.0452, 0.0662, 0.1204, 0.0630, 0.0460, 0.0537, 0.0554, 0.0421,\n                          0.0955, 0.0471, 0.0807, 0.0543, 0.0824, 0.0752, 0.0658, 0.0818, 0.0561,\n                          0.0522, 0.0262, 0.0561, 0.0628, 0.0689, 0.2464, 0.0486, 0.0527, 0.0653,\n                          0.0891, 0.0636, 0.0511, 0.0726, 0.0799, 0.0783, 0.0657, 0.0713, 0.1094,\n                          0.0932, 0.0641, 0.0588, 0.0735, 0.0544, 0.0499, 0.0802, 0.0711, 0.0642,\n                          0.0402, 0.0475, 0.0913, 0.0868, 0.0899, 0.0638, 0.0434, 0.0704, 0.0798,\n                          0.0701, 0.0557, 0.0562, 0.0614, 0.0986, 0.0646, 0.0539, 0.0521, 0.0659,\n                          0.0562, 0.0911, 0.0651, 0.0379, 0.0648, 0.0773, 0.0663, 0.0849, 0.0603,\n                          0.0536, 0.0710, 0.0462, 0.0663, 0.0485, 0.0708, 0.0733, 0.0410, 0.0506,\n                          0.0690, 0.0794, 0.0617, 0.0574, 0.0732, 0.0810, 0.0499, 0.0694, 0.0752,\n                          0.0582, 0.0653, 0.0603, 0.0651, 0.0868, 0.0727, 0.0528, 0.0552, 0.0764,\n                          0.0490, 0.0009, 0.0018, 0.0818, 0.0542, 0.0693, 0.0589, 0.0959, 0.0543,\n                          0.0998, 0.0703, 0.0928, 0.0636, 0.0723, 0.0759, 0.0537, 0.0700, 0.0776,\n                          0.0738, 0.0502, 0.0638, 0.0536, 0.0595, 0.0450, 0.0796, 0.0682, 0.0710,\n                          0.1022, 0.0716, 0.0373, 0.0627, 0.0831, 0.0649, 0.0684, 0.0669, 0.0550,\n                          0.0554, 0.0843, 0.0531, 0.0514, 0.0624, 0.0461, 0.0512, 0.0736, 0.0921,\n                          0.0682, 0.0611, 0.0509, 0.0740, 0.0602, 0.0709, 0.0756, 0.0915, 0.0851,\n                          0.0607, 0.0652, 0.0717, 0.0373, 0.0573, 0.0601, 0.0684, 0.0769, 0.0617,\n                          0.0928, 0.0574, 0.0783, 0.0760, 0.0694, 0.0891, 0.0572, 0.0647, 0.0711,\n                          0.0372, 0.0737, 0.0236, 0.1007, 0.0685, 0.0414, 0.0840, 0.1105, 0.0688,\n                          0.0504, 0.0671, 0.0621, 0.0579, 0.1137, 0.0842, 0.0593, 0.0603, 0.0844,\n                          0.0624, 0.0637, 0.0603, 0.0598, 0.0383, 0.0707, 0.0579, 0.0559, 0.0617,\n                          0.0633, 0.0873, 0.0779, 0.0669, 0.0838, 0.0601, 0.0904, 0.0719, 0.0484,\n                          0.0608, 0.0479, 0.0776, 0.0617, 0.0652, 0.0762, 0.0481, 0.0597, 0.0641,\n                          0.0368, 0.0639, 0.0702, 0.0628, 0.0603, 0.0893, 0.0326, 0.0314, 0.0390,\n                          0.0377, 0.0519, 0.0569, 0.0601, 0.0426, 0.0508, 0.0558, 0.0494, 0.0474,\n                          0.0778, 0.0745, 0.0300, 0.0647, 0.0626, 0.0798, 0.0482, 0.0668, 0.0575,\n                          0.1119, 0.0613, 0.0546, 0.0703, 0.0741, 0.0478, 0.0355, 0.0693, 0.0570,\n                          0.0508, 0.0621, 0.0512, 0.0998, 0.0732, 0.0625, 0.0756, 0.1020, 0.0645,\n                          0.0263, 0.0655, 0.0651, 0.0623, 0.0474, 0.0464, 0.0503, 0.0455, 0.0959,\n                          0.0962, 0.0554, 0.0561, 0.0532, 0.0423, 0.0472, 0.0653, 0.0585, 0.0784,\n                          0.0718, 0.0659, 0.0730, 0.0734, 0.0754, 0.0684, 0.0263, 0.0553, 0.0678,\n                          0.0817, 0.0497, 0.0399, 0.0918, 0.0649, 0.0855, 0.0423, 0.0930, 0.0754,\n                          0.0805, 0.0486, 0.0644, 0.0525, 0.1022, 0.0383, 0.0793, 0.0662, 0.0721,\n                          0.0939, 0.0419, 0.0720, 0.0656, 0.0722, 0.0748, 0.0613, 0.0573, 0.0676,\n                          0.0507, 0.0498, 0.0592, 0.0480, 0.0702, 0.0581, 0.0873, 0.0570, 0.1577,\n                          0.0840, 0.0438, 0.0945, 0.0371, 0.0850, 0.0486, 0.0404, 0.0803, 0.0768,\n                          0.0528, 0.0861, 0.0547, 0.0700, 0.0645, 0.0678, 0.0909, 0.0924, 0.0541,\n                          0.0691, 0.0799, 0.0650, 0.0832, 0.0872, 0.0563, 0.0576, 0.0468, 0.0631,\n                          0.0565, 0.0630, 0.0837, 0.0009, 0.0453, 0.0647, 0.0587, 0.0504, 0.0855,\n                          0.0574, 0.0638, 0.1004, 0.0418, 0.0016, 0.0593, 0.0733, 0.0655, 0.0462,\n                          0.0679, 0.0628, 0.0888, 0.0683, 0.0554, 0.0822, 0.0906, 0.0636, 0.0766,\n                          0.0614, 0.1002, 0.0711, 0.0596, 0.0426, 0.0735, 0.0913, 0.0581, 0.0694,\n                          0.0695, 0.0414, 0.0768, 0.0501, 0.0574, 0.0836, 0.0416, 0.0580, 0.0679])\n                )\n              )\n            )\n            (2): ReLU6(inplace=True)\n          )\n          (1): Module(\n            (0): ConvBn2d(\n              576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False\n              (bn): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n                fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0036, 0.0210, 0.0153, 0.0022, 0.0066, 0.0048, 0.0221, 0.0021, 0.0060,\n                        0.0234, 0.0029, 0.0096, 0.0128, 0.0382, 0.0028, 0.0208, 0.0134, 0.0148,\n                        0.0020, 0.0061, 0.0078, 0.0162, 0.0235, 0.0197, 0.0063, 0.0005, 0.0172,\n                        0.0100, 0.0172, 0.0153, 0.0029, 0.0162, 0.0111, 0.0188, 0.0023, 0.0193,\n                        0.0226, 0.0275, 0.0155, 0.0080, 0.0175, 0.0072, 0.0168, 0.0081, 0.0269,\n                        0.0185, 0.0117, 0.0099, 0.0090, 0.0073, 0.0256, 0.0222, 0.0090, 0.0019,\n                        0.0127, 0.0166, 0.0084, 0.0154, 0.0021, 0.0093, 0.0023, 0.0139, 0.0110,\n                        0.0230, 0.0081, 0.0095, 0.0105, 0.0114, 0.0031, 0.0160, 0.0148, 0.0057,\n                        0.0141, 0.0133, 0.0025, 0.0244, 0.0029, 0.0124, 0.0095, 0.0347, 0.0166,\n                        0.0045, 0.0092, 0.0227, 0.0159, 0.0028, 0.0023, 0.0128, 0.0018, 0.0085,\n                        0.0184, 0.0081, 0.0121, 0.0124, 0.0235, 0.0168, 0.0096, 0.0088, 0.0084,\n                        0.0018, 0.0143, 0.0093, 0.0103, 0.0019, 0.0081, 0.0023, 0.0030, 0.0023,\n                        0.0206, 0.0191, 0.0234, 0.0023, 0.0135, 0.0087, 0.0212, 0.0186, 0.0021,\n                        0.0021, 0.0078, 0.0115, 0.0148, 0.0157, 0.0011, 0.0019, 0.0050, 0.0261,\n                        0.0130, 0.0179, 0.0229, 0.0018, 0.0269, 0.0027, 0.0014, 0.0148, 0.0203,\n                        0.0173, 0.0113, 0.0148, 0.0093, 0.0163, 0.0180, 0.0238, 0.0172, 0.0113,\n                        0.0159, 0.0430, 0.0064, 0.0272, 0.0149, 0.0088, 0.0035, 0.0218, 0.0123,\n                        0.0021, 0.0122, 0.0020, 0.0030, 0.0132, 0.0191, 0.0113, 0.0056, 0.0058,\n                        0.0109, 0.0175, 0.0231, 0.0022, 0.0136, 0.0085, 0.0145, 0.0137, 0.0136,\n                        0.0024, 0.0021, 0.0174, 0.0024, 0.0393, 0.0029, 0.0021, 0.0023, 0.0120,\n                        0.0084, 0.0245, 0.0093, 0.0020, 0.0017, 0.0212, 0.0165, 0.0148, 0.0228,\n                        0.0134, 0.0029, 0.0191, 0.0314, 0.0125, 0.0021, 0.0158, 0.0068, 0.0165,\n                        0.0167, 0.0307, 0.0138, 0.0188, 0.0024, 0.0073, 0.0309, 0.0132, 0.0055,\n                        0.0019, 0.0116, 0.0155, 0.0225, 0.0024, 0.0024, 0.0140, 0.0188, 0.0038,\n                        0.0181, 0.0142, 0.0127, 0.0028, 0.0026, 0.0154, 0.0023, 0.0093, 0.0015,\n                        0.0706, 0.0048, 0.0039, 0.0283, 0.0087, 0.0038, 0.0203, 0.0115, 0.0018,\n                        0.0077, 0.0026, 0.0132, 0.0140, 0.0136, 0.0137, 0.0063, 0.0162, 0.0277,\n                        0.0021, 0.0071, 0.0124, 0.0079, 0.0154, 0.0100, 0.0026, 0.0090, 0.0135,\n                        0.0091, 0.0108, 0.0133, 0.0147, 0.0086, 0.0302, 0.0059, 0.0300, 0.0192,\n                        0.0155, 0.0247, 0.0027, 0.0191, 0.0023, 0.0053, 0.0025, 0.0048, 0.0084,\n                        0.0027, 0.0094, 0.0186, 0.0079, 0.0027, 0.0080, 0.0228, 0.0071, 0.0151,\n                        0.0143, 0.0003, 0.0017, 0.0036, 0.0028, 0.0028, 0.0101, 0.0044, 0.0039,\n                        0.0107, 0.0095, 0.0025, 0.0085, 0.0171, 0.0022, 0.0138, 0.0233, 0.0172,\n                        0.0129, 0.0030, 0.0195, 0.0170, 0.0121, 0.0048, 0.0200, 0.0178, 0.0131,\n                        0.0069, 0.0031, 0.0160, 0.0128, 0.0027, 0.0263, 0.0188, 0.0148, 0.0024,\n                        0.0186, 0.0029, 0.0264, 0.0086, 0.0083, 0.0148, 0.0028, 0.0155, 0.0201,\n                        0.0183, 0.0124, 0.0033, 0.0025, 0.0031, 0.0162, 0.0053, 0.0096, 0.0334,\n                        0.0107, 0.0028, 0.0017, 0.0131, 0.0024, 0.0140, 0.0152, 0.0205, 0.0122,\n                        0.0155, 0.0021, 0.0020, 0.0173, 0.0140, 0.0072, 0.0026, 0.0021, 0.0027,\n                        0.0115, 0.0200, 0.0259, 0.0097, 0.0087, 0.0057, 0.0153, 0.0020, 0.0073,\n                        0.0335, 0.0126, 0.0215, 0.0046, 0.0042, 0.0149, 0.0037, 0.0086, 0.0151,\n                        0.0098, 0.0132, 0.0170, 0.0189, 0.0131, 0.0056, 0.0026, 0.0148, 0.0131,\n                        0.0105, 0.0134, 0.0175, 0.0071, 0.0168, 0.0178, 0.0024, 0.0079, 0.0021,\n                        0.0229, 0.0121, 0.0078, 0.0089, 0.0178, 0.0200, 0.0019, 0.0129, 0.0019,\n                        0.0034, 0.0101, 0.0141, 0.0211, 0.0026, 0.0142, 0.0205, 0.0222, 0.0129,\n                        0.0117, 0.0084, 0.0167, 0.0116, 0.0167, 0.0105, 0.0022, 0.0131, 0.0186,\n                        0.0018, 0.0180, 0.0123, 0.0106, 0.0031, 0.0047, 0.0218, 0.0172, 0.0106,\n                        0.0023, 0.0142, 0.0145, 0.0234, 0.0108, 0.0110, 0.0093, 0.0029, 0.0219,\n                        0.0117, 0.0027, 0.0171, 0.0117, 0.0073, 0.0145, 0.0141, 0.0084, 0.0027,\n                        0.0294, 0.0144, 0.0166, 0.0095, 0.0136, 0.0028, 0.0149, 0.0339, 0.0022,\n                        0.0109, 0.0140, 0.0040, 0.0025, 0.0233, 0.0139, 0.0016, 0.0027, 0.0119,\n                        0.0145, 0.0194, 0.0165, 0.0107, 0.0213, 0.0054, 0.0072, 0.0144, 0.0077,\n                        0.0163, 0.0153, 0.0163, 0.0060, 0.0054, 0.0192, 0.0123, 0.0171, 0.0100,\n                        0.0033, 0.0183, 0.0219, 0.0262, 0.0047, 0.0102, 0.0120, 0.0015, 0.0088,\n                        0.0100, 0.0125, 0.0110, 0.0177, 0.0463, 0.0022, 0.0062, 0.0364, 0.0195,\n                        0.0122, 0.0163, 0.0097, 0.0028, 0.0193, 0.0216, 0.0020, 0.0118, 0.0049,\n                        0.0260, 0.0097, 0.0069, 0.0339, 0.0143, 0.0063, 0.0237, 0.0080, 0.0078,\n                        0.0125, 0.0076, 0.0162, 0.0129, 0.0146, 0.0135, 0.0134, 0.0018, 0.0211,\n                        0.0170, 0.0052, 0.0211, 0.0112, 0.0021, 0.0137, 0.0232, 0.0054, 0.0174,\n                        0.0126, 0.0024, 0.0114, 0.0006, 0.0144, 0.0024, 0.0194, 0.0157, 0.0028,\n                        0.0193, 0.0210, 0.0027, 0.0073, 0.0006, 0.0123, 0.0052, 0.0132, 0.0032,\n                        0.0096, 0.0084, 0.0056, 0.0035, 0.0110, 0.0127, 0.0137, 0.0020, 0.0019,\n                        0.0157, 0.0094, 0.0259, 0.0023, 0.0126, 0.0111, 0.0072, 0.0290, 0.0180,\n                        0.0135, 0.0250, 0.0218, 0.0152, 0.0082, 0.0028, 0.0030, 0.0141, 0.0048]), zero_point=tensor([ 127,    0,    0,  127, -128,    0,    0,  127,  127,    0,  127,    0,\n                           0,    0,  127,    0,    0,    0,  127, -128, -128,    0,    0,    0,\n                        -128,    0,    0,    0,    0,    0,  127,    0,    0,    0,  127,    0,\n                           0,    0,    0,    0,    0,    0,    0, -128,    0,    0,    0,    0,\n                           0,    0,    0,    0,    0,  127,    0,    0,    0,    0,  127, -128,\n                         127,    0, -128,    0, -128, -128,    0,    0,  127,    0,    0, -128,\n                           0,    0,  127,    0,  127,    0,    0,    0,    0,  127, -128,    0,\n                           0,  127,  127,    0,  127,    0,    0,    0,    0,    0,    0,    0,\n                           0,    0,    0,  127,    0, -128,    0,  127,    0,  127,  127,  127,\n                           0,    0,    0,  127,    0,    0,    0,    0,  127,  127, -128,    0,\n                           0,    0,    0,  127, -128,    0,    0,    0,    0,  127,    0,  127,\n                           0,    0,    0,    0,    0,    0, -128,    0,    0,    0,    0, -128,\n                           0,    0,    0,    0,    0,    0,  127,    0,    0,  127,    0,  127,\n                         127,    0,    0, -128,    0,    0,    0,    0,    0,  127,    0, -128,\n                           0,    0,    0,  127,  127,    0,  127,    0,  127,  127,  127,    0,\n                           0,    0,    0,  127,  127,    0,    0,    0,    0,    0,  127,    0,\n                           0,    0,  127,    0, -128,    0,    0,    0,    0,    0,  127,    0,\n                           0,    0, -128,  127,    0,    0,    0,  127,  127,    0,    0,  127,\n                           0,    0,    0,  127,  127,    0,  127,    0,  127,    0, -128,  127,\n                           0, -128,  127,    0,    0,  127,    0,  127,    0,    0,    0,    0,\n                        -128,    0,    0,  127, -128,    0,    0,    0,    0,  127, -128,    0,\n                           0,    0,    0,    0, -128,    0, -128,    0,    0,    0,    0,  127,\n                           0,  127,    0,  127,    0,    0,  127, -128,    0,    0,  127, -128,\n                           0, -128,    0,    0,    0,    0, -128,  127,  127,    0,  127,  127,\n                        -128,    0,  127,    0,    0,  127,    0,    0,    0,    0,  127,    0,\n                           0,    0,    0,    0,    0,    0,    0,  127,    0,    0,  127,    0,\n                           0,    0,  127,    0,  127,    0, -128, -128,    0,  127,    0,    0,\n                           0,    0,  127,  127,  127,    0,    0,    0,    0,    0,  127,  127,\n                        -128,  127,    0,    0,    0,    0,    0,  127,  127,    0,    0, -128,\n                         127,  127,  127,    0,    0,    0,    0, -128, -128,    0,  127,    0,\n                           0,    0,    0, -128, -128,    0,  127,    0,    0,    0,    0,    0,\n                           0,    0, -128,  127,    0,    0,    0,    0,    0, -128,    0,    0,\n                         127,    0,  127,    0,    0,    0,    0,    0,    0,  127,    0,  127,\n                         127,    0,    0,    0,  127,    0,    0,    0,    0,    0,    0,    0,\n                           0,    0,    0,  127,    0,    0,  127,    0, -128,    0,  127, -128,\n                           0,    0,    0,  127,    0,    0,    0,    0,    0, -128,  127,    0,\n                           0,  127,    0,    0, -128,    0,    0,    0,  127,    0,    0,    0,\n                           0,    0,  127,    0,    0,  127,    0,    0, -128,  127,    0,    0,\n                         127,  127,    0,    0,    0,    0,    0,    0, -128,  127,    0,    0,\n                           0,    0,    0,    0,    0,    0,    0,    0,    0,  127,    0,    0,\n                           0,    0,    0,    0,  127,    0,    0,    0,    0,    0,    0,  127,\n                           0,    0,    0,    0,    0,    0,  127,    0,    0,  127,    0,  127,\n                           0,    0,    0,    0,    0, -128,    0,    0,    0,    0, -128,    0,\n                           0,    0,    0,    0,  127,    0,    0, -128,    0, -128,  127,    0,\n                           0, -128,    0,    0,  127,    0,    0,    0,  127,    0,    0,  127,\n                           0,    0,  127,    0,    0,    0,    0,    0,  127,    0,    0, -128,\n                         127,    0,    0,    0,  127,  127,    0,    0,    0,  127,    0,    0,\n                           0,    0,    0,    0,    0,    0,    0,    0,  127,  127,    0, -128],\n                       dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n                (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n                  min_val=tensor([-9.1947e-01, -2.6918e+00, -1.9380e-01, -5.6546e-01,  8.7016e-03,\n                          -6.1387e-01, -1.2384e+00, -5.4330e-01, -1.5239e+00, -1.6277e+00,\n                          -7.4639e-01, -1.2247e+00, -5.2811e-01, -4.5683e-01, -7.0233e-01,\n                          -1.3160e+00, -5.3764e-01, -1.5690e+00, -5.0382e-01,  6.3408e-02,\n                           2.4241e-01, -1.7269e+00, -3.0087e+00, -7.3765e-01,  1.9937e-01,\n                          -2.8162e-02, -4.7881e-01, -1.2864e+00, -1.1172e+00, -1.2728e+00,\n                          -7.3159e-01, -1.4887e+00, -1.1154e+00, -1.3974e+00, -5.7962e-01,\n                          -1.1790e+00, -1.4042e+00, -1.7048e+00, -4.5010e-02, -1.0251e+00,\n                          -9.7064e-01, -9.2559e-01, -9.2387e-01,  2.1859e-02, -1.5055e+00,\n                          -2.3701e+00, -1.5023e+00, -2.2488e-01, -9.4123e-01, -8.4935e-01,\n                          -4.6790e-01, -1.3313e+00, -9.5143e-01, -4.9173e-01, -1.6193e+00,\n                          -2.1304e+00, -2.3186e-03, -2.0367e-01, -5.3748e-01,  9.5927e-02,\n                          -5.9716e-01, -8.2729e-01,  6.0527e-01, -2.9430e+00,  2.9130e-01,\n                           1.0901e-01, -1.1388e+00, -1.4117e+00, -8.0115e-01, -7.2041e-01,\n                          -1.8997e+00,  2.5654e-01, -2.3993e-01, -1.0196e+00, -6.2529e-01,\n                          -2.2023e+00, -7.3922e-01, -1.3127e+00, -9.3506e-01, -4.4353e+00,\n                          -1.0674e-01, -1.1404e+00,  1.5599e-01, -2.9034e+00, -4.1965e-01,\n                          -7.1167e-01, -5.9098e-01, -4.0547e-01, -4.4860e-01, -1.3317e-01,\n                          -7.8722e-02, -8.8512e-01, -8.5522e-01, -6.5845e-02, -5.4530e-01,\n                          -1.6267e+00, -1.0038e-01, -1.1305e+00, -8.2258e-01, -4.6482e-01,\n                          -7.7607e-01,  9.4421e-02, -1.3219e+00, -4.9017e-01, -1.0389e+00,\n                          -5.8682e-01, -7.5619e-01, -5.8578e-01, -1.4935e-01, -2.4507e+00,\n                          -3.2130e-01, -5.8247e-01, -5.0455e-01, -1.5212e-01, -1.0839e+00,\n                          -3.2161e-01, -5.2966e-01, -5.3207e-01,  1.4233e-02, -1.6047e-01,\n                          -1.8939e+00, -1.8637e-01, -1.1864e-01, -4.9059e-01,  5.8003e-02,\n                          -1.4626e+00, -9.3332e-01, -5.9828e-01, -7.9682e-01, -4.6126e-01,\n                          -1.1085e+00, -6.7915e-01, -3.7527e-02, -1.8487e+00, -4.4846e-01,\n                          -7.8049e-01, -1.3132e+00, -5.1131e-01,  2.4994e-01, -1.1298e+00,\n                          -8.2665e-01, -3.5487e-01, -1.1555e+00,  8.7021e-02, -6.3520e-01,\n                          -1.5719e+00, -8.1437e-01, -2.1757e-01, -1.3841e+00, -8.6390e-01,\n                          -8.9531e-01, -1.8656e+00, -1.5766e+00, -5.3541e-01, -1.5581e+00,\n                          -5.1622e-01, -7.7502e-01, -4.4918e-01, -1.3765e+00,  9.7837e-02,\n                          -7.1203e-01, -7.4626e-01, -1.3947e+00, -1.7920e-01, -3.6103e-01,\n                          -5.5744e-01, -7.7329e-01,  2.2590e-01, -8.6100e-01, -8.4006e-01,\n                          -1.4387e+00, -6.2265e-01, -5.2288e-01, -1.1001e-01, -6.2362e-01,\n                          -2.1602e-01, -7.3123e-01, -5.3695e-01, -5.7443e-01, -3.8281e-01,\n                          -9.2851e-01, -5.2486e-01, -1.1315e+00, -5.1066e-01, -4.3717e-01,\n                          -1.4546e-01, -1.0330e+00, -8.9809e-01, -1.7220e+00, -6.1831e-01,\n                          -7.4856e-01, -5.8654e-01, -1.8770e+00, -1.3680e+00, -5.2479e-01,\n                          -3.7318e-01,  1.5265e-01, -9.7082e-01, -1.3592e-01, -1.0940e+00,\n                          -4.9010e-01, -1.2364e-01, -6.0478e-01, -1.4535e-02, -3.8368e-02,\n                          -1.2340e+00,  1.9517e-01, -4.8924e-01, -1.4904e+00, -9.4092e-01,\n                          -1.6642e+00, -6.0570e-01, -6.1069e-01, -6.1177e-01, -4.5663e-02,\n                          -9.7395e-01, -4.2860e-01, -7.8784e-01, -9.4862e-01, -7.1006e-01,\n                          -6.7419e-01, -1.2274e+00, -5.9111e-01, -1.1896e+00, -3.7669e-01,\n                          -9.0339e+00,  3.0920e-01, -9.8552e-01, -2.7063e+00,  8.2848e-04,\n                          -9.6098e-01, -1.9043e+00, -6.4845e-01, -4.6728e-01, -9.8587e-01,\n                          -6.7285e-01, -1.6849e+00, -1.2050e+00, -9.5365e-01, -1.1473e+00,\n                           2.3504e-02, -1.3873e+00, -3.6010e-01, -5.4763e-01,  2.6140e-02,\n                          -1.2055e+00, -8.1324e-01, -6.4320e-01, -9.4567e-01, -6.5833e-01,\n                           1.4023e-01, -1.1852e+00, -2.2090e-01, -8.6568e-01, -1.1330e+00,\n                          -1.1748e+00,  1.5496e-01, -3.8645e+00,  8.1916e-02, -3.8340e+00,\n                          -1.2781e+00, -1.1324e+00, -1.7909e+00, -6.8828e-01, -1.2691e+00,\n                          -5.9234e-01, -6.8454e-01, -6.2802e-01, -6.1784e-01, -7.2022e-01,\n                          -6.9790e-01,  4.8862e-02, -2.3783e+00, -1.0075e+00, -6.9419e-01,\n                           7.9710e-02, -1.4536e+00,  1.3763e-01, -1.0320e+00, -8.9548e-01,\n                          -3.2083e-02, -5.1919e-03,  1.1845e-01, -7.2430e-01, -7.0398e-01,\n                          -1.1606e+00, -1.1225e+00, -1.0048e+00,  1.4635e-01, -1.2131e+00,\n                          -6.4427e-01, -1.0884e+00, -1.1898e-01, -5.6392e-01, -1.1010e+00,\n                          -4.3148e-01, -6.9433e-01, -1.0889e+00, -7.6154e-01, -2.4995e+00,\n                          -3.6672e-02, -1.2282e+00, -6.1295e-01, -6.2287e-01, -4.8661e-01,\n                          -8.3898e-01, -6.6733e-01, -7.8911e-01, -1.2411e+00, -2.9453e-01,\n                          -6.8771e-01, -1.8391e-01, -2.4000e+00, -3.5865e-01, -6.0309e-01,\n                          -2.3772e+00, -7.3985e-01, -1.4440e+00,  2.2678e-01,  1.4390e-01,\n                          -1.1163e+00, -7.1180e-01, -1.9780e+00, -2.5742e+00, -1.1696e+00,\n                          -1.0169e+00, -8.3436e-01, -6.4915e-01, -7.9147e-01, -1.5497e-02,\n                          -6.8419e-01, -4.3702e-02, -1.0895e+00, -1.3673e+00, -7.0543e-01,\n                          -4.2388e-01,  3.6196e-01, -6.0946e-01, -9.7110e-01, -8.9048e-02,\n                          -2.6221e+00, -1.2018e+00, -9.8132e-01, -5.4346e-01, -5.0107e-01,\n                          -8.1132e-01, -7.7620e-01,  5.3628e-02, -6.7396e-01, -5.2707e-01,\n                          -6.9868e-01, -5.4498e-01, -9.1757e-01, -7.8948e-01, -1.2358e+00,\n                           1.4648e-01,  5.6664e-02, -1.9598e+00, -5.0864e-01, -7.1619e-02,\n                          -3.1355e+00, -3.5827e-01, -1.0622e+00,  7.2199e-02,  2.8747e-02,\n                          -8.4025e-01, -9.3513e-01, -9.8111e-01, -1.9335e+00, -9.5271e-01,\n                          -1.6158e+00, -2.5113e-01, -1.0872e+00, -1.6824e+00,  1.1591e-01,\n                          -6.7140e-01, -4.6672e-02, -1.2160e+00, -7.6515e-01, -1.0116e+00,\n                          -8.8335e-01,  5.8382e-02, -7.1013e-01, -6.6816e-01, -6.1698e-01,\n                          -8.4277e-01, -5.4362e-01, -5.6516e-01, -9.7603e-01, -9.2364e-01,\n                          -9.3387e-01, -8.8247e-01, -1.0489e+00, -4.9380e-01, -7.6100e-01,\n                          -4.7916e-01, -8.7011e-01, -9.7127e-01, -1.8051e+00, -1.8118e+00,\n                          -6.6025e-01, -6.2354e-01, -1.8004e+00, -2.3159e-01, -1.0276e+00,\n                          -1.0068e+00, -9.7068e-01, -1.1643e+00, -1.1737e+00, -1.7308e+00,\n                          -4.2956e-01, -5.7267e-01, -1.0848e+00, -1.3825e+00, -4.6168e-01,\n                          -1.6017e-01,  3.9420e-02, -1.1659e-01, -7.8302e-01,  1.8053e-02,\n                          -7.2048e-01, -3.7997e-02, -5.4567e-01, -5.9639e-01, -1.6072e-01,\n                          -1.8522e+00, -1.7913e-02, -1.1181e+00, -9.3983e-01,  2.2525e-01,\n                          -7.3640e-01, -7.1964e-01, -7.6641e-01, -6.9512e-01, -4.5905e-01,\n                          -1.0443e+00,  2.1061e-01, -8.4249e-01, -1.3024e+00, -1.0756e+00,\n                          -6.9643e-01, -2.6870e+00, -1.4243e-01, -1.3047e+00, -3.3709e-02,\n                          -8.6985e-01, -7.0612e-01, -6.6224e-01, -3.1833e-01, -5.7209e-01,\n                          -1.3701e-02, -7.7767e-01,  2.9836e-01, -6.5004e-01, -1.3746e+00,\n                          -1.7810e+00, -4.1673e-01, -6.9129e-01, -1.2350e+00, -8.7052e-01,\n                          -1.3443e+00, -3.7607e-01, -9.5358e-01, -5.4115e-01,  2.3899e-01,\n                          -1.8315e+00, -1.8433e+00, -1.3982e-01, -1.3832e+00, -2.1713e-01,\n                          -1.3682e+00, -7.7000e-01, -6.9619e-01, -3.9240e-01, -1.5775e+00,\n                          -4.4812e-01, -1.2772e+00, -8.4782e-01, -1.1826e+00, -2.8058e+00,\n                          -1.0263e+00, -6.0228e-01, -8.8130e-01, -1.5333e+00, -3.7425e-01,\n                          -1.1307e+00, -5.4832e-01, -1.5991e+00, -1.4074e+00, -1.2768e-02,\n                          -1.0893e+00, -5.5856e-01, -7.8907e-01, -5.9201e-01, -6.8046e-01,\n                          -1.3378e+00, -1.2602e+00, -1.2352e+00, -7.2472e-01, -6.2412e-01,\n                          -1.0519e+00, -5.2225e-01, -1.2784e+00, -1.2442e+00, -1.4530e+00,\n                          -1.2423e+00, -7.9162e-01, -2.1505e-01, -5.1075e-01,  3.1899e-02,\n                          -5.4968e-01, -1.0227e+00, -1.1843e-01, -4.3770e-01,  1.3238e-01,\n                          -1.1756e+00, -1.4163e-01, -5.3551e-01, -1.2753e+00, -1.7146e+00,\n                          -4.5866e-01, -4.2002e-01, -8.5804e-01,  2.1890e-01, -1.2281e+00,\n                           2.6455e-02, -5.4312e-01, -8.7589e-02, -7.2217e-01,  2.2431e-01,\n                          -8.3853e-02, -1.3858e+00, -6.1551e-01, -4.3835e-01, -3.9755e-02,\n                          -9.7205e-01, -6.0989e-01, -1.0096e+00, -7.2688e-01, -7.0164e-01,\n                          -1.8507e-01, -2.0174e-01, -6.8432e-01, -7.1473e-01, -5.6278e-02,\n                          -1.5799e+00, -6.5924e-01, -1.4094e-01, -8.2862e-01, -9.2067e-01,\n                          -9.2817e-01,  2.0251e-02, -9.0379e-01, -4.1170e-01, -5.0407e-01,\n                          -1.2394e+00, -5.0716e-01, -4.8255e-01, -2.0053e-01, -1.1053e+00,\n                          -2.1991e+00, -5.7694e-01, -1.6172e+00, -4.7014e-01, -6.9998e-01,\n                          -1.4833e-01, -9.4574e-01, -1.2507e+00, -1.0965e+00, -9.9027e-01,\n                          -1.2362e+00, -7.9448e-01, -7.0856e-01, -7.6779e-01, -1.6252e-01,\n                           1.4387e-01]), max_val=tensor([-2.4242e-01,  1.1964e+00,  1.9438e+00, -1.5443e-01,  1.6882e+00,\n                           1.1892e-01,  2.8083e+00, -3.2658e-01, -2.3094e-01,  2.9724e+00,\n                          -2.0503e-01,  5.1996e-01,  1.6225e+00,  4.8536e+00, -2.5792e-01,\n                           2.6436e+00,  1.6993e+00,  1.8811e+00, -1.5742e-01,  1.5525e+00,\n                           2.0004e+00,  2.0583e+00,  2.2722e+00,  2.4968e+00,  1.6023e+00,\n                           6.0347e-02,  2.1798e+00,  6.9428e-01,  2.1789e+00,  1.9422e+00,\n                          -4.1143e-01,  2.0567e+00,  1.4142e+00,  2.3875e+00, -1.5501e-01,\n                           2.4566e+00,  2.8663e+00,  3.4902e+00,  1.9653e+00,  9.4119e-01,\n                           2.2255e+00,  4.0348e-03,  2.1298e+00,  2.0571e+00,  3.4187e+00,\n                           2.0228e+00,  9.0463e-01,  1.2596e+00,  1.1382e+00,  9.2693e-01,\n                           3.2519e+00,  2.8161e+00,  1.1455e+00, -2.1629e-01,  1.3104e+00,\n                           7.2143e-01,  1.0676e+00,  1.9508e+00, -1.2189e-01,  2.3650e+00,\n                          -2.2279e-01,  1.7647e+00,  2.8132e+00,  2.2034e+00,  2.0677e+00,\n                           2.4290e+00,  1.3300e+00,  1.4486e+00, -1.5048e-01,  2.0303e+00,\n                           1.8629e+00,  1.4525e+00,  1.7955e+00,  1.6930e+00, -2.5249e-01,\n                           3.0954e+00, -2.7045e-01,  1.5738e+00,  1.2088e+00,  2.4489e+00,\n                           2.1104e+00, -2.0528e-01,  2.3528e+00,  1.4696e+00,  2.0153e+00,\n                          -9.3011e-02, -2.6459e-01,  1.6310e+00, -1.7315e-01,  1.0739e+00,\n                           2.3422e+00,  1.0295e+00,  1.5405e+00,  1.5757e+00,  2.9875e+00,\n                           2.1316e+00,  1.2129e+00,  7.5438e-01,  1.0700e+00, -1.7431e-01,\n                           1.8163e+00,  2.3701e+00,  1.1500e+00, -8.5373e-02,  9.1861e-01,\n                          -1.7485e-01, -4.2113e-01, -2.3202e-01,  2.6145e+00,  9.5826e-01,\n                           2.9750e+00, -2.1423e-01,  1.7137e+00,  1.1032e+00,  2.6865e+00,\n                           2.3595e+00, -3.6469e-01, -2.2236e-01,  1.9789e+00,  1.4614e+00,\n                           1.5662e+00,  1.9999e+00,  1.3858e-01, -1.3979e-01,  1.2856e+00,\n                           3.3087e+00,  1.6563e+00,  2.2770e+00,  2.9052e+00, -2.6888e-01,\n                           3.4193e+00, -1.8324e-01,  1.8397e-01,  1.8802e+00,  2.5827e+00,\n                           2.2005e+00,  1.4400e+00,  1.8766e+00,  2.3688e+00,  2.0699e+00,\n                           2.2858e+00,  3.0258e+00,  2.1903e+00,  2.8913e+00,  2.0203e+00,\n                           5.4660e+00,  6.1097e-01,  3.4547e+00,  1.8862e+00,  1.1172e+00,\n                          -3.1912e-01,  2.7726e+00,  9.5683e-01, -3.4511e-01,  1.4827e+00,\n                          -2.0492e-01, -2.1638e-01,  1.6779e+00,  2.4207e+00,  2.8787e+00,\n                           6.0261e-01,  2.0383e-02,  1.3051e+00,  2.2226e+00,  2.9339e+00,\n                          -2.9774e-01,  1.7218e+00,  2.1790e+00,  1.8439e+00,  1.7425e+00,\n                           1.7254e+00, -2.0734e-01, -2.2167e-01,  2.2066e+00, -2.0736e-01,\n                           4.9874e+00, -2.5431e-01, -1.7402e-01, -1.5213e-01,  1.5183e+00,\n                           1.0711e+00,  3.1135e+00,  1.1824e+00, -2.2420e-01, -2.8898e-01,\n                           2.6873e+00,  2.0909e+00,  1.8819e+00,  2.8945e+00,  1.7041e+00,\n                          -2.9061e-01,  2.4206e+00,  3.9934e+00,  1.5821e+00, -2.3626e-01,\n                           2.0074e+00,  1.7392e+00,  2.0946e+00,  2.1158e+00,  3.8944e+00,\n                           1.7499e+00,  2.3909e+00, -2.2869e-01,  9.2398e-01,  3.9276e+00,\n                           1.6703e+00,  1.4109e+00, -1.4888e-01,  1.1483e+00,  1.9743e+00,\n                           2.8573e+00, -1.9699e-01, -3.4191e-01,  1.7834e+00,  2.3884e+00,\n                          -2.1880e-01,  2.2963e+00,  1.8025e+00,  1.6069e+00, -2.4184e-01,\n                          -1.8767e-01,  1.9513e+00, -2.5595e-01,  1.0352e+00, -2.8297e-01,\n                           4.6850e+00,  1.2336e+00, -2.2986e-01,  3.5981e+00,  2.2103e+00,\n                          -1.3606e-01,  2.5807e+00,  1.4594e+00, -2.1424e-01,  5.4197e-01,\n                          -1.8354e-01,  1.2179e+00,  1.7747e+00,  1.7329e+00,  1.7433e+00,\n                           1.5949e+00,  2.0574e+00,  3.5132e+00, -8.5702e-02,  1.8083e+00,\n                           1.5716e+00,  9.9783e-01,  1.9605e+00,  1.2692e+00, -8.9260e-02,\n                           2.2829e+00,  1.7207e+00,  1.1496e+00,  1.3759e+00,  1.6829e+00,\n                           1.8719e+00,  2.1833e+00,  2.7230e+00,  1.5003e+00,  3.1269e+00,\n                           2.4382e+00,  1.9717e+00,  3.1415e+00, -3.5098e-01,  2.4231e+00,\n                          -1.5468e-01,  3.2532e-01, -1.6917e-01,  1.7127e-03,  1.0623e+00,\n                          -3.1960e-01,  2.3981e+00,  1.0414e+00,  9.2241e-01, -1.3825e-01,\n                           2.0281e+00,  2.8984e+00,  1.8026e+00,  1.9132e+00,  1.8137e+00,\n                           4.2940e-02,  2.1732e-01,  9.0555e-01, -3.7718e-01, -2.9293e-01,\n                           1.2878e+00, -5.7520e-01, -2.3838e-01,  2.7177e+00,  2.6931e-01,\n                          -1.5705e-01,  4.2040e-02,  2.1667e+00, -2.3622e-01,  1.7472e+00,\n                           2.9581e+00,  2.1852e+00,  1.6368e+00, -2.3140e-01,  1.8548e+00,\n                           2.1539e+00,  1.5309e+00,  2.0309e-01,  2.5375e+00,  2.2585e+00,\n                           1.6660e+00,  8.7513e-01, -5.3344e-01,  2.0275e+00,  1.6212e+00,\n                          -2.3992e-01,  3.3426e+00,  1.7167e+00,  1.8746e+00, -1.3697e-01,\n                           1.6976e+00, -4.3732e-01,  3.3483e+00,  2.1828e+00,  2.1115e+00,\n                           1.8780e+00, -3.9756e-02,  1.1801e+00,  1.5643e+00,  2.3274e+00,\n                           1.5713e+00, -1.5582e-01, -8.7352e-02, -4.0054e-02,  2.0517e+00,\n                           1.5199e-01,  1.2221e+00,  4.2451e+00,  1.2230e+00, -5.6120e-02,\n                          -1.1488e-01,  3.3421e+00, -3.9952e-01,  1.7842e+00,  1.9277e+00,\n                           7.7011e-01,  1.5556e+00,  1.9660e+00, -2.5470e-01, -1.2361e-01,\n                           2.1918e+00,  1.7800e+00,  1.8400e+00, -3.3053e-01, -2.2706e-01,\n                          -1.9191e-01,  1.4545e+00,  2.5412e+00,  3.2911e+00,  9.4238e-01,\n                           2.2244e+00,  1.4546e+00,  9.8629e-01, -1.9255e-01,  9.2163e-01,\n                           4.2559e+00,  1.6020e+00,  2.7315e+00,  1.1754e+00,  1.0691e+00,\n                           1.8874e+00, -2.2287e-01,  1.0920e+00,  1.0317e+00,  1.2443e+00,\n                           1.6793e+00,  2.1594e+00,  2.3973e+00,  1.3534e+00,  1.4299e+00,\n                          -2.2860e-01,  1.8769e+00,  1.6661e+00,  1.3350e+00,  1.7038e+00,\n                           2.2177e+00,  1.8113e+00,  2.1297e+00,  2.2643e+00, -2.9633e-01,\n                           1.0017e+00, -2.2661e-01,  2.9105e+00,  1.5404e+00,  9.9163e-01,\n                           1.1270e+00,  2.2571e+00,  2.5411e+00, -2.0331e-02,  1.6350e+00,\n                          -2.8819e-01, -2.1240e-01,  1.2796e+00,  1.1456e+00,  2.6770e+00,\n                          -1.4244e-01,  1.8088e+00,  2.6050e+00,  2.8202e+00,  1.6363e+00,\n                           1.4915e+00,  1.0606e+00,  2.1175e+00,  1.4698e+00,  2.1235e+00,\n                           1.3345e+00, -2.0746e-01,  1.6615e+00,  2.3639e+00, -1.8288e-01,\n                           2.2817e+00,  3.1242e+00,  1.3468e+00, -1.8447e-01,  1.1928e+00,\n                           2.7655e+00,  2.1808e+00,  1.3467e+00, -4.5840e-02,  1.7975e+00,\n                           1.7779e+00,  2.9709e+00,  1.3726e+00,  1.3934e+00,  2.3822e+00,\n                          -1.3942e-01,  2.7815e+00,  1.4822e+00, -1.5627e-01,  2.1701e+00,\n                           1.4909e+00,  1.8642e+00,  1.8455e+00,  1.7970e+00,  9.4309e-01,\n                          -3.4806e-02,  3.7393e+00,  1.8344e+00,  2.1099e+00,  1.2119e+00,\n                           1.7228e+00, -1.2565e-01,  1.8936e+00,  4.3109e+00, -1.3888e-01,\n                           1.3891e+00,  1.7828e+00,  1.0153e+00, -2.8905e-01,  2.9541e+00,\n                           8.5410e-01, -1.3699e-01, -3.8384e-01,  1.5112e+00,  1.8360e+00,\n                           2.4687e+00,  2.1006e+00,  1.3568e+00,  2.7064e+00,  1.3803e+00,\n                          -1.0819e-01,  1.7426e+00,  9.7908e-01,  2.0758e+00,  1.9417e+00,\n                           2.0736e+00,  7.5359e-01,  4.5721e-02,  2.4426e+00,  1.3025e-01,\n                           2.1758e+00,  8.4595e-01, -2.4974e-01,  2.3198e+00,  1.4246e+00,\n                           3.3231e+00,  1.6747e-02,  1.3012e+00,  7.5887e-01, -1.1670e-01,\n                           1.0384e+00,  1.2646e+00,  1.5383e+00,  1.2991e+00,  2.2530e+00,\n                           5.8811e+00, -2.0932e-01,  3.9042e-02,  4.6171e+00,  2.4778e+00,\n                           1.5553e+00,  2.0693e+00,  1.1527e+00, -3.5207e-01,  2.4465e+00,\n                           2.7486e+00, -1.3225e-01,  1.5033e+00, -1.0027e-01,  3.2996e+00,\n                           7.9115e-01,  8.7205e-01,  4.3001e+00,  1.8158e+00,  1.6044e+00,\n                           3.0159e+00,  8.6846e-01,  9.8817e-01,  1.5812e+00,  1.9298e+00,\n                           2.0625e+00,  1.6336e+00,  1.8502e+00,  1.7173e+00,  1.0468e+00,\n                          -1.9146e-01,  2.6771e+00,  2.1641e+00,  1.3143e+00,  2.6810e+00,\n                           2.8647e+00, -1.3558e-01,  1.7455e+00,  2.9428e+00,  1.3648e+00,\n                           2.2073e+00,  1.5996e+00, -2.9552e-01,  1.4434e+00,  7.4425e-02,\n                           1.8245e+00, -2.9620e-01,  2.4629e+00,  1.9917e+00, -1.0614e-01,\n                           2.4496e+00,  2.6728e+00, -2.0603e-01,  9.2115e-01,  7.7149e-02,\n                           1.5362e+00,  3.9332e-02,  1.6723e+00, -3.2545e-01,  1.2248e+00,\n                           1.0640e+00,  1.4373e+00, -4.8006e-02,  1.3973e+00,  1.6067e+00,\n                           1.7432e+00, -9.7235e-02, -1.6157e-01,  1.9916e+00,  1.1891e+00,\n                           3.2870e+00, -2.6078e-01,  1.3951e+00,  1.4074e+00,  9.1983e-01,\n                           3.6866e+00,  2.2914e+00,  1.7162e+00,  3.1726e+00,  2.7671e+00,\n                           1.9299e+00,  1.0422e+00, -2.7073e-01, -3.6847e-01,  1.7935e+00,\n                           1.2220e+00])\n                )\n              )\n            )\n            (2): ReLU6(inplace=True)\n          )\n          (2): ConvBn2d(\n            576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0024, 0.0052, 0.0032, 0.0024, 0.0034, 0.0056, 0.0029, 0.0031, 0.0024,\n                      0.0045, 0.0028, 0.0030, 0.0026, 0.0021, 0.0036, 0.0054, 0.0033, 0.0028,\n                      0.0038, 0.0069, 0.0031, 0.0039, 0.0034, 0.0030, 0.0054, 0.0035, 0.0027,\n                      0.0030, 0.0025, 0.0025, 0.0030, 0.0030, 0.0084, 0.0033, 0.0029, 0.0046,\n                      0.0028, 0.0034, 0.0027, 0.0034, 0.0020, 0.0039, 0.0037, 0.0027, 0.0025,\n                      0.0028, 0.0027, 0.0027, 0.0044, 0.0037, 0.0018, 0.0029, 0.0019, 0.0034,\n                      0.0085, 0.0049, 0.0031, 0.0019, 0.0041, 0.0027, 0.0031, 0.0037, 0.0033,\n                      0.0054, 0.0025, 0.0022, 0.0028, 0.0033, 0.0027, 0.0034, 0.0055, 0.0047,\n                      0.0033, 0.0031, 0.0028, 0.0048, 0.0097, 0.0036, 0.0020, 0.0020, 0.0035,\n                      0.0054, 0.0053, 0.0030, 0.0030, 0.0028, 0.0079, 0.0036, 0.0026, 0.0030,\n                      0.0043, 0.0030, 0.0030, 0.0037, 0.0074, 0.0031]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                     dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n                min_val=tensor([-0.3081, -0.4051, -0.2637, -0.3045, -0.4268, -0.4365, -0.3734, -0.3569,\n                        -0.3069, -0.5725, -0.2460, -0.3352, -0.3367, -0.2683, -0.4246, -0.6891,\n                        -0.3683, -0.3203, -0.4882, -0.3841, -0.3908, -0.4957, -0.4307, -0.3896,\n                        -0.3457, -0.3959, -0.3398, -0.3526, -0.3143, -0.2990, -0.3277, -0.3761,\n                        -0.3777, -0.4150, -0.3658, -0.5939, -0.3517, -0.4358, -0.3491, -0.3142,\n                        -0.2389, -0.5035, -0.4679, -0.3441, -0.3234, -0.3262, -0.3099, -0.3045,\n                        -0.4189, -0.4078, -0.2269, -0.3716, -0.2406, -0.3477, -0.5168, -0.6212,\n                        -0.3913, -0.2460, -0.3692, -0.3461, -0.2919, -0.4686, -0.2604, -0.3365,\n                        -0.2078, -0.2854, -0.3643, -0.3690, -0.3094, -0.4394, -0.5579, -0.6005,\n                        -0.4226, -0.3455, -0.3093, -0.6115, -1.2466, -0.4063, -0.2502, -0.2286,\n                        -0.4029, -0.6928, -0.6766, -0.3328, -0.3621, -0.3216, -1.0095, -0.4617,\n                        -0.3338, -0.3800, -0.4615, -0.3868, -0.3786, -0.3362, -0.4057, -0.3298]), max_val=tensor([0.3001, 0.6581, 0.4042, 0.2376, 0.4336, 0.7054, 0.2817, 0.3965, 0.2287,\n                        0.4547, 0.3527, 0.3858, 0.2983, 0.2707, 0.4523, 0.4725, 0.4184, 0.3581,\n                        0.3738, 0.8772, 0.3397, 0.4892, 0.2410, 0.3707, 0.6810, 0.4432, 0.3453,\n                        0.3864, 0.2322, 0.3138, 0.3749, 0.3859, 1.0616, 0.4234, 0.2839, 0.3770,\n                        0.3504, 0.4216, 0.3026, 0.4264, 0.2600, 0.3981, 0.4144, 0.3355, 0.2713,\n                        0.3544, 0.3473, 0.3491, 0.5571, 0.4648, 0.2284, 0.3097, 0.2207, 0.4306,\n                        1.0733, 0.5624, 0.3306, 0.1971, 0.5189, 0.3317, 0.3965, 0.3689, 0.4186,\n                        0.6839, 0.3129, 0.2499, 0.3349, 0.4149, 0.3370, 0.3966, 0.6997, 0.5362,\n                        0.3340, 0.3981, 0.3526, 0.3724, 0.3598, 0.4625, 0.2274, 0.2500, 0.4411,\n                        0.5521, 0.6138, 0.3760, 0.3836, 0.3528, 0.4828, 0.4070, 0.2999, 0.2782,\n                        0.5518, 0.3329, 0.3104, 0.4755, 0.9396, 0.3953])\n              )\n            )\n          )\n        )\n      )\n      (14): Module(\n        (conv): Module(\n          (0): Module(\n            (0): ConvBn2d(\n              96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False\n              (bn): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n                fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([4.0051e-04, 9.0168e-04, 8.1152e-04, 8.4301e-04, 5.5881e-04, 4.3283e-04,\n                        4.6759e-04, 6.9043e-04, 8.0544e-04, 7.1430e-04, 5.8002e-04, 5.4078e-04,\n                        5.6623e-04, 4.4510e-04, 6.6953e-04, 6.8125e-04, 3.8093e-04, 5.6259e-04,\n                        6.1417e-04, 8.8105e-04, 4.9928e-04, 7.5327e-04, 7.2790e-04, 6.1158e-04,\n                        5.0890e-04, 2.4135e-04, 6.1077e-04, 6.0675e-04, 7.5205e-04, 6.4799e-04,\n                        6.3489e-04, 5.5921e-04, 9.0370e-04, 8.0654e-04, 9.3338e-04, 8.6399e-04,\n                        6.4616e-04, 6.3943e-04, 7.4950e-04, 7.6878e-04, 4.1377e-04, 5.2266e-04,\n                        5.5396e-04, 9.5852e-04, 7.3271e-04, 5.5420e-04, 5.7853e-04, 5.3002e-04,\n                        6.2288e-04, 6.5070e-04, 5.8276e-04, 7.2577e-04, 5.2698e-04, 6.3104e-04,\n                        6.2012e-04, 5.9613e-04, 6.3780e-04, 5.6990e-04, 9.1654e-04, 4.6325e-04,\n                        5.8091e-04, 9.4831e-04, 5.9034e-04, 5.1218e-04, 4.1858e-04, 6.2869e-04,\n                        7.0095e-04, 7.1687e-04, 5.5675e-04, 7.6968e-04, 4.5789e-04, 6.5343e-04,\n                        6.6095e-04, 5.3699e-04, 5.4219e-04, 7.0704e-04, 6.5217e-04, 8.5885e-04,\n                        2.3635e-04, 5.2966e-04, 5.4639e-04, 5.2821e-04, 7.5294e-04, 6.7431e-04,\n                        8.8479e-04, 6.3128e-04, 5.6854e-04, 7.5228e-04, 5.1398e-04, 4.9048e-04,\n                        5.0525e-04, 8.0239e-04, 9.5652e-04, 9.3978e-04, 5.5866e-04, 6.9203e-04,\n                        4.4494e-04, 4.6951e-04, 6.9827e-04, 7.2791e-04, 9.3282e-04, 7.6151e-04,\n                        8.7694e-04, 1.0779e-03, 6.2839e-04, 9.3735e-04, 7.0679e-04, 5.9408e-04,\n                        1.0902e-03, 6.0339e-04, 6.1344e-04, 8.5461e-04, 7.9176e-04, 4.4054e-04,\n                        5.6719e-04, 8.4974e-04, 8.2958e-04, 7.5674e-04, 7.2940e-04, 9.2716e-04,\n                        7.6003e-04, 2.6131e-04, 8.2775e-04, 5.1982e-04, 5.3506e-04, 5.1547e-04,\n                        9.9542e-04, 5.0727e-04, 5.9525e-04, 4.4968e-04, 4.7722e-04, 7.7055e-04,\n                        7.7274e-04, 6.3312e-04, 3.7908e-04, 2.7684e-04, 5.7213e-04, 8.0533e-04,\n                        1.1221e-03, 5.2797e-04, 6.4685e-04, 7.0314e-04, 3.1026e-04, 5.8147e-04,\n                        4.1937e-04, 5.8840e-04, 5.5206e-04, 2.4448e-04, 6.4969e-04, 7.0570e-04,\n                        5.1244e-04, 5.2829e-04, 6.5211e-04, 5.9007e-04, 7.6251e-04, 5.8893e-04,\n                        6.3490e-04, 9.6692e-04, 7.6568e-04, 6.4778e-04, 5.0923e-04, 7.0056e-04,\n                        6.0810e-04, 5.3594e-04, 1.2086e-03, 7.4305e-04, 4.5674e-04, 5.8536e-04,\n                        6.2171e-04, 5.8725e-04, 5.5961e-04, 1.0611e-03, 5.7705e-04, 6.3834e-04,\n                        5.3897e-04, 6.5800e-04, 5.3548e-04, 5.9284e-04, 5.4473e-04, 4.9564e-04,\n                        2.5126e-04, 7.5367e-04, 7.0306e-04, 6.9612e-04, 6.9182e-04, 5.9299e-04,\n                        6.7229e-04, 5.2425e-04, 7.5082e-04, 5.4223e-04, 7.2232e-04, 5.2223e-04,\n                        5.4720e-04, 5.3692e-04, 1.0937e-03, 4.7999e-04, 5.1438e-04, 4.7570e-04,\n                        4.6628e-04, 6.5669e-04, 5.7295e-04, 8.8391e-04, 6.6195e-04, 1.2913e-03,\n                        8.1684e-04, 5.0766e-04, 7.3253e-04, 5.6528e-04, 6.4431e-04, 8.7617e-04,\n                        8.2115e-04, 6.2853e-04, 5.6507e-04, 5.5513e-04, 7.3777e-04, 5.8774e-04,\n                        4.6257e-04, 7.4543e-04, 5.5329e-04, 6.8422e-04, 4.9827e-04, 4.4848e-04,\n                        5.7458e-04, 6.2032e-04, 7.3211e-04, 4.4283e-04, 7.6992e-04, 5.5402e-04,\n                        1.0109e-03, 5.3009e-04, 8.0974e-04, 4.7947e-04, 5.3829e-04, 7.6450e-04,\n                        6.7124e-04, 4.2686e-04, 7.6160e-04, 5.5249e-04, 3.6644e-04, 5.3034e-04,\n                        6.3588e-04, 8.3580e-04, 6.3058e-04, 5.7644e-04, 4.9844e-04, 4.9793e-04,\n                        5.0970e-04, 6.2262e-04, 4.7929e-04, 5.5099e-04, 3.9900e-04, 5.1728e-04,\n                        1.1178e-03, 5.6317e-04, 6.3070e-04, 4.1011e-04, 5.6203e-04, 3.9669e-04,\n                        6.5357e-04, 3.6619e-04, 6.8852e-04, 5.9284e-04, 4.8017e-04, 7.1265e-04,\n                        3.3992e-04, 6.0701e-04, 6.6436e-04, 3.0949e-04, 7.6352e-04, 8.4265e-04,\n                        8.1057e-04, 6.5945e-04, 1.0132e-03, 5.9967e-04, 7.2180e-04, 4.8511e-04,\n                        7.8718e-04, 7.0229e-04, 6.0214e-04, 7.7618e-04, 5.2148e-04, 7.5797e-04,\n                        8.0800e-04, 4.5925e-04, 4.8130e-04, 6.4055e-04, 6.9065e-04, 6.7391e-04,\n                        6.0262e-04, 7.8284e-04, 7.8542e-04, 7.4634e-04, 7.1965e-04, 6.1859e-04,\n                        5.7214e-04, 5.9384e-04, 4.6215e-04, 4.0268e-04, 5.4859e-04, 4.8409e-04,\n                        7.2065e-04, 4.1126e-04, 5.1416e-04, 5.8749e-04, 8.5146e-06, 8.2202e-04,\n                        9.6606e-04, 7.2301e-04, 6.1658e-04, 6.8058e-04, 7.0023e-04, 6.5502e-04,\n                        5.5638e-04, 5.0736e-04, 5.0466e-04, 4.2412e-04, 1.4037e-03, 7.5556e-04,\n                        5.5221e-04, 4.6173e-04, 5.6244e-04, 5.6625e-04, 6.6842e-04, 5.3185e-04,\n                        5.8519e-04, 6.5108e-04, 5.6746e-04, 7.3845e-04, 1.0851e-03, 9.4242e-04,\n                        5.5177e-04, 6.2455e-04, 6.9299e-04, 2.2811e-04, 3.7095e-04, 4.8655e-04,\n                        5.5306e-04, 5.9667e-04, 6.2259e-04, 5.5745e-04, 7.2494e-04, 6.3319e-04,\n                        5.7337e-04, 9.0765e-04, 5.5235e-04, 5.8483e-04, 7.7306e-04, 7.5702e-04,\n                        6.7042e-04, 4.3433e-04, 4.8197e-04, 6.0690e-04, 4.7494e-04, 5.9809e-04,\n                        6.6133e-04, 5.6612e-04, 7.3782e-04, 4.0084e-04, 6.3442e-04, 5.3657e-04,\n                        5.3459e-04, 7.4408e-04, 6.4764e-04, 7.3528e-04, 5.7906e-04, 5.5044e-04,\n                        5.7339e-04, 4.1191e-04, 7.6829e-04, 5.6428e-04, 2.2040e-04, 6.3208e-04,\n                        5.7833e-04, 7.7936e-04, 8.0285e-04, 5.7509e-04, 6.6140e-04, 6.3089e-04,\n                        7.1796e-04, 6.8385e-04, 7.7455e-04, 8.6179e-04, 6.0543e-04, 5.7203e-04,\n                        6.1699e-04, 7.5941e-04, 5.8415e-04, 7.4324e-04, 5.9494e-04, 5.9762e-04,\n                        5.9805e-04, 5.8946e-04, 5.7276e-04, 5.4641e-04, 4.8443e-04, 5.3309e-04,\n                        7.9163e-04, 1.0065e-03, 1.1698e-03, 5.3711e-04, 7.0814e-04, 5.1636e-04,\n                        2.2479e-04, 7.2779e-04, 5.9977e-04, 5.8357e-04, 5.6116e-04, 7.1279e-04,\n                        5.5520e-04, 6.3959e-04, 7.8135e-04, 3.7672e-04, 6.3408e-04, 7.4962e-04,\n                        6.7031e-04, 5.2946e-04, 3.2955e-04, 7.1516e-04, 7.2594e-04, 7.4104e-04,\n                        7.2352e-04, 6.1543e-04, 6.0440e-04, 1.1220e-04, 6.6280e-04, 5.5621e-04,\n                        5.7589e-04, 8.7930e-04, 6.2237e-04, 3.8808e-04, 7.0460e-04, 3.9757e-04,\n                        2.7025e-04, 8.2379e-04, 4.3206e-04, 6.2407e-04, 5.4030e-04, 7.4857e-04,\n                        4.1183e-04, 4.0672e-04, 6.7843e-04, 7.3355e-04, 7.1095e-04, 6.1691e-04,\n                        4.9636e-04, 3.3472e-04, 6.3072e-04, 5.9574e-04, 5.7818e-04, 5.6572e-04,\n                        6.0448e-04, 7.1359e-04, 6.6494e-04, 5.6059e-04, 7.4963e-04, 1.5901e-04,\n                        3.1519e-04, 6.3399e-04, 6.2590e-04, 5.6785e-04, 4.7713e-04, 5.6732e-04,\n                        5.4244e-04, 8.7423e-04, 2.1661e-04, 6.5171e-04, 4.2839e-04, 4.6544e-04,\n                        6.1800e-04, 6.4912e-04, 8.1812e-04, 5.9378e-04, 6.4147e-04, 4.9274e-04,\n                        9.0894e-04, 4.0821e-04, 6.7922e-04, 9.4689e-04, 4.9070e-04, 6.5087e-04,\n                        5.3243e-04, 5.3224e-04, 5.1398e-04, 5.6763e-04, 6.1457e-04, 6.5233e-04,\n                        7.3134e-04, 5.8927e-04, 6.0400e-04, 6.5571e-04, 6.7053e-04, 8.8092e-04,\n                        5.5840e-04, 8.3294e-04, 4.6633e-04, 7.6594e-04, 5.3378e-04, 6.3126e-04,\n                        5.4359e-04, 7.5541e-04, 5.2578e-04, 5.8872e-04, 7.1424e-04, 8.5244e-04,\n                        7.0935e-04, 5.9353e-04, 5.1388e-04, 4.7397e-04, 1.9084e-04, 6.7768e-04,\n                        5.0060e-04, 5.1244e-04, 4.6455e-04, 5.9293e-04, 7.1912e-04, 7.4372e-04,\n                        7.4327e-04, 7.6702e-04, 6.1528e-04, 5.7416e-04, 6.7123e-04, 6.3675e-04,\n                        6.6795e-04, 8.8661e-04, 8.0310e-04, 5.3379e-04, 4.7152e-04, 7.6923e-04,\n                        5.3839e-04, 6.1909e-04, 6.0300e-04, 5.8869e-04, 8.2670e-04, 8.0147e-04,\n                        6.8320e-04, 5.2891e-04, 5.2886e-04, 6.4396e-04, 6.7505e-04, 4.6194e-04,\n                        8.1067e-04, 4.8702e-04, 9.8793e-04, 1.2559e-05, 6.3138e-04, 4.6993e-04,\n                        8.6387e-04, 3.3234e-04, 1.3594e-03, 6.2995e-04, 6.4401e-04, 5.2776e-04,\n                        6.4887e-04, 5.4325e-04, 4.7120e-04, 7.3638e-04, 5.3786e-04, 6.1259e-04,\n                        6.0354e-04, 4.7960e-04, 6.4313e-04, 1.0325e-03, 7.8348e-04, 3.9256e-04,\n                        5.1235e-04, 8.6634e-04, 3.8521e-04, 7.0340e-04, 6.8299e-04, 4.7107e-04,\n                        5.8582e-04, 6.5223e-04, 7.0496e-04, 7.3368e-04, 7.6730e-04, 6.5771e-04]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                       dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n                (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n                  min_val=tensor([-0.0493, -0.1154, -0.0777, -0.0760, -0.0595, -0.0554, -0.0529, -0.0799,\n                          -0.0785, -0.0914, -0.0742, -0.0553, -0.0725, -0.0570, -0.0857, -0.0872,\n                          -0.0421, -0.0544, -0.0786, -0.0621, -0.0487, -0.0781, -0.0932, -0.0783,\n                          -0.0651, -0.0309, -0.0782, -0.0736, -0.0587, -0.0724, -0.0813, -0.0671,\n                          -0.0734, -0.1032, -0.1131, -0.0628, -0.0827, -0.0818, -0.0915, -0.0768,\n                          -0.0374, -0.0593, -0.0709, -0.1227, -0.0938, -0.0709, -0.0516, -0.0678,\n                          -0.0797, -0.0728, -0.0746, -0.0929, -0.0675, -0.0442, -0.0671, -0.0646,\n                          -0.0816, -0.0618, -0.0713, -0.0543, -0.0744, -0.0513, -0.0756, -0.0656,\n                          -0.0510, -0.0772, -0.0762, -0.0918, -0.0713, -0.0585, -0.0571, -0.0712,\n                          -0.0625, -0.0517, -0.0694, -0.0905, -0.0809, -0.1063, -0.0247, -0.0678,\n                          -0.0699, -0.0676, -0.0806, -0.0863, -0.0898, -0.0808, -0.0728, -0.0963,\n                          -0.0658, -0.0568, -0.0517, -0.1027, -0.1224, -0.1203, -0.0715, -0.0628,\n                          -0.0434, -0.0540, -0.0617, -0.0793, -0.1187, -0.0742, -0.1122, -0.1130,\n                          -0.0804, -0.1200, -0.0763, -0.0760, -0.1395, -0.0772, -0.0785, -0.1094,\n                          -0.0775, -0.0466, -0.0726, -0.1088, -0.0654, -0.0969, -0.0902, -0.0901,\n                          -0.0973, -0.0299, -0.0665, -0.0472, -0.0685, -0.0660, -0.1274, -0.0649,\n                          -0.0762, -0.0468, -0.0611, -0.0746, -0.0989, -0.0719, -0.0485, -0.0324,\n                          -0.0732, -0.0611, -0.1436, -0.0676, -0.0547, -0.0900, -0.0382, -0.0699,\n                          -0.0537, -0.0595, -0.0707, -0.0313, -0.0628, -0.0903, -0.0656, -0.0578,\n                          -0.0835, -0.0755, -0.0823, -0.0576, -0.0813, -0.1238, -0.0980, -0.0829,\n                          -0.0526, -0.0611, -0.0477, -0.0469, -0.0788, -0.0896, -0.0585, -0.0707,\n                          -0.0757, -0.0752, -0.0716, -0.0865, -0.0562, -0.0817, -0.0663, -0.0842,\n                          -0.0685, -0.0759, -0.0697, -0.0634, -0.0228, -0.0955, -0.0900, -0.0777,\n                          -0.0886, -0.0759, -0.0861, -0.0494, -0.0907, -0.0694, -0.0819, -0.0668,\n                          -0.0700, -0.0499, -0.0836, -0.0502, -0.0658, -0.0609, -0.0573, -0.0713,\n                          -0.0733, -0.1131, -0.0847, -0.1653, -0.0854, -0.0650, -0.0938, -0.0724,\n                          -0.0825, -0.0692, -0.1051, -0.0795, -0.0634, -0.0711, -0.0944, -0.0618,\n                          -0.0566, -0.0893, -0.0575, -0.0876, -0.0638, -0.0574, -0.0680, -0.0794,\n                          -0.0937, -0.0415, -0.0985, -0.0674, -0.0915, -0.0679, -0.0564, -0.0568,\n                          -0.0630, -0.0817, -0.0859, -0.0540, -0.0975, -0.0575, -0.0404, -0.0609,\n                          -0.0814, -0.1070, -0.0807, -0.0738, -0.0623, -0.0589, -0.0652, -0.0797,\n                          -0.0516, -0.0623, -0.0511, -0.0602, -0.1034, -0.0721, -0.0807, -0.0525,\n                          -0.0709, -0.0508, -0.0702, -0.0469, -0.0881, -0.0759, -0.0615, -0.0912,\n                          -0.0300, -0.0620, -0.0801, -0.0320, -0.0977, -0.1079, -0.1038, -0.0713,\n                          -0.0808, -0.0768, -0.0895, -0.0621, -0.0617, -0.0533, -0.0661, -0.0833,\n                          -0.0667, -0.0797, -0.0947, -0.0588, -0.0509, -0.0820, -0.0884, -0.0466,\n                          -0.0735, -0.1001, -0.1005, -0.0692, -0.0921, -0.0792, -0.0664, -0.0513,\n                          -0.0540, -0.0475, -0.0519, -0.0620, -0.0852, -0.0526, -0.0658, -0.0752,\n                          -0.0011, -0.1052, -0.1237, -0.0798, -0.0616, -0.0789, -0.0704, -0.0618,\n                          -0.0676, -0.0518, -0.0646, -0.0543, -0.1797, -0.0800, -0.0606, -0.0591,\n                          -0.0720, -0.0586, -0.0856, -0.0598, -0.0590, -0.0797, -0.0726, -0.0945,\n                          -0.1389, -0.0659, -0.0706, -0.0799, -0.0743, -0.0283, -0.0475, -0.0623,\n                          -0.0708, -0.0473, -0.0593, -0.0471, -0.0679, -0.0750, -0.0734, -0.0811,\n                          -0.0624, -0.0516, -0.0761, -0.0969, -0.0780, -0.0514, -0.0610, -0.0777,\n                          -0.0608, -0.0587, -0.0846, -0.0628, -0.0944, -0.0513, -0.0812, -0.0687,\n                          -0.0684, -0.0952, -0.0829, -0.0727, -0.0553, -0.0574, -0.0734, -0.0527,\n                          -0.0983, -0.0722, -0.0282, -0.0809, -0.0740, -0.0927, -0.1028, -0.0736,\n                          -0.0847, -0.0808, -0.0836, -0.0875, -0.0823, -0.1103, -0.0775, -0.0685,\n                          -0.0548, -0.0972, -0.0644, -0.0951, -0.0762, -0.0765, -0.0626, -0.0627,\n                          -0.0686, -0.0669, -0.0587, -0.0682, -0.0655, -0.0662, -0.0726, -0.0688,\n                          -0.0906, -0.0661, -0.0288, -0.0673, -0.0768, -0.0716, -0.0589, -0.0700,\n                          -0.0594, -0.0571, -0.1000, -0.0404, -0.0812, -0.0862, -0.0614, -0.0678,\n                          -0.0422, -0.0915, -0.0635, -0.0655, -0.0713, -0.0771, -0.0774, -0.0105,\n                          -0.0715, -0.0712, -0.0650, -0.1126, -0.0713, -0.0476, -0.0735, -0.0509,\n                          -0.0346, -0.1054, -0.0553, -0.0799, -0.0692, -0.0657, -0.0392, -0.0452,\n                          -0.0868, -0.0939, -0.0910, -0.0790, -0.0635, -0.0428, -0.0807, -0.0763,\n                          -0.0740, -0.0724, -0.0530, -0.0913, -0.0851, -0.0718, -0.0960, -0.0204,\n                          -0.0403, -0.0812, -0.0801, -0.0694, -0.0510, -0.0726, -0.0492, -0.0826,\n                          -0.0259, -0.0834, -0.0548, -0.0557, -0.0791, -0.0831, -0.1047, -0.0470,\n                          -0.0641, -0.0631, -0.0994, -0.0523, -0.0664, -0.1212, -0.0472, -0.0833,\n                          -0.0682, -0.0681, -0.0658, -0.0727, -0.0787, -0.0823, -0.0658, -0.0754,\n                          -0.0683, -0.0786, -0.0858, -0.1128, -0.0692, -0.0923, -0.0597, -0.0532,\n                          -0.0612, -0.0642, -0.0696, -0.0849, -0.0673, -0.0754, -0.0724, -0.1091,\n                          -0.0908, -0.0736, -0.0658, -0.0495, -0.0244, -0.0867, -0.0641, -0.0614,\n                          -0.0558, -0.0638, -0.0520, -0.0740, -0.0951, -0.0982, -0.0788, -0.0652,\n                          -0.0859, -0.0721, -0.0791, -0.0553, -0.0842, -0.0683, -0.0584, -0.0832,\n                          -0.0689, -0.0544, -0.0772, -0.0656, -0.0630, -0.1026, -0.0874, -0.0620,\n                          -0.0677, -0.0677, -0.0746, -0.0591, -0.1038, -0.0623, -0.1265, -0.0016,\n                          -0.0808, -0.0438, -0.1106, -0.0385, -0.0780, -0.0538, -0.0824, -0.0424,\n                          -0.0469, -0.0695, -0.0498, -0.0943, -0.0641, -0.0620, -0.0773, -0.0614,\n                          -0.0702, -0.1322, -0.0606, -0.0502, -0.0598, -0.1109, -0.0493, -0.0900,\n                          -0.0874, -0.0457, -0.0697, -0.0786, -0.0902, -0.0939, -0.0960, -0.0842]), max_val=tensor([0.0509, 0.0553, 0.1031, 0.1071, 0.0710, 0.0454, 0.0594, 0.0877, 0.1023,\n                          0.0891, 0.0633, 0.0687, 0.0702, 0.0515, 0.0648, 0.0793, 0.0484, 0.0714,\n                          0.0642, 0.1119, 0.0634, 0.0957, 0.0732, 0.0749, 0.0569, 0.0303, 0.0624,\n                          0.0771, 0.0955, 0.0823, 0.0708, 0.0710, 0.1148, 0.0719, 0.1185, 0.1097,\n                          0.0491, 0.0641, 0.0952, 0.0976, 0.0525, 0.0664, 0.0656, 0.0798, 0.0651,\n                          0.0528, 0.0735, 0.0432, 0.0578, 0.0826, 0.0668, 0.0893, 0.0618, 0.0801,\n                          0.0788, 0.0757, 0.0712, 0.0724, 0.1164, 0.0588, 0.0604, 0.1204, 0.0749,\n                          0.0610, 0.0532, 0.0798, 0.0890, 0.0793, 0.0548, 0.0977, 0.0582, 0.0830,\n                          0.0839, 0.0682, 0.0611, 0.0846, 0.0828, 0.1091, 0.0300, 0.0574, 0.0539,\n                          0.0352, 0.0956, 0.0613, 0.1124, 0.0671, 0.0611, 0.0599, 0.0615, 0.0623,\n                          0.0642, 0.0720, 0.0640, 0.0663, 0.0683, 0.0879, 0.0565, 0.0596, 0.0887,\n                          0.0924, 0.1185, 0.0967, 0.0812, 0.1369, 0.0735, 0.0948, 0.0898, 0.0736,\n                          0.0752, 0.0722, 0.0481, 0.1071, 0.1006, 0.0559, 0.0531, 0.0498, 0.1054,\n                          0.0801, 0.0926, 0.1177, 0.0773, 0.0332, 0.1051, 0.0660, 0.0623, 0.0490,\n                          0.0587, 0.0579, 0.0648, 0.0571, 0.0409, 0.0979, 0.0701, 0.0804, 0.0348,\n                          0.0352, 0.0664, 0.1023, 0.0769, 0.0633, 0.0821, 0.0809, 0.0394, 0.0738,\n                          0.0493, 0.0747, 0.0644, 0.0264, 0.0825, 0.0520, 0.0525, 0.0671, 0.0818,\n                          0.0748, 0.0968, 0.0748, 0.0638, 0.0764, 0.0575, 0.0606, 0.0647, 0.0890,\n                          0.0772, 0.0681, 0.1535, 0.0944, 0.0501, 0.0743, 0.0790, 0.0623, 0.0584,\n                          0.1348, 0.0733, 0.0599, 0.0684, 0.0565, 0.0595, 0.0698, 0.0396, 0.0605,\n                          0.0319, 0.0957, 0.0762, 0.0884, 0.0648, 0.0665, 0.0622, 0.0666, 0.0954,\n                          0.0517, 0.0917, 0.0558, 0.0535, 0.0682, 0.1389, 0.0610, 0.0611, 0.0535,\n                          0.0592, 0.0834, 0.0540, 0.1050, 0.0596, 0.0856, 0.1037, 0.0598, 0.0682,\n                          0.0520, 0.0573, 0.1113, 0.0763, 0.0798, 0.0718, 0.0486, 0.0758, 0.0746,\n                          0.0587, 0.0947, 0.0703, 0.0792, 0.0617, 0.0388, 0.0730, 0.0665, 0.0623,\n                          0.0562, 0.0749, 0.0704, 0.1284, 0.0506, 0.1028, 0.0609, 0.0684, 0.0971,\n                          0.0769, 0.0542, 0.0873, 0.0702, 0.0465, 0.0674, 0.0631, 0.0660, 0.0763,\n                          0.0702, 0.0633, 0.0632, 0.0609, 0.0624, 0.0609, 0.0700, 0.0469, 0.0657,\n                          0.1420, 0.0649, 0.0594, 0.0389, 0.0714, 0.0455, 0.0830, 0.0325, 0.0739,\n                          0.0597, 0.0555, 0.0862, 0.0432, 0.0771, 0.0844, 0.0393, 0.0623, 0.0739,\n                          0.0731, 0.0837, 0.1287, 0.0511, 0.0917, 0.0588, 0.1000, 0.0892, 0.0765,\n                          0.0986, 0.0568, 0.0963, 0.1026, 0.0568, 0.0611, 0.0631, 0.0638, 0.0856,\n                          0.0765, 0.0994, 0.0658, 0.0948, 0.0576, 0.0520, 0.0727, 0.0754, 0.0587,\n                          0.0511, 0.0697, 0.0470, 0.0915, 0.0508, 0.0616, 0.0538, 0.0011, 0.0861,\n                          0.1058, 0.0918, 0.0783, 0.0864, 0.0889, 0.0832, 0.0707, 0.0644, 0.0549,\n                          0.0515, 0.1205, 0.0960, 0.0701, 0.0562, 0.0633, 0.0719, 0.0730, 0.0675,\n                          0.0743, 0.0827, 0.0624, 0.0922, 0.0545, 0.1197, 0.0551, 0.0612, 0.0880,\n                          0.0290, 0.0455, 0.0603, 0.0644, 0.0758, 0.0791, 0.0708, 0.0921, 0.0804,\n                          0.0717, 0.1153, 0.0701, 0.0743, 0.0982, 0.0589, 0.0851, 0.0552, 0.0612,\n                          0.0599, 0.0507, 0.0760, 0.0810, 0.0719, 0.0518, 0.0494, 0.0659, 0.0427,\n                          0.0585, 0.0743, 0.0740, 0.0934, 0.0735, 0.0699, 0.0588, 0.0501, 0.0890,\n                          0.0707, 0.0253, 0.0746, 0.0619, 0.0990, 0.0785, 0.0674, 0.0737, 0.0562,\n                          0.0912, 0.0826, 0.0984, 0.0786, 0.0546, 0.0726, 0.0784, 0.0632, 0.0742,\n                          0.0887, 0.0687, 0.0565, 0.0760, 0.0749, 0.0727, 0.0694, 0.0615, 0.0628,\n                          0.1005, 0.1278, 0.1486, 0.0658, 0.0666, 0.0505, 0.0279, 0.0924, 0.0579,\n                          0.0741, 0.0713, 0.0905, 0.0705, 0.0812, 0.0745, 0.0478, 0.0721, 0.0952,\n                          0.0851, 0.0521, 0.0402, 0.0831, 0.0922, 0.0941, 0.0919, 0.0782, 0.0517,\n                          0.0142, 0.0842, 0.0675, 0.0731, 0.0925, 0.0790, 0.0493, 0.0895, 0.0498,\n                          0.0334, 0.0903, 0.0505, 0.0613, 0.0610, 0.0951, 0.0523, 0.0517, 0.0581,\n                          0.0757, 0.0846, 0.0640, 0.0522, 0.0343, 0.0691, 0.0698, 0.0692, 0.0549,\n                          0.0768, 0.0690, 0.0815, 0.0616, 0.0897, 0.0168, 0.0362, 0.0521, 0.0721,\n                          0.0721, 0.0606, 0.0637, 0.0689, 0.1110, 0.0275, 0.0700, 0.0534, 0.0591,\n                          0.0685, 0.0655, 0.0640, 0.0754, 0.0815, 0.0594, 0.1154, 0.0403, 0.0863,\n                          0.0623, 0.0623, 0.0629, 0.0657, 0.0669, 0.0558, 0.0570, 0.0505, 0.0828,\n                          0.0929, 0.0689, 0.0767, 0.0833, 0.0702, 0.0895, 0.0709, 0.1058, 0.0467,\n                          0.0973, 0.0678, 0.0802, 0.0677, 0.0959, 0.0603, 0.0563, 0.0907, 0.0688,\n                          0.0730, 0.0754, 0.0652, 0.0602, 0.0225, 0.0709, 0.0552, 0.0651, 0.0590,\n                          0.0753, 0.0913, 0.0945, 0.0765, 0.0611, 0.0692, 0.0729, 0.0701, 0.0809,\n                          0.0848, 0.1126, 0.1020, 0.0574, 0.0599, 0.0977, 0.0525, 0.0786, 0.0653,\n                          0.0748, 0.1050, 0.0524, 0.0725, 0.0672, 0.0637, 0.0818, 0.0857, 0.0547,\n                          0.0512, 0.0546, 0.0904, 0.0009, 0.0591, 0.0597, 0.0950, 0.0422, 0.1726,\n                          0.0800, 0.0457, 0.0670, 0.0824, 0.0561, 0.0598, 0.0922, 0.0683, 0.0778,\n                          0.0670, 0.0461, 0.0817, 0.0855, 0.0995, 0.0380, 0.0651, 0.0587, 0.0346,\n                          0.0604, 0.0833, 0.0598, 0.0744, 0.0828, 0.0838, 0.0744, 0.0974, 0.0613])\n                )\n              )\n            )\n            (2): ReLU6(inplace=True)\n          )\n          (1): Module(\n            (0): ConvBn2d(\n              576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False\n              (bn): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n                fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0019, 0.0020, 0.0036, 0.0030, 0.0034, 0.0021, 0.0045, 0.0030, 0.0033,\n                        0.0034, 0.0029, 0.0018, 0.0032, 0.0019, 0.0032, 0.0045, 0.0022, 0.0020,\n                        0.0037, 0.0035, 0.0023, 0.0030, 0.0022, 0.0023, 0.0016, 0.0039, 0.0047,\n                        0.0020, 0.0029, 0.0027, 0.0019, 0.0015, 0.0025, 0.0040, 0.0025, 0.0020,\n                        0.0020, 0.0033, 0.0013, 0.0023, 0.0030, 0.0034, 0.0028, 0.0031, 0.0016,\n                        0.0033, 0.0045, 0.0022, 0.0039, 0.0038, 0.0029, 0.0026, 0.0023, 0.0020,\n                        0.0025, 0.0030, 0.0042, 0.0024, 0.0032, 0.0038, 0.0030, 0.0025, 0.0032,\n                        0.0026, 0.0020, 0.0022, 0.0036, 0.0013, 0.0030, 0.0026, 0.0018, 0.0030,\n                        0.0028, 0.0022, 0.0029, 0.0029, 0.0030, 0.0017, 0.0172, 0.0015, 0.0031,\n                        0.0022, 0.0034, 0.0038, 0.0038, 0.0013, 0.0025, 0.0030, 0.0027, 0.0021,\n                        0.0040, 0.0043, 0.0028, 0.0039, 0.0020, 0.0030, 0.0025, 0.0021, 0.0036,\n                        0.0025, 0.0026, 0.0037, 0.0043, 0.0039, 0.0023, 0.0034, 0.0026, 0.0041,\n                        0.0034, 0.0041, 0.0044, 0.0036, 0.0028, 0.0018, 0.0020, 0.0038, 0.0029,\n                        0.0036, 0.0031, 0.0028, 0.0030, 0.0120, 0.0046, 0.0024, 0.0061, 0.0019,\n                        0.0033, 0.0020, 0.0031, 0.0019, 0.0033, 0.0025, 0.0039, 0.0031, 0.0021,\n                        0.0032, 0.0030, 0.0026, 0.0040, 0.0026, 0.0028, 0.0030, 0.0086, 0.0030,\n                        0.0018, 0.0031, 0.0044, 0.0083, 0.0029, 0.0022, 0.0029, 0.0017, 0.0039,\n                        0.0029, 0.0025, 0.0067, 0.0026, 0.0016, 0.0022, 0.0038, 0.0040, 0.0033,\n                        0.0034, 0.0017, 0.0034, 0.0026, 0.0028, 0.0016, 0.0022, 0.0030, 0.0016,\n                        0.0031, 0.0019, 0.0033, 0.0018, 0.0030, 0.0027, 0.0022, 0.0024, 0.0024,\n                        0.0168, 0.0019, 0.0027, 0.0030, 0.0015, 0.0031, 0.0019, 0.0039, 0.0016,\n                        0.0026, 0.0020, 0.0019, 0.0043, 0.0017, 0.0017, 0.0037, 0.0023, 0.0022,\n                        0.0019, 0.0016, 0.0026, 0.0029, 0.0028, 0.0025, 0.0026, 0.0021, 0.0023,\n                        0.0035, 0.0021, 0.0025, 0.0026, 0.0037, 0.0015, 0.0058, 0.0027, 0.0045,\n                        0.0029, 0.0037, 0.0022, 0.0042, 0.0019, 0.0022, 0.0033, 0.0039, 0.0025,\n                        0.0017, 0.0032, 0.0034, 0.0029, 0.0020, 0.0027, 0.0029, 0.0021, 0.0031,\n                        0.0018, 0.0021, 0.0040, 0.0029, 0.0022, 0.0019, 0.0042, 0.0034, 0.0033,\n                        0.0038, 0.0026, 0.0025, 0.0017, 0.0042, 0.0024, 0.0019, 0.0019, 0.0018,\n                        0.0028, 0.0033, 0.0042, 0.0022, 0.0039, 0.0024, 0.0030, 0.0094, 0.0031,\n                        0.0018, 0.0022, 0.0033, 0.0103, 0.0030, 0.0042, 0.0090, 0.0029, 0.0046,\n                        0.0022, 0.0033, 0.0032, 0.0026, 0.0019, 0.0028, 0.0018, 0.0032, 0.0030,\n                        0.0016, 0.0017, 0.0035, 0.0031, 0.0028, 0.0021, 0.0036, 0.0024, 0.0071,\n                        0.0019, 0.0024, 0.0027, 0.0021, 0.0031, 0.0022, 0.0042, 0.0020, 0.0023,\n                        0.0022, 0.0023, 0.0021, 0.0026, 0.0018, 0.0017, 0.0019, 0.0016, 0.0029,\n                        0.0026, 0.0030, 0.0020, 0.0040, 0.0041, 0.0019, 0.0039, 0.0022, 0.0016,\n                        0.0020, 0.0030, 0.0045, 0.0046, 0.0023, 0.0035, 0.0028, 0.0016, 0.0020,\n                        0.0038, 0.0034, 0.0031, 0.0029, 0.0025, 0.0034, 0.0021, 0.0047, 0.0038,\n                        0.0176, 0.0059, 0.0036, 0.0055, 0.0043, 0.0038, 0.0019, 0.0023, 0.0038,\n                        0.0016, 0.0029, 0.0038, 0.0036, 0.0018, 0.0020, 0.0031, 0.0023, 0.0023,\n                        0.0017, 0.0021, 0.0031, 0.0024, 0.0023, 0.0038, 0.0023, 0.0017, 0.0019,\n                        0.0016, 0.0016, 0.0018, 0.0026, 0.0019, 0.0035, 0.0040, 0.0022, 0.0028,\n                        0.0029, 0.0027, 0.0082, 0.0030, 0.0025, 0.0024, 0.0015, 0.0021, 0.0024,\n                        0.0031, 0.0033, 0.0030, 0.0049, 0.0037, 0.0040, 0.0038, 0.0032, 0.0033,\n                        0.0025, 0.0032, 0.0032, 0.0031, 0.0027, 0.0018, 0.0027, 0.0039, 0.0030,\n                        0.0029, 0.0019, 0.0030, 0.0029, 0.0045, 0.0027, 0.0108, 0.0037, 0.0050,\n                        0.0020, 0.0022, 0.0030, 0.0029, 0.0033, 0.0056, 0.0102, 0.0023, 0.0032,\n                        0.0020, 0.0033, 0.0015, 0.0030, 0.0035, 0.0041, 0.0020, 0.0021, 0.0027,\n                        0.0144, 0.0040, 0.0039, 0.0051, 0.0030, 0.0040, 0.0019, 0.0030, 0.0019,\n                        0.0113, 0.0038, 0.0019, 0.0015, 0.0030, 0.0031, 0.0021, 0.0022, 0.0018,\n                        0.0022, 0.0016, 0.0042, 0.0022, 0.0023, 0.0027, 0.0024, 0.0031, 0.0041,\n                        0.0032, 0.0013, 0.0019, 0.0029, 0.0023, 0.0102, 0.0030, 0.0018, 0.0034,\n                        0.0021, 0.0018, 0.0015, 0.0020, 0.0025, 0.0031, 0.0020, 0.0022, 0.0038,\n                        0.0018, 0.0020, 0.0025, 0.0024, 0.0033, 0.0019, 0.0025, 0.0023, 0.0041,\n                        0.0018, 0.0019, 0.0036, 0.0019, 0.0031, 0.0025, 0.0015, 0.0057, 0.0038,\n                        0.0020, 0.0036, 0.0020, 0.0030, 0.0037, 0.0032, 0.0025, 0.0029, 0.0020,\n                        0.0021, 0.0023, 0.0032, 0.0095, 0.0020, 0.0018, 0.0019, 0.0040, 0.0047,\n                        0.0034, 0.0027, 0.0023, 0.0021, 0.0136, 0.0037, 0.0018, 0.0030, 0.0015,\n                        0.0033, 0.0022, 0.0015, 0.0036, 0.0020, 0.0028, 0.0027, 0.0028, 0.0032,\n                        0.0017, 0.0032, 0.0025, 0.0028, 0.0037, 0.0018, 0.0023, 0.0021, 0.0036,\n                        0.0019, 0.0020, 0.0018, 0.0028, 0.0035, 0.0043, 0.0028, 0.0126, 0.0022,\n                        0.0029, 0.0022, 0.0045, 0.0027, 0.0021, 0.0020, 0.0027, 0.0021, 0.0028,\n                        0.0039, 0.0058, 0.0030, 0.0024, 0.0022, 0.0040, 0.0024, 0.0025, 0.0037,\n                        0.0024, 0.0020, 0.0024, 0.0032, 0.0034, 0.0022, 0.0018, 0.0036, 0.0021,\n                        0.0016, 0.0031, 0.0019, 0.0024, 0.0037, 0.0028, 0.0019, 0.0033, 0.0017]), zero_point=tensor([ 127,  127, -128, -128, -128,  127, -128, -128, -128, -128, -128,  127,\n                        -128,  127, -128, -128,  127,  127, -128, -128,  127, -128,  127, -128,\n                         127,  127, -128,  127, -128, -128,  127,  127, -128, -128, -128,  127,\n                         127, -128,  127,  127,  127, -128, -128, -128,  127, -128, -128,  127,\n                        -128, -128, -128, -128,  127,  127, -128, -128, -128,  127, -128, -128,\n                        -128, -128, -128,  127,  127,  127, -128,  127, -128,  127,  127, -128,\n                        -128,  127, -128, -128, -128,  127,    0,  127, -128,  127, -128, -128,\n                        -128,  127,  127, -128, -128,  127, -128, -128, -128, -128,  127, -128,\n                         127,  127, -128,  127, -128, -128, -128, -128, -128, -128, -128, -128,\n                        -128, -128, -128, -128, -128,  127,  127, -128, -128, -128, -128, -128,\n                        -128,    0, -128, -128,    0,  127, -128,  127, -128,  127, -128, -128,\n                        -128, -128,  127,  127, -128, -128, -128, -128, -128, -128,    0, -128,\n                         127, -128, -128,    0, -128,  127, -128,  127, -128, -128, -128, -128,\n                        -128,  127,  127, -128, -128, -128, -128,  127, -128, -128, -128,  127,\n                         127, -128,  127, -128,  127, -128,  127, -128, -128,  127,  127,  127,\n                           0,  127, -128, -128,  127, -128, -128, -128, -128,  127,  127,  127,\n                        -128,  127,  127, -128,  127,  127,  127,  127, -128, -128, -128, -128,\n                        -128,  127,  127, -128,  127, -128, -128, -128,  127,    0, -128,    0,\n                        -128, -128,  127, -128,  127,  127, -128, -128, -128,  127, -128, -128,\n                        -128,  127, -128, -128,  127, -128,  127,  127, -128, -128,  127,  127,\n                        -128, -128, -128, -128, -128,  127,  127, -128,  127,  127,  127,  127,\n                        -128, -128, -128,  127, -128,  127, -128,    0, -128,  127,  127, -128,\n                           0, -128, -128,    0, -128, -128,  127, -128, -128, -128,  127, -128,\n                         127, -128, -128,  127,  127, -128, -128, -128,  127, -128,  127,    0,\n                         127, -128, -128,  127, -128,  127, -128,  127, -128,  127,  127,  127,\n                        -128,  127,  127,  127,    0, -128, -128, -128,  127, -128, -128,  127,\n                        -128,  127,  127,  127, -128, -128, -128,  127, -128, -128,  127,  127,\n                        -128, -128, -128, -128, -128, -128,  127, -128, -128,    0,    0, -128,\n                        -128, -128, -128,  127, -128, -128,  127, -128, -128, -128,  127,  127,\n                        -128,  127,  127,  127,  127, -128, -128, -128, -128,  127,  127,  127,\n                         127,  127,  127, -128,  127, -128, -128,  127, -128, -128,  127,    0,\n                        -128, -128, -128,  127,  127, -128, -128, -128, -128, -128, -128, -128,\n                        -128, -128, -128, -128, -128, -128, -128, -128,  127, -128, -128, -128,\n                        -128,  127, -128, -128, -128, -128,    0, -128, -128,  127,  127, -128,\n                        -128, -128,    0,    0, -128, -128,  127, -128,  127, -128, -128, -128,\n                        -128,  127, -128,    0, -128, -128, -128, -128, -128,  127, -128,  127,\n                           0, -128,  127,  127, -128, -128,  127,  127,  127,  127,  127, -128,\n                         127,  127, -128,  127, -128, -128, -128,  127,  127, -128, -128,    0,\n                         127,  127, -128,  127,  127,  127,  127, -128,  127,  127,  127, -128,\n                         127,  127, -128,  127, -128,  127, -128,  127, -128,  127,  127, -128,\n                         127, -128,  127,  127,    0, -128,  127, -128,  127, -128, -128, -128,\n                        -128, -128,  127,  127,  127, -128,    0,  127,  127,  127, -128, -128,\n                        -128, -128, -128,  127,    0, -128,  127, -128,  127, -128,  127,  127,\n                        -128,  127, -128,  127, -128, -128,  127, -128, -128, -128, -128,  127,\n                         127,  127, -128,  127, -128,  127, -128, -128, -128, -128,    0,  127,\n                        -128,  127, -128,    0,  127,  127, -128,  127, -128, -128,    0, -128,\n                         127,  127, -128,  127,  127, -128,  127,  127, -128, -128, -128,  127,\n                         127, -128,  127,  127, -128,  127,  127, -128, -128,  127, -128,  127],\n                       dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n                (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n                  min_val=tensor([-4.7384e-01, -5.2004e-01,  2.0057e-01,  2.0346e-01,  2.2007e-01,\n                          -5.2302e-01,  2.3313e-01,  2.1859e-01,  1.3340e-01,  2.3782e-01,\n                           1.7122e-01, -4.6455e-01,  2.1932e-01, -4.7617e-01,  1.6143e-01,\n                           3.6596e-01, -5.6442e-01, -5.0393e-01,  1.9786e-01,  1.7690e-01,\n                          -5.8579e-01,  1.5760e-01, -5.6070e-01,  1.6408e-01, -4.1900e-01,\n                          -9.9963e-01,  2.9738e-01, -5.2005e-01,  1.1931e-01,  1.6568e-01,\n                          -4.8563e-01, -3.7372e-01,  1.5319e-01,  2.8608e-01,  1.2132e-01,\n                          -5.1806e-01, -5.1446e-01,  2.0458e-01, -3.2119e-01, -5.9626e-01,\n                          -7.5854e-01,  2.4447e-01,  2.0788e-01,  1.8390e-01, -4.0734e-01,\n                           1.5656e-01,  3.1252e-01, -5.6862e-01,  2.2576e-01,  2.0811e-01,\n                           2.2331e-01,  1.5569e-01, -5.9118e-01, -5.2160e-01,  1.6499e-01,\n                           1.4060e-01,  2.8273e-01, -6.0879e-01,  1.8650e-01,  2.4331e-01,\n                           1.9307e-01,  1.4584e-01,  1.5685e-01, -6.7185e-01, -4.9958e-01,\n                          -5.4951e-01,  1.1633e-01, -3.2535e-01,  1.8091e-01, -6.5080e-01,\n                          -4.5828e-01,  2.2622e-01,  1.8143e-01, -5.5639e-01,  1.7128e-01,\n                           1.7095e-01,  1.4801e-01, -4.2707e-01, -5.9295e-01, -3.8903e-01,\n                           2.3843e-01, -5.6880e-01,  2.3167e-01,  2.4548e-01,  9.6565e-02,\n                          -3.3027e-01, -6.3441e-01,  2.1321e-01,  1.9197e-01, -5.3341e-01,\n                           1.3460e-01,  2.7853e-01,  1.5820e-01,  2.4166e-01, -5.1749e-01,\n                           1.1970e-01, -6.3681e-01, -5.4045e-01,  1.9833e-01, -6.4428e-01,\n                           2.2094e-01,  1.8249e-01,  2.8033e-01,  4.1114e-01,  1.3674e-01,\n                           2.2560e-01,  1.7109e-01,  3.0118e-01,  2.0941e-01,  3.4375e-01,\n                           3.1573e-01,  2.5176e-01,  1.2436e-01, -4.5250e-01, -5.2175e-01,\n                           1.9802e-01,  1.3380e-01,  2.1027e-01,  1.8008e-01,  1.4259e-01,\n                           1.4479e-01, -1.5407e+00,  4.9737e-01,  1.1913e-01, -2.6438e-04,\n                          -4.8299e-01,  1.2752e-01, -5.1664e-01,  1.9100e-01, -4.7187e-01,\n                           2.3400e-01,  1.5559e-01,  2.8969e-01,  1.7631e-01, -5.2496e-01,\n                          -8.0454e-01,  2.2266e-01,  1.5201e-01,  2.6636e-01,  1.4378e-01,\n                           1.4038e-01,  1.7162e-01, -1.0956e+00,  1.5575e-01, -4.6382e-01,\n                           1.8993e-01,  3.4049e-01, -1.0577e+00,  1.9287e-01, -5.7369e-01,\n                           2.1307e-01, -4.3203e-01,  2.4909e-01,  1.8753e-01,  1.5005e-01,\n                           4.7293e-01,  1.5810e-01, -3.9675e-01, -5.6880e-01,  2.1615e-01,\n                           2.5393e-01,  1.3385e-01,  1.9122e-01, -4.4450e-01,  2.5277e-01,\n                           1.4500e-01,  6.0946e-02, -4.1697e-01, -5.6446e-01,  1.6968e-01,\n                          -4.1803e-01,  1.7915e-01, -4.9100e-01,  3.1881e-01, -4.6248e-01,\n                           1.5343e-01,  1.5428e-01, -5.5752e-01, -6.0451e-01, -6.1294e-01,\n                          -1.8023e+00, -4.8245e-01,  1.4347e-01,  9.4005e-02, -3.8605e-01,\n                           2.0670e-01,  2.1197e-01,  1.5911e-01,  7.8705e-02, -6.5769e-01,\n                          -5.1490e-01, -4.7814e-01,  1.6328e-01, -4.3886e-01, -4.3888e-01,\n                           2.0066e-01, -5.8291e-01, -5.5759e-01, -4.9424e-01, -4.0115e-01,\n                           1.7906e-01,  1.2757e-01,  2.1423e-01,  1.3962e-01,  2.5039e-01,\n                          -5.2617e-01, -5.8972e-01,  1.9271e-01, -5.4201e-01,  5.5881e-02,\n                           2.1700e-01,  2.2373e-01, -3.7432e-01, -7.4647e-01,  1.7631e-01,\n                          -2.4534e-01,  1.2705e-01,  2.6063e-01, -5.6640e-01,  4.2788e-01,\n                          -4.8591e-01, -5.5580e-01,  2.3020e-01,  2.3497e-01,  1.5343e-01,\n                          -4.4392e-01,  1.7057e-01,  1.7819e-01,  1.7199e-01, -4.9775e-01,\n                           1.9885e-01,  1.7720e-01, -5.3461e-01,  1.9445e-01, -4.4662e-01,\n                          -5.4550e-01,  3.7059e-01,  2.1087e-01, -5.6679e-01, -4.7224e-01,\n                           2.2909e-01,  2.0282e-01,  2.0182e-01,  3.2680e-01,  1.5714e-01,\n                          -6.3501e-01, -4.3521e-01,  2.6851e-01, -6.2215e-01, -4.9661e-01,\n                          -4.9102e-01, -4.5476e-01,  1.5482e-01,  2.2604e-01,  2.4185e-01,\n                          -5.7018e-01,  2.9646e-01, -6.1587e-01,  1.8148e-01, -7.1100e-01,\n                           1.6813e-01, -4.6188e-01, -5.7133e-01,  2.4977e-01, -1.3129e+00,\n                           1.7249e-01,  2.8061e-01, -1.1567e+00,  1.3974e-01,  3.2214e-01,\n                          -5.4948e-01,  2.2517e-01,  1.2951e-01,  1.4433e-01, -4.9007e-01,\n                           2.2349e-01, -4.5918e-01,  2.4969e-01,  1.9698e-01, -4.2012e-01,\n                          -4.4134e-01,  1.6379e-01,  1.5083e-01,  2.2543e-01, -5.3838e-01,\n                           1.7563e-01, -6.0711e-01, -5.3049e-02, -4.8718e-01,  1.0611e-01,\n                           1.7674e-01, -5.3808e-01,  1.6001e-01, -5.6447e-01,  2.5831e-01,\n                          -5.1477e-01,  1.7269e-01, -5.6393e-01, -5.9247e-01, -5.4313e-01,\n                           1.4275e-01, -4.5525e-01, -4.3256e-01, -4.9105e-01, -3.8232e-02,\n                           1.5603e-01,  1.5218e-01,  1.6180e-01, -5.0958e-01,  3.8537e-01,\n                           3.1315e-01, -4.9370e-01,  2.3855e-01, -5.5824e-01, -4.2040e-01,\n                          -5.2045e-01,  2.7049e-01,  1.4601e-01,  1.7938e-01, -5.9162e-01,\n                           2.3623e-01,  1.7435e-01, -4.0014e-01, -5.0352e-01,  1.8423e-01,\n                           2.6058e-01,  1.7385e-01,  2.0583e-01,  1.3584e-01,  1.8547e-01,\n                          -5.4329e-01,  3.7591e-01,  1.9661e-01, -8.1733e-01, -7.2912e-01,\n                           2.2764e-01,  3.2432e-01,  2.7620e-01,  2.3563e-01, -4.8108e-01,\n                           4.3267e-02,  2.6034e-01, -4.0875e-01,  1.6049e-01,  2.1845e-01,\n                           1.8846e-01, -4.6624e-01, -4.9828e-01,  1.4972e-01, -5.9517e-01,\n                          -5.8231e-01, -4.2237e-01, -5.3163e-01,  1.5204e-01,  1.2288e-01,\n                           1.0437e-01,  2.5942e-01, -5.7771e-01, -4.3213e-01, -4.8919e-01,\n                          -4.0271e-01, -4.1780e-01, -4.6503e-01,  1.4227e-01, -4.8285e-01,\n                           1.4028e-01,  2.3588e-01, -5.7223e-01,  1.7855e-01,  2.2454e-01,\n                          -6.9532e-01, -6.5292e-02,  1.7950e-01,  1.2873e-01,  1.0569e-01,\n                          -3.7308e-01, -5.4035e-01,  1.6649e-01,  2.6536e-01,  2.1700e-01,\n                           1.2491e-01,  3.5489e-01,  2.6769e-01,  2.9977e-01,  1.4142e-01,\n                           1.9846e-01,  1.6255e-01,  1.5275e-01,  1.7444e-01,  2.3516e-01,\n                           2.5662e-01,  1.6888e-01, -4.5183e-01,  1.7016e-01,  2.6296e-01,\n                           2.1417e-01,  9.6042e-02, -4.9387e-01,  1.2075e-01,  1.2480e-01,\n                           2.6943e-01,  1.6952e-01, -1.3326e+00,  2.5861e-01,  2.6473e-01,\n                          -5.1350e-01, -5.6033e-01,  2.7432e-01,  1.3716e-01,  2.1045e-01,\n                          -9.4064e-02, -1.0262e+00,  1.5187e-01,  2.3529e-01, -5.1160e-01,\n                           2.9411e-01, -3.8383e-01,  1.2574e-01,  2.0370e-01,  2.8657e-01,\n                           7.6616e-02, -5.3543e-01,  1.9804e-01, -1.8473e+00,  2.5035e-01,\n                           3.3020e-01,  2.8205e-01,  1.8661e-01,  3.1235e-01, -4.7346e-01,\n                           1.9723e-01, -4.9243e-01, -1.0704e+00,  1.9691e-01, -4.8871e-01,\n                          -3.7510e-01,  1.3834e-01,  2.3757e-01, -5.3702e-01, -5.5788e-01,\n                          -4.6268e-01, -5.5012e-01, -4.0508e-01,  3.1774e-01, -5.5753e-01,\n                          -5.8419e-01,  1.9325e-01, -6.1569e-01,  2.0317e-01,  2.4388e-01,\n                           1.8446e-01, -3.3120e-01, -4.9194e-01,  2.1521e-01,  1.8200e-01,\n                          -1.3114e+00, -7.6914e-01, -4.6487e-01,  2.2075e-01, -5.4402e-01,\n                          -4.5978e-01, -3.7302e-01, -5.1522e-01,  1.1865e-01, -7.9365e-01,\n                          -5.2148e-01, -5.6047e-01,  2.2652e-01, -4.6167e-01, -5.0565e-01,\n                           1.9386e-01, -6.1899e-01,  1.7918e-01, -4.8343e-01,  1.2078e-01,\n                          -5.9831e-01,  3.2473e-01, -4.6270e-01, -4.7565e-01,  2.5241e-01,\n                          -4.8485e-01,  2.0714e-01, -6.3102e-01, -3.7149e-01, -4.2525e-01,\n                           2.6725e-01, -5.0221e-01,  2.1609e-01, -5.1373e-01,  1.9990e-01,\n                           2.5343e-01,  3.0794e-01,  1.5317e-01,  2.1052e-01, -5.0980e-01,\n                          -5.3829e-01, -5.9089e-01,  2.2037e-01, -3.1654e-01, -4.9890e-01,\n                          -4.6450e-01, -4.7300e-01,  2.9410e-01,  2.9202e-01,  1.8328e-01,\n                           1.0835e-01,  2.3390e-01, -5.2531e-01, -1.4208e+00,  2.3375e-01,\n                          -4.5175e-01,  1.8253e-01, -3.8459e-01,  1.8454e-01, -5.6170e-01,\n                          -3.9120e-01,  2.6061e-01, -5.1688e-01,  1.6796e-01, -6.9581e-01,\n                           1.5325e-01,  2.3525e-01, -4.3800e-01,  1.7896e-01,  1.4343e-01,\n                           1.2432e-01,  1.9557e-01, -4.6772e-01, -5.9583e-01, -5.3010e-01,\n                           2.3192e-01, -4.9534e-01,  6.5090e-02, -4.5576e-01,  1.6701e-01,\n                           3.1771e-01,  2.9029e-01,  1.6578e-01, -1.2496e-01, -5.6126e-01,\n                           1.8583e-01, -5.6613e-01,  2.8221e-01, -2.5368e-01, -5.2640e-01,\n                          -5.2210e-01,  1.1991e-01, -5.3005e-01,  1.6944e-01,  2.3367e-01,\n                          -8.4862e-02,  2.3347e-01, -5.9936e-01, -5.6839e-01,  2.2982e-01,\n                          -6.1938e-01, -6.2904e-01,  2.0567e-01, -6.1144e-01, -5.1354e-01,\n                           1.8276e-01,  2.0655e-01,  1.6213e-01, -5.5982e-01, -4.5250e-01,\n                           2.0215e-01, -5.2611e-01, -4.0720e-01,  1.7708e-01, -4.9408e-01,\n                          -6.1563e-01,  2.6382e-01,  1.2688e-01, -4.7855e-01,  1.7582e-01,\n                          -4.4052e-01]), max_val=tensor([-0.1465, -0.1028,  0.9151,  0.7740,  0.8648, -0.1087,  1.1530,  0.7563,\n                           0.8328,  0.8593,  0.7481, -0.1235,  0.8033, -0.1037,  0.8104,  1.1513,\n                          -0.2229, -0.1000,  0.9312,  0.9042, -0.1647,  0.7705, -0.1047,  0.5904,\n                          -0.1363, -0.0994,  1.2094, -0.1295,  0.7434,  0.6853, -0.1439, -0.1626,\n                           0.6470,  1.0280,  0.6418, -0.1284, -0.1109,  0.8289, -0.0815, -0.1531,\n                          -0.0701,  0.8613,  0.7200,  0.7919, -0.1808,  0.8380,  1.1540, -0.1171,\n                           0.9960,  0.9725,  0.7317,  0.6691, -0.1380, -0.1579,  0.6303,  0.7775,\n                           1.0584, -0.1851,  0.8197,  0.9635,  0.7551,  0.6306,  0.8254, -0.1693,\n                          -0.1356, -0.1188,  0.9244, -0.1225,  0.7629, -0.1643, -0.0821,  0.7772,\n                           0.7095, -0.1858,  0.7463,  0.7300,  0.7633, -0.1718,  2.1805, -0.1319,\n                           0.7791, -0.2113,  0.8663,  0.9661,  0.9631, -0.1687, -0.1657,  0.7564,\n                           0.6895, -0.1153,  1.0255,  1.1078,  0.7171,  1.0008, -0.1553,  0.7680,\n                          -0.1261, -0.1845,  0.9188, -0.2848,  0.6696,  0.9399,  1.0840,  0.9997,\n                           0.5969,  0.8574,  0.6637,  1.0396,  0.8550,  1.0508,  1.1172,  0.9291,\n                           0.7073, -0.1781, -0.1565,  0.9681,  0.7309,  0.9105,  0.7878,  0.7244,\n                           0.7572,  0.9805,  1.1793,  0.6172,  0.7727, -0.1283,  0.8403, -0.1083,\n                           0.7903, -0.1120,  0.8452,  0.6439,  0.9986,  0.7901, -0.1740, -0.1324,\n                           0.7607,  0.6609,  1.0243,  0.6686,  0.7101,  0.7594,  0.8662,  0.7717,\n                          -0.1556,  0.7963,  1.1225,  0.1411,  0.7445, -0.2130,  0.7311, -0.1719,\n                           0.9946,  0.7349,  0.6251,  1.7031,  0.6516, -0.0964, -0.2108,  0.9676,\n                           1.0106,  0.8339,  0.8683, -0.1549,  0.8770,  0.6561,  0.7149, -0.1126,\n                          -0.1273,  0.7624, -0.1423,  0.8031, -0.1684,  0.8497, -0.1333,  0.7683,\n                           0.6857, -0.1299, -0.1116, -0.2461,  2.1391, -0.1107,  0.6921,  0.7599,\n                          -0.1483,  0.7938,  0.4895,  1.0017,  0.4200, -0.1157, -0.1580, -0.1954,\n                           1.0878, -0.2349, -0.1518,  0.9433, -0.1630, -0.1980, -0.0503, -0.1233,\n                           0.6527,  0.7330,  0.7182,  0.6283,  0.6512, -0.1765, -0.2090,  0.8915,\n                          -0.1391,  0.6253,  0.6741,  0.9419, -0.1671,  0.3229,  0.6984,  0.5739,\n                           0.7438,  0.9495, -0.1654,  1.0595, -0.2290, -0.1598,  0.8454,  0.9984,\n                           0.6424, -0.1328,  0.8136,  0.8654,  0.7516, -0.2434,  0.6881,  0.7404,\n                          -0.1459,  0.7990, -0.1155, -0.1461,  1.0148,  0.7310, -0.0788, -0.1346,\n                           1.0621,  0.8623,  0.8513,  0.9608,  0.6698, -0.1982, -0.1113,  1.0708,\n                          -0.1893, -0.1333, -0.1500, -0.1365,  0.7267,  0.8392,  1.0660, -0.1398,\n                           1.0050, -0.1791,  0.7666,  1.1893,  0.7965, -0.2309, -0.1621,  0.8318,\n                           0.6162,  0.7751,  1.0625,  0.3158,  0.7510,  1.1660, -0.1119,  0.8501,\n                           0.8118,  0.6556, -0.1643,  0.7187, -0.1205,  0.8114,  0.7547, -0.1109,\n                          -0.1641,  0.8937,  0.7995,  0.7258, -0.1629,  0.9147, -0.1461,  0.9046,\n                          -0.1553,  0.6094,  0.6968, -0.1457,  0.7822, -0.1706,  1.0617, -0.1121,\n                           0.5983, -0.2190, -0.1928, -0.2074,  0.6701, -0.1234, -0.1846, -0.0991,\n                           0.2007,  0.7351,  0.6649,  0.7665, -0.1176,  1.0282,  1.0552, -0.1206,\n                           1.0064, -0.1560, -0.1294, -0.1196,  0.7703,  1.1508,  1.1829, -0.1778,\n                           0.8982,  0.7168, -0.2640, -0.1274,  0.9736,  0.8645,  0.7945,  0.7427,\n                           0.6354,  0.8629, -0.1713,  1.2112,  0.9597,  2.2340,  0.7517,  0.9122,\n                           1.3989,  1.0935,  0.9751, -0.1566,  0.5985,  0.9662, -0.1032,  0.7381,\n                           0.9637,  0.9072, -0.1453, -0.1301,  0.7910, -0.1311, -0.2056, -0.2172,\n                          -0.1812,  0.7931,  0.6007,  0.5813,  0.9684, -0.0991, -0.1193, -0.1493,\n                          -0.1873, -0.1667, -0.1409,  0.6506, -0.1407,  0.9046,  1.0189, -0.1618,\n                           0.7216,  0.7520, -0.0958,  1.0376,  0.7581,  0.6458,  0.6243, -0.1019,\n                          -0.1403,  0.6083,  0.7861,  0.8303,  0.7574,  1.2553,  0.9509,  1.0238,\n                           0.9813,  0.8186,  0.8533,  0.6477,  0.8207,  0.8273,  0.8004,  0.6973,\n                          -0.1455,  0.6855,  1.0071,  0.7677,  0.7442, -0.1345,  0.7547,  0.7397,\n                           1.1462,  0.6882,  1.3673,  0.9443,  1.2723, -0.2118, -0.1614,  0.7561,\n                           0.7426,  0.8405,  0.7058,  1.2974,  0.5852,  0.8160, -0.1716,  0.8445,\n                          -0.0788,  0.7661,  0.8889,  1.0493,  0.5125, -0.1356,  0.6823,  1.6693,\n                           1.0150,  0.9818,  1.2973,  0.7593,  1.0131, -0.1588,  0.7752, -0.1445,\n                           1.4322,  0.9706, -0.1573, -0.2165,  0.7714,  0.8010, -0.1710, -0.0997,\n                          -0.1695, -0.1146, -0.1320,  1.0600, -0.0989, -0.1205,  0.6946, -0.1261,\n                           0.7798,  1.0421,  0.8103, -0.1401, -0.1316,  0.7416,  0.5956,  0.1307,\n                          -0.1209, -0.1303,  0.8561, -0.1519, -0.1925, -0.1461, -0.0748,  0.6352,\n                          -0.1562, -0.2310, -0.1490,  0.9727, -0.1734, -0.1623,  0.6500, -0.1559,\n                           0.8506, -0.1725,  0.6415, -0.1430,  1.0342, -0.1593, -0.0915,  0.9080,\n                          -0.1077,  0.7997, -0.1576, -0.1193,  0.7240,  0.9571, -0.1294,  0.9247,\n                          -0.1372,  0.7605,  0.9485,  0.8204,  0.6445,  0.7471, -0.1163, -0.1163,\n                          -0.1554,  0.8220,  1.2069, -0.1012, -0.1668, -0.0928,  1.0139,  1.2073,\n                           0.8724,  0.6989,  0.5854, -0.0881,  1.7267,  0.9387, -0.1655,  0.7535,\n                          -0.1332,  0.8334, -0.1235, -0.1911,  0.9084, -0.1387,  0.7026, -0.1876,\n                           0.7154,  0.8108, -0.1022,  0.8121,  0.6360,  0.7092,  0.9489, -0.1555,\n                          -0.1774, -0.1063,  0.9221, -0.1708,  0.5177, -0.1876,  0.7172,  0.8971,\n                           1.0884,  0.7033,  1.6034, -0.1823,  0.7359, -0.2079,  1.1587,  0.3369,\n                          -0.1320, -0.2047,  0.6923, -0.1446,  0.7263,  0.9956,  0.7406,  0.7604,\n                          -0.1324, -0.1383,  1.0269, -0.2238, -0.1266,  0.9393, -0.1650, -0.1274,\n                           0.6204,  0.8275,  0.8616, -0.1487, -0.1445,  0.9123, -0.2534, -0.0983,\n                           0.7970, -0.1229, -0.1646,  0.9560,  0.7224, -0.1470,  0.8522, -0.2005])\n                )\n              )\n            )\n            (2): ReLU6(inplace=True)\n          )\n          (2): ConvBn2d(\n            576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0016, 0.0016, 0.0019, 0.0021, 0.0018, 0.0017, 0.0017, 0.0021, 0.0019,\n                      0.0017, 0.0017, 0.0012, 0.0014, 0.0017, 0.0011, 0.0013, 0.0015, 0.0015,\n                      0.0016, 0.0016, 0.0017, 0.0015, 0.0017, 0.0014, 0.0014, 0.0017, 0.0022,\n                      0.0017, 0.0015, 0.0018, 0.0021, 0.0018, 0.0016, 0.0017, 0.0017, 0.0020,\n                      0.0024, 0.0017, 0.0018, 0.0018, 0.0018, 0.0018, 0.0017, 0.0024, 0.0015,\n                      0.0018, 0.0012, 0.0015, 0.0019, 0.0014, 0.0019, 0.0014, 0.0016, 0.0014,\n                      0.0017, 0.0019, 0.0021, 0.0016, 0.0017, 0.0016, 0.0018, 0.0024, 0.0017,\n                      0.0019, 0.0020, 0.0013, 0.0017, 0.0015, 0.0021, 0.0016, 0.0016, 0.0014,\n                      0.0017, 0.0014, 0.0015, 0.0021, 0.0019, 0.0019, 0.0017, 0.0016, 0.0016,\n                      0.0019, 0.0017, 0.0022, 0.0018, 0.0014, 0.0016, 0.0019, 0.0018, 0.0017,\n                      0.0016, 0.0016, 0.0017, 0.0014, 0.0017, 0.0021, 0.0020, 0.0022, 0.0017,\n                      0.0018, 0.0015, 0.0020, 0.0016, 0.0019, 0.0021, 0.0014, 0.0016, 0.0016,\n                      0.0016, 0.0018, 0.0015, 0.0018, 0.0019, 0.0014, 0.0019, 0.0016, 0.0014,\n                      0.0016, 0.0013, 0.0017, 0.0017, 0.0015, 0.0019, 0.0020, 0.0020, 0.0015,\n                      0.0016, 0.0015, 0.0015, 0.0017, 0.0020, 0.0016, 0.0014, 0.0015, 0.0018,\n                      0.0020, 0.0016, 0.0020, 0.0019, 0.0013, 0.0013, 0.0016, 0.0015, 0.0016,\n                      0.0013, 0.0017, 0.0022, 0.0018, 0.0015, 0.0020, 0.0018, 0.0017, 0.0023,\n                      0.0016, 0.0017, 0.0018, 0.0015, 0.0015, 0.0016, 0.0014]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n                min_val=tensor([-0.1985, -0.1981, -0.1566, -0.1889, -0.1641, -0.2119, -0.1689, -0.2731,\n                        -0.1846, -0.2149, -0.1715, -0.1451, -0.1772, -0.2187, -0.1408, -0.1711,\n                        -0.1682, -0.1910, -0.2053, -0.1804, -0.1740, -0.1979, -0.1884, -0.1852,\n                        -0.1551, -0.2116, -0.2858, -0.2141, -0.1902, -0.2318, -0.2049, -0.1957,\n                        -0.1750, -0.2162, -0.1954, -0.2542, -0.3024, -0.1639, -0.1859, -0.2127,\n                        -0.2321, -0.1844, -0.2045, -0.1744, -0.1642, -0.2248, -0.1597, -0.1876,\n                        -0.2225, -0.1823, -0.1600, -0.1742, -0.1964, -0.1609, -0.2194, -0.1935,\n                        -0.2485, -0.2058, -0.2129, -0.2088, -0.2346, -0.2057, -0.1544, -0.2419,\n                        -0.1913, -0.1618, -0.1777, -0.1959, -0.2672, -0.1894, -0.2029, -0.1834,\n                        -0.2005, -0.1842, -0.1938, -0.2637, -0.2092, -0.1860, -0.2168, -0.1873,\n                        -0.2096, -0.2107, -0.2116, -0.2878, -0.2069, -0.1821, -0.1987, -0.1738,\n                        -0.2324, -0.2212, -0.1997, -0.2007, -0.1900, -0.1720, -0.2174, -0.1718,\n                        -0.2244, -0.2604, -0.2203, -0.2341, -0.1812, -0.2382, -0.1632, -0.2452,\n                        -0.2676, -0.1530, -0.2059, -0.2082, -0.2110, -0.1892, -0.1664, -0.2323,\n                        -0.2390, -0.1663, -0.2474, -0.1989, -0.1641, -0.1965, -0.1692, -0.2175,\n                        -0.1605, -0.1867, -0.2039, -0.2595, -0.1974, -0.1671, -0.1854, -0.1752,\n                        -0.1977, -0.1801, -0.2123, -0.1689, -0.1738, -0.1945, -0.2293, -0.2499,\n                        -0.1627, -0.1821, -0.2476, -0.1645, -0.1689, -0.1976, -0.1915, -0.1746,\n                        -0.1612, -0.2013, -0.2776, -0.2279, -0.1857, -0.2530, -0.1955, -0.2195,\n                        -0.2896, -0.1911, -0.2130, -0.2291, -0.1889, -0.1868, -0.1856, -0.1557]), max_val=tensor([0.1759, 0.1993, 0.2356, 0.2611, 0.2326, 0.1588, 0.2127, 0.2026, 0.2419,\n                        0.1893, 0.2156, 0.1547, 0.1795, 0.2103, 0.1399, 0.1623, 0.1929, 0.1750,\n                        0.1906, 0.2083, 0.2203, 0.1771, 0.2159, 0.1780, 0.1773, 0.2020, 0.1609,\n                        0.1987, 0.1905, 0.2157, 0.2723, 0.2276, 0.2008, 0.2014, 0.2139, 0.2496,\n                        0.1610, 0.2211, 0.2329, 0.2247, 0.2026, 0.2223, 0.2160, 0.3079, 0.1924,\n                        0.1657, 0.1480, 0.1399, 0.2386, 0.1820, 0.2462, 0.1770, 0.2081, 0.1738,\n                        0.1962, 0.2468, 0.2645, 0.1744, 0.1973, 0.1771, 0.2309, 0.3107, 0.2119,\n                        0.1971, 0.2497, 0.1628, 0.2135, 0.1923, 0.2379, 0.1999, 0.1797, 0.1742,\n                        0.2126, 0.1664, 0.1784, 0.1819, 0.2411, 0.2454, 0.2150, 0.1974, 0.1894,\n                        0.2423, 0.1655, 0.2455, 0.2239, 0.1752, 0.1738, 0.2405, 0.1953, 0.1788,\n                        0.1509, 0.1858, 0.2194, 0.1725, 0.2099, 0.2613, 0.2525, 0.2756, 0.1527,\n                        0.2240, 0.1960, 0.2600, 0.2059, 0.2180, 0.2056, 0.1722, 0.2094, 0.1991,\n                        0.1725, 0.2297, 0.1856, 0.1809, 0.2285, 0.1828, 0.1660, 0.1755, 0.1728,\n                        0.2000, 0.1542, 0.2164, 0.2141, 0.1757, 0.2394, 0.1852, 0.2510, 0.1884,\n                        0.2069, 0.1949, 0.1635, 0.2108, 0.2510, 0.2009, 0.1640, 0.1516, 0.1611,\n                        0.1904, 0.1988, 0.2597, 0.1614, 0.1695, 0.1591, 0.2007, 0.1679, 0.1981,\n                        0.1405, 0.2120, 0.1833, 0.2294, 0.1864, 0.1921, 0.2267, 0.2177, 0.2158,\n                        0.2071, 0.1902, 0.2008, 0.1590, 0.1542, 0.2035, 0.1817])\n              )\n            )\n          )\n        )\n      )\n      (15): Module(\n        (conv): Module(\n          (0): Module(\n            (0): ConvBn2d(\n              160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False\n              (bn): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n                fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0011, 0.0008, 0.0006, 0.0012, 0.0008, 0.0009, 0.0008, 0.0006, 0.0012,\n                        0.0010, 0.0006, 0.0006, 0.0012, 0.0013, 0.0009, 0.0009, 0.0014, 0.0011,\n                        0.0006, 0.0013, 0.0005, 0.0010, 0.0013, 0.0008, 0.0011, 0.0011, 0.0007,\n                        0.0007, 0.0007, 0.0010, 0.0005, 0.0009, 0.0007, 0.0009, 0.0016, 0.0007,\n                        0.0010, 0.0008, 0.0006, 0.0009, 0.0011, 0.0010, 0.0012, 0.0010, 0.0005,\n                        0.0009, 0.0011, 0.0009, 0.0009, 0.0008, 0.0014, 0.0010, 0.0011, 0.0012,\n                        0.0007, 0.0009, 0.0005, 0.0008, 0.0014, 0.0010, 0.0013, 0.0011, 0.0010,\n                        0.0007, 0.0008, 0.0011, 0.0005, 0.0012, 0.0010, 0.0008, 0.0010, 0.0008,\n                        0.0010, 0.0005, 0.0009, 0.0010, 0.0011, 0.0007, 0.0008, 0.0009, 0.0010,\n                        0.0009, 0.0008, 0.0010, 0.0011, 0.0010, 0.0010, 0.0007, 0.0009, 0.0005,\n                        0.0012, 0.0012, 0.0004, 0.0007, 0.0007, 0.0009, 0.0011, 0.0007, 0.0007,\n                        0.0007, 0.0013, 0.0006, 0.0008, 0.0007, 0.0008, 0.0014, 0.0007, 0.0007,\n                        0.0007, 0.0012, 0.0010, 0.0008, 0.0005, 0.0007, 0.0007, 0.0016, 0.0006,\n                        0.0009, 0.0006, 0.0013, 0.0012, 0.0006, 0.0008, 0.0010, 0.0009, 0.0007,\n                        0.0007, 0.0010, 0.0017, 0.0008, 0.0011, 0.0008, 0.0007, 0.0012, 0.0005,\n                        0.0007, 0.0007, 0.0009, 0.0010, 0.0008, 0.0009, 0.0010, 0.0006, 0.0011,\n                        0.0007, 0.0016, 0.0006, 0.0010, 0.0012, 0.0007, 0.0006, 0.0010, 0.0010,\n                        0.0006, 0.0010, 0.0013, 0.0007, 0.0009, 0.0008, 0.0008, 0.0010, 0.0015,\n                        0.0010, 0.0014, 0.0012, 0.0013, 0.0011, 0.0012, 0.0013, 0.0009, 0.0007,\n                        0.0006, 0.0009, 0.0009, 0.0010, 0.0004, 0.0009, 0.0005, 0.0009, 0.0014,\n                        0.0009, 0.0008, 0.0007, 0.0008, 0.0013, 0.0009, 0.0012, 0.0007, 0.0005,\n                        0.0010, 0.0008, 0.0011, 0.0005, 0.0011, 0.0007, 0.0015, 0.0009, 0.0012,\n                        0.0007, 0.0008, 0.0008, 0.0005, 0.0012, 0.0008, 0.0004, 0.0005, 0.0009,\n                        0.0003, 0.0008, 0.0008, 0.0010, 0.0007, 0.0011, 0.0006, 0.0009, 0.0007,\n                        0.0007, 0.0008, 0.0007, 0.0010, 0.0005, 0.0009, 0.0014, 0.0011, 0.0008,\n                        0.0011, 0.0005, 0.0010, 0.0007, 0.0006, 0.0008, 0.0013, 0.0007, 0.0010,\n                        0.0008, 0.0006, 0.0008, 0.0013, 0.0007, 0.0008, 0.0009, 0.0005, 0.0013,\n                        0.0008, 0.0010, 0.0009, 0.0009, 0.0007, 0.0007, 0.0010, 0.0009, 0.0008,\n                        0.0007, 0.0005, 0.0008, 0.0005, 0.0013, 0.0010, 0.0008, 0.0006, 0.0011,\n                        0.0010, 0.0008, 0.0008, 0.0009, 0.0010, 0.0010, 0.0010, 0.0009, 0.0005,\n                        0.0009, 0.0010, 0.0006, 0.0005, 0.0009, 0.0008, 0.0011, 0.0009, 0.0004,\n                        0.0008, 0.0006, 0.0012, 0.0006, 0.0007, 0.0018, 0.0009, 0.0008, 0.0007,\n                        0.0012, 0.0014, 0.0007, 0.0005, 0.0008, 0.0008, 0.0007, 0.0008, 0.0005,\n                        0.0012, 0.0007, 0.0007, 0.0009, 0.0010, 0.0008, 0.0011, 0.0009, 0.0009,\n                        0.0023, 0.0006, 0.0015, 0.0013, 0.0008, 0.0011, 0.0005, 0.0008, 0.0007,\n                        0.0011, 0.0007, 0.0006, 0.0013, 0.0008, 0.0009, 0.0010, 0.0004, 0.0011,\n                        0.0011, 0.0007, 0.0007, 0.0008, 0.0013, 0.0008, 0.0006, 0.0011, 0.0008,\n                        0.0009, 0.0011, 0.0011, 0.0011, 0.0005, 0.0008, 0.0011, 0.0005, 0.0006,\n                        0.0010, 0.0006, 0.0009, 0.0013, 0.0006, 0.0010, 0.0007, 0.0006, 0.0010,\n                        0.0008, 0.0011, 0.0008, 0.0009, 0.0007, 0.0010, 0.0008, 0.0006, 0.0007,\n                        0.0011, 0.0010, 0.0015, 0.0011, 0.0009, 0.0006, 0.0008, 0.0013, 0.0005,\n                        0.0008, 0.0006, 0.0012, 0.0006, 0.0007, 0.0010, 0.0011, 0.0008, 0.0009,\n                        0.0015, 0.0008, 0.0011, 0.0010, 0.0007, 0.0009, 0.0009, 0.0007, 0.0008,\n                        0.0009, 0.0009, 0.0014, 0.0014, 0.0005, 0.0008, 0.0009, 0.0007, 0.0007,\n                        0.0007, 0.0007, 0.0009, 0.0009, 0.0008, 0.0007, 0.0009, 0.0009, 0.0010,\n                        0.0006, 0.0010, 0.0008, 0.0012, 0.0011, 0.0010, 0.0008, 0.0008, 0.0010,\n                        0.0012, 0.0006, 0.0008, 0.0008, 0.0007, 0.0007, 0.0009, 0.0008, 0.0014,\n                        0.0008, 0.0009, 0.0010, 0.0007, 0.0007, 0.0006, 0.0007, 0.0010, 0.0009,\n                        0.0009, 0.0007, 0.0009, 0.0009, 0.0008, 0.0008, 0.0011, 0.0007, 0.0006,\n                        0.0008, 0.0008, 0.0012, 0.0005, 0.0008, 0.0009, 0.0007, 0.0006, 0.0009,\n                        0.0008, 0.0015, 0.0011, 0.0012, 0.0012, 0.0005, 0.0011, 0.0009, 0.0010,\n                        0.0013, 0.0011, 0.0006, 0.0011, 0.0007, 0.0013, 0.0007, 0.0005, 0.0010,\n                        0.0010, 0.0009, 0.0005, 0.0006, 0.0005, 0.0010, 0.0007, 0.0007, 0.0009,\n                        0.0009, 0.0007, 0.0005, 0.0010, 0.0009, 0.0009, 0.0010, 0.0005, 0.0006,\n                        0.0006, 0.0005, 0.0012, 0.0009, 0.0006, 0.0014, 0.0009, 0.0008, 0.0008,\n                        0.0009, 0.0009, 0.0007, 0.0011, 0.0009, 0.0009, 0.0008, 0.0007, 0.0005,\n                        0.0009, 0.0005, 0.0012, 0.0013, 0.0005, 0.0010, 0.0012, 0.0006, 0.0006,\n                        0.0016, 0.0011, 0.0011, 0.0006, 0.0007, 0.0007, 0.0008, 0.0006, 0.0012,\n                        0.0025, 0.0011, 0.0010, 0.0007, 0.0009, 0.0006, 0.0007, 0.0008, 0.0006,\n                        0.0007, 0.0007, 0.0006, 0.0013, 0.0015, 0.0013, 0.0011, 0.0009, 0.0011,\n                        0.0010, 0.0011, 0.0010, 0.0009, 0.0012, 0.0010, 0.0006, 0.0010, 0.0010,\n                        0.0010, 0.0013, 0.0007, 0.0008, 0.0009, 0.0003, 0.0009, 0.0007, 0.0006,\n                        0.0009, 0.0006, 0.0005, 0.0011, 0.0014, 0.0008, 0.0007, 0.0010, 0.0005,\n                        0.0009, 0.0008, 0.0008, 0.0005, 0.0012, 0.0011, 0.0013, 0.0005, 0.0014,\n                        0.0013, 0.0009, 0.0006, 0.0007, 0.0005, 0.0007, 0.0013, 0.0010, 0.0012,\n                        0.0009, 0.0012, 0.0009, 0.0011, 0.0004, 0.0010, 0.0009, 0.0010, 0.0010,\n                        0.0008, 0.0009, 0.0009, 0.0009, 0.0008, 0.0009, 0.0009, 0.0010, 0.0008,\n                        0.0007, 0.0006, 0.0009, 0.0011, 0.0009, 0.0005, 0.0009, 0.0006, 0.0007,\n                        0.0009, 0.0007, 0.0010, 0.0011, 0.0006, 0.0004, 0.0013, 0.0010, 0.0007,\n                        0.0008, 0.0011, 0.0014, 0.0006, 0.0011, 0.0009, 0.0010, 0.0007, 0.0011,\n                        0.0007, 0.0005, 0.0011, 0.0011, 0.0010, 0.0005, 0.0010, 0.0007, 0.0007,\n                        0.0006, 0.0007, 0.0011, 0.0008, 0.0014, 0.0010, 0.0013, 0.0010, 0.0007,\n                        0.0011, 0.0007, 0.0009, 0.0010, 0.0012, 0.0009, 0.0008, 0.0011, 0.0009,\n                        0.0006, 0.0005, 0.0011, 0.0011, 0.0008, 0.0008, 0.0008, 0.0009, 0.0008,\n                        0.0013, 0.0008, 0.0013, 0.0010, 0.0010, 0.0006, 0.0010, 0.0007, 0.0012,\n                        0.0006, 0.0009, 0.0007, 0.0009, 0.0010, 0.0012, 0.0010, 0.0007, 0.0011,\n                        0.0005, 0.0004, 0.0006, 0.0010, 0.0010, 0.0008, 0.0009, 0.0013, 0.0008,\n                        0.0010, 0.0008, 0.0008, 0.0010, 0.0009, 0.0006, 0.0009, 0.0007, 0.0009,\n                        0.0012, 0.0009, 0.0008, 0.0009, 0.0011, 0.0006, 0.0009, 0.0007, 0.0007,\n                        0.0008, 0.0011, 0.0011, 0.0008, 0.0007, 0.0008, 0.0009, 0.0007, 0.0010,\n                        0.0010, 0.0006, 0.0010, 0.0010, 0.0015, 0.0015, 0.0007, 0.0009, 0.0008,\n                        0.0012, 0.0008, 0.0009, 0.0009, 0.0013, 0.0014, 0.0007, 0.0015, 0.0008,\n                        0.0010, 0.0010, 0.0009, 0.0005, 0.0009, 0.0011, 0.0007, 0.0006, 0.0012,\n                        0.0011, 0.0009, 0.0007, 0.0007, 0.0010, 0.0007, 0.0009, 0.0006, 0.0009,\n                        0.0006, 0.0007, 0.0012, 0.0011, 0.0006, 0.0010, 0.0008, 0.0005, 0.0009,\n                        0.0007, 0.0013, 0.0010, 0.0009, 0.0006, 0.0006, 0.0004, 0.0008, 0.0010,\n                        0.0008, 0.0009, 0.0008, 0.0006, 0.0005, 0.0010, 0.0009, 0.0010, 0.0008,\n                        0.0012, 0.0008, 0.0005, 0.0007, 0.0007, 0.0012, 0.0008, 0.0012, 0.0007,\n                        0.0013, 0.0012, 0.0010, 0.0009, 0.0009, 0.0008, 0.0006, 0.0006, 0.0005,\n                        0.0011, 0.0006, 0.0012, 0.0006, 0.0009, 0.0006, 0.0005, 0.0012, 0.0004,\n                        0.0008, 0.0005, 0.0008, 0.0008, 0.0009, 0.0012, 0.0006, 0.0019, 0.0008,\n                        0.0008, 0.0006, 0.0008, 0.0013, 0.0010, 0.0010, 0.0008, 0.0007, 0.0007,\n                        0.0007, 0.0011, 0.0010, 0.0011, 0.0006, 0.0012, 0.0008, 0.0010, 0.0006,\n                        0.0008, 0.0008, 0.0010, 0.0005, 0.0015, 0.0006, 0.0008, 0.0010, 0.0009,\n                        0.0009, 0.0011, 0.0010, 0.0008, 0.0008, 0.0009, 0.0010, 0.0015, 0.0007,\n                        0.0012, 0.0013, 0.0005, 0.0005, 0.0012, 0.0012, 0.0028, 0.0010, 0.0006,\n                        0.0008, 0.0011, 0.0008, 0.0010, 0.0009, 0.0009, 0.0012, 0.0008, 0.0006,\n                        0.0009, 0.0013, 0.0011, 0.0008, 0.0006, 0.0012, 0.0004, 0.0012, 0.0008,\n                        0.0006, 0.0007, 0.0007, 0.0011, 0.0013, 0.0007, 0.0008, 0.0006, 0.0010,\n                        0.0008, 0.0007, 0.0010, 0.0010, 0.0008, 0.0008, 0.0012, 0.0010, 0.0010,\n                        0.0008, 0.0008, 0.0007, 0.0013, 0.0010, 0.0007, 0.0011, 0.0010, 0.0010,\n                        0.0010, 0.0004, 0.0009, 0.0012, 0.0007, 0.0009, 0.0009, 0.0011, 0.0004,\n                        0.0005, 0.0013, 0.0011, 0.0010, 0.0005, 0.0010, 0.0008, 0.0010, 0.0009,\n                        0.0008, 0.0010, 0.0009, 0.0006, 0.0010, 0.0004, 0.0007, 0.0011, 0.0013,\n                        0.0007, 0.0011, 0.0009, 0.0007, 0.0009, 0.0010, 0.0009, 0.0008, 0.0009,\n                        0.0008, 0.0003, 0.0006, 0.0013, 0.0010, 0.0009, 0.0007, 0.0008, 0.0008,\n                        0.0008, 0.0007, 0.0010, 0.0008, 0.0005, 0.0006]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                       dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n                (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n                  min_val=tensor([-0.0882, -0.0900, -0.0752, -0.1255, -0.1020, -0.1161, -0.1064, -0.0778,\n                          -0.1578, -0.1134, -0.0731, -0.0714, -0.1300, -0.1623, -0.1150, -0.1171,\n                          -0.1749, -0.1346, -0.0697, -0.1727, -0.0684, -0.1070, -0.1694, -0.0958,\n                          -0.1247, -0.1408, -0.0824, -0.0936, -0.0927, -0.0929, -0.0642, -0.1107,\n                          -0.0868, -0.1167, -0.1542, -0.0868, -0.0952, -0.0828, -0.0764, -0.1100,\n                          -0.1399, -0.0941, -0.1404, -0.1169, -0.0613, -0.1141, -0.1360, -0.1157,\n                          -0.1136, -0.1049, -0.1615, -0.1236, -0.1453, -0.1488, -0.0863, -0.1205,\n                          -0.0522, -0.1086, -0.1521, -0.1335, -0.1698, -0.1084, -0.1244, -0.0953,\n                          -0.0967, -0.1105, -0.0638, -0.1259, -0.1268, -0.1079, -0.1114, -0.1022,\n                          -0.1283, -0.0668, -0.1089, -0.1149, -0.1459, -0.0861, -0.0980, -0.1179,\n                          -0.1215, -0.1142, -0.0839, -0.1222, -0.1190, -0.0880, -0.0928, -0.0894,\n                          -0.0852, -0.0611, -0.1239, -0.1161, -0.0474, -0.0841, -0.0766, -0.1126,\n                          -0.0928, -0.0758, -0.0801, -0.0811, -0.1644, -0.0669, -0.1068, -0.0935,\n                          -0.0915, -0.1425, -0.0887, -0.0820, -0.0858, -0.1362, -0.1155, -0.0856,\n                          -0.0700, -0.0810, -0.0798, -0.2019, -0.0675, -0.1148, -0.0730, -0.1720,\n                          -0.1302, -0.0815, -0.0989, -0.1104, -0.1145, -0.0940, -0.0870, -0.0790,\n                          -0.1310, -0.0888, -0.1030, -0.0967, -0.0885, -0.1370, -0.0680, -0.0935,\n                          -0.0881, -0.1162, -0.1086, -0.0804, -0.1073, -0.1233, -0.0732, -0.1417,\n                          -0.0790, -0.1302, -0.0790, -0.1061, -0.1527, -0.0778, -0.0746, -0.1037,\n                          -0.1291, -0.0742, -0.1246, -0.1246, -0.0817, -0.1134, -0.1023, -0.0970,\n                          -0.1275, -0.1868, -0.1228, -0.1111, -0.1577, -0.1433, -0.1347, -0.1485,\n                          -0.1614, -0.1192, -0.0866, -0.0760, -0.0979, -0.1098, -0.1144, -0.0458,\n                          -0.1145, -0.0644, -0.1125, -0.1836, -0.0775, -0.0973, -0.0827, -0.0881,\n                          -0.1618, -0.1161, -0.1506, -0.0796, -0.0584, -0.1291, -0.0846, -0.1400,\n                          -0.0608, -0.1428, -0.0699, -0.1897, -0.1113, -0.1473, -0.0785, -0.0941,\n                          -0.1005, -0.0563, -0.1436, -0.0910, -0.0477, -0.0656, -0.1004, -0.0377,\n                          -0.0988, -0.1024, -0.1325, -0.0672, -0.1433, -0.0785, -0.0966, -0.0913,\n                          -0.0849, -0.0738, -0.0936, -0.1005, -0.0668, -0.1059, -0.1485, -0.1422,\n                          -0.0993, -0.1431, -0.0606, -0.1116, -0.0799, -0.0728, -0.1000, -0.1088,\n                          -0.0859, -0.1287, -0.1074, -0.0511, -0.0746, -0.1490, -0.0730, -0.1004,\n                          -0.1049, -0.0665, -0.1697, -0.0678, -0.1170, -0.1129, -0.1047, -0.0879,\n                          -0.0854, -0.1287, -0.1028, -0.0787, -0.0928, -0.0545, -0.0966, -0.0634,\n                          -0.1638, -0.1292, -0.0962, -0.0606, -0.1365, -0.1241, -0.1021, -0.1051,\n                          -0.0894, -0.0977, -0.0976, -0.1150, -0.1095, -0.0699, -0.1160, -0.1253,\n                          -0.0637, -0.0578, -0.1116, -0.1053, -0.1350, -0.1179, -0.0504, -0.0951,\n                          -0.0725, -0.1267, -0.0748, -0.0762, -0.2321, -0.1155, -0.0807, -0.0908,\n                          -0.1472, -0.1209, -0.0939, -0.0535, -0.0989, -0.0976, -0.0897, -0.0807,\n                          -0.0577, -0.1420, -0.0675, -0.0661, -0.1131, -0.1287, -0.0766, -0.1237,\n                          -0.1136, -0.1089, -0.2767, -0.0761, -0.1959, -0.1524, -0.0790, -0.1242,\n                          -0.0678, -0.1083, -0.0666, -0.1170, -0.0842, -0.0451, -0.1422, -0.0677,\n                          -0.0838, -0.1337, -0.0521, -0.0873, -0.1420, -0.0894, -0.0832, -0.1043,\n                          -0.1610, -0.0949, -0.0736, -0.0905, -0.0972, -0.1204, -0.1398, -0.1422,\n                          -0.1371, -0.0588, -0.1031, -0.1390, -0.0696, -0.0663, -0.1170, -0.0793,\n                          -0.1093, -0.1409, -0.0774, -0.1215, -0.0803, -0.0479, -0.1202, -0.1072,\n                          -0.1406, -0.0646, -0.1076, -0.0833, -0.1315, -0.0839, -0.0758, -0.0892,\n                          -0.1453, -0.1270, -0.1529, -0.1457, -0.1107, -0.0828, -0.0911, -0.1690,\n                          -0.0497, -0.1029, -0.0696, -0.1050, -0.0743, -0.0920, -0.1270, -0.1444,\n                          -0.1068, -0.1121, -0.1360, -0.1063, -0.0922, -0.1342, -0.0910, -0.1154,\n                          -0.1212, -0.0865, -0.0937, -0.1144, -0.1183, -0.1814, -0.1367, -0.0659,\n                          -0.1058, -0.0646, -0.0857, -0.0820, -0.0656, -0.0702, -0.0975, -0.1118,\n                          -0.1011, -0.0782, -0.1114, -0.1127, -0.1163, -0.0619, -0.1311, -0.1020,\n                          -0.1518, -0.1207, -0.1279, -0.0982, -0.0796, -0.0848, -0.1537, -0.0615,\n                          -0.1032, -0.1020, -0.0928, -0.0926, -0.1202, -0.0964, -0.1740, -0.1035,\n                          -0.1203, -0.1223, -0.0834, -0.0805, -0.0673, -0.0848, -0.1223, -0.1142,\n                          -0.1178, -0.0896, -0.0986, -0.1165, -0.1052, -0.1005, -0.1439, -0.0913,\n                          -0.0648, -0.1035, -0.1059, -0.1477, -0.0648, -0.0885, -0.1029, -0.0805,\n                          -0.0772, -0.1107, -0.0910, -0.1862, -0.1359, -0.0924, -0.1283, -0.0526,\n                          -0.1405, -0.0896, -0.1307, -0.1154, -0.1405, -0.0779, -0.1432, -0.0833,\n                          -0.1679, -0.0939, -0.0593, -0.1315, -0.1316, -0.1049, -0.0642, -0.0782,\n                          -0.0575, -0.1159, -0.0825, -0.0809, -0.1149, -0.1179, -0.0898, -0.0593,\n                          -0.1343, -0.1118, -0.1093, -0.1120, -0.0676, -0.0793, -0.0754, -0.0448,\n                          -0.1295, -0.1031, -0.0801, -0.1429, -0.1114, -0.0942, -0.1060, -0.1164,\n                          -0.1026, -0.0757, -0.0768, -0.1199, -0.1055, -0.1000, -0.0805, -0.0638,\n                          -0.1042, -0.0651, -0.1578, -0.1592, -0.0592, -0.1334, -0.1233, -0.0803,\n                          -0.0771, -0.2108, -0.1410, -0.0802, -0.0817, -0.0630, -0.0885, -0.1008,\n                          -0.0783, -0.1501, -0.2650, -0.1398, -0.1235, -0.0847, -0.1097, -0.0723,\n                          -0.0915, -0.0914, -0.0724, -0.0894, -0.0920, -0.0713, -0.1622, -0.1403,\n                          -0.1614, -0.1392, -0.1107, -0.1368, -0.1089, -0.0979, -0.1282, -0.1107,\n                          -0.1479, -0.1238, -0.0790, -0.1011, -0.1238, -0.1150, -0.1297, -0.0784,\n                          -0.0938, -0.1144, -0.0441, -0.1055, -0.0958, -0.0781, -0.1033, -0.0730,\n                          -0.0653, -0.1462, -0.1199, -0.1068, -0.0884, -0.1286, -0.0608, -0.1135,\n                          -0.1026, -0.0998, -0.0612, -0.1127, -0.1453, -0.1254, -0.0641, -0.1483,\n                          -0.1644, -0.1108, -0.0826, -0.0773, -0.0598, -0.0728, -0.1678, -0.0985,\n                          -0.1301, -0.1079, -0.1242, -0.1215, -0.1373, -0.0557, -0.1094, -0.1088,\n                          -0.1279, -0.1292, -0.1035, -0.1169, -0.1110, -0.1207, -0.0874, -0.1205,\n                          -0.1093, -0.1268, -0.1026, -0.0651, -0.0511, -0.1091, -0.1380, -0.0873,\n                          -0.0662, -0.1088, -0.0781, -0.0841, -0.1032, -0.0816, -0.1232, -0.1428,\n                          -0.0718, -0.0568, -0.1676, -0.1296, -0.0830, -0.0977, -0.1025, -0.1515,\n                          -0.0702, -0.1392, -0.0962, -0.1327, -0.0726, -0.1447, -0.0928, -0.0641,\n                          -0.1261, -0.1377, -0.1234, -0.0654, -0.1239, -0.0870, -0.0755, -0.0753,\n                          -0.0891, -0.1391, -0.0911, -0.1226, -0.1293, -0.1655, -0.1276, -0.0911,\n                          -0.1357, -0.0871, -0.0985, -0.1177, -0.1522, -0.1116, -0.0815, -0.0701,\n                          -0.1135, -0.0574, -0.0638, -0.1176, -0.1426, -0.1068, -0.0978, -0.1000,\n                          -0.1175, -0.0942, -0.1526, -0.0905, -0.1622, -0.1247, -0.0829, -0.0693,\n                          -0.1185, -0.0885, -0.0882, -0.0714, -0.1153, -0.0905, -0.0946, -0.1315,\n                          -0.0957, -0.1277, -0.0861, -0.1353, -0.0575, -0.0456, -0.0690, -0.1308,\n                          -0.1192, -0.0924, -0.0744, -0.1077, -0.0732, -0.1123, -0.1019, -0.1041,\n                          -0.1229, -0.0972, -0.0679, -0.1214, -0.0927, -0.0794, -0.1226, -0.0944,\n                          -0.1007, -0.1164, -0.1428, -0.0823, -0.1153, -0.0828, -0.0951, -0.0840,\n                          -0.1209, -0.1344, -0.1011, -0.0790, -0.0880, -0.0738, -0.0816, -0.1293,\n                          -0.1243, -0.0763, -0.0922, -0.1328, -0.1277, -0.1878, -0.0894, -0.0899,\n                          -0.0898, -0.1586, -0.0634, -0.1026, -0.1165, -0.1144, -0.1304, -0.0795,\n                          -0.1410, -0.1027, -0.1217, -0.1247, -0.0881, -0.0628, -0.0988, -0.1243,\n                          -0.0764, -0.0812, -0.1147, -0.1400, -0.1144, -0.0817, -0.0656, -0.1265,\n                          -0.0887, -0.1167, -0.0678, -0.1121, -0.0740, -0.0813, -0.1554, -0.0810,\n                          -0.0790, -0.0961, -0.0812, -0.0659, -0.1042, -0.0894, -0.1260, -0.1171,\n                          -0.1174, -0.0632, -0.0653, -0.0466, -0.0981, -0.1079, -0.0852, -0.1148,\n                          -0.0918, -0.0520, -0.0665, -0.1079, -0.1103, -0.1241, -0.1058, -0.1183,\n                          -0.0968, -0.0619, -0.0908, -0.0488, -0.1570, -0.0936, -0.1475, -0.0808,\n                          -0.1709, -0.1535, -0.1276, -0.0685, -0.0923, -0.1051, -0.0748, -0.0818,\n                          -0.0649, -0.1351, -0.0732, -0.1523, -0.0789, -0.1106, -0.0574, -0.0594,\n                          -0.1182, -0.0399, -0.1037, -0.0689, -0.0990, -0.0975, -0.1198, -0.1555,\n                          -0.0780, -0.1630, -0.0890, -0.0969, -0.0736, -0.1006, -0.1606, -0.1259,\n                          -0.1307, -0.0979, -0.0789, -0.0834, -0.0879, -0.1371, -0.1247, -0.1467,\n                          -0.0753, -0.1397, -0.0945, -0.1132, -0.0614, -0.0770, -0.1077, -0.1288,\n                          -0.0536, -0.1216, -0.0773, -0.0857, -0.1343, -0.0976, -0.1098, -0.1131,\n                          -0.1205, -0.0801, -0.0900, -0.1057, -0.1318, -0.1288, -0.0678, -0.1561,\n                          -0.1007, -0.0610, -0.0655, -0.1571, -0.1477, -0.3569, -0.1113, -0.0808,\n                          -0.1044, -0.1419, -0.0995, -0.1272, -0.1099, -0.1132, -0.1598, -0.1048,\n                          -0.0796, -0.0861, -0.1451, -0.1136, -0.1006, -0.0627, -0.1501, -0.0437,\n                          -0.1084, -0.0999, -0.0813, -0.0861, -0.0920, -0.1218, -0.1137, -0.0769,\n                          -0.0941, -0.0698, -0.1267, -0.0851, -0.0896, -0.1136, -0.1146, -0.1033,\n                          -0.0976, -0.1569, -0.1337, -0.1225, -0.1073, -0.0837, -0.0815, -0.1351,\n                          -0.1326, -0.0894, -0.1181, -0.1217, -0.1262, -0.1294, -0.0567, -0.0919,\n                          -0.1311, -0.0736, -0.0766, -0.1111, -0.1283, -0.0505, -0.0642, -0.1005,\n                          -0.1268, -0.1240, -0.0694, -0.1269, -0.0961, -0.1260, -0.0932, -0.0949,\n                          -0.1256, -0.1081, -0.0588, -0.1014, -0.0476, -0.0742, -0.1375, -0.1719,\n                          -0.0739, -0.0785, -0.1053, -0.0818, -0.1049, -0.1161, -0.0919, -0.0969,\n                          -0.0915, -0.1052, -0.0338, -0.0713, -0.1562, -0.1264, -0.1062, -0.0865,\n                          -0.0993, -0.1044, -0.0940, -0.0741, -0.1325, -0.0751, -0.0600, -0.0758]), max_val=tensor([0.1355, 0.1028, 0.0538, 0.1585, 0.0789, 0.0907, 0.0732, 0.0643, 0.1243,\n                          0.1218, 0.0713, 0.0823, 0.1508, 0.1641, 0.1078, 0.1033, 0.1417, 0.0968,\n                          0.0766, 0.1430, 0.0555, 0.1316, 0.1284, 0.1021, 0.1441, 0.1018, 0.0932,\n                          0.0807, 0.0849, 0.1317, 0.0591, 0.0904, 0.0624, 0.0835, 0.1984, 0.0729,\n                          0.1286, 0.0998, 0.0651, 0.1095, 0.1120, 0.1226, 0.1461, 0.1306, 0.0589,\n                          0.1110, 0.1118, 0.1003, 0.1032, 0.0866, 0.1734, 0.1239, 0.1407, 0.1331,\n                          0.0649, 0.0651, 0.0631, 0.0812, 0.1808, 0.0921, 0.1488, 0.1343, 0.1304,\n                          0.0832, 0.0908, 0.1349, 0.0643, 0.1476, 0.0880, 0.1046, 0.1284, 0.1070,\n                          0.1022, 0.0660, 0.0905, 0.1309, 0.1258, 0.0737, 0.1036, 0.1147, 0.1259,\n                          0.1100, 0.0959, 0.1319, 0.1371, 0.1285, 0.1241, 0.0716, 0.1176, 0.0540,\n                          0.1552, 0.1485, 0.0485, 0.0921, 0.0925, 0.0896, 0.1334, 0.0843, 0.0850,\n                          0.0891, 0.1105, 0.0719, 0.1061, 0.0913, 0.0968, 0.1794, 0.0809, 0.0916,\n                          0.0790, 0.1532, 0.1314, 0.1076, 0.0686, 0.0899, 0.0912, 0.1696, 0.0789,\n                          0.0697, 0.0565, 0.1349, 0.1529, 0.0813, 0.1052, 0.1228, 0.0976, 0.0899,\n                          0.0799, 0.1227, 0.2141, 0.1043, 0.1454, 0.0680, 0.0524, 0.1471, 0.0623,\n                          0.0901, 0.0858, 0.0975, 0.1220, 0.0995, 0.1084, 0.1152, 0.0528, 0.1058,\n                          0.0851, 0.2037, 0.0632, 0.1315, 0.1132, 0.0908, 0.0794, 0.1218, 0.1045,\n                          0.0735, 0.1164, 0.1613, 0.0837, 0.1024, 0.0987, 0.1053, 0.1110, 0.1498,\n                          0.0848, 0.1833, 0.1359, 0.1613, 0.1229, 0.1226, 0.1364, 0.1144, 0.0862,\n                          0.0733, 0.1196, 0.1042, 0.1295, 0.0518, 0.1021, 0.0535, 0.0934, 0.1567,\n                          0.1197, 0.0828, 0.0936, 0.1024, 0.1123, 0.1043, 0.1508, 0.0827, 0.0590,\n                          0.1122, 0.0976, 0.1449, 0.0539, 0.1414, 0.0830, 0.1929, 0.0964, 0.1480,\n                          0.0929, 0.1079, 0.0741, 0.0645, 0.1517, 0.1029, 0.0430, 0.0598, 0.1202,\n                          0.0431, 0.0747, 0.0965, 0.0988, 0.0866, 0.1243, 0.0695, 0.1104, 0.0779,\n                          0.0868, 0.1049, 0.0838, 0.1315, 0.0610, 0.1129, 0.1830, 0.1040, 0.0761,\n                          0.1383, 0.0608, 0.1239, 0.0862, 0.0803, 0.0834, 0.1672, 0.0774, 0.1039,\n                          0.0873, 0.0809, 0.0986, 0.1637, 0.0863, 0.0888, 0.1167, 0.0436, 0.1476,\n                          0.0989, 0.1223, 0.1137, 0.1112, 0.0819, 0.0823, 0.0719, 0.1109, 0.0968,\n                          0.0845, 0.0695, 0.0885, 0.0581, 0.1611, 0.1219, 0.0825, 0.0706, 0.1326,\n                          0.0911, 0.0947, 0.0853, 0.1096, 0.1222, 0.1219, 0.1322, 0.1015, 0.0685,\n                          0.1055, 0.1134, 0.0793, 0.0495, 0.0879, 0.0804, 0.1211, 0.0905, 0.0527,\n                          0.0971, 0.0804, 0.1483, 0.0804, 0.0856, 0.1718, 0.0988, 0.1047, 0.0886,\n                          0.1351, 0.1735, 0.0888, 0.0638, 0.0833, 0.1020, 0.0931, 0.0972, 0.0585,\n                          0.1571, 0.0868, 0.0849, 0.1018, 0.1219, 0.1030, 0.1411, 0.1061, 0.0867,\n                          0.2940, 0.0813, 0.1591, 0.1685, 0.0966, 0.1373, 0.0618, 0.1037, 0.0864,\n                          0.1340, 0.0629, 0.0816, 0.1680, 0.1057, 0.1196, 0.1227, 0.0502, 0.1339,\n                          0.1385, 0.0939, 0.0940, 0.0768, 0.1363, 0.1007, 0.0763, 0.1413, 0.0799,\n                          0.1003, 0.1067, 0.1342, 0.0931, 0.0681, 0.0837, 0.0978, 0.0683, 0.0795,\n                          0.1223, 0.0715, 0.1070, 0.1685, 0.0720, 0.1287, 0.0913, 0.0717, 0.1249,\n                          0.0901, 0.1156, 0.0966, 0.1181, 0.0904, 0.1267, 0.1013, 0.0677, 0.0674,\n                          0.1191, 0.1208, 0.1908, 0.1222, 0.0867, 0.0611, 0.1054, 0.1293, 0.0620,\n                          0.0954, 0.0786, 0.1539, 0.0517, 0.0714, 0.0972, 0.1352, 0.1029, 0.0995,\n                          0.1854, 0.0810, 0.1339, 0.1095, 0.0852, 0.1200, 0.1032, 0.0904, 0.0999,\n                          0.1090, 0.1203, 0.1479, 0.1720, 0.0599, 0.0717, 0.1181, 0.0888, 0.0898,\n                          0.0846, 0.0947, 0.1166, 0.1087, 0.1017, 0.0928, 0.1070, 0.0852, 0.1266,\n                          0.0719, 0.0928, 0.0814, 0.1398, 0.1448, 0.1151, 0.0904, 0.0962, 0.1224,\n                          0.1211, 0.0705, 0.0898, 0.1036, 0.0916, 0.0812, 0.1051, 0.1072, 0.1732,\n                          0.1063, 0.1111, 0.0708, 0.0911, 0.0926, 0.0739, 0.0889, 0.1252, 0.1015,\n                          0.1112, 0.0750, 0.1119, 0.1066, 0.0956, 0.0918, 0.1253, 0.0862, 0.0738,\n                          0.0960, 0.0966, 0.1363, 0.0589, 0.1015, 0.1082, 0.0829, 0.0749, 0.0981,\n                          0.1078, 0.1246, 0.0845, 0.1553, 0.1539, 0.0652, 0.1423, 0.1084, 0.1160,\n                          0.1645, 0.1127, 0.0727, 0.1274, 0.0675, 0.1617, 0.0634, 0.0613, 0.1193,\n                          0.1194, 0.1109, 0.0580, 0.0801, 0.0598, 0.1297, 0.0873, 0.0857, 0.0968,\n                          0.0889, 0.0761, 0.0573, 0.0788, 0.0854, 0.0963, 0.1305, 0.0664, 0.0780,\n                          0.0649, 0.0652, 0.1573, 0.1161, 0.0728, 0.1725, 0.1014, 0.0981, 0.0898,\n                          0.0924, 0.1120, 0.0901, 0.1454, 0.1187, 0.1167, 0.0836, 0.0934, 0.0668,\n                          0.1080, 0.0542, 0.1434, 0.1602, 0.0602, 0.0967, 0.1507, 0.0796, 0.0803,\n                          0.1905, 0.1286, 0.1396, 0.0753, 0.0875, 0.0890, 0.1059, 0.0620, 0.1186,\n                          0.3129, 0.1029, 0.0884, 0.0898, 0.1109, 0.0682, 0.0706, 0.0959, 0.0655,\n                          0.0654, 0.0941, 0.0688, 0.1275, 0.1942, 0.1245, 0.1338, 0.1183, 0.0870,\n                          0.1304, 0.1349, 0.1153, 0.1105, 0.1193, 0.1218, 0.0813, 0.1277, 0.0910,\n                          0.1214, 0.1647, 0.0850, 0.0979, 0.1123, 0.0433, 0.1112, 0.0758, 0.0772,\n                          0.1099, 0.0805, 0.0626, 0.1439, 0.1758, 0.0871, 0.0913, 0.1067, 0.0633,\n                          0.1150, 0.1047, 0.0859, 0.0550, 0.1500, 0.1130, 0.1688, 0.0636, 0.1727,\n                          0.1396, 0.0994, 0.0811, 0.0826, 0.0569, 0.0869, 0.1520, 0.1210, 0.1574,\n                          0.1137, 0.1508, 0.1055, 0.1030, 0.0539, 0.1267, 0.1013, 0.0922, 0.1129,\n                          0.0807, 0.0847, 0.1088, 0.1020, 0.1015, 0.1157, 0.1114, 0.1078, 0.0875,\n                          0.0828, 0.0739, 0.0959, 0.1195, 0.1135, 0.0652, 0.1194, 0.0751, 0.0831,\n                          0.1180, 0.0910, 0.0769, 0.1390, 0.0663, 0.0502, 0.1280, 0.0925, 0.0915,\n                          0.0897, 0.1353, 0.1804, 0.0774, 0.1219, 0.1128, 0.0932, 0.0897, 0.0981,\n                          0.0736, 0.0409, 0.1382, 0.1052, 0.0899, 0.0530, 0.1079, 0.0898, 0.0941,\n                          0.0775, 0.0922, 0.1160, 0.0977, 0.1830, 0.1299, 0.1563, 0.1028, 0.0830,\n                          0.1388, 0.0945, 0.1087, 0.1317, 0.1382, 0.1107, 0.1071, 0.1371, 0.0962,\n                          0.0736, 0.0673, 0.1349, 0.1397, 0.0701, 0.0891, 0.0913, 0.0936, 0.0966,\n                          0.1708, 0.1021, 0.1312, 0.1233, 0.1262, 0.0727, 0.1332, 0.0837, 0.1482,\n                          0.0686, 0.1099, 0.0790, 0.1097, 0.1047, 0.1528, 0.1270, 0.0626, 0.1212,\n                          0.0660, 0.0500, 0.0795, 0.1077, 0.1326, 0.0986, 0.1088, 0.1631, 0.0956,\n                          0.1251, 0.0925, 0.0932, 0.0806, 0.1086, 0.0766, 0.1069, 0.0744, 0.1127,\n                          0.1487, 0.1121, 0.0948, 0.0818, 0.1381, 0.0784, 0.1190, 0.0951, 0.0834,\n                          0.0985, 0.1429, 0.1286, 0.1018, 0.0843, 0.0987, 0.1118, 0.0839, 0.1167,\n                          0.1074, 0.0724, 0.1237, 0.1015, 0.1947, 0.1961, 0.0753, 0.1140, 0.1068,\n                          0.1552, 0.1017, 0.1133, 0.1153, 0.1660, 0.1727, 0.0840, 0.1856, 0.0899,\n                          0.0843, 0.1307, 0.1195, 0.0578, 0.1149, 0.1424, 0.0841, 0.0722, 0.1484,\n                          0.1200, 0.1033, 0.0834, 0.0851, 0.1201, 0.0780, 0.1118, 0.0784, 0.0924,\n                          0.0723, 0.0852, 0.1444, 0.1413, 0.0701, 0.1213, 0.1011, 0.0629, 0.1155,\n                          0.0894, 0.1696, 0.1214, 0.0890, 0.0736, 0.0822, 0.0523, 0.1003, 0.1254,\n                          0.1044, 0.1160, 0.1003, 0.0724, 0.0689, 0.1216, 0.0815, 0.0872, 0.0807,\n                          0.1488, 0.1036, 0.0590, 0.0694, 0.0919, 0.1193, 0.1017, 0.1285, 0.0942,\n                          0.1514, 0.0948, 0.1171, 0.1137, 0.1159, 0.0843, 0.0740, 0.0713, 0.0690,\n                          0.1295, 0.0798, 0.1361, 0.0644, 0.1080, 0.0809, 0.0564, 0.1486, 0.0502,\n                          0.0928, 0.0688, 0.1023, 0.0919, 0.1118, 0.1039, 0.0687, 0.2376, 0.1069,\n                          0.0764, 0.0710, 0.0870, 0.1188, 0.1043, 0.0943, 0.0931, 0.0941, 0.0799,\n                          0.0934, 0.1247, 0.1052, 0.1282, 0.0765, 0.1564, 0.1071, 0.1243, 0.0706,\n                          0.1040, 0.0859, 0.1228, 0.0599, 0.1851, 0.0804, 0.1021, 0.1226, 0.1147,\n                          0.1098, 0.1371, 0.1312, 0.0998, 0.1042, 0.1181, 0.0934, 0.1870, 0.0841,\n                          0.1342, 0.1613, 0.0597, 0.0629, 0.1202, 0.1407, 0.2238, 0.1307, 0.0756,\n                          0.1068, 0.1248, 0.0738, 0.1091, 0.0957, 0.0933, 0.1117, 0.0965, 0.0774,\n                          0.1125, 0.1665, 0.1374, 0.0704, 0.0812, 0.1484, 0.0546, 0.1467, 0.0846,\n                          0.0800, 0.0703, 0.0812, 0.1362, 0.1650, 0.0912, 0.1011, 0.0823, 0.1301,\n                          0.0991, 0.0849, 0.1333, 0.1255, 0.1016, 0.0958, 0.1566, 0.0948, 0.1311,\n                          0.0788, 0.0986, 0.0908, 0.1683, 0.0845, 0.0900, 0.1339, 0.1083, 0.1058,\n                          0.1252, 0.0520, 0.1206, 0.1539, 0.0876, 0.1205, 0.0896, 0.1440, 0.0526,\n                          0.0638, 0.1705, 0.1400, 0.0932, 0.0617, 0.1077, 0.0904, 0.1059, 0.1140,\n                          0.1033, 0.0967, 0.1191, 0.0752, 0.1209, 0.0489, 0.0873, 0.1016, 0.1521,\n                          0.0881, 0.1394, 0.1109, 0.0853, 0.1123, 0.1217, 0.1165, 0.0607, 0.1088,\n                          0.0927, 0.0295, 0.0532, 0.1650, 0.1125, 0.1081, 0.0717, 0.0991, 0.1042,\n                          0.1059, 0.0870, 0.1115, 0.1060, 0.0555, 0.0653])\n                )\n              )\n            )\n            (2): ReLU6(inplace=True)\n          )\n          (1): Module(\n            (0): ConvBn2d(\n              960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False\n              (bn): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n                fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0088, 0.0221, 0.0158, 0.0018, 0.0067, 0.0021, 0.0129, 0.0153, 0.0020,\n                        0.0022, 0.0029, 0.0228, 0.0031, 0.0022, 0.0169, 0.0081, 0.0018, 0.0048,\n                        0.0098, 0.0021, 0.0124, 0.0123, 0.0017, 0.0099, 0.0117, 0.0159, 0.0127,\n                        0.0031, 0.0026, 0.0091, 0.0095, 0.0111, 0.0125, 0.0044, 0.0019, 0.0120,\n                        0.0102, 0.0049, 0.0066, 0.0097, 0.0019, 0.0126, 0.0076, 0.0131, 0.0171,\n                        0.0022, 0.0144, 0.0144, 0.0086, 0.0048, 0.0017, 0.0019, 0.0080, 0.0091,\n                        0.0106, 0.0113, 0.0235, 0.0118, 0.0121, 0.0185, 0.0036, 0.0017, 0.0086,\n                        0.0082, 0.0020, 0.0087, 0.0112, 0.0022, 0.0022, 0.0115, 0.0114, 0.0111,\n                        0.0117, 0.0175, 0.0101, 0.0176, 0.0024, 0.0188, 0.0105, 0.0131, 0.0109,\n                        0.0018, 0.0027, 0.0022, 0.0021, 0.0046, 0.0111, 0.0101, 0.0057, 0.0170,\n                        0.0016, 0.0016, 0.0428, 0.0085, 0.0088, 0.0126, 0.0083, 0.0189, 0.0151,\n                        0.0155, 0.0084, 0.0119, 0.0217, 0.0180, 0.0031, 0.0065, 0.0152, 0.0112,\n                        0.0146, 0.0188, 0.0017, 0.0025, 0.0210, 0.0114, 0.0274, 0.0039, 0.0137,\n                        0.0123, 0.0254, 0.0029, 0.0014, 0.0107, 0.0130, 0.0015, 0.0124, 0.0135,\n                        0.0126, 0.0026, 0.0073, 0.0171, 0.0085, 0.0110, 0.0183, 0.0171, 0.0124,\n                        0.0116, 0.0141, 0.0055, 0.0019, 0.0087, 0.0110, 0.0093, 0.0114, 0.0074,\n                        0.0060, 0.0080, 0.0130, 0.0016, 0.0023, 0.0153, 0.0133, 0.0146, 0.0161,\n                        0.0104, 0.0020, 0.0021, 0.0115, 0.0135, 0.0159, 0.0122, 0.0116, 0.0040,\n                        0.0159, 0.0019, 0.0018, 0.0018, 0.0020, 0.0022, 0.0111, 0.0076, 0.0115,\n                        0.0110, 0.0102, 0.0104, 0.0075, 0.0133, 0.0102, 0.0256, 0.0024, 0.0020,\n                        0.0113, 0.0050, 0.0020, 0.0103, 0.0094, 0.0130, 0.0059, 0.0133, 0.0117,\n                        0.0023, 0.0154, 0.0127, 0.0134, 0.0016, 0.0026, 0.0096, 0.0022, 0.0083,\n                        0.0123, 0.0122, 0.0030, 0.0226, 0.0044, 0.0111, 0.0116, 0.0147, 0.0133,\n                        0.0327, 0.0179, 0.0070, 0.0078, 0.0147, 0.0089, 0.0299, 0.0096, 0.0075,\n                        0.0081, 0.0170, 0.0089, 0.0125, 0.0029, 0.0107, 0.0034, 0.0018, 0.0100,\n                        0.0021, 0.0188, 0.0132, 0.0086, 0.0170, 0.0066, 0.0020, 0.0127, 0.0099,\n                        0.0128, 0.0170, 0.0231, 0.0117, 0.0119, 0.0064, 0.0123, 0.0186, 0.0024,\n                        0.0109, 0.0074, 0.0129, 0.0149, 0.0234, 0.0115, 0.0126, 0.0034, 0.0097,\n                        0.0021, 0.0154, 0.0240, 0.0135, 0.0017, 0.0085, 0.0180, 0.0225, 0.0122,\n                        0.0109, 0.0021, 0.0074, 0.0083, 0.0202, 0.0189, 0.0016, 0.0137, 0.0033,\n                        0.0019, 0.0019, 0.0149, 0.0129, 0.0109, 0.0108, 0.0109, 0.0067, 0.0256,\n                        0.0081, 0.0174, 0.0039, 0.0115, 0.0144, 0.0082, 0.0086, 0.0067, 0.0101,\n                        0.0021, 0.0023, 0.0022, 0.0124, 0.0200, 0.0025, 0.0101, 0.0099, 0.0124,\n                        0.0023, 0.0027, 0.0169, 0.0019, 0.0055, 0.0032, 0.0144, 0.0096, 0.0115,\n                        0.0021, 0.0161, 0.0016, 0.0156, 0.0145, 0.0097, 0.0295, 0.0053, 0.0112,\n                        0.0026, 0.0142, 0.0143, 0.0019, 0.0103, 0.0150, 0.0076, 0.0215, 0.0088,\n                        0.0018, 0.0198, 0.0127, 0.0082, 0.0098, 0.0120, 0.0149, 0.0061, 0.0112,\n                        0.0174, 0.0026, 0.0043, 0.0088, 0.0083, 0.0021, 0.0021, 0.0203, 0.0189,\n                        0.0108, 0.0120, 0.0105, 0.0022, 0.0129, 0.0123, 0.0126, 0.0117, 0.0021,\n                        0.0023, 0.0055, 0.0188, 0.0101, 0.0118, 0.0131, 0.0076, 0.0108, 0.0117,\n                        0.0018, 0.0082, 0.0077, 0.0077, 0.0082, 0.0207, 0.0038, 0.0025, 0.0167,\n                        0.0173, 0.0143, 0.0071, 0.0131, 0.0576, 0.0092, 0.0270, 0.0140, 0.0178,\n                        0.0023, 0.0095, 0.0114, 0.0108, 0.0152, 0.0020, 0.0099, 0.0121, 0.0390,\n                        0.0188, 0.0015, 0.0034, 0.0074, 0.0121, 0.0166, 0.0238, 0.0072, 0.0117,\n                        0.0153, 0.0117, 0.0149, 0.0111, 0.0161, 0.0188, 0.0181, 0.0110, 0.0071,\n                        0.0147, 0.0018, 0.0125, 0.0023, 0.0110, 0.0040, 0.0016, 0.0107, 0.0044,\n                        0.0033, 0.0170, 0.0110, 0.0147, 0.0120, 0.0182, 0.0101, 0.0022, 0.0020,\n                        0.0098, 0.0019, 0.0115, 0.0021, 0.0144, 0.0168, 0.0082, 0.0118, 0.0028,\n                        0.0086, 0.0108, 0.0022, 0.0025, 0.0023, 0.0024, 0.0019, 0.0132, 0.0112,\n                        0.0121, 0.0129, 0.0043, 0.0646, 0.0022, 0.0079, 0.0081, 0.0024, 0.0246,\n                        0.0108, 0.0027, 0.0141, 0.0091, 0.0114, 0.0115, 0.0021, 0.0152, 0.0020,\n                        0.0036, 0.0020, 0.0121, 0.0080, 0.0081, 0.0068, 0.0106, 0.0104, 0.0067,\n                        0.0035, 0.0113, 0.0183, 0.0076, 0.0056, 0.0063, 0.0113, 0.0100, 0.0020,\n                        0.0141, 0.0127, 0.0125, 0.0121, 0.0102, 0.0019, 0.0049, 0.0210, 0.0099,\n                        0.0159, 0.0224, 0.0023, 0.0079, 0.0088, 0.0017, 0.0095, 0.0093, 0.0086,\n                        0.0056, 0.0053, 0.0032, 0.0118, 0.0017, 0.0024, 0.0142, 0.0113, 0.0133,\n                        0.0062, 0.0131, 0.0030, 0.0017, 0.0203, 0.0134, 0.0082, 0.0096, 0.0136,\n                        0.0101, 0.0094, 0.0114, 0.0136, 0.0030, 0.0121, 0.0158, 0.0148, 0.0022,\n                        0.0025, 0.0110, 0.0107, 0.0125, 0.0168, 0.0116, 0.0084, 0.0084, 0.0123,\n                        0.0124, 0.0020, 0.0113, 0.0048, 0.0041, 0.0142, 0.0034, 0.0078, 0.0099,\n                        0.0020, 0.0111, 0.0017, 0.0036, 0.0018, 0.0126, 0.0377, 0.0150, 0.0186,\n                        0.0104, 0.0019, 0.0233, 0.0114, 0.0050, 0.0333, 0.0120, 0.0122, 0.0179,\n                        0.0023, 0.0029, 0.0126, 0.0020, 0.0090, 0.0067, 0.0139, 0.0099, 0.0373,\n                        0.0025, 0.0136, 0.0109, 0.0142, 0.0074, 0.0020, 0.0014, 0.0144, 0.0033,\n                        0.0099, 0.0020, 0.0183, 0.0129, 0.0111, 0.0123, 0.0040, 0.0144, 0.0016,\n                        0.0033, 0.0149, 0.0085, 0.0021, 0.0202, 0.0040, 0.0022, 0.0023, 0.0105,\n                        0.0134, 0.0017, 0.0018, 0.0129, 0.0110, 0.0077, 0.0142, 0.0133, 0.0020,\n                        0.0086, 0.0154, 0.0271, 0.0021, 0.0110, 0.0156, 0.0022, 0.0130, 0.0038,\n                        0.0055, 0.0210, 0.0097, 0.0024, 0.0235, 0.0276, 0.0017, 0.0086, 0.0108,\n                        0.0033, 0.0016, 0.0023, 0.0113, 0.0018, 0.0022, 0.0022, 0.0089, 0.0019,\n                        0.0165, 0.0261, 0.0022, 0.0079, 0.0086, 0.0129, 0.0098, 0.0116, 0.0098,\n                        0.0111, 0.0093, 0.0081, 0.0015, 0.0017, 0.0074, 0.0017, 0.0125, 0.0154,\n                        0.0096, 0.0032, 0.0120, 0.0122, 0.0018, 0.0169, 0.0091, 0.0053, 0.0104,\n                        0.0114, 0.0104, 0.0115, 0.0273, 0.0094, 0.0140, 0.0170, 0.0108, 0.0141,\n                        0.0018, 0.0103, 0.0081, 0.0084, 0.0107, 0.0171, 0.0071, 0.0125, 0.0118,\n                        0.0341, 0.0021, 0.0123, 0.0121, 0.0029, 0.0065, 0.0102, 0.0129, 0.0095,\n                        0.0138, 0.0134, 0.0031, 0.0022, 0.0127, 0.0134, 0.0056, 0.0161, 0.0113,\n                        0.0068, 0.0024, 0.0200, 0.0103, 0.0066, 0.0096, 0.0057, 0.0165, 0.0085,\n                        0.0018, 0.0122, 0.0437, 0.0040, 0.0020, 0.0101, 0.0074, 0.0110, 0.0186,\n                        0.0022, 0.0098, 0.0139, 0.0023, 0.0108, 0.0119, 0.0119, 0.0108, 0.0026,\n                        0.0027, 0.0118, 0.0096, 0.0071, 0.0042, 0.0018, 0.0100, 0.0111, 0.0047,\n                        0.0024, 0.0079, 0.0133, 0.0094, 0.0021, 0.0041, 0.0163, 0.0019, 0.0028,\n                        0.0151, 0.0156, 0.0143, 0.0168, 0.0019, 0.0091, 0.0110, 0.0122, 0.0104,\n                        0.0022, 0.0132, 0.0110, 0.0102, 0.0089, 0.0130, 0.0047, 0.0090, 0.0120,\n                        0.0236, 0.0191, 0.0023, 0.0113, 0.0139, 0.0095, 0.0110, 0.0137, 0.0132,\n                        0.0085, 0.0097, 0.0096, 0.0024, 0.0164, 0.0103, 0.0237, 0.0106, 0.0015,\n                        0.0087, 0.0025, 0.0084, 0.0116, 0.0123, 0.0114, 0.0111, 0.0023, 0.0182,\n                        0.0018, 0.0142, 0.0116, 0.0268, 0.0107, 0.0041, 0.0089, 0.0022, 0.0122,\n                        0.0076, 0.0019, 0.0021, 0.0253, 0.0024, 0.0031, 0.0133, 0.0191, 0.0147,\n                        0.0138, 0.0129, 0.0112, 0.0181, 0.0071, 0.0144, 0.0178, 0.0058, 0.0262,\n                        0.0036, 0.0086, 0.0114, 0.0107, 0.0095, 0.0031, 0.0133, 0.0021, 0.0145,\n                        0.0023, 0.0075, 0.0131, 0.0016, 0.0096, 0.0022, 0.0025, 0.0118, 0.0119,\n                        0.0113, 0.0022, 0.0141, 0.0017, 0.0051, 0.0022, 0.0088, 0.0138, 0.0151,\n                        0.0024, 0.0017, 0.0022, 0.0148, 0.0026, 0.0101, 0.0021, 0.0069, 0.0127,\n                        0.0075, 0.0018, 0.0152, 0.0108, 0.0137, 0.0090, 0.0023, 0.0020, 0.0135,\n                        0.0069, 0.0016, 0.0190, 0.0111, 0.0040, 0.0024, 0.0261, 0.0160, 0.0065,\n                        0.0200, 0.0023, 0.0121, 0.0052, 0.0150, 0.0025, 0.0200, 0.0160, 0.0206,\n                        0.0175, 0.0045, 0.0207, 0.0188, 0.0163, 0.0018, 0.0664, 0.0025, 0.0074,\n                        0.0117, 0.0110, 0.0095, 0.0096, 0.0023, 0.0102, 0.0109, 0.0123, 0.0125,\n                        0.0106, 0.0136, 0.0110, 0.0080, 0.0018, 0.0167, 0.0022, 0.0019, 0.0020,\n                        0.0105, 0.0128, 0.0080, 0.0019, 0.0157, 0.0028, 0.0101, 0.0022, 0.0027,\n                        0.0020, 0.0203, 0.0124, 0.0161, 0.0187, 0.0112, 0.0116, 0.0091, 0.0210,\n                        0.0134, 0.0121, 0.0057, 0.0022, 0.0097, 0.0018, 0.0090, 0.0126, 0.0032,\n                        0.0114, 0.0138, 0.0025, 0.0123, 0.0020, 0.0171, 0.0166, 0.0097, 0.0018,\n                        0.0020, 0.0083, 0.0162, 0.0121, 0.0023, 0.0083, 0.0173, 0.0058, 0.0271,\n                        0.0084, 0.0373, 0.0125, 0.0018, 0.0095, 0.0106, 0.0144, 0.0131, 0.0022,\n                        0.0096, 0.0125, 0.0039, 0.0025, 0.0200, 0.0134]), zero_point=tensor([-128,    0,    0,  127, -128,  127,    0,    0,  127,  127,  127,    0,\n                        -128,  127,    0,    0,  127,    0,    0,  127,    0,    0,  127,    0,\n                           0,    0,    0,  127,  127,    0,    0,    0,    0,    0,  127,    0,\n                           0,    0,  127,    0,  127,    0,    0,    0,    0,  127,    0,    0,\n                        -128,    0,  127,  127, -128,    0,    0,    0,    0,    0,    0,    0,\n                        -128,  127,    0,    0,  127,    0,    0,  127,  127,    0,    0,    0,\n                           0,    0,    0,    0,  127,    0,    0,    0,    0,  127,  127,  127,\n                         127, -128,    0,    0,    0,    0,  127,  127,    0,    0,    0,    0,\n                           0,    0,    0,    0,    0,    0,    0,    0,  127, -128,    0,    0,\n                           0,    0,  127,  127,    0,    0,    0,    0,    0,    0,    0,  127,\n                         127,    0,    0,  127,    0,    0,    0,  127,    0,    0,    0,    0,\n                           0,    0,    0,    0,    0, -128,  127,    0,    0,    0,    0,    0,\n                         127,    0,    0,  127,  127,    0,    0,    0,    0,    0,  127,  127,\n                           0,    0,    0,    0,    0, -128,    0,  127,  127,  127,  127,  127,\n                           0, -128,    0,    0,    0,    0,    0, -128,    0,    0,  127,  127,\n                           0,    0,  127,    0,    0,    0, -128,    0,    0,  127,    0,    0,\n                           0,  127,  127,    0,  127,    0,    0,    0,  127,    0,    0,    0,\n                           0,    0,    0,    0,    0,    0, -128,    0,    0,    0,    0, -128,\n                           0,    0,    0,    0,  127,    0,    0,  127,    0,  127,    0,    0,\n                           0,    0,    0,  127,    0,    0,    0,    0,    0,    0,    0, -128,\n                           0,    0,  127,    0,    0,    0,    0,    0,    0,    0, -128,    0,\n                         127,    0,    0,    0,  127,    0,    0,    0,    0,    0,  127,    0,\n                           0,    0,    0,  127,    0,  127,  127,  127,    0,    0,    0,    0,\n                           0, -128,    0,    0,    0,    0,    0,    0,    0,    0, -128,    0,\n                         127,  127,  127,    0,    0,  127,    0,    0,    0,  127,  127,    0,\n                         127, -128,  127,    0,    0,    0,  127,    0,  127,    0,    0,    0,\n                           0, -128,    0,  127,    0,    0,  127,    0,    0,    0,    0,    0,\n                         127,    0,    0, -128,    0,    0,    0, -128,    0,    0,  127,    0,\n                           0, -128,  127,  127,    0,    0,    0,    0,    0,  127,    0,    0,\n                           0,    0,  127,  127,    0,    0,    0,    0,    0,    0,    0,    0,\n                         127,    0,    0,    0,    0,    0,  127,  127,    0,    0,    0,    0,\n                           0,    0, -128,    0,    0,    0,  127,    0,    0,    0,    0,  127,\n                           0,    0,    0,    0,  127,    0, -128,    0,    0,    0,    0,    0,\n                           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,  127,    0,\n                         127,    0,    0,  127,    0, -128,    0,    0,    0,    0,    0,    0,\n                           0,  127,  127,    0,  127,    0,  127,    0,    0,    0,    0,  127,\n                           0,    0,  127,  127,  127,  127,  127,    0,    0,    0,    0,    0,\n                           0,  127,    0,  127,  127,    0,    0,  127,    0,    0,    0,    0,\n                         127,    0,  127,    0,  127,    0,    0,    0,    0,    0,    0, -128,\n                           0,    0,    0,    0,  127, -128,    0,    0,  127,    0,    0,    0,\n                           0,    0,  127,    0,    0,    0,    0,    0,  127, -128,    0,  127,\n                           0,    0,    0, -128, -128, -128,    0,  127,  127,    0,    0,    0,\n                           0,    0,  127,  127,    0,    0,    0,    0,    0,  127,    0,    0,\n                           0,  127,    0,    0,    0,  127,  127,    0,    0,    0,    0,    0,\n                           0, -128,    0,    0,  127,    0, -128,    0,    0,    0, -128,    0,\n                         127,    0,  127,    0,  127,    0,    0,    0,    0,    0,  127,    0,\n                           0, -128,    0,    0,    0,    0, -128,  127,    0,  127,    0, -128,\n                           0,    0,    0,  127,    0,    0,    0,    0,  127,  127,    0,    0,\n                           0,  127,    0,    0, -128,    0,    0,    0,  127,    0,    0,    0,\n                         127,    0,    0,  127,  127,    0,    0,  127,  127,    0,    0,    0,\n                           0,    0,  127,    0,    0,    0,  127,    0,    0,  127,    0,  127,\n                        -128,    0,    0,  127,    0,    0,  127,    0,    0, -128,  127,  127,\n                           0,  127,  127,  127,    0,  127,    0,    0,  127,    0,    0,    0,\n                           0,    0, -128,    0,    0,    0,  127,  127,    0,  127,    0,    0,\n                           0,  127,    0,    0,  127,    0,    0, -128,    0,    0,    0,    0,\n                           0,    0,    0,    0,    0,    0,  127,    0, -128, -128,    0,    0,\n                           0,    0,    0,    0,  127,    0,    0,  127,    0,    0,    0,    0,\n                           0,    0,  127,  127,    0,    0, -128,    0,    0,    0,  127,    0,\n                           0, -128,    0,    0,    0,    0,  127,    0,    0, -128,  127,    0,\n                           0,    0, -128,  127,    0,    0,  127,    0,    0,    0,    0,  127,\n                         127,    0,    0,    0,    0,  127,    0,    0, -128,  127,    0,    0,\n                           0,  127, -128,    0,  127,  127,    0,    0,    0,    0,  127,    0,\n                           0,    0,    0,  127,    0,    0,    0,    0, -128, -128,    0,    0,\n                           0,    0,  127,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n                         127,    0,    0,    0,    0,  127,    0,  127,    0,    0,    0,    0,\n                           0,  127,    0,  127,    0,    0,    0,    0,    0,    0,  127,    0,\n                        -128,  127,  127,    0,  127,  127,    0,    0,    0,    0,    0,    0,\n                           0,    0,    0,    0,    0,    0,  127,    0,    0,    0,    0,  127,\n                           0,  127,    0,  127, -128,    0,  127,    0,  127,  127,    0,    0,\n                           0,  127,    0,  127,  127,  127,    0,    0,    0,  127,  127,  127,\n                           0,  127,    0,  127,    0,    0,    0,  127,    0,    0,    0,    0,\n                         127,  127,    0,    0,  127,    0,    0,    0,  127,    0,    0, -128,\n                           0,  127,    0, -128,    0,  127,    0,    0,    0,    0,    0,    0,\n                           0,    0,  127,    0,  127, -128,    0,    0,    0,    0,  127,    0,\n                           0,    0,    0,    0,    0,    0,    0,  127,    0,  127,  127,  127,\n                           0,    0,    0,  127,    0,  127,    0,  127,  127,  127, -128,    0,\n                           0,    0,    0,    0,    0,    0,    0,    0,    0,  127,    0,  127,\n                           0,    0,  127,    0,    0,  127,    0,  127,    0,    0,    0,  127,\n                         127,    0,    0,    0,  127,    0,    0,    0,    0,    0,    0,    0,\n                         127,    0,    0,    0,    0,  127,    0,    0, -128,  127,    0,    0],\n                       dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n                (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n                  min_val=tensor([ 1.1003e-01, -1.0031e+00, -2.0260e+00, -4.5455e-01,  1.1718e-01,\n                          -5.3112e-01, -9.5797e-01, -1.9536e+00, -5.1752e-01, -5.6238e-01,\n                          -7.3710e-01, -1.7043e-01,  1.0171e-01, -5.5119e-01, -1.2321e+00,\n                          -1.0363e+00, -4.5689e-01, -6.1144e-01, -1.1849e+00, -5.2762e-01,\n                          -1.3716e+00, -1.0542e+00, -4.2961e-01, -1.0282e+00, -1.4974e+00,\n                          -4.5628e-01, -1.3037e+00, -7.8379e-01, -6.5612e-01, -8.8668e-01,\n                          -8.5742e-01, -1.4202e+00, -1.6040e+00, -5.6093e-01, -4.9155e-01,\n                          -1.1210e+00, -1.3074e+00, -6.0239e-01, -1.6881e+00, -1.2385e+00,\n                          -4.9181e-01, -3.1042e-01, -1.6680e-01, -1.6787e+00, -8.2388e-01,\n                          -5.6405e-01, -1.8481e+00, -1.7716e-03,  8.0602e-02, -6.1594e-01,\n                          -4.2103e-01, -4.7934e-01,  1.7190e-01, -1.1602e+00, -9.7257e-01,\n                          -1.4115e+00, -3.0066e+00, -1.0476e+00, -1.5542e+00, -2.3676e+00,\n                           9.7835e-02, -4.4349e-01, -1.1070e+00, -1.0444e+00, -5.1741e-01,\n                          -1.1149e+00, -1.4016e+00, -5.6184e-01, -5.6755e-01, -1.4134e+00,\n                          -1.4609e+00, -1.0360e+00, -1.4980e+00, -4.3929e-01, -1.2129e+00,\n                          -3.6889e-01, -6.1260e-01, -1.3287e+00, -1.3456e+00, -3.6705e-02,\n                          -1.3552e+00, -4.6877e-01, -6.8452e-01, -5.5728e-01, -5.2345e-01,\n                           1.6761e-01, -1.4157e+00, -9.3908e-01, -7.2582e-01, -1.5434e+00,\n                          -4.1802e-01, -4.0730e-01, -6.0422e-01, -1.0834e+00, -1.0088e+00,\n                          -5.6113e-01, -9.8396e-01, -4.5125e-01, -2.3268e-01, -1.4195e+00,\n                          -1.0716e+00, -1.2498e+00, -1.1462e+00, -2.3099e+00, -7.9328e-01,\n                           4.1002e-02, -1.9443e+00, -9.5354e-01, -1.2597e+00, -9.9779e-01,\n                          -4.2144e-01, -6.4570e-01, -1.1244e+00, -9.5046e-01, -7.6442e-01,\n                          -4.9681e-01, -9.4890e-01, -1.5756e+00, -8.5599e-01, -7.3862e-01,\n                          -3.4746e-01, -1.3681e+00, -5.8646e-01, -3.9219e-01, -1.0235e+00,\n                          -1.3005e+00, -1.2073e+00, -6.5957e-01, -5.6996e-02, -2.4967e-03,\n                          -6.8325e-01, -1.4097e+00, -1.3733e+00, -1.9569e-01, -1.2686e+00,\n                          -1.3934e+00, -4.7230e-01,  3.1368e-02, -4.7411e-01, -7.6745e-01,\n                          -9.2209e-01, -1.1892e+00, -1.2547e+00, -8.0301e-02, -1.5369e+00,\n                          -1.0239e+00, -1.1531e+00, -4.1565e-01, -5.9754e-01, -1.2471e+00,\n                          -1.2989e+00, -1.8681e+00, -1.2949e+00, -1.1988e+00, -5.0199e-01,\n                          -5.3020e-01, -1.4694e+00, -1.0136e+00, -2.0207e+00, -1.5559e+00,\n                          -1.4813e+00,  9.3370e-02, -1.5448e+00, -4.8288e-01, -4.6463e-01,\n                          -4.6189e-01, -5.0610e-01, -5.5470e-01, -1.4218e+00,  8.6586e-02,\n                          -1.3834e+00, -7.3351e-01, -9.6775e-01, -1.2583e+00, -8.1199e-01,\n                           5.8818e-01, -4.0544e-02, -7.0038e-01, -6.0853e-01, -5.1500e-01,\n                          -8.6696e-01, -6.4360e-01, -5.1269e-01, -7.9987e-01, -1.1991e+00,\n                          -1.0648e+00,  4.1289e-02, -8.1282e-01, -1.3214e+00, -5.8613e-01,\n                          -1.7549e+00, -1.6314e+00, -9.0327e-01, -4.0443e-01, -6.5033e-01,\n                          -7.0321e-01, -5.7187e-01, -6.8130e-02, -1.0358e+00, -1.1771e+00,\n                          -7.7164e-01, -8.2130e-01, -5.6780e-01, -1.4254e+00, -1.2006e+00,\n                          -1.8836e+00, -6.8718e-02, -4.1821e+00, -2.2967e+00, -5.8015e-01,\n                           1.0729e-01, -1.8762e+00, -1.0104e+00, -9.9580e-01, -1.2305e+00,\n                           6.6413e-02, -6.5804e-01, -2.1736e+00, -7.9309e-01, -1.5998e+00,\n                          -7.3007e-01, -9.0988e-01, -4.3510e-01, -4.6669e-01, -1.1480e+00,\n                          -5.4545e-01, -9.3418e-01, -1.6922e+00, -7.9259e-01, -1.2601e+00,\n                          -8.4831e-01, -5.2008e-01, -1.2490e+00, -1.1204e+00, -1.3936e+00,\n                          -1.3316e+00, -1.7060e-01, -6.5371e-01, -1.5241e+00,  1.2273e-01,\n                          -1.1895e+00, -5.4755e-01, -6.0323e-01, -1.1449e+00, -5.6828e-01,\n                          -1.6514e+00, -1.9130e+00, -2.9997e+00, -9.5407e-01, -1.1700e+00,\n                           7.8026e-02, -1.2429e+00, -5.3716e-01, -1.1595e+00, -3.0776e+00,\n                          -1.0697e+00, -4.2699e-01, -9.9456e-02, -1.1681e+00, -1.9938e+00,\n                          -5.6764e-02, -1.2064e+00, -5.2955e-01, -9.4688e-01, -8.9695e-01,\n                          -7.6376e-01, -1.5890e-01, -4.1514e-01, -8.8847e-01, -8.3295e-01,\n                          -4.8423e-01, -4.9197e-01, -1.2234e+00, -1.4951e+00, -1.1440e+00,\n                          -1.2314e+00, -1.2272e+00,  1.8266e-02, -2.1369e+00, -1.0393e+00,\n                          -5.1609e-01, -5.0228e-01, -5.5266e-01, -1.7266e+00, -9.2380e-01,\n                          -9.1954e-03,  5.6245e-03, -1.2941e+00, -5.4806e-01, -5.9513e-01,\n                          -5.6799e-01, -1.2317e+00, -2.5159e+00, -6.4554e-01, -7.0141e-01,\n                          -3.4246e-01, -1.4361e+00, -5.7632e-01, -6.9181e-01, -2.1471e-01,\n                          -4.8256e-01,  1.4306e-01, -8.1559e-01, -6.3319e-01, -1.2334e+00,\n                          -9.4011e-01, -5.4038e-01, -1.4310e+00, -4.1164e-01, -2.0017e+00,\n                          -1.6403e+00, -7.2691e-01, -3.7799e+00,  6.2405e-02, -9.7652e-01,\n                          -6.6835e-01, -1.1200e+00, -1.2560e+00, -4.9508e-01, -7.7200e-01,\n                          -8.5799e-01, -7.7385e-01, -2.7496e+00, -1.1212e+00, -4.5922e-01,\n                          -1.2792e+00, -9.9276e-01,  7.0854e-02, -1.2512e+00, -1.3505e+00,\n                          -1.6038e+00,  2.7534e-01, -1.0368e+00, -1.8888e-03, -6.5082e-01,\n                          -5.4625e-01, -4.2526e-01,  3.3549e-02, -5.2480e-01, -5.4085e-01,\n                          -1.1273e+00, -2.4146e+00, -1.3870e+00, -1.2423e+00, -1.3382e+00,\n                          -5.6010e-01, -1.4359e+00, -1.5800e+00, -1.6135e+00, -1.0916e+00,\n                          -5.4160e-01, -5.7642e-01, -6.9949e-01, -6.4342e-01, -3.0081e-01,\n                          -1.1652e+00, -8.7877e-01, -9.4281e-01, -1.3795e+00, -1.2691e+00,\n                          -4.6653e-01, -1.0524e+00, -9.7960e-01, -7.5429e-01, -5.4562e-01,\n                          -9.9995e-01, -9.6781e-01, -6.4539e-01, -1.7113e+00, -5.6302e-02,\n                          -8.8642e-01, -9.1447e-01, -1.2941e+00, -7.2611e+00,  4.5687e-02,\n                          -3.4540e+00, -1.7983e+00, -2.2813e+00, -5.9468e-01, -1.3942e-01,\n                          -1.0633e+00, -1.3799e+00, -1.1195e+00, -5.2015e-01, -7.3596e-02,\n                          -1.2357e+00, -9.0121e-01, -6.3544e-01, -3.7020e-01, -4.3021e-01,\n                           1.5806e-02, -1.4151e+00, -1.8348e+00, -3.0515e+00, -2.4536e-01,\n                          -1.5031e+00, -1.1699e+00, -1.3503e+00, -1.1938e+00, -7.6613e-01,\n                          -8.7003e-01, -1.3129e+00, -1.2802e+00, -1.4038e+00, -8.3894e-01,\n                          -3.3269e-01, -4.4887e-01, -1.3978e+00, -5.7928e-01, -2.0107e-02,\n                          -5.1521e-01, -4.0979e-01, -8.3585e-01,  1.4754e-01, -4.2748e-01,\n                          -1.3099e+00, -1.0809e+00, -1.8838e+00, -9.5995e-01, -1.7518e+00,\n                          -8.1923e-01, -5.5108e-01, -5.1346e-01, -1.2591e+00, -4.9128e-01,\n                          -1.0955e+00, -5.4755e-01, -9.9114e-01, -1.1995e+00, -1.8928e-01,\n                          -1.4774e+00, -7.0598e-01, -1.0952e+00, -1.2350e+00, -5.6099e-01,\n                          -6.4960e-01, -5.8407e-01, -6.0120e-01, -4.9168e-01, -1.6935e+00,\n                          -1.2378e+00, -1.4603e+00, -1.2835e+00, -5.4590e-01, -7.0235e+00,\n                          -5.5864e-01, -7.2513e-01, -2.0581e+00, -6.1056e-01, -8.0355e-01,\n                          -1.3786e+00, -6.9389e-01, -1.8067e+00, -1.1662e+00, -1.4632e+00,\n                          -1.3477e+00, -5.2343e-01, -1.9410e+00, -5.1877e-01, -4.5626e-01,\n                          -5.1954e-01, -1.2028e+00, -9.9726e-01, -1.0378e+00, -5.0719e-01,\n                          -1.3594e+00, -1.2097e+00,  2.7615e-02, -4.4991e-01, -1.2697e+00,\n                          -1.6926e+00, -8.2960e-01, -1.4322e+00,  2.2251e-01, -5.5131e-01,\n                          -1.2833e+00, -5.0550e-01, -5.8157e-01, -1.0322e+00, -1.5968e+00,\n                          -5.0082e-01, -1.0346e+00, -4.8373e-01, -6.2192e-01, -1.2762e+00,\n                          -1.0454e+00, -8.1084e-01, -2.9124e-01, -5.9141e-01,  1.7240e-01,\n                          -9.3446e-01, -4.4477e-01, -1.2139e+00, -9.3564e-01, -1.1009e+00,\n                           7.7528e-02,  5.6630e-02,  2.7518e-01, -1.0994e+00, -4.3954e-01,\n                          -6.0088e-01, -1.8227e+00, -1.2101e+00, -1.3154e+00, -6.3883e-01,\n                          -1.2154e+00, -7.5466e-01, -4.4370e-01, -1.1775e+00, -8.7540e-01,\n                          -1.0498e+00, -1.2337e+00, -1.0241e+00, -2.5700e+00, -1.2080e+00,\n                          -1.3972e+00, -1.0792e+00, -7.6532e-01, -1.3639e+00, -2.0226e+00,\n                          -1.2613e+00, -5.6441e-01, -6.3948e-01, -1.4078e+00, -1.3724e+00,\n                          -1.5776e+00, -1.2980e+00, -7.9734e-02, -5.6239e-01,  1.5931e-01,\n                          -1.0805e+00, -1.5884e+00, -5.1992e-01, -9.7773e-01,  1.4260e-01,\n                          -5.3093e-01, -1.8145e+00, -4.3546e-01,  1.0178e-01, -8.9300e-01,\n                          -5.0461e-01, -1.4162e+00, -4.2096e-01, -4.5957e-01, -4.4926e-01,\n                          -1.5332e-02, -1.2996e+00, -1.9241e+00, -7.4790e-01, -1.2123e+00,\n                          -4.8644e-01, -1.5826e+00, -1.4429e+00,  1.3762e-01, -1.9683e+00,\n                          -1.3122e+00, -1.5660e+00, -1.2370e+00,  8.6862e-02, -7.4643e-01,\n                          -1.5524e+00, -5.0703e-01, -1.0674e+00,  1.2218e-01, -1.0908e+00,\n                          -1.2395e+00, -1.1657e+00, -6.3338e-01, -1.7381e+00, -1.3913e+00,\n                          -1.5492e+00, -6.4421e-01, -5.0103e-01, -3.6635e-01, -1.1497e+00,\n                          -4.2047e-01, -9.9197e-02, -5.1463e-01, -2.3363e+00, -1.4159e+00,\n                           1.4696e-01, -1.5705e+00, -5.1747e-01, -1.0081e+00, -4.0087e-01,\n                          -4.2640e-01, -1.9045e+00, -1.0850e+00, -5.4191e-01, -1.4066e+00,\n                          -5.1249e-01, -5.5605e-01, -5.9508e-01, -1.2902e+00, -9.7784e-01,\n                          -4.4103e-01, -4.6180e-01, -2.7262e-01, -1.3250e+00, -9.8499e-01,\n                          -1.8186e+00, -1.7040e+00, -5.2071e-01, -1.0854e+00, -1.5751e+00,\n                          -4.4792e-01, -5.2567e-01, -1.1306e+00, -1.3994e+00, -5.5526e-01,\n                          -9.6404e-01, -9.6963e-01,  6.4930e-02, -6.9500e-01, -9.5930e-01,\n                          -6.1056e-01, -2.2401e-01, -2.7727e+00, -4.4253e-01, -1.1070e+00,\n                          -1.1770e+00,  1.7626e-01, -4.1854e-01, -5.9431e-01, -1.1765e+00,\n                          -4.5814e-01, -5.6208e-01, -5.6470e-01, -8.4292e-01, -4.8418e-01,\n                          -1.0971e+00, -2.4099e+00, -5.5650e-01, -9.3378e-01, -8.4869e-01,\n                          -1.2793e+00, -7.5026e-01, -1.1699e+00,  1.1621e-01, -1.3913e+00,\n                          -1.1936e+00, -1.0369e+00, -3.9112e-01, -4.4041e-01, -9.2191e-01,\n                          -4.2939e-01, -8.5656e-01, -1.3560e+00, -1.2225e+00, -8.2128e-01,\n                          -2.5810e-02, -1.5634e+00, -4.5259e-01, -2.1584e+00, -1.1056e+00,\n                           5.0697e-02, -1.3293e+00, -1.2483e+00, -1.1332e+00, -1.4692e+00,\n                          -3.4908e+00, -1.1741e+00, -1.7546e+00, -7.9989e-01, -1.0743e+00,\n                          -7.6427e-02, -4.6769e-01, -9.3668e-01,  1.7951e-01,  6.6589e-02,\n                          -7.6519e-01, -2.1919e+00, -9.0995e-01, -9.8943e-01, -1.2187e+00,\n                          -2.2428e+00, -5.3744e-01, -1.0496e+00, -4.9259e-01, -7.3350e-01,\n                          -8.3837e-01, -7.1484e-01, -9.8085e-01, -9.0368e-01, -1.0862e+00,\n                          -1.4981e+00, -7.8506e-01, -5.4956e-01, -1.2469e+00, -1.3918e+00,\n                           4.0518e-02, -2.0649e+00, -7.7769e-02, -8.6861e-01, -6.0557e-01,\n                          -4.4067e-02, -1.3132e+00,  2.3586e-02, -1.2259e+00, -7.2520e-01,\n                          -9.7026e-01, -1.0019e+00, -4.4899e-01, -1.3265e+00, -4.4345e-01,\n                           2.6186e-02, -5.1974e-01, -1.2028e+00, -9.4750e-01, -5.6782e-01,\n                           5.3685e-02, -5.5379e-01, -1.0351e+00, -1.7855e+00, -5.8071e-01,\n                          -1.0534e+00, -1.5234e+00, -1.1769e+00, -1.1267e+00, -6.6238e-01,\n                          -6.8223e-01, -1.2273e+00, -8.5336e-02, -6.9467e-01, -5.3749e-01,\n                          -4.5232e-01, -1.0936e+00, -1.0141e+00,  9.8659e-02, -6.0335e-01,\n                          -8.9249e-01, -1.1385e+00, -1.1132e+00, -5.4583e-01,  1.2858e-01,\n                          -5.2739e-01, -4.8149e-01, -7.2280e-01, -9.9543e-01, -5.3328e-01,\n                          -7.4769e-01, -1.2540e+00, -4.8808e-01, -9.9865e-01, -1.4089e+00,\n                          -1.5654e+00, -1.3364e+00, -5.6384e-01, -1.4318e+00, -9.5065e-01,\n                          -1.2060e+00, -1.1383e+00,  1.6692e-01,  5.5557e-02, -2.9849e-01,\n                          -1.0855e-01, -4.0712e-01, -2.4477e+00, -5.7878e-01, -9.7804e-01,\n                          -1.3373e+00, -9.0219e-01, -7.9104e-01, -1.1485e+00, -1.4953e+00,\n                          -1.5566e-01, -3.3481e-01, -1.2306e+00, -6.1487e-01, -8.9063e-01,\n                          -1.3245e+00, -3.0298e+00, -9.5065e-01, -3.9497e-01, -5.3043e-01,\n                          -6.3856e-01, -8.9720e-01, -1.0129e+00, -1.5791e+00, -1.1157e+00,\n                          -1.1024e+00, -5.8456e-01, -1.8425e+00, -4.7014e-01, -9.2066e-02,\n                          -1.2605e+00, -5.0038e-01, -1.2933e+00, -5.2862e-01, -1.1355e+00,\n                          -5.6126e-01, -1.2084e+00,  2.2753e-01, -4.9188e-01, -5.4483e-01,\n                          -3.2343e+00, -6.1852e-01, -7.9170e-01, -1.4520e+00, -2.2727e+00,\n                          -1.4439e+00, -1.7601e+00, -1.3113e+00, -4.8998e-02, -1.2504e+00,\n                          -6.2663e-01, -1.6735e+00, -1.2568e+00, -5.9276e-01, -1.4156e+00,\n                          -9.0551e-01, -8.0705e-01, -1.4222e+00, -1.3742e+00, -1.2218e+00,\n                          -7.8620e-01, -1.2033e+00, -5.2469e-01, -1.6014e+00, -5.9569e-01,\n                           1.2432e-02, -1.6799e+00, -4.0194e-01, -5.5892e-02, -5.5199e-01,\n                          -6.3262e-01, -1.1939e+00, -1.0032e+00, -1.4461e+00, -5.5041e-01,\n                          -1.8036e+00, -4.2858e-01, -1.2919e+00, -5.5362e-01, -9.7720e-03,\n                          -3.4536e-01, -9.4453e-01, -6.2065e-01, -4.3766e-01, -5.5891e-01,\n                          -8.9541e-01, -6.7289e-01, -8.8432e-01, -5.3322e-01, -8.8108e-01,\n                          -1.6289e+00, -9.5647e-01, -4.6377e-01, -1.0793e+00, -1.1861e+00,\n                          -2.6124e-01, -8.4597e-01, -5.9416e-01, -5.1970e-01, -1.4264e+00,\n                          -8.5345e-01, -3.9925e-01, -1.0968e+00, -1.1427e+00, -5.0639e-01,\n                          -6.1261e-01, -3.3361e+00, -2.0434e+00,  5.7353e-02, -1.6318e+00,\n                          -5.8504e-01, -1.5427e+00,  1.4958e-01, -1.9255e+00, -6.2969e-01,\n                          -9.4765e-01, -1.2339e+00, -2.6368e+00, -2.2405e+00, -5.7308e-01,\n                          -7.9963e-02, -2.0051e-01, -1.1801e+00, -4.5810e-01, -8.5008e+00,\n                          -6.3987e-01,  2.3841e-01, -1.4696e+00, -1.1518e+00, -1.2061e+00,\n                          -1.2322e+00, -5.8531e-01, -1.3106e+00, -6.3209e-01, -1.2635e+00,\n                          -1.5975e+00, -4.9377e-01, -9.4020e-01, -1.2665e+00, -5.7143e-01,\n                          -4.6754e-01, -7.0710e-02, -5.5703e-01, -4.8385e-01, -5.0183e-01,\n                          -1.3439e+00, -1.4228e+00, -1.0241e+00, -4.8933e-01, -2.6326e-01,\n                          -7.0762e-01, -9.5381e-01, -5.4980e-01, -6.9059e-01, -5.1212e-01,\n                           6.7283e-01, -9.1356e-01, -2.0661e+00, -8.4448e-02, -1.4318e+00,\n                          -2.5905e-01, -5.1235e-01, -9.6520e-01, -5.9708e-01, -1.3711e+00,\n                          -7.2672e-01, -5.6750e-01, -8.9269e-01, -4.5728e-01, -1.1581e+00,\n                          -1.3834e+00, -8.2371e-01, -1.3890e-01, -1.7650e+00, -6.2598e-01,\n                          -8.8593e-01, -5.1381e-01, -1.8712e+00, -5.5645e-01, -1.1417e+00,\n                          -4.6256e-01, -5.0961e-01, -8.6283e-01, -1.5080e+00, -1.2938e+00,\n                          -5.8420e-01, -4.6127e-01, -1.8621e+00, -7.4275e-01, -4.8471e-01,\n                          -2.7462e-01, -1.9459e+00, -9.9045e-01, -4.6518e-01, -1.2171e+00,\n                          -1.3541e+00, -9.8107e-02, -1.6775e+00, -5.5981e-01, -1.0620e+00,\n                          -1.3921e+00,  5.0813e-02, -6.4889e-01, -2.1466e+00, -1.2962e+00]), max_val=tensor([ 2.2362e+00,  2.8117e+00,  1.4973e+00, -3.2624e-01,  1.7055e+00,\n                          -1.5038e-01,  1.6346e+00,  1.0590e+00, -2.3833e-01, -2.8775e-01,\n                          -2.6771e-01,  2.8993e+00,  7.8655e-01, -2.6991e-01,  2.1487e+00,\n                           9.7127e-01, -2.1256e-01,  3.4320e-01,  1.2433e+00, -7.7817e-02,\n                           1.5717e+00,  1.5590e+00, -9.7992e-02,  1.2616e+00,  7.9857e-01,\n                           2.0213e+00,  1.6110e+00, -1.7571e-01, -1.5828e-01,  1.1608e+00,\n                           1.2069e+00,  1.0467e+00,  1.4788e+00,  2.1388e-01, -2.7197e-01,\n                           1.5284e+00,  7.5593e-01,  6.1811e-01, -1.3086e-01,  1.0937e+00,\n                          -2.2526e-01,  1.6023e+00,  9.6128e-01,  1.4840e+00,  2.1657e+00,\n                          -1.4106e-01,  1.6216e+00,  1.8274e+00,  2.1975e+00,  4.2925e-01,\n                          -2.0837e-01, -6.7641e-02,  2.0518e+00,  1.1162e+00,  1.3414e+00,\n                           1.4391e+00,  3.2169e-02,  1.5009e+00,  1.3713e+00,  1.2605e+00,\n                           9.1196e-01, -2.4949e-01,  1.0415e+00,  8.6372e-01, -2.8738e-01,\n                           6.7758e-01,  1.4254e+00, -3.0478e-01, -5.4844e-02,  1.4618e+00,\n                           8.6815e-01,  1.4158e+00,  8.6113e-01,  2.2180e+00,  1.2889e+00,\n                           2.2406e+00, -3.9760e-01,  2.3932e+00,  1.1527e+00,  1.6590e+00,\n                           1.3820e+00, -1.7189e-01, -2.4166e-01, -2.2734e-01, -7.1235e-02,\n                           1.1709e+00,  9.4178e-01,  1.2836e+00,  6.7681e-01,  2.1573e+00,\n                          -9.5713e-02, -2.0045e-01,  5.4392e+00,  1.0373e+00,  1.1230e+00,\n                           1.6041e+00,  1.0554e+00,  2.3988e+00,  1.9153e+00,  1.9745e+00,\n                           1.0131e+00,  1.5114e+00,  2.7589e+00,  1.3193e+00, -1.8812e-01,\n                           1.6526e+00,  1.8921e+00,  1.4217e+00,  1.8603e+00,  2.3901e+00,\n                          -4.8439e-02, -3.2212e-01,  2.6673e+00,  1.4428e+00,  3.4762e+00,\n                           2.8309e-01,  1.7454e+00,  1.3211e+00,  3.2206e+00, -3.2561e-01,\n                          -1.9157e-01,  9.7264e-01,  1.6448e+00, -1.3363e-01,  1.5807e+00,\n                           1.7172e+00,  1.5982e+00, -2.5488e-01,  9.2087e-01,  2.1714e+00,\n                           1.0747e+00,  1.0971e+00,  2.3276e+00,  2.1658e+00,  1.5707e+00,\n                           1.4693e+00,  1.7917e+00,  1.4062e+00, -2.2444e-01,  1.0994e+00,\n                           1.3986e+00,  9.5002e-01,  1.4522e+00,  9.4281e-01, -2.2456e-01,\n                           6.2131e-01,  1.6492e+00, -2.2305e-01, -3.1609e-01,  1.9418e+00,\n                           1.6943e+00,  1.6889e+00,  2.0468e+00,  1.3226e+00, -1.8742e-01,\n                          -2.9440e-01,  1.4478e+00,  1.7179e+00,  2.0184e+00,  7.6225e-01,\n                           9.2722e-01,  1.0294e+00,  2.0152e+00, -3.1073e-01, -1.0393e-01,\n                          -2.2722e-01, -2.6654e-01, -2.4591e-01,  1.3628e+00,  1.9305e+00,\n                           1.4655e+00,  1.3945e+00,  1.2957e+00,  1.3256e+00,  9.5215e-01,\n                           3.3903e+00,  1.2917e+00,  3.2460e+00, -3.8320e-01, -4.6492e-02,\n                           1.4306e+00,  2.8871e-01, -2.4741e-01,  1.3031e+00,  8.6071e-01,\n                           1.6555e+00,  1.5042e+00,  1.6877e+00,  1.4868e+00, -2.1857e-01,\n                           1.9530e+00,  1.2347e+00,  1.6988e+00, -9.2784e-02, -3.0792e-01,\n                           1.2234e+00, -3.2893e-01,  1.0596e+00,  1.5667e+00,  1.5497e+00,\n                          -3.5725e-01,  2.8715e+00,  1.2576e-01,  7.1714e-01,  1.4773e+00,\n                           1.4922e+00,  1.6943e+00,  4.1309e+00,  1.3652e+00,  8.8560e-01,\n                           1.9997e+00,  2.4296e-02,  1.1301e+00,  3.7946e+00,  1.0101e+00,\n                           1.9121e+00,  1.0254e+00,  1.1628e+00,  1.1291e+00,  1.2552e+00,\n                          -4.4038e-01,  1.3586e+00,  1.2259e-02, -2.4564e-01,  1.2685e+00,\n                          -1.9391e-01,  2.3870e+00,  1.2666e+00,  1.0883e+00,  2.1562e+00,\n                           3.9514e-01, -2.7526e-01,  1.6149e+00,  1.2517e+00,  1.6260e+00,\n                           2.1645e+00,  2.9383e+00,  1.4810e+00,  1.0827e+00,  1.6413e+00,\n                           1.5596e+00,  2.3640e+00, -1.9912e-01,  1.3820e+00,  9.4275e-01,\n                           1.0232e+00,  5.2495e-01,  6.9482e-01,  1.4575e+00,  1.6017e+00,\n                           8.6200e-01,  6.6797e-01, -1.4062e-01,  1.9547e+00,  9.1062e-01,\n                           1.7122e+00, -2.0237e-01,  1.0777e+00,  2.2799e+00,  2.8554e+00,\n                           1.5432e+00,  1.3784e+00, -1.5372e-01,  6.7529e-01,  1.0570e+00,\n                           2.5596e+00,  2.3982e+00, -1.0015e-01,  1.7435e+00, -3.2613e-01,\n                          -2.6897e-01, -1.3925e-01,  1.8949e+00,  1.6428e+00,  1.3834e+00,\n                           1.3698e+00,  1.3851e+00,  1.7033e+00,  3.2515e+00,  8.8385e-01,\n                           2.2124e+00,  1.2343e-01,  1.4627e+00,  1.8247e+00,  1.0381e+00,\n                           1.0905e+00,  1.7166e+00,  1.1618e+00, -2.0973e-01, -2.6774e-01,\n                          -1.9678e-01,  1.5783e+00,  2.5436e+00, -8.2749e-02,  1.2778e+00,\n                           1.2611e+00,  1.5757e+00, -2.3188e-01, -2.7589e-01,  2.1457e+00,\n                          -2.3138e-01,  1.3959e+00, -1.4132e-01,  1.8317e+00,  1.1661e+00,\n                           1.4604e+00, -1.8792e-01,  2.0406e+00, -2.2611e-01,  5.0316e-01,\n                           1.8428e+00,  1.2316e+00,  1.4616e+00,  1.3627e+00,  1.4173e+00,\n                          -2.7140e-01,  1.8048e+00,  1.8200e+00, -3.0280e-01,  1.3038e+00,\n                           1.9002e+00,  9.6836e-01,  2.5827e+00,  1.0925e+00, -1.3510e-01,\n                           2.5091e+00,  1.6189e+00,  2.0926e+00,  9.3306e-01,  1.5219e+00,\n                           1.8962e+00,  1.5629e+00,  1.4211e+00,  2.2035e+00, -2.1653e-01,\n                           4.8757e-02,  1.1131e+00,  2.1141e+00, -2.3087e-01, -2.0416e-01,\n                           2.5728e+00,  1.3297e+00,  1.0431e+00,  1.5275e+00,  1.1283e+00,\n                          -2.2914e-01,  1.6327e+00,  7.3971e-01,  1.2029e+00,  1.4844e+00,\n                          -2.0523e-01, -2.2854e-01,  8.5820e-02,  2.3871e+00,  1.2808e+00,\n                           1.4960e+00,  1.6620e+00,  9.6554e-01,  3.7314e-02,  1.4884e+00,\n                          -9.6579e-02,  1.0155e+00,  7.8065e-01,  9.7192e-01,  1.0353e+00,\n                           2.6309e+00, -1.9717e-01, -2.6250e-01,  2.1259e+00,  2.1941e+00,\n                           1.8108e+00,  4.4362e-01,  1.6609e+00,  7.3138e+00,  2.3572e+00,\n                           1.6740e+00,  1.6650e+00,  1.2801e+00, -1.6792e-01,  1.2052e+00,\n                           1.4505e+00,  1.2543e+00,  1.9305e+00, -1.7885e-01,  1.2604e+00,\n                           1.5359e+00,  4.9557e+00,  2.3868e+00, -6.9523e-02,  5.3107e-02,\n                           1.8916e+00,  1.5372e+00,  2.1093e+00,  1.7176e+00,  9.1478e-01,\n                           1.1079e+00,  1.9426e+00,  1.4913e+00,  1.8971e+00,  1.4044e+00,\n                           2.0396e+00,  2.3902e+00,  2.3044e+00,  9.9903e-01,  8.9873e-01,\n                           1.8690e+00, -2.1878e-01,  1.5864e+00, -3.1874e-01,  1.4025e+00,\n                           4.8172e-02, -2.7780e-01,  1.3625e+00,  1.1158e+00,  5.2471e-02,\n                           2.1537e+00,  1.3997e+00,  5.8722e-01,  1.5291e+00,  2.3167e+00,\n                           1.2837e+00, -2.1534e-01, -1.8298e-01,  8.9124e-01, -6.6423e-02,\n                           1.4570e+00, -2.7266e-01,  1.8289e+00,  2.1342e+00,  1.0363e+00,\n                           1.4935e+00, -1.0261e-01,  9.2570e-01,  1.3714e+00, -2.2149e-01,\n                          -1.5593e-01, -3.0728e-01, -2.8748e-01, -1.7719e-01,  1.3164e+00,\n                           1.4181e+00,  1.5412e+00,  1.6370e+00,  1.7759e-02,  8.2093e+00,\n                          -3.3038e-01,  1.0054e+00, -1.1595e-01, -3.4446e-01,  3.1243e+00,\n                           1.0978e+00, -1.2013e-01,  1.5796e+00,  1.0342e+00,  1.4368e+00,\n                           1.4639e+00, -7.6646e-02,  1.6076e+00, -2.0698e-01,  5.0534e-02,\n                          -2.6091e-01,  1.5323e+00,  1.0196e+00,  1.0295e+00,  8.6716e-01,\n                           1.0708e+00,  1.3156e+00,  1.7073e+00,  7.9903e-02,  1.4392e+00,\n                           2.3213e+00,  9.6463e-01, -2.2319e-03,  1.6091e+00,  1.4313e+00,\n                           1.2634e+00, -2.4793e-01,  1.7933e+00,  1.6144e+00,  1.5918e+00,\n                           1.5407e+00,  1.2902e+00, -2.6572e-01,  4.3191e-01,  2.6624e+00,\n                           1.2531e+00,  2.0239e+00,  2.8413e+00, -2.3508e-01,  2.0150e+00,\n                           1.1119e+00, -6.4123e-02,  1.1691e+00,  1.1769e+00,  9.2206e-01,\n                           1.4317e+00,  1.3463e+00,  8.2647e-01,  1.4924e+00, -6.5710e-02,\n                          -1.3342e-01,  1.1108e+00,  1.4318e+00,  1.6913e+00,  7.9005e-01,\n                           1.6681e+00, -1.4114e-01, -1.3886e-01,  2.5771e+00,  1.7008e+00,\n                           6.1507e-01,  1.1728e+00,  1.7301e+00, -1.0033e-01,  9.7713e-01,\n                           1.4439e+00,  1.7245e+00, -2.1701e-01,  1.5368e+00,  1.2695e+00,\n                           1.8825e+00, -2.0849e-01, -4.3037e-02,  1.1551e+00,  1.0282e+00,\n                           1.5887e+00,  2.1391e+00,  1.4780e+00,  1.0658e+00,  2.1481e+00,\n                           1.5571e+00,  1.1807e+00, -7.3365e-02,  1.4351e+00,  1.2249e+00,\n                           2.1057e-01,  1.1573e+00,  2.2058e-03,  1.9857e+00,  1.2529e+00,\n                          -2.4976e-01,  1.0893e+00, -1.9761e-01,  1.4797e-01, -2.4814e-01,\n                           1.6027e+00,  4.7941e+00,  9.5964e-01,  2.3619e+00,  1.3160e+00,\n                          -2.0672e-01,  2.9563e+00,  1.4512e+00,  1.2667e+00,  4.2268e+00,\n                           1.5223e+00,  1.3165e+00,  2.2739e+00,  5.8394e-01, -8.9849e-02,\n                           1.5983e+00, -2.4078e-01,  1.1493e+00,  1.7069e+00,  1.7608e+00,\n                           1.2541e+00,  4.7341e+00, -2.9075e-01,  9.0013e-01,  1.1732e+00,\n                           1.8044e+00,  9.4529e-01, -2.3995e-02, -1.6993e-01,  1.8231e+00,\n                           2.6857e-01,  1.2516e+00, -2.8208e-01,  2.0121e+00,  1.6403e+00,\n                           2.8249e+00,  1.3685e+00,  2.7549e-02,  1.8333e+00, -3.5796e-02,\n                           9.7261e-02,  1.2418e+00,  8.3768e-01, -1.6537e-01,  2.5624e+00,\n                           1.1943e-01, -3.4058e-01, -2.6227e-01,  1.3287e+00,  1.6998e+00,\n                          -2.1160e-01, -1.9251e-01,  1.6440e+00,  1.3959e+00,  7.7833e-01,\n                           1.2045e+00,  8.4771e-01, -3.0723e-01,  1.0971e+00,  1.9573e+00,\n                           3.4452e+00, -1.5836e-01,  1.3951e+00,  1.9865e+00, -1.3111e-01,\n                           1.6502e+00, -1.1589e-01,  1.4047e+00,  2.6633e+00,  1.2360e+00,\n                          -2.9381e-01,  2.9822e+00,  3.5074e+00, -2.7748e-01,  7.9435e-01,\n                           1.3768e+00,  8.3439e-01, -1.9235e-01, -1.2182e-01,  1.4336e+00,\n                          -1.8947e-01, -1.7481e-01, -3.2939e-01,  1.1331e+00, -3.2670e-01,\n                           2.0979e+00,  3.3160e+00, -1.6457e-01,  1.0010e+00,  1.0952e+00,\n                           1.6348e+00,  1.2458e+00,  1.4764e+00,  2.5007e+00,  1.4054e+00,\n                           1.1831e+00,  6.6742e-01, -2.2815e-01, -2.1292e-01,  9.3523e-01,\n                          -8.1663e-02,  1.5885e+00,  1.9570e+00,  1.0708e+00, -1.3610e-01,\n                           1.5259e+00,  7.8777e-01, -1.5047e-01,  1.7043e+00,  1.1566e+00,\n                           1.3408e+00,  1.2049e+00,  1.4498e+00,  1.3260e+00,  1.3472e+00,\n                           1.4652e+00,  1.1995e+00,  1.7837e+00,  2.1617e+00,  1.3731e+00,\n                           1.7856e+00, -7.1240e-03,  1.3071e+00,  2.0530e+00,  2.1391e+00,\n                           1.3631e+00,  1.2003e+00,  7.7455e-01,  1.5841e+00,  1.4994e+00,\n                           4.3351e+00, -1.4034e-01,  1.5570e+00,  1.5394e+00, -9.8886e-02,\n                           8.2647e-01,  1.2934e+00,  1.6361e+00,  1.2057e+00,  1.7467e+00,\n                           1.7072e+00, -3.8136e-01, -3.0157e-01,  1.6076e+00,  1.6995e+00,\n                           1.4369e+00,  1.8879e+00,  1.4380e+00,  5.8798e-01, -3.1263e-01,\n                           2.5337e+00,  9.0205e-01,  1.6834e+00,  1.1183e+00,  3.9832e-02,\n                           2.0993e+00,  1.0733e+00, -1.7905e-01,  1.5529e+00,  5.5443e+00,\n                           1.0123e+00, -2.5307e-01,  1.2838e+00,  8.3026e-01,  1.3989e+00,\n                           4.7495e+00, -2.9137e-01,  1.2445e+00,  8.2883e-01, -2.4247e-01,\n                           1.3684e+00,  1.0724e+00,  1.5174e+00,  1.3677e+00, -1.5902e-01,\n                          -2.9948e-01,  1.4939e+00,  1.2174e+00,  8.9599e-01,  7.6311e-02,\n                          -2.7753e-01,  1.2694e+00,  1.4100e+00,  1.1951e+00, -2.6169e-01,\n                           1.0039e+00,  1.6828e+00,  1.1995e+00, -2.2929e-01,  1.0475e+00,\n                           2.0740e+00, -2.7391e-01, -1.2235e-01,  1.9177e+00,  1.9783e+00,\n                           1.8210e+00,  2.1328e+00, -1.7146e-01,  1.1528e+00,  1.1988e+00,\n                           1.1502e+00,  1.0464e+00, -2.5098e-01,  1.6717e+00,  1.3921e+00,\n                           1.2924e+00,  6.9653e-01,  3.3171e+00,  1.2054e+00,  1.1402e+00,\n                           1.5299e+00,  2.9919e+00,  6.1463e-01, -3.2454e-01,  1.4311e+00,\n                           1.7628e+00,  1.2076e+00,  1.4000e+00,  1.7342e+00,  1.6759e+00,\n                           1.0841e+00,  1.2308e+00,  1.1840e+00, -2.0430e-01,  2.0783e+00,\n                           1.2136e+00,  1.1057e+00,  1.3475e+00, -2.2262e-01,  1.1016e+00,\n                          -1.5904e-01,  1.0715e+00,  1.4706e+00,  1.4558e+00,  1.4510e+00,\n                           1.4090e+00, -1.7823e-01,  2.3118e+00, -1.5171e-01,  1.8065e+00,\n                           1.4774e+00,  3.4094e+00,  1.3648e+00,  1.4216e-01,  1.0933e+00,\n                          -2.5906e-01,  1.5488e+00,  1.9300e+00, -1.3063e-01, -1.7681e-01,\n                           1.7504e+00, -1.1923e-01, -1.2774e-01,  1.6929e+00,  2.4204e+00,\n                           1.8689e+00,  1.0705e+00,  1.6402e+00,  1.4252e+00,  2.3040e+00,\n                           9.0322e-01,  1.8237e+00,  2.2592e+00,  7.3445e-01,  3.3225e+00,\n                          -2.0453e-01,  1.0945e+00,  1.4476e+00,  1.1639e+00,  9.6715e-01,\n                          -2.2365e-02,  1.6894e+00, -6.4340e-02,  1.8460e+00, -3.7195e-01,\n                           1.9086e+00,  1.2559e+00, -1.3644e-01,  1.2175e+00, -3.0029e-01,\n                          -1.7184e-01,  1.5028e+00,  1.5098e+00,  1.3882e+00, -8.4090e-02,\n                           1.0091e+00, -2.6438e-01, -1.2595e-01, -7.3225e-02,  1.1114e+00,\n                           1.7514e+00,  1.9187e+00, -2.1133e-01, -2.0481e-01, -1.9618e-01,\n                           1.8824e+00, -2.4891e-01,  1.2771e+00, -3.3796e-01,  8.4792e-01,\n                           9.3360e-01,  8.9476e-01, -3.2516e-01,  1.9284e+00,  1.3708e+00,\n                           1.7410e+00,  1.1459e+00, -2.2192e-01, -1.7862e-01,  1.7141e+00,\n                           8.7193e-01, -1.4473e-01,  2.4133e+00,  1.4047e+00,  3.4215e-02,\n                          -3.6482e-01,  2.9696e-01,  1.4621e+00,  1.6652e+00,  2.5376e+00,\n                          -1.4695e-01,  1.1601e+00,  1.3374e+00,  1.1866e+00, -1.6862e-01,\n                           2.5460e+00,  2.0328e+00,  1.5020e+00,  8.9675e-01,  2.8057e-02,\n                           2.6289e+00,  2.3824e+00,  2.0679e+00, -5.3998e-02,  3.3092e+00,\n                          -2.3755e-01,  1.8815e+00,  1.4817e+00,  1.3969e+00,  1.2100e+00,\n                           1.1164e+00, -3.6031e-01,  1.2009e+00,  1.3898e+00,  1.5598e+00,\n                           1.1672e+00,  1.3419e+00,  1.7267e+00,  1.4009e+00,  1.0124e+00,\n                          -2.3990e-01,  2.1168e+00, -1.0686e-02, -1.8683e-01, -2.8557e-01,\n                           1.1381e+00,  1.6283e+00,  1.0116e+00, -9.7329e-02,  1.9877e+00,\n                          -3.2456e-01,  1.2780e+00, -1.0253e-01, -1.9714e-01, -3.3519e-01,\n                           5.1762e+00,  1.5777e+00,  5.1791e-01,  2.3752e+00,  1.1850e+00,\n                           1.4739e+00,  1.1583e+00,  2.6620e+00,  1.7048e+00,  1.5311e+00,\n                           7.0798e-01, -2.8795e-01,  1.2332e+00, -1.8504e-01,  1.0412e+00,\n                           1.6044e+00, -1.7881e-02,  1.4453e+00,  1.0403e+00, -3.1145e-01,\n                           1.5658e+00, -2.5610e-01,  2.1756e+00,  2.1046e+00,  1.2263e+00,\n                          -3.2259e-01, -2.7556e-01,  1.0580e+00,  2.0532e+00,  1.5413e+00,\n                          -1.6470e-01,  1.0547e+00,  2.1978e+00,  3.5151e-01,  3.4386e+00,\n                           1.0614e+00,  4.7386e+00,  1.5848e+00, -2.3722e-01,  9.0881e-01,\n                           6.8539e-01,  1.8302e+00,  1.4532e+00, -2.0692e-01,  1.2223e+00,\n                           1.5883e+00,  9.8903e-01, -1.6935e-01,  2.5441e+00,  1.6981e+00])\n                )\n              )\n            )\n            (2): ReLU6(inplace=True)\n          )\n          (2): ConvBn2d(\n            960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0022, 0.0017, 0.0017, 0.0018, 0.0013, 0.0016, 0.0013, 0.0023, 0.0016,\n                      0.0012, 0.0018, 0.0018, 0.0015, 0.0016, 0.0016, 0.0012, 0.0018, 0.0014,\n                      0.0016, 0.0017, 0.0017, 0.0024, 0.0015, 0.0020, 0.0017, 0.0018, 0.0017,\n                      0.0014, 0.0016, 0.0019, 0.0016, 0.0017, 0.0014, 0.0017, 0.0014, 0.0016,\n                      0.0015, 0.0015, 0.0016, 0.0014, 0.0016, 0.0015, 0.0020, 0.0014, 0.0014,\n                      0.0016, 0.0014, 0.0015, 0.0019, 0.0017, 0.0018, 0.0019, 0.0020, 0.0014,\n                      0.0015, 0.0020, 0.0018, 0.0016, 0.0016, 0.0019, 0.0016, 0.0014, 0.0013,\n                      0.0014, 0.0015, 0.0015, 0.0017, 0.0018, 0.0024, 0.0016, 0.0017, 0.0014,\n                      0.0014, 0.0014, 0.0019, 0.0015, 0.0017, 0.0024, 0.0019, 0.0019, 0.0017,\n                      0.0014, 0.0019, 0.0014, 0.0016, 0.0017, 0.0011, 0.0018, 0.0017, 0.0017,\n                      0.0020, 0.0017, 0.0013, 0.0019, 0.0020, 0.0014, 0.0014, 0.0015, 0.0019,\n                      0.0016, 0.0014, 0.0015, 0.0016, 0.0017, 0.0016, 0.0020, 0.0015, 0.0015,\n                      0.0015, 0.0019, 0.0012, 0.0023, 0.0020, 0.0015, 0.0016, 0.0015, 0.0014,\n                      0.0014, 0.0022, 0.0016, 0.0013, 0.0014, 0.0015, 0.0017, 0.0021, 0.0020,\n                      0.0018, 0.0015, 0.0012, 0.0015, 0.0018, 0.0016, 0.0014, 0.0014, 0.0014,\n                      0.0019, 0.0013, 0.0016, 0.0015, 0.0016, 0.0016, 0.0015, 0.0016, 0.0016,\n                      0.0019, 0.0016, 0.0020, 0.0014, 0.0016, 0.0018, 0.0014, 0.0019, 0.0016,\n                      0.0017, 0.0022, 0.0026, 0.0018, 0.0017, 0.0015, 0.0014]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n                min_val=tensor([-0.2812, -0.1783, -0.1963, -0.2250, -0.1704, -0.2038, -0.1697, -0.2902,\n                        -0.2018, -0.1557, -0.2279, -0.2000, -0.1779, -0.1798, -0.1434, -0.1506,\n                        -0.2226, -0.1808, -0.1986, -0.2174, -0.2115, -0.3020, -0.1964, -0.1720,\n                        -0.1718, -0.1895, -0.1771, -0.1794, -0.1932, -0.2484, -0.1670, -0.2205,\n                        -0.1732, -0.1996, -0.1679, -0.1750, -0.1905, -0.1759, -0.2016, -0.1618,\n                        -0.2056, -0.1883, -0.2190, -0.1636, -0.1732, -0.1994, -0.1819, -0.1901,\n                        -0.2448, -0.2184, -0.2362, -0.2425, -0.2549, -0.1846, -0.1944, -0.2565,\n                        -0.2348, -0.1629, -0.1653, -0.1811, -0.1780, -0.1793, -0.1525, -0.1804,\n                        -0.1492, -0.1943, -0.2152, -0.1791, -0.3041, -0.2047, -0.2154, -0.1758,\n                        -0.1852, -0.1851, -0.1855, -0.1863, -0.2187, -0.1971, -0.2483, -0.1990,\n                        -0.1831, -0.1848, -0.1635, -0.1826, -0.1949, -0.2152, -0.1311, -0.2300,\n                        -0.2122, -0.2187, -0.2534, -0.2133, -0.1713, -0.2439, -0.2577, -0.1796,\n                        -0.1839, -0.1857, -0.2045, -0.2109, -0.1848, -0.1821, -0.2003, -0.1674,\n                        -0.2106, -0.1942, -0.1622, -0.1754, -0.1975, -0.2080, -0.1337, -0.2989,\n                        -0.2519, -0.1896, -0.2005, -0.1939, -0.1737, -0.1650, -0.2836, -0.2049,\n                        -0.1674, -0.1598, -0.1542, -0.2079, -0.2732, -0.1930, -0.1593, -0.1874,\n                        -0.1553, -0.1602, -0.1751, -0.2112, -0.1807, -0.1758, -0.1786, -0.2092,\n                        -0.1518, -0.2021, -0.1861, -0.1828, -0.1951, -0.1952, -0.2005, -0.1730,\n                        -0.1680, -0.2096, -0.2532, -0.1790, -0.2060, -0.2004, -0.1777, -0.2442,\n                        -0.1986, -0.2131, -0.1975, -0.3267, -0.1911, -0.2238, -0.1861, -0.1759]), max_val=tensor([0.1681, 0.2214, 0.2138, 0.1790, 0.1557, 0.1801, 0.1664, 0.2163, 0.1862,\n                        0.1455, 0.2262, 0.2307, 0.1885, 0.1997, 0.2004, 0.1533, 0.2237, 0.1572,\n                        0.2039, 0.1833, 0.1570, 0.2670, 0.1968, 0.2531, 0.2195, 0.2293, 0.2173,\n                        0.1723, 0.1984, 0.1933, 0.2032, 0.1949, 0.1681, 0.2152, 0.1750, 0.1996,\n                        0.1786, 0.1890, 0.2065, 0.1752, 0.1812, 0.1831, 0.2489, 0.1805, 0.1574,\n                        0.2005, 0.1551, 0.1481, 0.1849, 0.1560, 0.2337, 0.1709, 0.1709, 0.1708,\n                        0.1813, 0.1816, 0.1980, 0.1973, 0.2071, 0.2474, 0.2038, 0.1701, 0.1672,\n                        0.1826, 0.1904, 0.1665, 0.1834, 0.2255, 0.1864, 0.1907, 0.1688, 0.1635,\n                        0.1794, 0.1658, 0.2430, 0.1736, 0.1906, 0.3082, 0.1408, 0.2386, 0.2174,\n                        0.1805, 0.2416, 0.1740, 0.2090, 0.2046, 0.1368, 0.1941, 0.2181, 0.2087,\n                        0.2079, 0.2078, 0.1618, 0.2001, 0.1800, 0.1689, 0.1638, 0.1760, 0.2454,\n                        0.1943, 0.1624, 0.1867, 0.1849, 0.2197, 0.1505, 0.2573, 0.1882, 0.1842,\n                        0.1648, 0.2417, 0.1554, 0.2191, 0.2093, 0.1813, 0.1697, 0.1866, 0.1744,\n                        0.1734, 0.2384, 0.1709, 0.1620, 0.1715, 0.1965, 0.2137, 0.1843, 0.2567,\n                        0.2248, 0.1692, 0.1528, 0.1886, 0.2234, 0.1578, 0.1637, 0.1604, 0.1624,\n                        0.2369, 0.1708, 0.1939, 0.1395, 0.2052, 0.2023, 0.1753, 0.1657, 0.2091,\n                        0.2420, 0.1993, 0.2019, 0.1763, 0.1738, 0.2234, 0.1594, 0.2199, 0.1726,\n                        0.2062, 0.2829, 0.2250, 0.2298, 0.1335, 0.1622, 0.1506])\n              )\n            )\n          )\n        )\n      )\n      (16): Module(\n        (conv): Module(\n          (0): Module(\n            (0): ConvBn2d(\n              160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False\n              (bn): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n                fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([4.4223e-04, 6.2033e-04, 4.7595e-04, 5.4475e-04, 8.5607e-04, 5.5120e-04,\n                        7.3473e-04, 3.5467e-04, 5.5353e-04, 5.1569e-04, 7.7232e-04, 4.8403e-04,\n                        5.7328e-04, 5.8708e-04, 6.5640e-04, 6.1928e-04, 5.9882e-04, 8.3005e-04,\n                        4.5624e-04, 5.9295e-04, 7.2116e-04, 5.7731e-04, 4.3988e-04, 5.2896e-04,\n                        3.8909e-04, 6.8745e-04, 6.1780e-04, 4.7326e-04, 6.6419e-04, 5.1472e-04,\n                        3.3499e-04, 1.0035e-03, 5.6569e-04, 1.0004e-03, 3.9233e-04, 4.2350e-04,\n                        4.7521e-04, 5.7244e-04, 5.6186e-04, 8.3088e-04, 7.7272e-04, 6.4988e-04,\n                        4.6669e-04, 8.0466e-04, 9.5296e-04, 5.4429e-04, 7.7731e-04, 1.0710e-03,\n                        7.4922e-04, 6.2934e-04, 6.1401e-04, 7.2624e-04, 6.8608e-04, 4.6914e-04,\n                        9.7595e-04, 3.3227e-04, 6.1764e-04, 5.5581e-04, 4.1251e-04, 5.5462e-04,\n                        8.9288e-04, 7.1486e-04, 5.6910e-04, 5.7847e-04, 5.8964e-04, 6.8222e-04,\n                        6.7957e-04, 7.5791e-04, 5.8684e-04, 7.6334e-04, 6.5571e-04, 5.0039e-04,\n                        8.7117e-04, 7.5687e-04, 4.1471e-04, 6.1813e-04, 8.0856e-04, 5.2910e-04,\n                        8.7183e-04, 7.3108e-04, 1.0424e-05, 7.2333e-04, 4.9084e-04, 6.0136e-04,\n                        5.5694e-04, 6.5635e-04, 6.6513e-04, 4.6732e-04, 9.2920e-04, 4.8080e-04,\n                        5.9987e-04, 6.9794e-04, 6.1503e-04, 5.1285e-04, 6.0161e-04, 7.2277e-04,\n                        8.5119e-04, 7.5624e-04, 7.8237e-04, 5.2881e-04, 7.1003e-04, 6.3116e-04,\n                        6.8003e-04, 9.5707e-04, 6.0873e-04, 7.6411e-04, 5.1451e-04, 7.9985e-04,\n                        5.2631e-04, 4.4588e-04, 5.7769e-04, 7.5236e-04, 8.8350e-04, 5.2890e-04,\n                        4.2873e-04, 7.7925e-04, 4.9827e-04, 3.3109e-04, 4.1783e-04, 5.1875e-04,\n                        6.8627e-04, 5.4157e-04, 9.1281e-04, 8.1197e-04, 5.7479e-04, 5.9466e-04,\n                        6.0143e-04, 5.0082e-04, 6.7043e-04, 6.0262e-04, 6.9663e-04, 5.1922e-04,\n                        5.2322e-04, 5.3902e-04, 7.2110e-04, 7.3584e-04, 4.2485e-04, 6.1662e-04,\n                        5.7441e-04, 5.3396e-04, 8.6980e-04, 5.5492e-04, 6.1292e-04, 7.0722e-04,\n                        7.7281e-04, 4.9481e-04, 4.6825e-04, 4.7815e-04, 5.4074e-04, 6.4679e-04,\n                        5.1799e-04, 4.8612e-04, 5.9132e-04, 7.2268e-04, 8.3409e-04, 8.1820e-04,\n                        7.1021e-04, 4.9485e-04, 8.7538e-04, 7.0088e-04, 6.7223e-04, 3.3431e-04,\n                        7.2808e-04, 8.6628e-04, 7.0454e-04, 9.2526e-04, 5.4539e-04, 6.8413e-04,\n                        6.4036e-04, 5.5688e-04, 5.7739e-04, 6.4113e-04, 7.1525e-04, 7.9524e-04,\n                        5.8201e-04, 7.1251e-04, 7.3502e-04, 5.9913e-04, 9.0962e-04, 2.5449e-04,\n                        5.9794e-04, 4.4666e-04, 8.2510e-04, 7.5229e-04, 7.2798e-04, 7.6561e-04,\n                        6.2260e-04, 4.9780e-04, 7.5864e-04, 7.2157e-04, 4.5425e-04, 7.2257e-04,\n                        8.5021e-04, 6.9209e-04, 7.4150e-04, 9.0899e-04, 5.2485e-04, 4.8764e-04,\n                        5.4640e-04, 6.4363e-04, 7.3480e-04, 5.6112e-04, 7.3077e-04, 5.0689e-04,\n                        5.1896e-04, 6.9891e-04, 9.2599e-04, 8.5414e-04, 4.5601e-04, 7.1982e-04,\n                        4.4489e-04, 6.1241e-04, 6.4066e-04, 4.8442e-04, 8.7010e-04, 5.1918e-04,\n                        5.1277e-04, 4.4244e-04, 9.0009e-04, 6.1336e-04, 4.9666e-04, 5.5235e-04,\n                        5.7788e-04, 7.6069e-04, 5.4940e-04, 7.7502e-04, 9.7948e-04, 6.6854e-04,\n                        5.9791e-04, 5.9358e-04, 8.3845e-04, 5.3334e-04, 6.4303e-04, 1.7106e-03,\n                        6.0772e-04, 9.6869e-04, 1.0344e-03, 7.3692e-04, 4.9870e-04, 5.1680e-04,\n                        5.2408e-04, 7.4078e-04, 5.3907e-04, 5.2666e-04, 4.4471e-04, 7.0366e-04,\n                        4.0149e-04, 5.4748e-04, 5.8806e-04, 5.3344e-04, 9.3826e-04, 4.2774e-04,\n                        3.8728e-04, 6.7519e-04, 7.5065e-04, 4.9497e-04, 6.2528e-04, 5.7261e-04,\n                        4.8075e-04, 4.5402e-04, 3.3904e-04, 8.5736e-04, 6.1116e-04, 5.8243e-04,\n                        4.9555e-04, 3.6339e-04, 6.3435e-04, 7.4331e-04, 4.7653e-04, 4.8887e-04,\n                        6.3478e-04, 5.3694e-04, 6.8932e-04, 4.4110e-04, 6.1602e-04, 7.2375e-04,\n                        6.4059e-04, 4.3318e-04, 5.2096e-04, 6.3288e-04, 9.1038e-04, 6.2121e-04,\n                        8.9361e-04, 7.3810e-04, 5.3848e-04, 4.5592e-04, 6.6801e-04, 4.6175e-04,\n                        7.7785e-04, 7.5307e-04, 8.0631e-04, 4.1195e-04, 7.6421e-04, 5.3668e-04,\n                        5.0186e-04, 6.8992e-04, 8.9705e-04, 4.7367e-04, 9.4228e-04, 4.3099e-04,\n                        6.8121e-04, 8.0955e-04, 3.6914e-04, 5.6750e-04, 5.8858e-04, 7.6065e-04,\n                        3.9323e-04, 8.6640e-04, 4.6979e-04, 5.6677e-04, 5.5244e-04, 4.8992e-04,\n                        7.1952e-04, 8.2541e-04, 6.5447e-04, 8.0855e-04, 5.2854e-04, 8.0540e-04,\n                        5.9213e-04, 6.8599e-04, 4.9419e-04, 5.1425e-04, 4.1486e-04, 1.0717e-03,\n                        6.7150e-04, 9.8245e-04, 5.3931e-04, 8.0164e-04, 9.6427e-04, 9.0570e-04,\n                        6.7492e-04, 7.5169e-04, 4.0339e-04, 8.8326e-04, 5.8368e-04, 5.6432e-04,\n                        5.8396e-04, 6.1276e-04, 7.1584e-04, 6.1223e-04, 6.2077e-04, 4.3873e-04,\n                        5.5238e-04, 3.7706e-04, 6.8576e-04, 5.7798e-04, 7.0620e-04, 8.9505e-04,\n                        9.8033e-06, 4.5737e-04, 5.3629e-04, 7.4856e-04, 8.5543e-04, 1.0202e-03,\n                        5.2087e-04, 7.2579e-04, 4.1193e-04, 6.7573e-04, 7.7527e-04, 6.3270e-04,\n                        7.0676e-04, 7.6180e-04, 8.3890e-04, 5.6344e-04, 6.1404e-04, 7.0652e-04,\n                        6.7472e-04, 5.0359e-04, 5.2212e-04, 8.1073e-04, 4.1815e-04, 6.7110e-04,\n                        8.8770e-04, 7.1346e-04, 4.7500e-04, 6.5283e-04, 4.9178e-04, 6.8948e-04,\n                        5.5965e-04, 7.2218e-04, 4.7751e-04, 5.9809e-04, 7.7393e-04, 5.7628e-04,\n                        6.1524e-04, 6.1615e-04, 5.0939e-04, 5.3699e-04, 5.1790e-04, 4.6516e-04,\n                        7.5472e-04, 5.1558e-04, 9.1372e-04, 7.1766e-04, 6.3087e-04, 3.1340e-04,\n                        6.5272e-04, 4.0203e-04, 5.9009e-04, 6.2832e-04, 6.4396e-04, 7.0482e-04,\n                        8.7280e-04, 6.0204e-04, 8.3806e-06, 5.6442e-04, 3.1510e-04, 5.7944e-04,\n                        6.0401e-04, 5.0615e-04, 5.3399e-04, 8.5796e-04, 4.6606e-04, 4.4364e-04,\n                        6.1129e-04, 6.2858e-04, 7.6470e-04, 5.6217e-04, 5.5088e-04, 3.2122e-04,\n                        7.2032e-04, 4.7230e-04, 5.6071e-04, 3.8182e-04, 6.1698e-04, 6.3757e-04,\n                        4.9481e-04, 5.7939e-04, 6.4681e-04, 8.1161e-04, 1.0136e-03, 4.3202e-04,\n                        5.0782e-04, 6.2789e-04, 5.6475e-04, 7.5313e-04, 6.7694e-04, 8.3198e-04,\n                        4.2063e-04, 7.2647e-04, 5.5151e-04, 8.0106e-04, 6.0495e-04, 3.8864e-04,\n                        6.0204e-04, 5.2265e-04, 6.4252e-04, 4.6291e-04, 5.6364e-04, 1.0732e-03,\n                        8.4478e-04, 5.2054e-04, 5.5672e-04, 5.9234e-04, 1.1979e-03, 4.3474e-04,\n                        4.2477e-04, 6.1231e-04, 6.2227e-04, 6.8661e-04, 7.4869e-04, 6.5396e-04,\n                        4.1375e-04, 7.6901e-04, 6.4917e-04, 4.8913e-04, 4.6857e-04, 5.7696e-04,\n                        6.0245e-04, 7.2624e-04, 5.0093e-04, 6.5978e-04, 4.6329e-04, 5.7171e-04,\n                        7.7697e-04, 6.0362e-04, 8.1735e-04, 9.1207e-04, 4.1940e-04, 3.3954e-04,\n                        7.4273e-04, 7.8533e-04, 7.5684e-04, 6.8820e-04, 5.8259e-04, 7.2254e-04,\n                        7.4587e-04, 8.4193e-04, 6.9375e-04, 6.8386e-04, 4.0024e-04, 6.0994e-04,\n                        8.5063e-04, 5.4509e-04, 6.4413e-04, 4.7204e-04, 5.9270e-04, 7.1197e-04,\n                        3.4424e-04, 5.1649e-04, 5.9034e-04, 6.6759e-04, 6.5887e-04, 2.7212e-04,\n                        6.8982e-04, 5.5274e-04, 8.0782e-04, 7.2538e-04, 5.2296e-04, 5.7445e-04,\n                        8.3061e-04, 5.3285e-04, 4.3423e-04, 7.4561e-04, 6.5918e-04, 5.5632e-04,\n                        5.1199e-04, 7.0621e-04, 4.4762e-04, 5.1971e-04, 7.4962e-04, 5.9122e-04,\n                        8.6997e-04, 5.0844e-04, 5.2100e-04, 5.2819e-04, 7.7276e-04, 6.9687e-04,\n                        8.1382e-04, 7.8529e-04, 7.3367e-04, 7.2790e-04, 6.9627e-04, 6.8359e-04,\n                        4.3170e-04, 5.9987e-04, 4.8379e-04, 5.1201e-04, 7.3004e-04, 6.7993e-04,\n                        7.5431e-04, 8.5066e-04, 5.0173e-04, 6.5780e-04, 3.8027e-04, 4.5006e-04,\n                        8.8283e-04, 4.9461e-04, 5.9349e-04, 6.6173e-04, 5.7446e-04, 7.6945e-04,\n                        5.3655e-04, 6.7284e-04, 8.7555e-04, 8.0134e-04, 6.7905e-04, 6.5973e-04,\n                        7.1794e-04, 5.5690e-04, 8.7861e-04, 7.1930e-04, 9.1437e-04, 5.7692e-04,\n                        5.6190e-04, 7.9748e-04, 6.1037e-04, 3.5653e-04, 6.4901e-04, 5.8319e-04,\n                        4.5520e-04, 6.5481e-04, 6.6711e-04, 3.2104e-04, 7.0256e-04, 6.2323e-04,\n                        8.8786e-04, 3.3319e-04, 5.8132e-04, 4.9844e-04, 6.4664e-04, 8.4477e-04,\n                        6.2032e-04, 8.5368e-04, 6.2970e-04, 2.8218e-04, 5.7814e-04, 8.1331e-04,\n                        8.7208e-04, 5.9616e-04, 8.5011e-04, 5.2241e-04, 6.9751e-04, 6.6532e-04,\n                        6.2769e-04, 6.4759e-04, 5.8708e-04, 6.1796e-04, 4.4263e-04, 4.8528e-04,\n                        4.9176e-04, 6.6810e-04, 7.2717e-04, 3.9796e-04, 7.7496e-04, 8.7090e-04,\n                        7.8457e-04, 7.9216e-04, 6.1397e-04, 3.8887e-04, 7.8937e-04, 5.7002e-04,\n                        4.3934e-04, 6.4534e-04, 8.5745e-04, 5.4398e-04, 7.0524e-04, 6.0227e-04,\n                        4.6973e-04, 5.9393e-04, 5.0108e-04, 7.8182e-04, 7.6593e-04, 7.4097e-04,\n                        9.0010e-04, 6.9427e-04, 6.6019e-04, 5.3083e-04, 4.6159e-04, 6.0978e-04,\n                        6.9861e-04, 4.4369e-04, 1.0031e-03, 6.7429e-04, 7.2070e-04, 6.6367e-04,\n                        4.5897e-04, 7.1182e-04, 5.5865e-04, 3.6344e-04, 5.5272e-04, 7.5814e-04,\n                        3.7249e-04, 5.1111e-04, 5.0230e-04, 8.7831e-04, 6.7300e-04, 7.4267e-04,\n                        5.2122e-04, 5.8807e-04, 6.0858e-04, 5.7884e-04, 4.0265e-04, 7.3527e-04,\n                        5.2395e-04, 5.7766e-04, 5.3817e-04, 4.9656e-04, 7.8026e-04, 6.7616e-04,\n                        6.4680e-04, 3.6712e-04, 5.3681e-04, 3.8249e-04, 6.9596e-04, 9.2110e-04,\n                        7.5650e-04, 6.3045e-04, 4.5863e-04, 7.1930e-04, 6.2805e-04, 6.1963e-04,\n                        8.5479e-04, 3.1186e-04, 5.7828e-04, 5.7791e-04, 9.2942e-04, 5.3908e-04,\n                        7.4767e-04, 5.1571e-04, 4.9409e-04, 6.4493e-04, 8.6601e-04, 5.9263e-04,\n                        8.1393e-04, 4.7973e-04, 4.1272e-04, 5.4670e-04, 5.7306e-04, 6.6268e-04,\n                        5.3309e-04, 6.1233e-04, 4.1726e-04, 5.4716e-04, 5.9769e-04, 7.7515e-04,\n                        6.3249e-04, 6.5153e-04, 8.1011e-04, 5.7969e-04, 6.9902e-04, 5.4739e-04,\n                        5.1182e-04, 7.1890e-04, 6.4410e-04, 7.3300e-04, 5.5287e-04, 3.5711e-04,\n                        1.2753e-03, 9.1067e-04, 5.3342e-04, 7.2462e-04, 4.3961e-04, 4.6195e-04,\n                        6.5575e-04, 4.8071e-04, 1.0738e-03, 4.6002e-04, 5.9231e-04, 5.2165e-04,\n                        4.3433e-04, 5.5861e-04, 6.5381e-04, 4.9864e-04, 5.5356e-04, 3.2995e-04,\n                        4.7954e-04, 4.9822e-04, 8.4997e-04, 6.1033e-04, 9.1868e-04, 3.7377e-04,\n                        6.5964e-04, 6.0566e-04, 1.0252e-03, 5.3871e-04, 6.2700e-04, 3.9278e-04,\n                        6.0508e-04, 5.9746e-04, 4.1154e-04, 6.4887e-04, 6.2549e-04, 4.4660e-04,\n                        5.5501e-04, 4.4355e-04, 7.1617e-04, 3.9800e-04, 7.4817e-04, 7.5582e-04,\n                        3.3770e-04, 5.3762e-04, 6.9194e-04, 8.2117e-04, 5.7925e-04, 4.0596e-04,\n                        8.1384e-04, 5.1507e-04, 4.3129e-04, 6.1236e-04, 4.8348e-04, 3.7635e-04,\n                        4.6329e-04, 5.5763e-04, 4.9530e-04, 4.3871e-04, 5.0241e-04, 6.3528e-04,\n                        5.7579e-04, 6.6222e-04, 6.8254e-04, 7.1528e-04, 9.2116e-04, 6.4223e-04,\n                        6.9787e-04, 2.9511e-04, 4.5023e-04, 7.2171e-04, 5.9939e-04, 5.4161e-04,\n                        4.3229e-04, 7.3979e-04, 6.8531e-04, 3.4110e-04, 2.9123e-04, 5.9013e-04,\n                        7.2133e-04, 7.2385e-04, 4.5869e-04, 6.3837e-04, 7.6710e-04, 4.2374e-04,\n                        4.3630e-04, 8.2076e-04, 4.5032e-04, 6.4417e-04, 6.5033e-04, 6.7010e-04,\n                        4.8196e-04, 4.2883e-04, 3.8767e-04, 7.6032e-04, 8.5405e-04, 5.6768e-04,\n                        7.4093e-04, 4.6468e-04, 7.6666e-04, 6.7518e-04, 2.4369e-04, 3.8357e-04,\n                        4.1947e-04, 2.2619e-04, 2.4564e-04, 7.3369e-04, 6.9601e-06, 5.0814e-04,\n                        4.2493e-04, 1.1458e-03, 7.4512e-04, 8.8232e-04, 5.8787e-04, 6.0678e-04,\n                        7.8227e-04, 5.5476e-04, 5.9916e-04, 6.4338e-04, 4.4775e-04, 4.5220e-04,\n                        7.3515e-04, 6.9116e-04, 7.5329e-04, 7.3642e-04, 4.4109e-04, 1.0378e-03,\n                        1.1045e-03, 5.3173e-04, 6.8654e-04, 7.3679e-04, 6.6146e-04, 8.7937e-04,\n                        6.6382e-04, 7.4426e-04, 7.4293e-04, 5.7813e-04, 4.1694e-04, 7.6153e-04,\n                        9.1736e-04, 7.6963e-04, 7.2428e-04, 5.9698e-04, 1.4051e-03, 4.7079e-04,\n                        4.9154e-04, 6.4570e-04, 7.0920e-04, 6.6234e-04, 4.1165e-04, 1.2723e-03,\n                        6.3907e-04, 6.2165e-04, 6.0558e-04, 7.4125e-04, 7.3703e-04, 8.6822e-04,\n                        4.3307e-04, 1.0836e-03, 7.2148e-04, 3.7216e-04, 2.9723e-04, 7.3625e-04,\n                        5.7854e-04, 8.2719e-04, 8.7801e-04, 7.8037e-04, 2.6509e-04, 6.6917e-04,\n                        4.0275e-04, 5.1725e-04, 7.3043e-04, 4.6928e-04, 5.6037e-04, 4.7403e-04,\n                        6.1704e-04, 9.0389e-04, 6.9107e-04, 1.0832e-03, 6.1249e-04, 5.3446e-04,\n                        5.7770e-04, 7.2438e-04, 5.6780e-04, 3.9219e-04, 3.5522e-04, 7.2473e-04,\n                        4.5778e-04, 6.4418e-04, 6.9429e-04, 6.3592e-04, 1.2745e-03, 7.4106e-04,\n                        6.2468e-04, 6.8087e-04, 6.6894e-04, 7.0380e-04, 3.5960e-04, 6.2991e-04,\n                        4.9508e-04, 1.0256e-03, 8.7075e-04, 5.9256e-04, 6.8961e-04, 6.1321e-04,\n                        5.6570e-04, 1.1435e-03, 4.7490e-04, 8.1814e-04, 9.6444e-04, 8.2636e-04,\n                        6.6115e-04, 5.2371e-04, 7.0107e-04, 5.0216e-04, 5.2380e-04, 5.4897e-04,\n                        5.7990e-04, 7.6032e-04, 5.6172e-04, 6.7357e-04, 7.1037e-04, 8.8976e-04,\n                        4.9393e-04, 3.8641e-04, 3.3883e-04, 7.4494e-04, 7.0938e-04, 5.1512e-04,\n                        7.1031e-04, 6.1273e-04, 5.7375e-04, 2.5201e-04, 8.0242e-04, 4.3008e-04,\n                        6.5281e-04, 5.9364e-04, 9.7048e-04, 4.8596e-04, 7.0968e-04, 6.3613e-04,\n                        6.8311e-04, 7.4698e-04, 6.2906e-04, 6.1862e-04, 7.2797e-04, 6.3938e-04,\n                        6.6743e-04, 5.2858e-04, 6.4197e-04, 4.4778e-04, 7.3021e-04, 5.6055e-04]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                       dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n                (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n                  min_val=tensor([-0.0407, -0.0794, -0.0609, -0.0697, -0.1096, -0.0706, -0.0922, -0.0379,\n                          -0.0618, -0.0549, -0.0817, -0.0620, -0.0623, -0.0553, -0.0810, -0.0690,\n                          -0.0678, -0.0844, -0.0584, -0.0613, -0.0859, -0.0692, -0.0563, -0.0677,\n                          -0.0497, -0.0880, -0.0791, -0.0512, -0.0850, -0.0659, -0.0429, -0.0996,\n                          -0.0724, -0.1110, -0.0423, -0.0542, -0.0518, -0.0733, -0.0696, -0.1064,\n                          -0.0989, -0.0766, -0.0597, -0.1030, -0.0901, -0.0669, -0.0816, -0.1371,\n                          -0.0917, -0.0753, -0.0643, -0.0841, -0.0758, -0.0600, -0.1092, -0.0416,\n                          -0.0791, -0.0711, -0.0495, -0.0710, -0.1082, -0.0865, -0.0728, -0.0735,\n                          -0.0751, -0.0873, -0.0870, -0.0970, -0.0697, -0.0922, -0.0839, -0.0469,\n                          -0.1115, -0.0887, -0.0527, -0.0791, -0.1035, -0.0614, -0.0961, -0.0856,\n                          -0.0013, -0.0926, -0.0628, -0.0639, -0.0640, -0.0665, -0.0850, -0.0587,\n                          -0.1189, -0.0615, -0.0768, -0.0893, -0.0747, -0.0646, -0.0770, -0.0796,\n                          -0.1090, -0.0786, -0.0785, -0.0677, -0.0832, -0.0799, -0.0836, -0.1179,\n                          -0.0779, -0.0830, -0.0594, -0.0771, -0.0674, -0.0571, -0.0739, -0.0963,\n                          -0.0838, -0.0557, -0.0479, -0.0997, -0.0638, -0.0379, -0.0447, -0.0466,\n                          -0.0878, -0.0640, -0.0946, -0.1039, -0.0545, -0.0582, -0.0648, -0.0641,\n                          -0.0858, -0.0623, -0.0700, -0.0634, -0.0670, -0.0690, -0.0923, -0.0787,\n                          -0.0485, -0.0789, -0.0638, -0.0683, -0.1113, -0.0694, -0.0785, -0.0905,\n                          -0.0989, -0.0489, -0.0558, -0.0612, -0.0692, -0.0751, -0.0663, -0.0541,\n                          -0.0677, -0.0666, -0.0661, -0.0890, -0.0680, -0.0552, -0.1120, -0.0897,\n                          -0.0860, -0.0359, -0.0465, -0.1109, -0.0902, -0.0925, -0.0698, -0.0750,\n                          -0.0820, -0.0588, -0.0739, -0.0821, -0.0916, -0.0806, -0.0716, -0.0681,\n                          -0.0941, -0.0748, -0.1164, -0.0326, -0.0765, -0.0572, -0.0833, -0.0904,\n                          -0.0932, -0.0980, -0.0797, -0.0637, -0.0790, -0.0924, -0.0575, -0.0925,\n                          -0.0797, -0.0850, -0.0842, -0.1164, -0.0672, -0.0614, -0.0699, -0.0824,\n                          -0.0747, -0.0588, -0.0935, -0.0649, -0.0664, -0.0895, -0.1185, -0.0973,\n                          -0.0525, -0.0921, -0.0569, -0.0775, -0.0820, -0.0620, -0.1114, -0.0504,\n                          -0.0656, -0.0561, -0.1147, -0.0778, -0.0614, -0.0529, -0.0740, -0.0974,\n                          -0.0703, -0.0992, -0.1020, -0.0856, -0.0573, -0.0760, -0.0860, -0.0646,\n                          -0.0571, -0.1826, -0.0778, -0.1240, -0.1101, -0.0943, -0.0618, -0.0662,\n                          -0.0652, -0.0743, -0.0690, -0.0575, -0.0569, -0.0577, -0.0455, -0.0701,\n                          -0.0726, -0.0594, -0.1117, -0.0541, -0.0394, -0.0805, -0.0961, -0.0588,\n                          -0.0769, -0.0733, -0.0492, -0.0563, -0.0434, -0.1097, -0.0712, -0.0746,\n                          -0.0542, -0.0385, -0.0618, -0.0951, -0.0610, -0.0479, -0.0760, -0.0687,\n                          -0.0723, -0.0498, -0.0789, -0.0926, -0.0820, -0.0552, -0.0630, -0.0810,\n                          -0.1165, -0.0670, -0.0903, -0.0833, -0.0689, -0.0577, -0.0855, -0.0591,\n                          -0.0996, -0.0964, -0.0883, -0.0450, -0.0798, -0.0687, -0.0642, -0.0611,\n                          -0.0940, -0.0606, -0.1206, -0.0487, -0.0843, -0.0824, -0.0434, -0.0726,\n                          -0.0753, -0.0974, -0.0503, -0.0775, -0.0601, -0.0725, -0.0666, -0.0488,\n                          -0.0921, -0.1057, -0.0732, -0.0827, -0.0677, -0.1031, -0.0727, -0.0811,\n                          -0.0528, -0.0592, -0.0531, -0.0654, -0.0854, -0.1258, -0.0551, -0.0785,\n                          -0.1234, -0.1159, -0.0864, -0.0780, -0.0483, -0.0797, -0.0747, -0.0670,\n                          -0.0747, -0.0663, -0.0916, -0.0784, -0.0795, -0.0562, -0.0707, -0.0476,\n                          -0.0638, -0.0608, -0.0736, -0.1146, -0.0013, -0.0585, -0.0686, -0.0840,\n                          -0.1095, -0.0855, -0.0667, -0.0929, -0.0527, -0.0865, -0.0877, -0.0595,\n                          -0.0905, -0.0975, -0.0909, -0.0563, -0.0786, -0.0904, -0.0864, -0.0587,\n                          -0.0668, -0.1038, -0.0522, -0.0679, -0.0926, -0.0913, -0.0601, -0.0584,\n                          -0.0629, -0.0883, -0.0716, -0.0924, -0.0530, -0.0766, -0.0751, -0.0738,\n                          -0.0788, -0.0476, -0.0652, -0.0687, -0.0663, -0.0595, -0.0957, -0.0578,\n                          -0.0923, -0.0740, -0.0764, -0.0376, -0.0616, -0.0505, -0.0755, -0.0804,\n                          -0.0798, -0.0902, -0.1117, -0.0686, -0.0010, -0.0692, -0.0349, -0.0550,\n                          -0.0773, -0.0578, -0.0627, -0.1030, -0.0597, -0.0470, -0.0675, -0.0805,\n                          -0.0979, -0.0578, -0.0593, -0.0345, -0.0762, -0.0605, -0.0673, -0.0458,\n                          -0.0790, -0.0712, -0.0614, -0.0664, -0.0828, -0.1039, -0.1215, -0.0441,\n                          -0.0650, -0.0804, -0.0723, -0.0857, -0.0866, -0.1065, -0.0538, -0.0754,\n                          -0.0706, -0.1025, -0.0774, -0.0497, -0.0564, -0.0669, -0.0742, -0.0593,\n                          -0.0708, -0.1374, -0.1081, -0.0666, -0.0713, -0.0758, -0.1533, -0.0462,\n                          -0.0544, -0.0784, -0.0781, -0.0879, -0.0958, -0.0783, -0.0430, -0.0847,\n                          -0.0656, -0.0547, -0.0600, -0.0712, -0.0626, -0.0788, -0.0588, -0.0845,\n                          -0.0593, -0.0732, -0.0794, -0.0682, -0.1046, -0.0924, -0.0498, -0.0435,\n                          -0.0570, -0.0729, -0.0969, -0.0881, -0.0746, -0.0708, -0.0861, -0.1078,\n                          -0.0708, -0.0549, -0.0512, -0.0781, -0.1089, -0.0698, -0.0422, -0.0604,\n                          -0.0747, -0.0828, -0.0352, -0.0661, -0.0756, -0.0855, -0.0689, -0.0348,\n                          -0.0862, -0.0563, -0.0983, -0.0928, -0.0561, -0.0735, -0.1063, -0.0595,\n                          -0.0556, -0.0832, -0.0844, -0.0619, -0.0621, -0.0610, -0.0546, -0.0665,\n                          -0.0960, -0.0616, -0.0952, -0.0651, -0.0667, -0.0603, -0.0989, -0.0892,\n                          -0.0746, -0.0885, -0.0824, -0.0832, -0.0846, -0.0663, -0.0553, -0.0768,\n                          -0.0619, -0.0655, -0.0934, -0.0870, -0.0966, -0.1089, -0.0421, -0.0842,\n                          -0.0378, -0.0477, -0.0616, -0.0627, -0.0539, -0.0748, -0.0569, -0.0985,\n                          -0.0687, -0.0681, -0.1121, -0.0823, -0.0717, -0.0844, -0.0919, -0.0525,\n                          -0.1125, -0.0918, -0.1170, -0.0738, -0.0603, -0.1021, -0.0781, -0.0442,\n                          -0.0727, -0.0603, -0.0405, -0.0586, -0.0854, -0.0411, -0.0899, -0.0706,\n                          -0.1136, -0.0351, -0.0570, -0.0638, -0.0742, -0.0863, -0.0661, -0.0908,\n                          -0.0798, -0.0361, -0.0668, -0.1041, -0.0963, -0.0369, -0.1088, -0.0558,\n                          -0.0893, -0.0756, -0.0803, -0.0828, -0.0626, -0.0700, -0.0567, -0.0621,\n                          -0.0571, -0.0725, -0.0688, -0.0509, -0.0919, -0.1049, -0.0925, -0.0800,\n                          -0.0786, -0.0498, -0.0846, -0.0730, -0.0562, -0.0826, -0.0814, -0.0691,\n                          -0.0737, -0.0688, -0.0601, -0.0688, -0.0641, -0.0864, -0.0980, -0.0894,\n                          -0.1152, -0.0889, -0.0845, -0.0549, -0.0591, -0.0781, -0.0894, -0.0552,\n                          -0.1093, -0.0775, -0.0767, -0.0847, -0.0561, -0.0818, -0.0715, -0.0465,\n                          -0.0609, -0.0970, -0.0438, -0.0654, -0.0643, -0.0697, -0.0650, -0.0949,\n                          -0.0606, -0.0753, -0.0712, -0.0741, -0.0515, -0.0801, -0.0671, -0.0635,\n                          -0.0689, -0.0636, -0.0999, -0.0829, -0.0689, -0.0402, -0.0687, -0.0488,\n                          -0.0883, -0.1179, -0.0878, -0.0759, -0.0493, -0.0852, -0.0804, -0.0660,\n                          -0.1094, -0.0329, -0.0740, -0.0740, -0.1190, -0.0690, -0.0957, -0.0660,\n                          -0.0538, -0.0757, -0.0966, -0.0625, -0.1042, -0.0549, -0.0405, -0.0700,\n                          -0.0734, -0.0848, -0.0682, -0.0613, -0.0528, -0.0674, -0.0765, -0.0992,\n                          -0.0580, -0.0653, -0.1037, -0.0687, -0.0895, -0.0637, -0.0490, -0.0920,\n                          -0.0595, -0.0559, -0.0603, -0.0437, -0.1632, -0.1166, -0.0679, -0.0928,\n                          -0.0563, -0.0562, -0.0839, -0.0615, -0.1374, -0.0589, -0.0639, -0.0643,\n                          -0.0477, -0.0715, -0.0821, -0.0638, -0.0709, -0.0410, -0.0614, -0.0498,\n                          -0.1088, -0.0781, -0.1176, -0.0428, -0.0842, -0.0775, -0.0909, -0.0541,\n                          -0.0660, -0.0412, -0.0675, -0.0765, -0.0433, -0.0831, -0.0801, -0.0572,\n                          -0.0634, -0.0546, -0.0917, -0.0509, -0.0851, -0.0657, -0.0432, -0.0552,\n                          -0.0886, -0.1051, -0.0638, -0.0466, -0.1042, -0.0659, -0.0552, -0.0604,\n                          -0.0619, -0.0416, -0.0506, -0.0714, -0.0634, -0.0482, -0.0643, -0.0690,\n                          -0.0701, -0.0566, -0.0874, -0.0916, -0.1179, -0.0555, -0.0893, -0.0199,\n                          -0.0576, -0.0890, -0.0767, -0.0693, -0.0553, -0.0947, -0.0877, -0.0353,\n                          -0.0373, -0.0706, -0.0923, -0.0770, -0.0569, -0.0683, -0.0982, -0.0542,\n                          -0.0558, -0.1051, -0.0533, -0.0809, -0.0832, -0.0803, -0.0617, -0.0529,\n                          -0.0496, -0.0776, -0.1093, -0.0701, -0.0639, -0.0567, -0.0981, -0.0701,\n                          -0.0237, -0.0491, -0.0476, -0.0267, -0.0246, -0.0838, -0.0008, -0.0642,\n                          -0.0447, -0.0890, -0.0657, -0.1060, -0.0752, -0.0756, -0.1001, -0.0710,\n                          -0.0767, -0.0824, -0.0349, -0.0579, -0.0796, -0.0885, -0.0964, -0.0880,\n                          -0.0565, -0.1328, -0.1414, -0.0606, -0.0879, -0.0943, -0.0847, -0.0983,\n                          -0.0586, -0.0953, -0.0951, -0.0645, -0.0320, -0.0975, -0.0963, -0.0985,\n                          -0.0846, -0.0713, -0.1799, -0.0554, -0.0629, -0.0610, -0.0908, -0.0759,\n                          -0.0464, -0.1629, -0.0721, -0.0730, -0.0600, -0.0949, -0.0735, -0.1111,\n                          -0.0554, -0.1004, -0.0894, -0.0476, -0.0380, -0.0942, -0.0741, -0.0835,\n                          -0.0774, -0.0671, -0.0330, -0.0811, -0.0516, -0.0662, -0.0798, -0.0601,\n                          -0.0717, -0.0587, -0.0790, -0.0648, -0.0623, -0.1386, -0.0784, -0.0684,\n                          -0.0739, -0.0927, -0.0662, -0.0465, -0.0397, -0.0917, -0.0586, -0.0649,\n                          -0.0810, -0.0814, -0.1631, -0.0944, -0.0496, -0.0872, -0.0838, -0.0867,\n                          -0.0460, -0.0550, -0.0634, -0.1313, -0.1115, -0.0758, -0.0883, -0.0694,\n                          -0.0606, -0.1464, -0.0572, -0.1047, -0.1234, -0.0766, -0.0846, -0.0670,\n                          -0.0888, -0.0643, -0.0433, -0.0703, -0.0742, -0.0789, -0.0681, -0.0725,\n                          -0.0901, -0.1139, -0.0632, -0.0495, -0.0332, -0.0866, -0.0761, -0.0659,\n                          -0.0909, -0.0784, -0.0734, -0.0312, -0.1027, -0.0534, -0.0836, -0.0760,\n                          -0.1242, -0.0622, -0.0662, -0.0707, -0.0806, -0.0779, -0.0805, -0.0676,\n                          -0.0858, -0.0691, -0.0854, -0.0677, -0.0793, -0.0510, -0.0929, -0.0718]), max_val=tensor([0.0562, 0.0772, 0.0537, 0.0659, 0.0776, 0.0686, 0.0933, 0.0450, 0.0703,\n                          0.0655, 0.0981, 0.0504, 0.0728, 0.0746, 0.0834, 0.0786, 0.0761, 0.1054,\n                          0.0557, 0.0753, 0.0916, 0.0733, 0.0515, 0.0666, 0.0494, 0.0823, 0.0640,\n                          0.0601, 0.0720, 0.0646, 0.0404, 0.1274, 0.0550, 0.1271, 0.0498, 0.0455,\n                          0.0604, 0.0676, 0.0714, 0.0837, 0.0838, 0.0825, 0.0516, 0.0818, 0.1210,\n                          0.0691, 0.0987, 0.0941, 0.0952, 0.0799, 0.0780, 0.0922, 0.0871, 0.0531,\n                          0.1239, 0.0422, 0.0560, 0.0582, 0.0524, 0.0510, 0.1134, 0.0908, 0.0657,\n                          0.0735, 0.0749, 0.0566, 0.0812, 0.0774, 0.0745, 0.0969, 0.0739, 0.0636,\n                          0.0825, 0.0961, 0.0527, 0.0739, 0.0827, 0.0672, 0.1107, 0.0928, 0.0009,\n                          0.0918, 0.0471, 0.0764, 0.0707, 0.0834, 0.0845, 0.0594, 0.1127, 0.0382,\n                          0.0623, 0.0655, 0.0781, 0.0651, 0.0683, 0.0918, 0.0765, 0.0960, 0.0994,\n                          0.0516, 0.0902, 0.0802, 0.0864, 0.1215, 0.0542, 0.0970, 0.0653, 0.1016,\n                          0.0543, 0.0536, 0.0607, 0.0767, 0.1122, 0.0672, 0.0544, 0.0799, 0.0615,\n                          0.0420, 0.0531, 0.0659, 0.0725, 0.0688, 0.1159, 0.0825, 0.0730, 0.0755,\n                          0.0764, 0.0478, 0.0698, 0.0765, 0.0885, 0.0659, 0.0524, 0.0612, 0.0575,\n                          0.0935, 0.0540, 0.0588, 0.0730, 0.0614, 0.0923, 0.0705, 0.0645, 0.0618,\n                          0.0709, 0.0628, 0.0595, 0.0606, 0.0601, 0.0821, 0.0611, 0.0617, 0.0751,\n                          0.0918, 0.1059, 0.1039, 0.0902, 0.0628, 0.0884, 0.0790, 0.0819, 0.0425,\n                          0.0925, 0.0842, 0.0877, 0.1175, 0.0655, 0.0869, 0.0647, 0.0707, 0.0519,\n                          0.0791, 0.0850, 0.1010, 0.0739, 0.0905, 0.0918, 0.0761, 0.0870, 0.0296,\n                          0.0612, 0.0529, 0.1048, 0.0955, 0.0760, 0.0883, 0.0492, 0.0503, 0.0963,\n                          0.0822, 0.0577, 0.0851, 0.1080, 0.0879, 0.0942, 0.0838, 0.0558, 0.0619,\n                          0.0639, 0.0707, 0.0933, 0.0713, 0.0659, 0.0481, 0.0591, 0.0784, 0.1066,\n                          0.1085, 0.0579, 0.0841, 0.0498, 0.0778, 0.0628, 0.0538, 0.0770, 0.0659,\n                          0.0560, 0.0562, 0.1143, 0.0779, 0.0631, 0.0701, 0.0605, 0.0940, 0.0637,\n                          0.0781, 0.1244, 0.0846, 0.0759, 0.0622, 0.1065, 0.0677, 0.0817, 0.2172,\n                          0.0648, 0.0843, 0.1314, 0.0718, 0.0633, 0.0645, 0.0666, 0.0941, 0.0627,\n                          0.0669, 0.0553, 0.0894, 0.0510, 0.0575, 0.0747, 0.0677, 0.1192, 0.0543,\n                          0.0492, 0.0857, 0.0822, 0.0629, 0.0794, 0.0659, 0.0611, 0.0577, 0.0402,\n                          0.1059, 0.0776, 0.0730, 0.0629, 0.0462, 0.0806, 0.0932, 0.0593, 0.0621,\n                          0.0806, 0.0585, 0.0875, 0.0560, 0.0751, 0.0793, 0.0497, 0.0550, 0.0662,\n                          0.0668, 0.0793, 0.0789, 0.1135, 0.0937, 0.0597, 0.0579, 0.0796, 0.0525,\n                          0.0853, 0.0867, 0.1024, 0.0523, 0.0971, 0.0531, 0.0510, 0.0876, 0.1139,\n                          0.0541, 0.0737, 0.0547, 0.0865, 0.1028, 0.0469, 0.0684, 0.0678, 0.0731,\n                          0.0484, 0.1100, 0.0566, 0.0709, 0.0702, 0.0622, 0.0736, 0.0828, 0.0831,\n                          0.1027, 0.0602, 0.0767, 0.0752, 0.0871, 0.0628, 0.0653, 0.0305, 0.1361,\n                          0.0853, 0.1157, 0.0685, 0.1018, 0.0982, 0.0988, 0.0640, 0.0955, 0.0512,\n                          0.1122, 0.0600, 0.0717, 0.0700, 0.0778, 0.0798, 0.0775, 0.0659, 0.0516,\n                          0.0660, 0.0479, 0.0871, 0.0734, 0.0897, 0.0785, 0.0012, 0.0550, 0.0645,\n                          0.0951, 0.0880, 0.1296, 0.0563, 0.0751, 0.0495, 0.0822, 0.0985, 0.0804,\n                          0.0762, 0.0832, 0.1065, 0.0716, 0.0412, 0.0665, 0.0752, 0.0640, 0.0460,\n                          0.0679, 0.0531, 0.0852, 0.1127, 0.0765, 0.0603, 0.0829, 0.0517, 0.0809,\n                          0.0688, 0.0689, 0.0606, 0.0626, 0.0983, 0.0694, 0.0774, 0.0783, 0.0646,\n                          0.0601, 0.0586, 0.0484, 0.0958, 0.0655, 0.1160, 0.0911, 0.0801, 0.0398,\n                          0.0829, 0.0511, 0.0637, 0.0665, 0.0818, 0.0799, 0.0928, 0.0765, 0.0011,\n                          0.0717, 0.0400, 0.0736, 0.0589, 0.0643, 0.0678, 0.1090, 0.0591, 0.0563,\n                          0.0776, 0.0740, 0.0782, 0.0714, 0.0700, 0.0408, 0.0915, 0.0520, 0.0712,\n                          0.0485, 0.0743, 0.0810, 0.0628, 0.0736, 0.0819, 0.0808, 0.1287, 0.0549,\n                          0.0626, 0.0643, 0.0617, 0.0956, 0.0653, 0.0799, 0.0457, 0.0923, 0.0609,\n                          0.0959, 0.0673, 0.0415, 0.0765, 0.0596, 0.0816, 0.0505, 0.0716, 0.1329,\n                          0.0783, 0.0618, 0.0552, 0.0740, 0.1399, 0.0552, 0.0444, 0.0647, 0.0790,\n                          0.0841, 0.0604, 0.0831, 0.0525, 0.0977, 0.0824, 0.0621, 0.0564, 0.0733,\n                          0.0765, 0.0922, 0.0636, 0.0790, 0.0541, 0.0671, 0.0987, 0.0767, 0.0776,\n                          0.1158, 0.0533, 0.0411, 0.0943, 0.0997, 0.0928, 0.0598, 0.0717, 0.0918,\n                          0.0947, 0.0806, 0.0881, 0.0869, 0.0501, 0.0725, 0.0999, 0.0655, 0.0818,\n                          0.0597, 0.0753, 0.0904, 0.0437, 0.0574, 0.0704, 0.0821, 0.0837, 0.0322,\n                          0.0876, 0.0702, 0.1026, 0.0813, 0.0664, 0.0552, 0.0739, 0.0677, 0.0526,\n                          0.0947, 0.0589, 0.0707, 0.0650, 0.0897, 0.0568, 0.0480, 0.0726, 0.0751,\n                          0.1105, 0.0633, 0.0641, 0.0671, 0.0935, 0.0850, 0.1034, 0.0997, 0.0932,\n                          0.0924, 0.0884, 0.0868, 0.0461, 0.0672, 0.0570, 0.0576, 0.0761, 0.0555,\n                          0.0904, 0.0919, 0.0637, 0.0732, 0.0483, 0.0572, 0.1121, 0.0628, 0.0754,\n                          0.0840, 0.0730, 0.0728, 0.0528, 0.0855, 0.0894, 0.1018, 0.0862, 0.0645,\n                          0.0698, 0.0707, 0.0981, 0.0914, 0.0793, 0.0662, 0.0714, 0.0956, 0.0707,\n                          0.0453, 0.0824, 0.0741, 0.0578, 0.0832, 0.0838, 0.0367, 0.0878, 0.0792,\n                          0.0988, 0.0423, 0.0738, 0.0513, 0.0821, 0.1073, 0.0788, 0.1084, 0.0800,\n                          0.0342, 0.0734, 0.0631, 0.1108, 0.0757, 0.0907, 0.0663, 0.0827, 0.0845,\n                          0.0756, 0.0822, 0.0746, 0.0785, 0.0452, 0.0545, 0.0625, 0.0848, 0.0924,\n                          0.0476, 0.0984, 0.1106, 0.0996, 0.1006, 0.0744, 0.0443, 0.1003, 0.0695,\n                          0.0502, 0.0642, 0.1089, 0.0691, 0.0896, 0.0765, 0.0540, 0.0754, 0.0472,\n                          0.0993, 0.0934, 0.0941, 0.0858, 0.0856, 0.0688, 0.0674, 0.0486, 0.0656,\n                          0.0873, 0.0563, 0.1274, 0.0856, 0.0915, 0.0843, 0.0583, 0.0904, 0.0504,\n                          0.0446, 0.0702, 0.0846, 0.0473, 0.0516, 0.0628, 0.1115, 0.0855, 0.0943,\n                          0.0662, 0.0661, 0.0773, 0.0714, 0.0416, 0.0934, 0.0588, 0.0734, 0.0661,\n                          0.0628, 0.0941, 0.0859, 0.0821, 0.0466, 0.0618, 0.0486, 0.0884, 0.1043,\n                          0.0961, 0.0801, 0.0582, 0.0914, 0.0796, 0.0787, 0.0858, 0.0396, 0.0705,\n                          0.0570, 0.0772, 0.0477, 0.0777, 0.0611, 0.0627, 0.0819, 0.1100, 0.0753,\n                          0.0747, 0.0609, 0.0524, 0.0692, 0.0670, 0.0709, 0.0639, 0.0778, 0.0530,\n                          0.0695, 0.0717, 0.0852, 0.0803, 0.0827, 0.0876, 0.0736, 0.0806, 0.0695,\n                          0.0650, 0.0806, 0.0818, 0.0931, 0.0702, 0.0454, 0.1443, 0.0983, 0.0677,\n                          0.0727, 0.0526, 0.0587, 0.0787, 0.0517, 0.1038, 0.0509, 0.0752, 0.0662,\n                          0.0552, 0.0636, 0.0830, 0.0582, 0.0636, 0.0419, 0.0605, 0.0633, 0.0639,\n                          0.0625, 0.0809, 0.0475, 0.0838, 0.0725, 0.1302, 0.0684, 0.0796, 0.0499,\n                          0.0768, 0.0754, 0.0523, 0.0636, 0.0755, 0.0538, 0.0705, 0.0563, 0.0793,\n                          0.0474, 0.0950, 0.0960, 0.0397, 0.0683, 0.0847, 0.0680, 0.0736, 0.0516,\n                          0.1024, 0.0632, 0.0518, 0.0778, 0.0473, 0.0478, 0.0588, 0.0578, 0.0534,\n                          0.0557, 0.0636, 0.0807, 0.0731, 0.0841, 0.0833, 0.0755, 0.1096, 0.0816,\n                          0.0707, 0.0375, 0.0555, 0.0917, 0.0731, 0.0657, 0.0452, 0.0851, 0.0639,\n                          0.0433, 0.0340, 0.0749, 0.0833, 0.0919, 0.0583, 0.0811, 0.0937, 0.0465,\n                          0.0535, 0.0700, 0.0572, 0.0818, 0.0656, 0.0851, 0.0559, 0.0545, 0.0476,\n                          0.0966, 0.0802, 0.0721, 0.0941, 0.0590, 0.0714, 0.0857, 0.0309, 0.0454,\n                          0.0533, 0.0287, 0.0312, 0.0932, 0.0009, 0.0645, 0.0540, 0.1455, 0.0946,\n                          0.1121, 0.0620, 0.0771, 0.0874, 0.0656, 0.0581, 0.0576, 0.0569, 0.0521,\n                          0.0934, 0.0719, 0.0724, 0.0935, 0.0498, 0.0721, 0.0872, 0.0675, 0.0713,\n                          0.0814, 0.0683, 0.1117, 0.0843, 0.0767, 0.0857, 0.0734, 0.0530, 0.0864,\n                          0.1165, 0.0822, 0.0920, 0.0758, 0.1138, 0.0598, 0.0490, 0.0820, 0.0837,\n                          0.0841, 0.0523, 0.0931, 0.0812, 0.0789, 0.0769, 0.0608, 0.0936, 0.0772,\n                          0.0537, 0.1376, 0.0916, 0.0442, 0.0269, 0.0802, 0.0666, 0.1051, 0.1115,\n                          0.0991, 0.0337, 0.0850, 0.0423, 0.0644, 0.0928, 0.0525, 0.0588, 0.0602,\n                          0.0777, 0.1148, 0.0878, 0.1003, 0.0613, 0.0654, 0.0508, 0.0890, 0.0721,\n                          0.0498, 0.0451, 0.0920, 0.0564, 0.0818, 0.0882, 0.0753, 0.1409, 0.0941,\n                          0.0793, 0.0738, 0.0850, 0.0894, 0.0452, 0.0800, 0.0607, 0.0711, 0.0891,\n                          0.0684, 0.0738, 0.0779, 0.0718, 0.1398, 0.0603, 0.0959, 0.0981, 0.1049,\n                          0.0829, 0.0608, 0.0890, 0.0475, 0.0665, 0.0641, 0.0702, 0.0966, 0.0713,\n                          0.0855, 0.0902, 0.0880, 0.0594, 0.0476, 0.0430, 0.0946, 0.0901, 0.0524,\n                          0.0509, 0.0701, 0.0719, 0.0320, 0.0870, 0.0546, 0.0812, 0.0661, 0.0850,\n                          0.0510, 0.0901, 0.0808, 0.0868, 0.0949, 0.0620, 0.0786, 0.0925, 0.0812,\n                          0.0639, 0.0648, 0.0815, 0.0569, 0.0927, 0.0704])\n                )\n              )\n            )\n            (2): ReLU6(inplace=True)\n          )\n          (1): Module(\n            (0): ConvBn2d(\n              960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False\n              (bn): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n                fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0212, 0.0092, 0.0174, 0.0028, 0.0063, 0.0055, 0.0135, 0.0190, 0.0046,\n                        0.0099, 0.0080, 0.0018, 0.0163, 0.0113, 0.0023, 0.0135, 0.0088, 0.0020,\n                        0.0167, 0.0022, 0.0018, 0.0123, 0.0121, 0.0116, 0.0096, 0.0108, 0.0014,\n                        0.0043, 0.0043, 0.0128, 0.0148, 0.0106, 0.0112, 0.0055, 0.0137, 0.0447,\n                        0.0135, 0.0043, 0.0069, 0.0104, 0.0066, 0.0019, 0.0167, 0.0159, 0.0084,\n                        0.0149, 0.0063, 0.0039, 0.0040, 0.0110, 0.0018, 0.0025, 0.0039, 0.0101,\n                        0.0064, 0.0103, 0.0137, 0.0022, 0.0078, 0.0170, 0.0116, 0.0184, 0.0090,\n                        0.0054, 0.0134, 0.0123, 0.0070, 0.0113, 0.0027, 0.0025, 0.0020, 0.0085,\n                        0.0016, 0.0107, 0.0154, 0.0029, 0.0038, 0.0139, 0.0038, 0.0110, 0.0002,\n                        0.0122, 0.0141, 0.0066, 0.0029, 0.0077, 0.0097, 0.0107, 0.0022, 0.0148,\n                        0.0022, 0.0035, 0.0089, 0.0030, 0.0024, 0.0107, 0.0053, 0.0096, 0.0072,\n                        0.0104, 0.0025, 0.0089, 0.0022, 0.0020, 0.0022, 0.0043, 0.0026, 0.0058,\n                        0.0123, 0.0178, 0.0086, 0.0023, 0.0039, 0.0173, 0.0095, 0.0083, 0.0102,\n                        0.0243, 0.0077, 0.0075, 0.0018, 0.0101, 0.0104, 0.0126, 0.0263, 0.0142,\n                        0.0048, 0.0078, 0.0089, 0.0066, 0.0024, 0.0118, 0.0058, 0.0094, 0.0095,\n                        0.0024, 0.0186, 0.0170, 0.0020, 0.0060, 0.0057, 0.0059, 0.0025, 0.0027,\n                        0.0020, 0.0211, 0.0076, 0.0019, 0.0029, 0.0099, 0.0093, 0.0088, 0.0103,\n                        0.0221, 0.0100, 0.0019, 0.0021, 0.0050, 0.0064, 0.0149, 0.0019, 0.0068,\n                        0.0016, 0.0019, 0.0201, 0.0025, 0.0149, 0.0089, 0.0028, 0.0081, 0.0022,\n                        0.0019, 0.0067, 0.0013, 0.0047, 0.0133, 0.0137, 0.0020, 0.0049, 0.0295,\n                        0.0125, 0.0097, 0.0156, 0.0030, 0.0022, 0.0024, 0.0021, 0.0129, 0.0038,\n                        0.0025, 0.0229, 0.0020, 0.0015, 0.0016, 0.0021, 0.0091, 0.0025, 0.0114,\n                        0.0105, 0.0023, 0.0174, 0.0084, 0.0154, 0.0060, 0.0094, 0.0048, 0.0093,\n                        0.0236, 0.0185, 0.0081, 0.0072, 0.0065, 0.0023, 0.0023, 0.0093, 0.0025,\n                        0.0025, 0.0166, 0.0160, 0.0082, 0.0074, 0.0100, 0.0052, 0.0017, 0.0246,\n                        0.0016, 0.0016, 0.0065, 0.0042, 0.0061, 0.0089, 0.0029, 0.0136, 0.0216,\n                        0.0022, 0.0016, 0.0041, 0.0063, 0.0099, 0.0146, 0.0055, 0.0018, 0.0030,\n                        0.0028, 0.0070, 0.0065, 0.0204, 0.0076, 0.0028, 0.0106, 0.0053, 0.0136,\n                        0.0301, 0.0021, 0.0029, 0.0018, 0.0109, 0.0064, 0.0063, 0.0035, 0.0274,\n                        0.0091, 0.0080, 0.0049, 0.0028, 0.0146, 0.0026, 0.0018, 0.0024, 0.0030,\n                        0.0016, 0.0024, 0.0044, 0.0106, 0.0041, 0.0039, 0.0144, 0.0113, 0.0159,\n                        0.0140, 0.0017, 0.0137, 0.0088, 0.0129, 0.0024, 0.0098, 0.0107, 0.0023,\n                        0.0076, 0.0033, 0.0095, 0.0082, 0.0105, 0.0073, 0.0090, 0.0026, 0.0016,\n                        0.0082, 0.0022, 0.0063, 0.0102, 0.0020, 0.0105, 0.0026, 0.0016, 0.0181,\n                        0.0151, 0.0027, 0.0086, 0.0055, 0.0189, 0.0083, 0.0130, 0.0058, 0.0175,\n                        0.0230, 0.0125, 0.0018, 0.0020, 0.0171, 0.0027, 0.0025, 0.0166, 0.0018,\n                        0.0046, 0.0045, 0.0106, 0.0384, 0.0046, 0.0109, 0.0130, 0.0213, 0.0116,\n                        0.0020, 0.0100, 0.0073, 0.0019, 0.0030, 0.0022, 0.0102, 0.0110, 0.0557,\n                        0.0067, 0.0168, 0.0152, 0.0023, 0.0101, 0.0067, 0.0010, 0.0139, 0.0085,\n                        0.0051, 0.0113, 0.0019, 0.0199, 0.0027, 0.0109, 0.0053, 0.0023, 0.0100,\n                        0.0016, 0.0056, 0.0019, 0.0127, 0.0070, 0.0028, 0.0025, 0.0121, 0.0084,\n                        0.0094, 0.0150, 0.0169, 0.0018, 0.0116, 0.0239, 0.0118, 0.0044, 0.0039,\n                        0.0059, 0.0105, 0.0174, 0.0050, 0.0124, 0.0021, 0.0074, 0.0095, 0.0138,\n                        0.0022, 0.0085, 0.0232, 0.0047, 0.0126, 0.0051, 0.0095, 0.0128, 0.0216,\n                        0.0135, 0.0147, 0.0017, 0.0017, 0.0015, 0.0020, 0.0046, 0.0045, 0.0004,\n                        0.0023, 0.0188, 0.0066, 0.0039, 0.0160, 0.0021, 0.0035, 0.0036, 0.0100,\n                        0.0108, 0.0052, 0.0017, 0.0028, 0.0170, 0.0162, 0.0234, 0.0095, 0.0073,\n                        0.0060, 0.0022, 0.0083, 0.0190, 0.0147, 0.0110, 0.0050, 0.0050, 0.0094,\n                        0.0089, 0.0103, 0.0147, 0.0018, 0.0037, 0.0045, 0.0172, 0.0027, 0.0124,\n                        0.0036, 0.0096, 0.0132, 0.0022, 0.0140, 0.0127, 0.0110, 0.0147, 0.0066,\n                        0.0025, 0.0121, 0.0298, 0.0110, 0.0039, 0.0125, 0.0056, 0.0141, 0.0045,\n                        0.0069, 0.0016, 0.0023, 0.0053, 0.0181, 0.0134, 0.0085, 0.0127, 0.0085,\n                        0.0075, 0.0028, 0.0026, 0.0019, 0.0104, 0.0129, 0.0022, 0.0104, 0.0023,\n                        0.0025, 0.0133, 0.0157, 0.0219, 0.0023, 0.0016, 0.0104, 0.0038, 0.0023,\n                        0.0020, 0.0059, 0.0029, 0.0051, 0.0154, 0.0021, 0.0070, 0.0084, 0.0021,\n                        0.0123, 0.0029, 0.0020, 0.0110, 0.0026, 0.0176, 0.0024, 0.0019, 0.0295,\n                        0.0044, 0.0024, 0.0021, 0.0021, 0.0066, 0.0118, 0.0020, 0.0017, 0.0059,\n                        0.0131, 0.0022, 0.0103, 0.0247, 0.0044, 0.0058, 0.0075, 0.0030, 0.0081,\n                        0.0019, 0.0018, 0.0062, 0.0058, 0.0018, 0.0054, 0.0024, 0.0018, 0.0022,\n                        0.0051, 0.0025, 0.0148, 0.0099, 0.0339, 0.0024, 0.0074, 0.0083, 0.0116,\n                        0.0065, 0.0057, 0.0131, 0.0077, 0.0084, 0.0142, 0.0024, 0.0125, 0.0083,\n                        0.0026, 0.0093, 0.0021, 0.0099, 0.0055, 0.0061, 0.0021, 0.0051, 0.0017,\n                        0.0067, 0.0270, 0.0062, 0.0061, 0.0023, 0.0110, 0.0139, 0.0015, 0.0090,\n                        0.0139, 0.0071, 0.0021, 0.0107, 0.0107, 0.0055, 0.0154, 0.0038, 0.0059,\n                        0.0058, 0.0147, 0.0028, 0.0237, 0.0085, 0.0097, 0.0092, 0.0094, 0.0099,\n                        0.0212, 0.0083, 0.0028, 0.0015, 0.0142, 0.0060, 0.0071, 0.0110, 0.0152,\n                        0.0026, 0.0139, 0.0085, 0.0081, 0.0148, 0.0081, 0.0043, 0.0020, 0.0026,\n                        0.0277, 0.0024, 0.0121, 0.0181, 0.0106, 0.0195, 0.0068, 0.0028, 0.0065,\n                        0.0125, 0.0063, 0.0155, 0.0142, 0.0022, 0.0025, 0.0024, 0.0092, 0.0142,\n                        0.0107, 0.0105, 0.0056, 0.0092, 0.0128, 0.0113, 0.0057, 0.0145, 0.0025,\n                        0.0041, 0.0576, 0.0053, 0.0055, 0.0133, 0.0066, 0.0199, 0.0112, 0.0078,\n                        0.0225, 0.0024, 0.0105, 0.0131, 0.0158, 0.0087, 0.0082, 0.0105, 0.0242,\n                        0.0163, 0.0021, 0.0043, 0.0050, 0.0158, 0.0046, 0.0134, 0.0103, 0.0036,\n                        0.0058, 0.0051, 0.0068, 0.0087, 0.0070, 0.0154, 0.0171, 0.0073, 0.0023,\n                        0.0101, 0.0055, 0.0182, 0.0092, 0.0032, 0.0117, 0.0019, 0.0186, 0.0064,\n                        0.0038, 0.0078, 0.0073, 0.0022, 0.0094, 0.0100, 0.0019, 0.0096, 0.0145,\n                        0.0019, 0.0086, 0.0155, 0.0023, 0.0121, 0.0145, 0.0113, 0.0035, 0.0057,\n                        0.0023, 0.0158, 0.0042, 0.0204, 0.0051, 0.0031, 0.0020, 0.0020, 0.0186,\n                        0.0115, 0.0061, 0.0101, 0.0110, 0.0154, 0.0221, 0.0104, 0.0021, 0.0129,\n                        0.0020, 0.0080, 0.0032, 0.0079, 0.0029, 0.0020, 0.0173, 0.0116, 0.0048,\n                        0.0155, 0.0095, 0.0024, 0.0154, 0.0027, 0.0132, 0.0092, 0.0120, 0.0074,\n                        0.0068, 0.0063, 0.0197, 0.0017, 0.0059, 0.0125, 0.0158, 0.0070, 0.0366,\n                        0.0129, 0.0021, 0.0040, 0.0021, 0.0111, 0.0162, 0.0121, 0.0120, 0.0019,\n                        0.0239, 0.0085, 0.0119, 0.0037, 0.0171, 0.0103, 0.0022, 0.0094, 0.0228,\n                        0.0032, 0.0092, 0.0228, 0.0127, 0.0029, 0.0177, 0.0050, 0.0033, 0.0155,\n                        0.0219, 0.0113, 0.0040, 0.0035, 0.0109, 0.0103, 0.0019, 0.0123, 0.0044,\n                        0.0137, 0.0130, 0.0035, 0.0057, 0.0022, 0.0139, 0.0086, 0.0020, 0.0187,\n                        0.0144, 0.0188, 0.0072, 0.0097, 0.0090, 0.0025, 0.0093, 0.0019, 0.0052,\n                        0.0107, 0.0019, 0.0153, 0.0110, 0.0055, 0.0046, 0.0058, 0.0159, 0.0141,\n                        0.0021, 0.0018, 0.0066, 0.0095, 0.0082, 0.0094, 0.0020, 0.0166, 0.0085,\n                        0.0115, 0.0183, 0.0304, 0.0019, 0.0004, 0.0148, 0.0128, 0.0097, 0.0018,\n                        0.0017, 0.0034, 0.0212, 0.0029, 0.0056, 0.0147, 0.0045, 0.0088, 0.0122,\n                        0.0048, 0.0052, 0.0085, 0.0022, 0.0172, 0.0058, 0.0087, 0.0032, 0.0158,\n                        0.0056, 0.0052, 0.0044, 0.0159, 0.0142, 0.0030, 0.0139, 0.0304, 0.0072,\n                        0.0075, 0.0051, 0.0014, 0.0065, 0.0087, 0.0036, 0.0099, 0.0040, 0.0135,\n                        0.0021, 0.0089, 0.0029, 0.0132, 0.0085, 0.0121, 0.0120, 0.0043, 0.0019,\n                        0.0125, 0.0023, 0.0018, 0.0140, 0.0521, 0.0064, 0.0096, 0.0084, 0.0045,\n                        0.0048, 0.0381, 0.0244, 0.0197, 0.0109, 0.0021, 0.0092, 0.0123, 0.0027,\n                        0.0027, 0.0067, 0.0052, 0.0028, 0.0025, 0.0128, 0.0086, 0.0021, 0.0025,\n                        0.0277, 0.0088, 0.0043, 0.0101, 0.0114, 0.0088, 0.0085, 0.0097, 0.0018,\n                        0.0090, 0.0024, 0.0071, 0.0058, 0.0138, 0.0164, 0.0129, 0.0048, 0.0085,\n                        0.0073, 0.0019, 0.0121, 0.0097, 0.0061, 0.0099, 0.0043, 0.0128, 0.0046,\n                        0.0098, 0.0260, 0.0054, 0.0069, 0.0098, 0.0097, 0.0026, 0.0118, 0.0108,\n                        0.0066, 0.0021, 0.0018, 0.0176, 0.0057, 0.0261, 0.0121, 0.0019, 0.0122,\n                        0.0113, 0.0016, 0.0019, 0.0151, 0.0015, 0.0110, 0.0023, 0.0067, 0.0081,\n                        0.0131, 0.0109, 0.0023, 0.0060, 0.0018, 0.0083, 0.0137, 0.0023, 0.0037,\n                        0.0036, 0.0092, 0.0091, 0.0174, 0.0090, 0.0065]), zero_point=tensor([   0, -128,    0,  127, -128, -128,    0,    0,  127,    0,    0,  127,\n                           0, -128,  127,    0,  127,  127,    0,  127,  127,    0,    0,    0,\n                           0,    0,  127,  127, -128,    0,    0,    0,    0, -128,    0,    0,\n                           0, -128,    0,    0, -128,  127,    0,    0,    0,    0, -128,    0,\n                           0,    0,  127,  127, -128,    0, -128,    0,    0,  127,  127,    0,\n                           0,    0, -128, -128,    0,    0, -128,    0,  127,  127,  127,    0,\n                         127,    0, -128,  127, -128,    0, -128,    0,    0,    0,    0, -128,\n                         127, -128, -128,    0,  127,    0,  127,  127, -128,  127,  127, -128,\n                        -128,    0, -128,    0,  127,    0,  127,  127,  127,    0,  127,    0,\n                           0,    0,    0,  127, -128,    0,  127,    0,    0,    0, -128, -128,\n                         127,    0,    0, -128,    0,    0,    0,    0,    0,  127,  127, -128,\n                         127,    0,    0,  127,    0,    0,  127, -128, -128, -128,  127,  127,\n                         127,    0,  127,  127,  127,    0,    0,    0,    0,    0,    0,  127,\n                         127,    0, -128,    0,  127, -128,  127,  127,    0,  127,    0, -128,\n                         127,    0,  127,  127, -128,  127, -128,    0,    0,  127,    0,    0,\n                           0,    0,    0,  127,  127,  127,  127,    0, -128,  127,    0,  127,\n                         127,  127,  127,    0,  127,    0,    0,  127,    0, -128,    0, -128,\n                           0, -128, -128,    0,    0,    0, -128, -128,  127,  127,    0,  127,\n                         127,    0,    0,    0, -128,    0,  127,  127,    0,  127,  127, -128,\n                         127, -128,    0, -128,    0,    0,  127,  127,    0,    0,    0,    0,\n                        -128,  127,  127,  127,  127, -128,    0,    0,  127,    0,    0,    0,\n                           0,  127,  127,  127, -128, -128,  127,  127,    0, -128, -128,    0,\n                         127, -128,  127,  127,  127,  127,  127,  127,    0, -128, -128,    0,\n                           0,    0,    0,    0,  127,    0,    0,    0,  127,    0,    0,  127,\n                           0,  127,    0,  127, -128, -128, -128,  127,  127,    0,  127,  127,\n                           0,  127,    0,  127,  127,    0,    0,  127,    0, -128,    0,  127,\n                           0, -128,    0,    0,    0,  127,  127,    0,  127,  127,    0,  127,\n                           0, -128,    0,    0, -128,    0,    0,    0, -128,  127,    0, -128,\n                         127,  127,  127,    0,    0,    0, -128,    0,    0,  127,    0, -128,\n                           0,    0,    0, -128,    0,  127,    0,  127,    0, -128,  127,    0,\n                         127, -128,  127,    0,  127,  127,  127,    0, -128,    0,    0,    0,\n                         127,    0,    0,    0,  127, -128,    0,    0, -128, -128,    0,  127,\n                           0,    0,    0,  127,    0,    0, -128, -128, -128, -128,    0,    0,\n                           0,    0,  127,  127,  127,  127,    0, -128,    0,  127,    0, -128,\n                        -128,    0,  127, -128,  127,    0,    0, -128,  127,  127,    0,    0,\n                           0,    0, -128, -128,  127,    0,    0,    0,    0, -128, -128,  127,\n                           0,    0,    0,  127,  127,    0,    0,  127, -128,  127,    0,    0,\n                         127,    0,    0, -128,    0, -128,  127,    0,    0,    0,    0,    0,\n                        -128,    0, -128, -128,  127,  127,  127,    0,    0, -128,    0,    0,\n                        -128,  127,  127,  127,    0,    0,  127, -128,  127,  127,    0,    0,\n                           0,  127,  127, -128,  127,  127,  127, -128,  127, -128,    0,  127,\n                           0,    0,  127,    0,  127,  127, -128,  127,    0,  127,  127,    0,\n                        -128,  127,  127,  127, -128,    0,  127,  127,  127,    0,  127,    0,\n                           0,  127, -128, -128,  127, -128,  127,  127, -128, -128,  127, -128,\n                         127,  127,  127, -128,  127,    0,    0,    0,  127, -128,    0,    0,\n                           0, -128,    0,    0, -128,    0,  127,    0,    0,  127,    0,  127,\n                        -128, -128, -128,  127, -128,  127,    0,    0, -128, -128,  127,    0,\n                           0,  127,    0,    0,    0,  127,    0,    0, -128,    0, -128, -128,\n                        -128,    0,  127,    0,    0,    0,    0,    0,    0,    0, -128,  127,\n                         127,    0, -128, -128,    0,    0,  127,    0,    0,    0,    0,  127,\n                        -128,  127,  127,    0,  127,    0,    0,    0,    0,  127,  127,    0,\n                           0, -128,    0,    0,  127,  127,  127, -128,    0, -128,    0, -128,\n                           0,    0,    0,    0,    0,  127, -128,    0, -128, -128,    0, -128,\n                           0, -128,    0,    0,  127,    0,    0,    0,    0,    0,    0,    0,\n                           0,  127, -128, -128,    0, -128,    0,    0,  127,  127, -128, -128,\n                           0,  127,    0, -128,    0,  127, -128, -128,    0,    0,  127,    0,\n                         127,    0, -128,  127,    0, -128,  127,    0,    0,  127,    0,    0,\n                         127,    0,    0,  127,    0,    0,    0,  127,  127,  127,    0,    0,\n                           0,  127, -128,  127,  127,    0,    0,    0,    0,    0,    0,    0,\n                           0,  127,    0,  127,    0,  127,    0,  127,  127,    0,    0, -128,\n                           0, -128,  127,    0,  127,    0, -128, -128, -128, -128, -128,    0,\n                         127, -128,    0,    0,    0,    0,    0,  127, -128,  127,    0,    0,\n                           0,    0,  127,    0,    0,    0, -128,    0,    0,  127,    0,    0,\n                         127,    0,    0,    0,  127,    0,    0,  127,    0,    0,    0,  127,\n                         127,    0,    0,  127, -128, -128,    0, -128, -128, -128,  127,    0,\n                         127,  127,    0,  127,    0, -128,    0,    0,  127, -128,  127,    0,\n                           0,  127,    0,    0, -128, -128, -128,    0,    0,  127,  127, -128,\n                           0,  127,    0,  127, -128,    0,    0, -128,    0,  127,    0,    0,\n                           0,    0,  127,  127,  127,    0,  127, -128,    0,    0,    0,    0,\n                        -128, -128,    0,  127,    0, -128,    0,  127,    0,    0,    0, -128,\n                           0,    0,  127,    0,    0, -128, -128, -128,  127, -128,  127,  127,\n                           0,  127,    0,  127, -128,  127,    0,    0,    0,    0,    0,  127,\n                           0,  127,  127,    0,    0, -128,    0, -128, -128, -128,    0,    0,\n                           0,    0,  127,    0,    0,  127,  127, -128,    0,  127,  127,    0,\n                           0,  127,  127,    0, -128, -128,    0, -128, -128,    0,    0,  127,\n                           0,  127, -128,    0,    0,    0,    0,    0, -128,    0,  127,    0,\n                           0, -128,    0, -128,    0, -128, -128,    0, -128, -128, -128,    0,\n                         127,    0,    0, -128,  127,  127,    0, -128,    0,    0,  127,    0,\n                           0,  127,  127, -128,  127,    0,  127, -128,    0,    0,    0,  127,\n                        -128,  127, -128,    0,  127, -128, -128,    0,    0,    0,    0, -128],\n                       dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n                (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n                  min_val=tensor([-1.3552e+00,  2.4548e-01, -1.1460e+00, -7.1093e-01,  3.7480e-02,\n                           7.0885e-02, -1.7307e+00, -5.4313e-01, -1.1717e+00, -1.2611e+00,\n                          -1.0189e+00, -4.6067e-01, -9.8441e-01,  1.2047e-01, -5.9346e-01,\n                          -1.7301e+00, -2.2433e+00, -5.0069e-01, -8.8032e-01, -5.6319e-01,\n                          -4.4822e-01, -1.2402e+00, -5.4910e-02, -1.0882e+00, -1.2342e+00,\n                          -1.0915e+00, -3.5199e-01, -1.0969e+00,  8.2470e-03, -1.6395e+00,\n                          -1.2970e-01, -4.8086e-03, -1.0623e+00,  1.0728e-01, -1.7529e+00,\n                          -2.9885e-02, -1.2279e-01,  1.9194e-01, -8.6788e-01, -1.3293e+00,\n                           1.5185e-01, -4.9025e-01, -2.2491e-01, -4.0219e-01, -1.8806e-02,\n                          -5.2668e-02,  1.0080e-02, -4.9786e-01, -5.1449e-01, -2.3862e-02,\n                          -4.5668e-01, -6.3902e-01,  2.0354e-02, -1.2200e+00,  4.4451e-02,\n                          -1.0550e+00, -1.0619e+00, -5.6900e-01, -1.9893e+00, -1.2304e+00,\n                          -1.7466e-02, -2.3598e+00,  7.7789e-02,  1.7874e-01, -2.6721e-02,\n                          -8.4118e-01,  3.4975e-02, -3.0257e-01, -6.7757e-01, -6.2665e-01,\n                          -5.2138e-01, -7.4051e-01, -3.9575e-01, -9.4730e-02,  1.8356e-01,\n                          -7.2985e-01,  1.8643e-01, -9.8854e-01,  1.9073e-01, -1.0758e-01,\n                          -3.3367e-03, -2.1629e-02, -1.8063e+00,  1.2307e-01, -7.3990e-01,\n                           2.4612e-01,  1.2586e-01, -2.0065e-01, -5.6951e-01, -1.1816e+00,\n                          -5.7351e-01, -9.0200e-01,  2.4671e-01, -7.5838e-01, -6.2022e-01,\n                           3.3973e-01,  9.5823e-02, -7.6644e-01,  4.1471e-02, -1.7776e-01,\n                          -6.4144e-01, -3.6794e-02, -5.5408e-01, -5.0478e-01, -5.6776e-01,\n                          -5.4557e-01, -6.6986e-01, -7.4669e-01, -1.2213e+00, -5.9869e-01,\n                          -2.6743e-01, -5.8522e-01,  4.9267e-02, -4.7682e-01, -2.4225e+00,\n                          -1.0619e+00, -1.0004e+00, -3.1112e+00,  2.2858e-01,  1.3319e-01,\n                          -4.5231e-01, -9.1573e-01, -4.7525e-01,  1.3972e-01, -1.5511e+00,\n                          -5.3283e-02, -6.1217e-01, -9.9831e-01, -3.0112e-02, -1.6717e+00,\n                          -6.2273e-01,  4.3143e-01, -1.4777e+00, -6.5547e-01, -8.9246e-01,\n                          -6.1107e-01, -7.9704e-01, -6.3808e-02, -5.0525e-01,  8.4176e-02,\n                           1.0727e-01,  6.9671e-02, -6.2985e-01, -6.7758e-01, -5.0780e-01,\n                          -1.3163e+00, -1.9394e+00, -4.8271e-01, -7.4586e-01, -1.2675e+00,\n                          -1.1642e+00, -3.1042e-01, -9.6212e-01, -5.0702e-02, -1.0796e-01,\n                          -4.8907e-01, -5.4597e-01, -6.4460e-01,  7.4639e-02, -1.7244e+00,\n                          -4.7324e-01,  3.5051e-01, -3.9680e-01, -4.7249e-01, -2.5742e+00,\n                          -6.3607e-01, -1.5829e+00,  2.1618e-01, -7.2408e-01, -6.0528e-01,\n                          -5.7145e-01, -4.8569e-01,  7.6486e-03, -3.3879e-01,  1.2930e-01,\n                          -1.7308e-01, -1.7527e+00, -5.1743e-01, -6.2669e-01, -2.0585e+00,\n                          -1.6060e+00, -9.0428e-01, -4.1796e-01, -7.6314e-01, -5.6118e-01,\n                          -6.0033e-01, -5.3995e-01, -1.6464e+00,  1.8473e-01, -6.2477e-01,\n                          -2.1487e+00, -5.0151e-01, -3.8578e-01, -4.1906e-01, -5.2299e-01,\n                          -1.1651e+00, -6.2589e-01, -1.3350e+00, -9.4607e-01, -5.8755e-01,\n                          -2.2279e+00,  3.5935e-01, -5.4045e-01,  3.2746e-01, -1.1982e+00,\n                           8.8168e-03,  1.5426e-01, -1.1208e+00, -9.6247e-01, -1.0316e+00,\n                           5.3489e-02,  1.1668e-01, -5.7950e-01, -5.9475e-01, -7.9509e-02,\n                          -6.3543e-01, -6.3863e-01, -7.2489e-01, -3.0286e-01, -1.1133e-01,\n                           3.3817e-02, -1.0882e+00, -1.3140e+00, -4.2161e-01, -3.1442e+00,\n                          -4.0369e-01, -4.0706e-01,  1.1747e-01, -1.0614e+00,  1.1159e-01,\n                          -7.9763e-01,  2.5213e-01, -1.0956e+00, -2.7654e+00, -5.6716e-01,\n                          -4.0292e-01, -5.2552e-01, -8.1230e-01, -4.7100e-01, -2.1572e-01,\n                           2.1822e-01, -4.6186e-01, -7.6104e-01, -7.2644e-01, -1.7874e+00,\n                           1.6255e-01, -9.9143e-01, -7.3649e-01, -7.0945e-01, -1.3475e+00,\n                          -6.8219e-01, -9.2733e-01, -9.2823e-01, -5.2500e-01, -7.3748e-01,\n                          -4.6764e-01,  1.3642e-01,  3.3097e-01, -1.6071e+00, -8.9023e-01,\n                          -2.4870e+00,  8.1425e-02,  8.1950e-02, -6.2872e-01, -7.2054e-01,\n                           7.5628e-02, -6.6333e-01, -4.6745e-01, -6.2459e-01, -7.5260e-01,\n                          -4.0895e-01, -6.1649e-01, -5.4021e-01,  1.3085e-01,  5.2619e-02,\n                          -5.0421e-01, -1.8428e+00, -3.2777e-01, -9.5468e-01, -1.7982e+00,\n                          -4.4125e-01, -9.3975e-01, -1.7610e-01, -8.0558e-02, -6.2274e-01,\n                          -8.7101e-01, -6.0045e-01, -5.8329e-01, -2.3009e-02, -8.4777e-01,\n                          -9.1642e-03, -2.0953e+00,  1.4227e-01,  1.0317e-01,  2.1320e-01,\n                          -6.7100e-01, -4.0881e-01, -1.0517e+00, -5.7052e-01, -1.6147e+00,\n                          -4.4928e-01, -5.0449e-01, -1.1296e+00, -6.6776e-01, -4.1195e-01,\n                          -7.1044e-01, -1.1116e+00, -6.7687e-01, -9.8504e-01,  1.5646e-01,\n                          -1.7065e-01, -2.1069e+00, -7.1268e-02,  2.1322e-02, -1.2883e+00,\n                          -3.3081e-01, -1.1298e+00, -4.6707e-01, -5.0001e-01, -4.1614e-01,\n                          -6.9587e-01, -6.4680e-01, -1.5805e+00, -4.7164e-01, -5.9174e-01,\n                           7.7076e-02, -1.3558e+00, -2.7764e-01,  8.3393e-02, -9.9841e-02,\n                          -3.4214e-01, -2.1484e-01,  3.6173e-01, -5.1156e-01, -1.6631e-01,\n                           1.2259e-01, -4.8280e-01, -7.5394e-01, -5.5640e-01, -1.1020e-01,\n                          -1.2021e+00, -7.1235e+00,  2.0373e-01, -1.9192e+00, -6.2730e-01,\n                          -5.9509e-01, -1.2885e+00,  1.1891e-01, -1.4725e-02, -5.6381e-01,\n                          -1.0904e+00,  7.0787e-02, -4.8180e-03, -4.7662e-01, -2.5424e+00,\n                          -6.9304e-01, -9.2879e-01,  1.1759e-01, -5.9877e-01, -1.6044e-02,\n                          -4.0844e-01,  1.1151e-01, -4.9110e-01, -1.6283e+00, -1.7821e+00,\n                          -7.1793e-01, -6.2855e-01, -1.5432e+00,  4.1782e-02, -1.2047e+00,\n                          -8.9712e-01, -1.4651e+00, -4.5528e-01, -2.5971e-01, -3.7341e-02,\n                          -7.2388e-01, -1.1230e+00,  2.8912e-02, -7.5705e-01, -4.1377e-01,\n                           5.4345e-01,  1.3431e-01, -9.6030e-01, -5.3616e-01, -9.1055e-01,\n                          -1.0881e+00, -5.6697e-01, -5.4868e-01, -1.0907e+00, -7.3678e-01,\n                           1.3113e-01,  5.5198e-02,  1.6378e-02,  4.3735e-01, -1.6433e+00,\n                          -3.6145e-01, -1.5167e+00, -1.0150e+00, -4.2980e-01, -4.4569e-01,\n                          -3.7537e-01, -5.1973e-01, -5.8327e-01,  9.5207e-02, -2.7016e-02,\n                          -5.9659e-01, -2.4009e+00,  9.2991e-02,  4.5803e-02, -1.3566e+00,\n                          -5.4177e-01,  3.0421e-02, -9.0981e-01, -1.2544e+00, -9.2323e-02,\n                           7.5671e-02, -4.2513e-01, -7.0721e-01, -8.6177e-01, -1.2072e+00,\n                          -2.4532e-01, -1.2126e+00,  1.8820e-01,  3.4918e-02, -5.6548e-01,\n                          -1.0649e+00, -1.5706e+00, -1.1007e+00, -6.9667e-01,  6.5942e-02,\n                           7.4760e-02, -2.3926e+00, -1.0750e+00, -9.1589e-01, -1.6183e+00,\n                          -4.6770e-01, -9.3700e-01, -5.7925e-01, -6.4290e-01, -6.7643e-01,\n                           9.1088e-02, -9.1058e-01, -4.4440e-02, -1.2117e+00, -5.5672e-01,\n                          -5.0701e-01, -1.2377e+00,  7.8574e-02, -1.1824e+00,  1.1657e-01,\n                          -6.3837e-01, -1.0025e+00, -4.6285e-01, -1.4141e+00, -4.9968e-01,\n                          -1.0232e+00,  5.3926e-02, -1.6581e+00,  2.1936e-02,  8.2577e-02,\n                          -4.0677e-01, -5.7533e-01, -1.3492e+00, -2.3209e+00, -1.0698e+00,\n                           1.8101e-01, -1.2905e+00, -8.8021e-01,  4.9420e-02, -7.2305e-01,\n                          -6.7384e-01, -4.8737e-01, -1.3333e+00, -1.6455e+00, -5.5975e-01,\n                           5.6296e-04, -5.7569e-01, -6.4029e-01, -1.0363e+00, -1.2287e+00,\n                          -1.8373e+00, -5.8330e-01, -4.0000e-01,  3.0374e-01, -9.6873e-01,\n                          -5.9622e-01, -5.1529e-01,  1.3632e-01, -7.4547e-01,  6.1006e-02,\n                          -1.5776e+00, -5.4306e-01, -1.4336e-02, -1.5011e-02, -5.4057e-01,\n                          -1.6615e-01, -7.2734e-01, -5.0526e-01,  2.5113e-01, -6.7047e-01,\n                          -1.2121e+00, -6.1647e-01, -4.8450e-01, -1.5868e-01,  5.7036e-02,\n                          -6.0900e-01, -5.4121e-01, -5.2951e-01,  1.4333e-02, -1.2710e+00,\n                          -5.0663e-01, -4.2855e-01, -1.4967e+00, -1.6718e+00, -5.5659e-01,\n                          -4.7847e-01, -3.2951e-01, -1.1243e+00,  1.5360e-01,  1.1758e-01,\n                          -7.5956e-01,  2.5878e-01, -4.7692e-01, -4.6498e-01,  1.0726e-01,\n                           4.5337e-02, -4.5928e-01,  1.0750e-01, -6.2224e-01, -4.5585e-01,\n                          -5.6088e-01,  4.7728e-02, -6.3007e-01, -9.1718e-02, -1.2652e+00,\n                          -1.4741e+00, -6.1732e-01,  5.5350e-02, -7.5674e-01, -1.4816e+00,\n                          -8.2761e-01,  1.4209e-01, -1.4858e+00, -9.8584e-01,  1.9041e-01,\n                          -1.4709e+00, -6.2055e-01, -7.4072e-01, -1.0653e+00, -6.5834e-01,\n                          -1.7136e-01, -5.4019e-01,  3.1730e-01,  3.6550e-02,  2.4647e-01,\n                          -5.4554e-01,  1.1224e-01, -4.4270e-01, -7.1273e-03, -1.2121e+00,\n                           2.4609e-02,  1.2001e-01, -5.8716e-01, -2.5792e-01, -1.2909e+00,\n                          -3.9273e-01, -1.0829e+00, -9.1077e-01, -9.0499e-01, -5.3614e-01,\n                          -1.2448e+00, -1.2428e+00,  1.2695e-01, -1.0679e+00,  1.5569e-01,\n                           5.0462e-02,  8.4791e-02, -2.3905e-01, -7.0484e-01, -1.2586e-01,\n                          -9.5020e-01, -5.8394e-02, -1.1728e+00, -4.2150e-01, -4.3909e-02,\n                          -3.7409e-01,  7.5816e-02, -7.1989e-01, -3.7029e-01, -1.0858e+00,\n                           6.3213e-02,  8.4005e-02, -1.2950e+00, -1.6420e-01, -6.7434e-01,\n                          -7.1510e-01, -1.0598e+00, -1.0387e+00, -2.5688e-02, -2.0687e+00,\n                           3.8723e-01, -5.1273e-01, -6.6718e-01, -2.7178e+00, -6.1427e-01,\n                          -3.6104e-01, -3.9471e-01, -1.3511e+00, -2.5865e-01, -1.7463e+00,\n                          -7.1094e-01, -8.3441e-01, -8.6906e-01,  7.5428e-02, -1.7134e-01,\n                          -4.8971e-01, -5.6446e-01, -6.3771e-01, -6.1089e-01,  1.7043e-01,\n                          -9.0453e-01,  7.2666e-03, -9.4680e-02,  2.8262e-02, -1.1754e+00,\n                          -1.5580e+00, -1.0884e+00, -7.3498e-01, -1.1586e+00, -6.4561e-01,\n                           1.6138e-01, -2.6701e+00,  2.4025e-01,  2.0102e-01, -1.4425e+00,\n                           1.7382e-01, -1.7480e+00,  1.0185e-01, -9.7241e-01, -1.2369e+00,\n                          -6.1741e-01, -1.3421e+00, -1.0111e+00, -8.6322e-01, -1.1129e+00,\n                          -1.9107e-01, -1.0662e+00, -2.2385e-03, -5.6327e-01, -5.3166e-01,\n                           1.4960e-01,  1.5517e-01, -1.3607e+00,  2.0608e-01, -1.1423e+00,\n                          -5.6651e-01, -9.0982e-01, -1.4914e+00,  3.4412e-02,  9.5511e-02,\n                          -8.0486e-01, -1.7855e+00, -3.0406e-01,  4.1887e-01, -9.3532e-01,\n                          -5.8127e-01,  1.2518e-01,  9.3351e-02, -1.5943e-01, -7.4359e-01,\n                          -8.1176e-01, -9.5930e-01, -4.8917e-01, -1.2914e+00,  1.1816e-02,\n                          -9.7583e-01, -2.6492e-01,  5.6895e-03, -5.5271e-01, -3.4490e-01,\n                          -1.0410e+00, -4.9021e-01, -1.2309e+00, -8.4860e-01, -4.9587e-01,\n                          -6.7869e-01, -1.9882e+00, -5.9173e-01, -1.5457e+00, -5.1056e-01,\n                          -3.1869e-01, -9.0500e-01, -1.4470e+00, -5.9402e-01, -3.2503e-01,\n                          -5.4013e-01, -2.3077e+00, -1.2906e+00,  6.7137e-02, -5.2036e-01,\n                          -4.9750e-01, -4.4734e-01, -8.6256e-01, -7.7539e-01, -7.4441e-01,\n                          -9.3084e-01, -1.1657e+00, -1.7771e+00, -5.3965e-01, -5.2982e-01,\n                          -1.6491e+00, -4.9890e-01, -1.0176e+00, -8.1072e-01, -1.0131e+00,\n                          -7.4178e-01, -5.0133e-01, -9.9028e-01, -1.1518e-01,  1.1458e-01,\n                          -1.2397e+00,  6.1068e-01, -6.1644e-01, -6.4451e-01, -6.9741e-01,\n                          -1.0857e+00,  5.8723e-02,  3.6451e-02,  1.4913e-02,  5.1813e-02,\n                           5.9625e-03, -2.5180e+00, -4.4554e-01,  1.2739e-01, -3.3241e-02,\n                          -1.6551e+00, -5.5412e-02, -1.8941e-02, -3.2165e-03, -5.4123e-01,\n                           3.2617e-01, -5.4341e-01, -6.6340e-01, -2.0685e+00, -1.5469e+00,\n                          -1.4412e+00, -4.7809e-01, -5.6111e-01, -1.0889e+00, -3.0687e-01,\n                           3.4232e-01, -6.5733e-01, -1.3196e+00, -5.6397e-01, -7.9168e-02,\n                          -2.1261e+00, -8.1943e-01, -9.4439e-01, -8.1931e-01, -8.0460e-01,\n                          -7.2715e-01, -3.2360e-01, -6.3951e-01, -8.5415e-01, -1.1152e+00,\n                          -2.1637e-02, -1.3649e+00, -1.0195e+00, -8.8011e-01, -1.3908e+00,\n                          -1.3159e+00, -4.7565e-01,  1.7181e-01,  7.2034e-02, -1.7544e+00,\n                           6.7588e-01,  1.5403e-01,  1.9544e-01, -5.6286e-01, -1.1544e+00,\n                          -2.1808e+00, -5.1236e-01, -2.3890e+00, -3.6702e+00, -1.9171e+00,\n                           7.8049e-02, -2.8171e-01, -5.6033e-01, -6.3865e-01,  4.0927e-02,\n                          -4.7310e-01, -6.6179e-01, -1.2711e+00, -4.7408e-01, -1.1976e+00,\n                          -1.4034e+00,  4.0440e-02,  1.5811e-01,  5.4746e-02, -2.0342e+00,\n                          -1.0757e+00, -5.2821e-01, -4.5419e-01,  2.1167e-01, -5.9750e-01,\n                          -2.0806e+00, -6.3415e-01, -5.1249e-01,  3.1402e-01, -8.9296e-01,\n                          -3.2266e-01,  5.2142e-01, -5.5692e-01, -4.9475e-01, -4.0095e-02,\n                          -1.6628e+00, -1.5802e+00, -3.6057e-02, -4.6246e-01, -4.3340e-01,\n                          -8.6323e-01, -1.0182e+00, -7.3637e-01,  4.3149e-02, -2.2457e-01,\n                          -5.7541e-01, -9.2791e-01, -1.0807e+00,  1.3642e-01,  2.6584e-02,\n                          -1.0890e+00, -5.4874e-01, -9.8396e-01,  7.8347e-02, -1.1115e+00,\n                          -8.1020e-01, -1.0130e+00, -7.2229e-01, -6.6788e-01,  1.2750e-01,\n                          -1.6501e-02, -1.1100e+00, -7.5701e-01, -4.7982e-01, -3.8959e+00,\n                           1.0077e-01,  1.1310e-01,  7.1477e-03, -3.6164e-01,  1.4101e-01,\n                          -2.2058e+00, -9.2724e-01, -2.7003e-02, -1.0089e+00, -1.0061e+00,\n                          -5.2346e-01,  2.1753e-01, -7.4652e-01, -1.2070e+00, -7.7526e-01,\n                          -1.9409e-01, -5.1074e-01, -5.5538e-01, -4.8542e-01, -9.9395e-01,\n                          -5.7923e-01, -4.5032e-01, -1.8439e-01, -6.4553e-01,  2.4186e-01,\n                          -9.7306e-01,  4.0396e-01,  3.7277e-02,  1.6971e-02, -4.8741e+00,\n                          -1.5775e+00, -1.2753e-01, -1.3981e+00, -5.2980e-01, -7.7348e-01,\n                          -3.9000e-01, -6.9812e-01, -6.9439e-01,  5.6165e-02, -6.6264e-01,\n                          -7.2391e-01, -6.4940e-01, -1.1335e+00, -1.0950e+00, -5.3186e-01,\n                          -6.4695e-01, -2.2477e+00,  1.1875e-01,  9.8008e-02, -1.2948e+00,\n                           3.8056e-02,  7.0269e-02, -6.7417e-01, -3.9756e-01, -4.5530e-01,\n                          -8.8230e-01, -6.0293e-01,  2.5179e-01, -7.3818e-01, -8.5653e-01,\n                          -2.1047e+00, -1.0510e+00, -6.1692e-01,  4.3324e-01, -9.2847e-01,\n                          -4.8042e-01, -5.9908e-01, -9.7818e-01,  1.2001e-01, -1.2242e+00,\n                           1.0306e-01, -1.1876e-02,  2.2027e-01,  6.7643e-02, -3.3238e+00,\n                           1.5213e-01,  3.7223e-02,  3.3027e-01, -2.6648e-01, -6.6918e-01,\n                          -1.1962e-01, -1.3852e+00,  9.0105e-02, -5.2932e-01, -4.4994e-01,\n                          -9.6999e-03,  3.4403e-02, -1.3049e+00, -1.3508e-01, -4.8184e-01,\n                          -9.3097e-01, -8.9784e-01, -4.0337e-01, -4.9415e-01,  3.8153e-01,\n                          -3.9099e-01, -1.0881e+00, -5.8692e-01,  6.4330e-02, -1.0711e-01,\n                          -1.3831e+00, -4.2184e-02, -5.8934e-01,  2.4298e-01, -4.6456e-01,\n                           1.9421e-01, -5.6285e-02, -5.7672e-01,  1.2774e-01,  1.5891e-01,\n                          -1.1765e+00, -5.7545e-01, -1.7106e+00, -1.0706e+00,  1.8165e-01]), max_val=tensor([ 2.6982e+00,  2.3447e+00,  2.2092e+00, -5.4720e-02,  1.5966e+00,\n                           1.4082e+00,  1.6856e+00,  2.4128e+00, -2.0218e-01,  1.0287e+00,\n                           5.4566e-01, -2.7903e-01,  2.0657e+00,  2.8855e+00, -1.9339e-01,\n                           1.1874e+00, -6.4870e-02, -2.1394e-01,  2.1272e+00, -2.3862e-01,\n                          -2.2656e-01,  1.5571e+00,  1.5327e+00,  1.4782e+00,  8.0211e-01,\n                           1.3744e+00, -6.6948e-03, -2.4557e-01,  1.0912e+00,  7.2050e-01,\n                           1.8738e+00,  1.3447e+00,  1.4219e+00,  1.4009e+00,  1.4333e+00,\n                           5.6777e+00,  1.7152e+00,  1.1047e+00,  8.7212e-01,  8.6503e-01,\n                           1.6752e+00, -3.3444e-01,  2.1209e+00,  2.0216e+00,  1.0665e+00,\n                           1.8864e+00,  1.6078e+00,  6.7657e-02,  3.7460e-03,  1.3992e+00,\n                          -2.8302e-01, -2.1131e-01,  9.9141e-01,  1.2820e+00,  1.6226e+00,\n                           1.3075e+00,  1.7455e+00, -2.5409e-01, -1.1798e-01,  2.1607e+00,\n                           1.4702e+00,  5.5720e-01,  2.2876e+00,  1.3778e+00,  1.7016e+00,\n                           1.5677e+00,  1.7848e+00,  1.4344e+00, -3.5392e-01, -2.4820e-01,\n                          -2.9388e-01,  1.0852e+00, -8.9364e-02,  1.3537e+00,  3.9362e+00,\n                          -2.2689e-01,  9.5835e-01,  1.7657e+00,  9.6003e-01,  1.3919e+00,\n                           2.6008e-02,  1.5464e+00,  1.2364e+00,  1.6728e+00, -2.8565e-01,\n                           1.9565e+00,  2.4837e+00,  1.3623e+00, -3.1803e-01,  1.8791e+00,\n                          -2.9305e-01, -2.7583e-01,  2.2678e+00, -6.1420e-02, -2.7328e-01,\n                           2.7334e+00,  1.3627e+00,  1.2212e+00,  1.8311e+00,  1.3183e+00,\n                          -1.9094e-01,  1.1262e+00, -1.4317e-01, -1.7607e-01, -2.6733e-01,\n                           8.8606e-02, -2.6677e-01,  4.6102e-01,  1.5620e+00,  2.2593e+00,\n                           1.0919e+00, -1.2962e-01,  9.9881e-01,  2.1987e+00, -2.8133e-03,\n                           7.2264e-01,  1.2967e+00,  9.8390e-01,  1.9742e+00,  1.9188e+00,\n                          -9.2802e-02,  1.2786e+00,  1.3245e+00,  3.2108e+00,  3.3427e+00,\n                           1.8095e+00,  1.1026e-01,  9.4885e-01,  1.1248e+00, -1.2715e-01,\n                          -1.7086e-01,  3.0044e+00, -2.4818e-01,  1.1926e+00,  1.2072e+00,\n                          -1.6095e-01,  2.3658e+00,  2.1579e+00, -2.4348e-01,  1.5372e+00,\n                           1.4611e+00,  1.4984e+00, -1.4874e-01, -3.0414e-01, -1.7852e-01,\n                           2.6860e+00, -1.0686e-01, -2.9927e-01, -2.5084e-01,  1.1225e+00,\n                           1.1765e+00,  1.1168e+00,  1.3050e+00,  2.8119e+00,  1.2692e+00,\n                          -2.5124e-01, -2.2125e-01,  5.3419e-02,  1.6299e+00,  1.8972e+00,\n                          -3.0006e-01,  1.7303e+00, -2.3236e-01, -1.7770e-01,  1.5257e+00,\n                          -3.5502e-01,  1.8879e+00,  2.2685e+00, -1.4166e-01,  1.0246e+00,\n                          -3.6813e-01, -2.6483e-01,  1.6970e+00, -1.5526e-01,  1.1972e+00,\n                           1.6903e+00,  8.3227e-01, -2.0876e-01,  5.1237e-02,  3.7491e+00,\n                           1.1887e+00,  1.2375e+00,  1.9764e+00, -5.7628e-02, -3.0342e-01,\n                          -2.6480e-01, -2.6527e-01,  2.1050e-01,  9.6933e-01, -2.4415e-01,\n                           2.9036e+00, -9.2143e-02, -2.3048e-01, -6.0903e-02, -1.9302e-01,\n                           9.7611e-01, -2.6416e-01,  1.4438e+00,  1.3387e+00, -7.6627e-02,\n                           1.5612e+00,  2.1296e+00,  1.9586e+00,  1.5422e+00,  8.8905e-01,\n                           1.2155e+00,  2.3658e+00,  2.9977e+00,  2.3436e+00,  8.8148e-01,\n                           1.8483e+00,  1.6633e+00, -8.3355e-02, -1.8063e-01,  1.1791e+00,\n                          -2.2991e-01, -5.2959e-02,  2.1119e+00,  2.0330e+00,  1.0355e+00,\n                           1.8791e+00,  1.2639e+00, -1.7451e-01, -2.7274e-01,  1.4130e+00,\n                          -2.9753e-01, -6.4103e-02,  1.6516e+00, -2.0615e-01,  1.5450e+00,\n                           1.1347e+00,  7.3634e-01,  1.7326e+00,  6.8851e-03, -2.1334e-01,\n                          -2.9284e-01,  1.9231e-01,  7.4627e-01,  1.2624e+00,  1.8504e+00,\n                           1.4043e+00, -3.2399e-01, -2.3947e-01, -2.2432e-01, -1.4421e-01,\n                           1.6509e+00,  2.5882e+00,  9.6720e-01, -1.0670e-01,  1.3407e+00,\n                           2.7930e-01,  1.7280e+00,  3.8200e+00, -2.8334e-01, -2.3975e-01,\n                          -2.6583e-01,  2.7841e+00,  1.6357e+00, -1.9659e-01, -1.9450e-01,\n                           3.4813e+00,  2.3249e+00,  2.0403e+00,  4.8798e-01, -3.2739e-01,\n                           3.7193e+00, -3.1675e-01, -2.6968e-01, -1.7535e-01, -8.1346e-02,\n                          -2.6878e-01, -2.9602e-01,  5.5740e-01,  2.6990e+00,  1.0376e+00,\n                           9.2237e-02,  1.4314e+00,  1.4410e+00,  2.0222e+00,  1.1804e+00,\n                          -2.3245e-01,  1.7359e+00,  1.1131e+00,  1.6323e+00, -1.3308e-01,\n                           1.2445e+00,  1.3651e+00, -3.9866e-01,  9.6910e-01, -7.4662e-03,\n                           1.2074e+00, -8.7453e-02,  2.6786e+00,  1.8615e+00,  2.2976e+00,\n                          -2.2693e-01, -2.5779e-01,  9.3386e-01, -7.3739e-02, -6.7827e-02,\n                           1.2961e+00, -1.4656e-01,  1.3316e+00, -2.5769e-01, -2.4643e-01,\n                           2.2952e+00,  1.9170e+00, -3.2291e-01,  1.0957e+00,  1.3958e+00,\n                           2.3986e+00, -9.4934e-02,  1.6503e+00,  1.4884e+00,  2.2183e+00,\n                           2.9156e+00,  1.5935e+00, -2.2792e-01, -3.3616e-01,  2.1747e+00,\n                          -1.9497e-01, -2.6768e-01,  2.1030e+00, -1.9684e-01,  1.6618e-01,\n                           1.1542e+00,  1.1237e+00,  4.8804e+00,  1.1850e+00,  1.3814e+00,\n                           1.6473e+00,  2.7021e+00,  2.9494e+00, -1.9216e-01,  1.2750e+00,\n                           1.8498e+00, -2.8797e-01, -1.2892e-01, -2.7682e-01,  1.2937e+00,\n                           1.3968e+00,  2.8599e+00,  1.7131e+00,  2.1357e+00,  1.9287e+00,\n                          -1.9446e-01,  7.9449e-01,  1.7098e+00,  1.2656e-01,  1.7702e+00,\n                           1.0115e+00,  1.2994e+00,  1.4304e+00, -1.6068e-01,  1.7936e+00,\n                          -2.6724e-01,  1.3836e+00,  1.3594e+00, -1.6727e-01,  1.2686e+00,\n                          -3.5889e-02,  1.4289e+00, -1.7087e-01,  1.2343e+00, -8.0718e-02,\n                          -3.6327e-02, -3.0782e-01,  1.5059e+00,  2.1377e+00,  1.0890e+00,\n                           1.9023e+00,  2.1521e+00, -1.6166e-01,  1.4732e+00,  3.0339e+00,\n                           1.5009e+00, -2.5678e-01,  9.9343e-01,  6.0931e-02,  1.3315e+00,\n                           4.4273e+00,  1.2805e+00,  1.5744e+00, -3.1104e-01,  9.3554e-01,\n                           1.2037e+00,  1.7560e+00, -3.4451e-01,  6.5114e-01,  2.9517e+00,\n                           1.1885e+00,  3.2225e+00,  1.3116e+00,  2.4155e+00,  1.0408e+00,\n                           2.7439e+00,  1.7184e+00,  1.8678e+00, -2.4982e-01, -2.7789e-01,\n                          -1.3267e-01, -2.2619e-01,  4.3785e-01,  1.1442e+00,  5.5673e-02,\n                          -2.0144e-01,  3.5148e-03,  1.6746e+00,  9.8580e-01,  2.0272e+00,\n                          -1.7472e-01,  8.8792e-01, -2.3785e-01,  1.2760e+00,  1.3761e+00,\n                           1.3159e+00, -2.0099e-01, -2.3282e-01,  2.1653e+00,  2.0585e+00,\n                           2.9717e+00,  1.1539e+00,  1.8736e+00,  1.5295e+00, -1.9343e-01,\n                           8.8120e-01,  2.4174e+00,  1.8712e+00,  1.3975e+00,  1.2655e+00,\n                           1.2688e+00, -6.3307e-02,  1.1301e+00,  1.3027e+00,  1.8694e+00,\n                          -1.5429e-01, -1.8321e-01,  2.4792e-01,  2.1866e+00, -3.1141e-01,\n                           3.1631e+00, -2.6216e-01,  1.2135e+00,  1.6787e+00, -2.7275e-01,\n                           1.7798e+00,  1.6071e+00,  2.8116e+00,  1.8705e+00,  1.6784e+00,\n                          -1.2009e-01,  1.5359e+00,  3.7810e+00,  1.0212e+00,  1.3163e-02,\n                           1.5849e+00,  1.4175e+00,  1.7870e+00,  1.1540e+00,  1.7684e+00,\n                          -2.1698e-01, -2.2638e-01, -2.6760e-02,  6.4983e-01,  1.6998e+00,\n                           2.1760e+00,  1.6099e+00,  1.0789e+00,  1.9053e+00, -2.6201e-01,\n                          -1.5566e-01, -3.1667e-01,  9.6098e-01,  1.0358e+00, -1.5740e-01,\n                           2.6491e+00, -1.3581e-01, -6.5696e-02,  1.6895e+00,  1.9977e+00,\n                           2.7752e+00, -2.6713e-01, -2.8903e-01,  2.6603e+00, -3.6695e-01,\n                          -3.5736e-01, -2.5227e-01,  1.5117e+00, -1.0559e-01,  1.2990e+00,\n                           1.9519e+00, -8.0888e-03,  8.8296e-01,  1.0613e+00, -1.1839e-01,\n                           1.5577e+00, -2.3785e-01, -6.8230e-02,  2.7943e+00, -2.5805e-01,\n                           2.2402e+00, -2.8626e-01, -2.8455e-01,  3.7487e+00,  1.1278e+00,\n                          -3.8996e-02, -2.8034e-01, -2.8655e-01,  1.6724e+00,  1.5030e+00,\n                          -8.9712e-04, -3.1417e-01, -6.3817e-02,  7.5500e-01, -2.8230e-01,\n                           1.3119e+00,  3.1401e+00, -3.6177e-01,  1.4830e+00,  1.9215e+00,\n                          -2.4534e-01,  2.0642e+00, -1.4050e-01, -3.3608e-01,  1.5748e+00,\n                           1.4886e+00, -8.1029e-02,  1.3813e+00, -1.1588e-01, -1.8026e-01,\n                          -2.7462e-01,  1.3129e+00, -2.0318e-01,  1.8802e+00,  1.2284e+00,\n                           4.3041e+00, -1.7537e-01,  1.8849e+00,  1.0498e+00,  9.4868e-01,\n                           6.0317e-01,  1.4570e+00,  1.6580e+00,  9.7316e-01,  2.1527e+00,\n                           1.7999e+00, -2.7716e-01,  1.5913e+00,  9.2744e-01, -3.1165e-01,\n                           1.1797e+00, -2.5022e-01,  2.5193e+00,  1.3922e+00,  1.5446e+00,\n                          -2.5300e-01,  1.2916e+00, -3.3517e-01,  8.5636e-01,  3.4350e+00,\n                           1.5821e+00,  1.5641e+00, -3.3332e-01,  1.3971e+00,  1.7672e+00,\n                          -7.6742e-02,  1.1444e+00,  1.7613e+00,  6.9054e-01, -3.6196e-01,\n                           1.3613e+00,  1.3554e+00,  1.3965e+00,  1.9540e+00,  9.7317e-01,\n                           1.5111e+00,  1.4687e+00,  1.8660e+00, -3.3260e-01,  3.0046e+00,\n                           1.0771e+00,  1.2267e+00,  9.4480e-01,  1.1877e+00,  1.2618e+00,\n                           2.6964e+00,  2.1218e+00, -1.1835e-01, -2.4172e-01,  1.7973e+00,\n                           1.5192e+00,  1.8028e+00,  1.3924e+00,  1.9355e+00, -2.0768e-01,\n                           1.7595e+00,  1.0786e+00,  9.8059e-01,  1.8754e+00, -5.8573e-02,\n                           1.0976e+00, -2.5392e-01, -8.3003e-02,  3.5155e+00, -3.1088e-01,\n                           1.5380e+00,  2.2961e+00,  3.6494e-01,  2.4809e+00, -2.6675e-01,\n                          -1.4899e-01,  4.6021e-01,  1.5864e+00,  1.6093e+00,  1.9639e+00,\n                           1.8017e+00, -1.1777e-01, -2.8081e-01, -3.6148e-01,  2.3492e+00,\n                           1.8069e+00,  2.7356e+00,  1.3363e+00,  1.4394e+00,  6.7844e-01,\n                           1.6319e+00,  1.4318e+00,  7.9523e-03,  1.8420e+00, -9.7639e-02,\n                           1.0460e+00,  7.3115e+00,  1.3488e+00,  1.3991e+00,  1.6843e+00,\n                           1.6864e+00,  2.5210e+00,  2.8678e+00,  9.9384e-01,  2.8603e+00,\n                          -3.6875e-01,  8.8364e-01,  1.6684e+00,  2.0069e+00,  1.0053e+00,\n                           1.0365e+00,  1.3394e+00,  3.0689e+00,  2.0758e+00, -2.8646e-01,\n                           1.0942e+00,  1.2792e+00,  2.0073e+00,  1.1849e+00,  1.7023e+00,\n                           1.3104e+00, -2.0335e-02, -1.6402e-01,  1.3019e+00,  1.7332e+00,\n                           1.0995e+00, -2.1736e-01,  1.9612e+00,  4.3514e+00,  2.7413e-01,\n                          -1.4334e-01,  2.5635e+00,  1.4006e+00,  2.3071e+00,  1.1632e+00,\n                          -3.0178e-01,  1.4825e+00, -2.9596e-01,  2.3660e+00,  1.6359e+00,\n                          -2.1160e-01,  9.9037e-01,  1.8732e+00, -1.9495e-01,  1.1881e+00,\n                           1.2700e+00, -2.4566e-01,  7.3825e-01,  1.8434e+00, -2.3687e-01,\n                           1.0976e+00,  1.5325e+00, -2.2079e-01,  1.2645e+00,  1.8471e+00,\n                           1.4318e+00, -2.4634e-01, -2.4706e-02, -2.2521e-01,  2.0030e+00,\n                           6.1207e-02,  2.5881e+00, -2.2001e-01,  7.9956e-01, -3.6249e-01,\n                          -2.5639e-01,  2.3562e+00,  1.4668e+00,  6.7145e-01,  1.2829e+00,\n                           1.4003e+00,  1.9522e+00,  2.8052e+00,  1.3149e+00, -1.6286e-01,\n                           8.7510e-01, -2.7486e-03,  9.9422e-01, -1.7029e-01,  8.6792e-01,\n                          -2.6244e-01, -2.8350e-01,  2.2034e+00,  1.4791e+00,  1.2333e+00,\n                           1.9642e+00,  2.4331e+00, -3.0954e-01,  1.9607e+00, -1.4950e-01,\n                           1.6754e+00,  2.3559e+00,  3.0566e+00,  1.8857e+00,  1.7305e+00,\n                           1.6059e+00,  1.7162e+00, -2.4939e-01,  1.5048e+00,  1.5921e+00,\n                           2.0017e+00,  8.8922e-01,  4.6539e+00,  1.6393e+00, -3.1111e-01,\n                           1.0187e+00, -3.3639e-01,  1.4139e+00,  1.9043e+00,  1.3885e+00,\n                           1.5201e+00, -2.1368e-01,  3.0329e+00,  8.4314e-01,  1.5050e+00,\n                           9.3500e-01,  2.1716e+00,  3.7291e-01, -1.8228e-01,  1.1968e+00,\n                           2.9005e+00, -7.4847e-02,  1.1644e+00,  2.8919e+00,  1.6188e+00,\n                          -2.2706e-01,  2.2481e+00,  5.5454e-02, -2.5785e-01,  1.9668e+00,\n                           2.7755e+00,  1.4308e+00, -1.3100e-01, -2.7217e-01,  8.3210e-01,\n                           1.0396e+00, -3.0217e-01,  3.1320e+00,  1.1335e+00,  1.2836e+00,\n                           3.3116e+00,  8.8798e-01,  1.4543e+00, -2.2526e-01,  1.7626e+00,\n                          -7.1135e-02, -8.3513e-02,  8.5374e-01, -5.6312e-02,  2.3826e+00,\n                           1.8320e+00,  1.2365e+00,  1.1453e+00, -2.0882e-01,  2.3655e+00,\n                          -1.2405e-01,  1.6305e-01,  1.3617e+00, -1.7536e-01,  1.9368e+00,\n                           1.1221e+00,  1.4111e+00,  1.1706e+00,  1.4877e+00,  1.7677e+00,\n                           1.7919e+00, -2.4772e-01, -9.0681e-02,  1.6796e+00,  1.2050e+00,\n                          -1.3462e-01,  1.1953e+00, -2.6574e-01,  4.2289e+00,  1.0818e+00,\n                           1.4570e+00,  4.6582e+00,  3.8590e+00, -1.5820e-01,  4.5308e-02,\n                           1.8756e+00,  1.6217e+00,  1.2379e+00, -2.7272e-01, -1.5030e-01,\n                          -3.0674e-01,  2.6954e+00, -2.0309e-01,  1.4184e+00,  1.8619e+00,\n                           9.5180e-02,  1.1199e+00,  1.5473e+00,  1.2247e+00,  1.3342e+00,\n                           1.0214e+00, -3.1783e-01,  2.1847e+00,  1.4663e+00,  6.1224e-01,\n                          -2.2020e-01,  2.0099e+00,  5.9047e-02,  5.8711e-01,  1.1312e+00,\n                           2.0220e+00,  1.8016e+00, -2.8319e-02,  1.7654e+00,  9.8659e-01,\n                           1.8300e+00,  1.9168e+00,  1.2997e+00, -3.9281e-02,  1.6620e+00,\n                          -2.0965e-02, -2.5083e-01,  1.2592e+00, -2.6967e-01,  1.7206e+00,\n                          -2.6573e-01,  2.2773e+00, -2.1015e-01,  1.6731e+00,  1.0752e+00,\n                           1.5368e+00,  1.5298e+00,  5.1723e-03, -2.5080e-01,  1.5836e+00,\n                          -1.8436e-01, -2.4047e-01,  1.7725e+00,  6.6184e+00,  1.6346e+00,\n                           1.2222e+00,  2.1423e+00,  1.1537e+00,  1.2366e+00,  1.9073e+00,\n                           3.0963e+00,  2.5039e+00,  9.7332e-01, -2.7160e-01,  1.1723e+00,\n                           1.5654e+00, -3.2646e-01, -6.0725e-02,  1.7159e+00,  2.4978e-01,\n                          -9.1162e-02, -3.0995e-01,  1.6212e+00,  9.3612e-01, -2.5286e-01,\n                          -2.2060e-01,  3.5175e+00,  2.2559e+00,  1.0839e+00,  1.1429e+00,\n                           2.9160e+00,  2.2329e+00,  1.0842e+00,  1.2377e+00, -2.5097e-01,\n                           1.1425e+00, -2.8837e-01,  1.8154e+00,  7.2510e-01,  1.7581e+00,\n                           1.3168e+00,  1.6443e+00,  3.3700e-01,  2.1744e+00,  7.5699e-02,\n                          -2.7553e-01,  1.5359e+00,  1.2315e+00,  1.5616e+00,  1.2617e+00,\n                           1.1092e+00,  1.6204e+00,  1.1653e+00,  2.4875e+00,  3.2559e+00,\n                           1.3760e+00,  1.7538e+00,  2.5000e+00,  1.2320e+00, -1.4842e-01,\n                           1.5023e+00,  1.1652e+00,  1.6921e+00, -2.4912e-01, -1.5959e-01,\n                           2.2364e+00,  1.4618e+00,  3.3192e+00,  1.5416e+00, -6.2804e-02,\n                           1.5465e+00,  1.4294e+00, -2.2600e-01, -3.1496e-01,  3.8483e+00,\n                          -2.0174e-01,  1.4012e+00, -3.7913e-01,  1.7137e+00,  1.0272e+00,\n                           1.6606e+00,  1.3897e+00, -1.9166e-01,  1.5189e+00, -1.4041e-01,\n                           2.1270e+00,  1.7399e+00, -1.9636e-01,  9.4115e-01,  9.0961e-01,\n                           7.4140e-01,  1.1617e+00,  2.2039e+00,  1.1439e+00,  1.6470e+00])\n                )\n              )\n            )\n            (2): ReLU6(inplace=True)\n          )\n          (2): ConvBn2d(\n            960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0029, 0.0025, 0.0029, 0.0027, 0.0024, 0.0028, 0.0027, 0.0030, 0.0026,\n                      0.0017, 0.0033, 0.0029, 0.0023, 0.0023, 0.0023, 0.0032, 0.0025, 0.0025,\n                      0.0026, 0.0026, 0.0029, 0.0026, 0.0026, 0.0024, 0.0027, 0.0030, 0.0034,\n                      0.0021, 0.0025, 0.0026, 0.0035, 0.0023, 0.0024, 0.0030, 0.0025, 0.0026,\n                      0.0026, 0.0028, 0.0023, 0.0026, 0.0026, 0.0030, 0.0025, 0.0035, 0.0019,\n                      0.0027, 0.0030, 0.0019, 0.0028, 0.0026, 0.0026, 0.0027, 0.0025, 0.0026,\n                      0.0024, 0.0039, 0.0024, 0.0024, 0.0032, 0.0026, 0.0026, 0.0035, 0.0018,\n                      0.0032, 0.0027, 0.0024, 0.0028, 0.0027, 0.0026, 0.0023, 0.0026, 0.0023,\n                      0.0023, 0.0021, 0.0027, 0.0023, 0.0024, 0.0023, 0.0024, 0.0032, 0.0026,\n                      0.0031, 0.0024, 0.0026, 0.0027, 0.0024, 0.0021, 0.0021, 0.0024, 0.0041,\n                      0.0034, 0.0021, 0.0023, 0.0027, 0.0020, 0.0019, 0.0027, 0.0024, 0.0028,\n                      0.0025, 0.0022, 0.0022, 0.0030, 0.0023, 0.0028, 0.0020, 0.0026, 0.0020,\n                      0.0028, 0.0025, 0.0019, 0.0026, 0.0026, 0.0022, 0.0023, 0.0024, 0.0022,\n                      0.0023, 0.0027, 0.0023, 0.0043, 0.0030, 0.0028, 0.0021, 0.0024, 0.0022,\n                      0.0025, 0.0028, 0.0023, 0.0024, 0.0025, 0.0030, 0.0024, 0.0019, 0.0048,\n                      0.0024, 0.0026, 0.0027, 0.0025, 0.0023, 0.0026, 0.0025, 0.0023, 0.0024,\n                      0.0024, 0.0022, 0.0025, 0.0026, 0.0029, 0.0021, 0.0020, 0.0030, 0.0022,\n                      0.0033, 0.0042, 0.0033, 0.0022, 0.0024, 0.0026, 0.0021]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n                min_val=tensor([-0.3180, -0.3158, -0.3703, -0.2684, -0.3094, -0.3351, -0.2877, -0.3786,\n                        -0.3335, -0.2170, -0.4283, -0.3710, -0.2957, -0.2555, -0.1952, -0.4113,\n                        -0.3222, -0.2617, -0.3340, -0.3276, -0.3655, -0.2518, -0.3371, -0.3001,\n                        -0.2794, -0.3829, -0.3304, -0.2672, -0.3176, -0.3373, -0.2906, -0.2824,\n                        -0.3052, -0.2859, -0.3207, -0.3331, -0.3378, -0.2770, -0.2843, -0.2962,\n                        -0.3294, -0.3833, -0.2354, -0.4441, -0.2449, -0.3424, -0.3788, -0.2262,\n                        -0.3465, -0.2922, -0.3326, -0.2642, -0.3139, -0.2742, -0.2492, -0.4938,\n                        -0.3037, -0.2801, -0.2973, -0.3294, -0.3354, -0.4463, -0.2056, -0.4048,\n                        -0.3433, -0.3076, -0.3553, -0.3489, -0.3381, -0.3005, -0.3048, -0.2912,\n                        -0.2917, -0.2688, -0.3421, -0.2759, -0.2705, -0.2772, -0.3014, -0.3916,\n                        -0.3228, -0.4003, -0.3040, -0.2742, -0.3395, -0.3120, -0.2702, -0.2106,\n                        -0.2947, -0.3558, -0.4380, -0.2333, -0.2464, -0.3039, -0.2554, -0.2482,\n                        -0.3250, -0.3120, -0.3537, -0.2646, -0.2877, -0.2796, -0.3165, -0.3005,\n                        -0.3537, -0.2290, -0.3206, -0.2547, -0.3063, -0.2609, -0.2421, -0.3026,\n                        -0.3072, -0.2864, -0.2806, -0.2766, -0.2791, -0.2941, -0.3421, -0.2921,\n                        -0.5456, -0.3304, -0.3556, -0.2626, -0.3136, -0.2329, -0.2877, -0.3627,\n                        -0.2890, -0.3065, -0.3145, -0.2921, -0.2644, -0.2305, -0.6124, -0.2874,\n                        -0.3357, -0.3344, -0.2941, -0.2940, -0.2828, -0.2989, -0.2736, -0.2851,\n                        -0.3054, -0.2363, -0.2803, -0.2887, -0.3696, -0.2078, -0.2513, -0.3198,\n                        -0.2867, -0.4191, -0.3105, -0.4276, -0.2773, -0.2605, -0.3276, -0.2346]), max_val=tensor([0.3634, 0.3025, 0.2955, 0.3410, 0.2605, 0.3503, 0.3377, 0.3042, 0.2535,\n                        0.2136, 0.2645, 0.2959, 0.2870, 0.2910, 0.2927, 0.3756, 0.2818, 0.3147,\n                        0.2579, 0.2562, 0.2137, 0.3251, 0.2772, 0.3040, 0.3383, 0.2921, 0.4307,\n                        0.2401, 0.3040, 0.2823, 0.4401, 0.2912, 0.2582, 0.3867, 0.2875, 0.2984,\n                        0.2909, 0.3617, 0.2878, 0.3265, 0.3160, 0.3216, 0.3137, 0.2630, 0.2450,\n                        0.2858, 0.2844, 0.2374, 0.3586, 0.3299, 0.3201, 0.3375, 0.3118, 0.3261,\n                        0.2986, 0.2816, 0.2888, 0.3014, 0.4020, 0.2841, 0.2725, 0.3093, 0.2306,\n                        0.2789, 0.2924, 0.2601, 0.3318, 0.3148, 0.2981, 0.2943, 0.3325, 0.2712,\n                        0.2797, 0.2193, 0.3081, 0.2913, 0.3002, 0.2913, 0.2182, 0.4118, 0.3311,\n                        0.2918, 0.2755, 0.3323, 0.2741, 0.2731, 0.2544, 0.2667, 0.3012, 0.5240,\n                        0.2500, 0.2672, 0.2982, 0.3451, 0.2485, 0.2457, 0.3428, 0.2716, 0.3289,\n                        0.3199, 0.2623, 0.2784, 0.3852, 0.2871, 0.2556, 0.2552, 0.3330, 0.2438,\n                        0.3530, 0.3126, 0.2073, 0.3287, 0.3256, 0.2719, 0.2879, 0.3106, 0.2497,\n                        0.2954, 0.3008, 0.2864, 0.3552, 0.3835, 0.3611, 0.2433, 0.2803, 0.2750,\n                        0.3128, 0.3013, 0.2784, 0.2554, 0.2948, 0.3810, 0.2986, 0.2470, 0.3329,\n                        0.3007, 0.3022, 0.3488, 0.3151, 0.2635, 0.3243, 0.3179, 0.2959, 0.3024,\n                        0.2503, 0.2763, 0.3234, 0.3347, 0.3470, 0.2606, 0.2576, 0.3758, 0.2585,\n                        0.2691, 0.5366, 0.2870, 0.2805, 0.3032, 0.2657, 0.2634])\n              )\n            )\n          )\n        )\n      )\n      (17): Module(\n        (conv): Module(\n          (0): Module(\n            (0): ConvBn2d(\n              160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False\n              (bn): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n                fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([2.0576e-04, 3.9906e-04, 3.0297e-04, 2.8021e-04, 2.3589e-04, 3.1121e-04,\n                        2.9281e-04, 2.8074e-04, 3.0464e-04, 3.4170e-04, 3.9315e-04, 4.1206e-04,\n                        3.0664e-04, 2.4507e-04, 2.5033e-04, 2.8404e-04, 2.7896e-04, 3.2371e-04,\n                        3.1882e-04, 2.7833e-04, 2.6878e-04, 2.9151e-04, 2.7909e-04, 3.4228e-04,\n                        5.2779e-04, 3.3884e-04, 2.9350e-04, 2.6036e-04, 2.4352e-04, 2.7413e-04,\n                        1.9869e-04, 2.9573e-04, 5.1645e-04, 1.6927e-06, 2.8767e-04, 3.2809e-04,\n                        2.5779e-04, 3.1735e-04, 3.0094e-04, 5.5596e-04, 3.5972e-04, 2.3662e-04,\n                        2.6024e-04, 2.7065e-04, 2.5469e-04, 4.9219e-04, 3.0985e-04, 4.4983e-04,\n                        2.3281e-04, 2.7102e-04, 3.5319e-04, 3.1715e-04, 2.7583e-04, 2.7818e-04,\n                        2.7314e-04, 2.3077e-04, 3.2022e-04, 2.7583e-04, 2.8789e-04, 2.4122e-04,\n                        2.4850e-04, 2.7474e-04, 2.3912e-04, 2.8125e-04, 2.3464e-04, 2.9099e-04,\n                        2.6057e-04, 2.4936e-04, 2.8856e-04, 4.7078e-04, 3.5077e-04, 2.8895e-04,\n                        2.5574e-04, 2.4469e-04, 2.7150e-04, 2.2749e-04, 2.5723e-04, 2.1499e-04,\n                        2.9744e-04, 9.7342e-04, 2.8006e-04, 2.9822e-04, 2.8229e-04, 3.1736e-04,\n                        2.9859e-04, 3.1260e-04, 2.5403e-04, 4.7832e-04, 2.7701e-04, 2.1815e-04,\n                        2.3965e-04, 5.3516e-04, 4.1240e-04, 2.4323e-04, 5.5247e-04, 4.2449e-04,\n                        2.4638e-04, 2.7088e-04, 3.2523e-04, 2.7195e-04, 3.6104e-04, 2.9772e-04,\n                        2.5332e-04, 2.7462e-04, 4.1740e-04, 3.1533e-04, 3.8577e-04, 2.5005e-04,\n                        2.3127e-04, 4.6879e-04, 3.1421e-04, 4.6138e-04, 3.3331e-04, 2.7965e-04,\n                        2.5655e-04, 2.3304e-04, 2.3730e-04, 2.5387e-04, 2.8872e-04, 3.3212e-04,\n                        2.9959e-04, 4.4325e-04, 2.8069e-04, 2.8528e-04, 3.4090e-04, 4.0775e-04,\n                        2.9212e-04, 2.0412e-04, 2.6256e-04, 3.1402e-04, 2.7140e-04, 3.0465e-04,\n                        2.0062e-04, 2.4645e-04, 2.5409e-04, 3.3119e-04, 2.5903e-04, 2.9217e-04,\n                        3.8151e-04, 3.1023e-04, 1.9381e-04, 5.8541e-04, 2.6740e-04, 2.8476e-04,\n                        3.2167e-04, 2.5944e-04, 2.7992e-04, 2.5469e-04, 2.1443e-04, 3.1720e-04,\n                        3.2803e-04, 3.0136e-04, 2.6639e-04, 2.2027e-04, 2.5917e-04, 3.1905e-04,\n                        2.6525e-04, 2.8211e-04, 3.2966e-04, 2.9614e-04, 2.4811e-04, 2.6127e-04,\n                        3.1016e-04, 2.8312e-04, 3.1421e-04, 2.4671e-04, 3.6344e-04, 3.7621e-04,\n                        2.6769e-04, 2.2805e-04, 3.2156e-04, 3.0817e-04, 3.6738e-04, 2.5268e-04,\n                        4.3096e-04, 2.4807e-04, 3.9085e-04, 4.2091e-04, 2.5250e-04, 2.6835e-04,\n                        3.0054e-04, 2.3942e-04, 2.4580e-04, 3.0655e-04, 2.7270e-04, 2.8745e-04,\n                        2.8008e-04, 3.9772e-04, 2.5159e-04, 2.5655e-04, 2.7095e-04, 2.5657e-04,\n                        2.3531e-04, 2.2596e-04, 3.9661e-04, 2.6292e-04, 2.3633e-04, 2.5689e-04,\n                        2.6219e-04, 3.2580e-04, 3.9541e-04, 3.5190e-04, 4.9258e-04, 2.4287e-04,\n                        2.5242e-04, 2.8230e-04, 3.1396e-04, 3.2687e-04, 2.8599e-04, 2.5242e-04,\n                        2.6440e-04, 2.5840e-04, 2.8846e-04, 3.1247e-04, 2.4255e-04, 2.8741e-04,\n                        5.3631e-04, 2.5405e-04, 3.0319e-04, 2.9665e-04, 2.7364e-04, 2.5093e-04,\n                        2.5226e-04, 2.5635e-04, 2.1885e-04, 2.1095e-04, 3.2239e-04, 3.0162e-04,\n                        3.3051e-04, 2.7712e-04, 2.9678e-04, 3.1651e-04, 2.4449e-04, 3.9465e-04,\n                        2.3659e-04, 3.5515e-04, 2.9539e-04, 3.5947e-04, 3.0785e-04, 2.2952e-04,\n                        2.4772e-04, 3.0956e-04, 3.5637e-04, 3.2048e-04, 3.1326e-04, 2.6831e-04,\n                        2.7691e-04, 3.3224e-04, 2.9076e-04, 3.5109e-04, 3.9183e-04, 2.8717e-04,\n                        2.8910e-04, 2.4351e-04, 2.3075e-04, 2.8143e-04, 2.2906e-04, 3.1050e-04,\n                        3.0215e-04, 2.8004e-04, 3.5314e-04, 3.3832e-04, 2.6125e-04, 4.1318e-04,\n                        2.1442e-04, 2.5644e-04, 4.7282e-04, 2.5460e-04, 2.2451e-04, 3.2385e-04,\n                        2.8613e-04, 2.6317e-04, 2.5085e-04, 2.8291e-04, 2.7867e-04, 3.0355e-04,\n                        3.5279e-04, 4.3681e-04, 2.1992e-04, 3.4692e-04, 4.7553e-04, 2.5429e-04,\n                        2.2912e-04, 3.3848e-04, 2.1821e-04, 2.6660e-04, 2.7274e-04, 2.3711e-04,\n                        2.9288e-04, 2.8215e-04, 2.5206e-04, 3.1856e-04, 3.3290e-04, 3.1575e-04,\n                        3.1882e-04, 2.8144e-04, 3.9948e-04, 2.7601e-04, 4.3469e-04, 3.7866e-04,\n                        3.1390e-04, 3.6356e-04, 2.7225e-04, 2.6244e-04, 2.0216e-04, 3.8121e-04,\n                        4.7247e-04, 3.8574e-04, 3.2113e-04, 2.5274e-04, 2.1381e-04, 2.7895e-04,\n                        2.8178e-04, 3.8601e-04, 3.3222e-04, 2.3617e-04, 3.4890e-04, 2.7561e-04,\n                        2.6110e-04, 3.1997e-04, 2.6849e-04, 4.3828e-04, 3.5190e-04, 5.0671e-04,\n                        2.6518e-04, 7.0331e-04, 2.4903e-04, 2.9335e-04, 3.3457e-04, 2.9348e-04,\n                        2.3067e-04, 2.8067e-04, 2.6583e-04, 4.1497e-04, 4.3773e-04, 2.3202e-04,\n                        3.1513e-04, 2.9241e-04, 2.4498e-04, 2.3270e-04, 2.6455e-04, 4.2830e-04,\n                        2.9618e-04, 3.0033e-04, 2.7050e-04, 3.8783e-04, 2.9907e-04, 3.0747e-04,\n                        2.9497e-04, 2.9330e-04, 3.2757e-04, 2.6681e-04, 3.3920e-04, 3.0619e-04,\n                        4.7510e-04, 2.9189e-04, 3.4737e-04, 3.0650e-04, 2.9725e-04, 2.7073e-04,\n                        2.2017e-04, 2.6124e-04, 2.9487e-04, 2.7874e-04, 3.9510e-04, 4.1140e-04,\n                        3.4600e-04, 5.0587e-04, 2.7870e-04, 3.0213e-04, 2.8784e-04, 2.8244e-04,\n                        5.4488e-04, 2.8822e-04, 2.7380e-04, 2.7140e-04, 2.6955e-04, 4.9059e-04,\n                        3.0520e-04, 2.7530e-04, 3.2042e-04, 2.9161e-04, 2.5347e-04, 2.9828e-04,\n                        3.2280e-04, 2.3189e-04, 2.0590e-04, 3.5057e-04, 2.7492e-04, 2.6790e-04,\n                        1.9033e-04, 4.8841e-04, 4.3405e-04, 2.9123e-04, 3.6035e-04, 2.6169e-04,\n                        2.7307e-04, 2.4844e-04, 3.3139e-04, 2.8098e-04, 4.2873e-04, 4.3917e-04,\n                        3.3709e-04, 3.8299e-04, 4.7586e-04, 2.8804e-04, 6.4589e-04, 5.0931e-04,\n                        2.8492e-04, 2.2357e-04, 3.7707e-04, 2.4707e-04, 2.5550e-04, 3.4557e-04,\n                        3.0508e-04, 3.9295e-04, 3.1427e-04, 3.7639e-04, 2.9702e-04, 2.8999e-04,\n                        2.4224e-04, 5.0030e-04, 3.3641e-04, 3.3815e-04, 2.3634e-04, 2.3986e-04,\n                        2.2877e-04, 2.7431e-04, 2.9661e-04, 4.5599e-04, 2.5814e-04, 2.3068e-04,\n                        2.7706e-04, 2.1371e-04, 3.6738e-04, 2.9388e-04, 3.2929e-04, 3.2095e-04,\n                        2.6752e-04, 2.8732e-04, 3.2160e-04, 1.4678e-04, 2.1180e-04, 3.3237e-04,\n                        2.7789e-04, 2.8520e-04, 2.9601e-04, 3.1649e-04, 3.5846e-04, 3.0975e-04,\n                        3.5665e-04, 2.8948e-04, 5.2757e-04, 2.8024e-04, 2.4356e-04, 3.2203e-04,\n                        2.4007e-04, 2.4637e-04, 2.9301e-04, 2.1744e-04, 2.6389e-04, 2.8479e-04,\n                        4.0052e-04, 2.7474e-04, 2.2529e-04, 3.1547e-04, 2.9679e-04, 2.5324e-04,\n                        2.8070e-04, 2.8530e-04, 2.4619e-04, 3.5661e-04, 3.0647e-04, 3.0650e-04,\n                        2.8166e-04, 2.8349e-04, 2.3835e-04, 3.3161e-04, 4.2084e-04, 3.3415e-04,\n                        3.0000e-04, 3.1909e-04, 3.5733e-04, 4.2889e-04, 3.0133e-04, 3.1946e-04,\n                        3.0644e-04, 2.3772e-04, 3.4837e-04, 5.6861e-04, 2.5199e-04, 3.0068e-04,\n                        2.9573e-04, 4.9414e-04, 3.0112e-04, 2.4257e-04, 3.3901e-04, 4.6892e-04,\n                        2.4901e-04, 2.9888e-04, 2.5757e-04, 4.3639e-04, 3.0908e-04, 5.7870e-04,\n                        2.7440e-04, 3.0172e-04, 2.3398e-04, 4.3407e-04, 3.9792e-04, 3.7227e-04,\n                        2.9843e-04, 2.6889e-04, 2.8888e-04, 2.7917e-04, 3.3800e-04, 2.8445e-04,\n                        2.5131e-04, 2.0680e-04, 3.4708e-04, 2.3698e-04, 2.6969e-04, 2.3902e-04,\n                        2.5549e-04, 3.1819e-04, 4.1276e-04, 2.6546e-04, 2.5891e-04, 3.2050e-04,\n                        2.7098e-04, 2.7380e-04, 4.1767e-04, 2.1474e-04, 3.3201e-04, 2.5196e-04,\n                        2.6862e-04, 2.4769e-04, 2.7365e-04, 3.2348e-04, 2.6876e-04, 2.4395e-04,\n                        2.9787e-04, 2.1517e-04, 4.2065e-04, 2.2376e-04, 2.6159e-04, 4.1777e-04,\n                        3.1264e-04, 3.6299e-04, 3.1247e-04, 3.2240e-04, 5.6409e-04, 2.8374e-04,\n                        5.6437e-04, 2.6488e-04, 2.6103e-04, 3.6711e-04, 3.2327e-04, 2.3301e-04,\n                        4.7632e-04, 3.3816e-04, 3.1792e-04, 3.0338e-04, 3.8749e-04, 2.8281e-04,\n                        3.6999e-04, 2.0795e-04, 2.7285e-04, 3.0966e-04, 2.3795e-04, 2.4925e-04,\n                        3.2676e-04, 2.5931e-04, 2.4904e-04, 2.9481e-04, 2.3352e-04, 2.6249e-04,\n                        2.5617e-04, 2.7623e-04, 2.6393e-04, 3.7953e-04, 2.2418e-04, 3.4737e-04,\n                        2.5418e-04, 2.1691e-04, 2.6491e-04, 2.7815e-04, 3.0380e-04, 2.7117e-04,\n                        2.9856e-04, 2.7122e-04, 3.1304e-04, 2.3901e-04, 3.4835e-04, 2.9895e-04,\n                        3.4044e-04, 2.7298e-04, 3.6305e-04, 3.1280e-04, 4.4241e-04, 2.8665e-04,\n                        2.7794e-04, 3.2595e-04, 2.4690e-04, 2.7338e-04, 2.3967e-04, 4.5144e-04,\n                        2.9417e-04, 2.3396e-04, 2.9354e-04, 3.3066e-04, 2.5333e-04, 2.6888e-04,\n                        2.7616e-04, 2.7640e-04, 2.4944e-04, 3.4225e-04, 3.6813e-04, 2.7688e-04,\n                        2.5979e-04, 2.9038e-04, 2.8768e-04, 2.3422e-04, 2.6852e-04, 3.9074e-04,\n                        5.5502e-04, 2.7022e-04, 2.6119e-04, 3.1044e-04, 4.7654e-04, 5.3940e-04,\n                        3.3666e-04, 4.6373e-04, 3.0750e-04, 2.0721e-04, 2.7911e-04, 3.5137e-04,\n                        2.6268e-04, 3.7331e-04, 2.3580e-04, 2.6178e-04, 3.2588e-04, 4.2111e-04,\n                        3.5624e-04, 3.1323e-04, 2.0948e-04, 2.9622e-04, 3.0157e-04, 2.6839e-04,\n                        3.0542e-04, 3.0066e-04, 3.8769e-04, 2.5308e-04, 5.7071e-04, 2.5566e-04,\n                        2.5519e-04, 2.2428e-04, 2.6900e-04, 4.8930e-04, 2.2388e-04, 2.8228e-04,\n                        2.7124e-04, 2.7978e-04, 3.2831e-04, 4.0525e-04, 2.6844e-04, 3.3318e-04,\n                        2.2877e-04, 2.4030e-04, 2.7597e-04, 4.5927e-04, 4.1741e-04, 3.1552e-04,\n                        3.0947e-04, 2.7038e-04, 2.8858e-04, 2.9775e-04, 3.8132e-04, 3.6971e-04,\n                        2.2157e-04, 3.8714e-04, 9.8322e-05, 1.9777e-04, 2.8990e-04, 2.8767e-04,\n                        2.3900e-04, 3.8988e-04, 2.8269e-04, 2.0017e-04, 3.2408e-04, 2.4848e-04,\n                        4.3511e-04, 3.0663e-04, 2.5434e-04, 2.5905e-04, 3.3366e-04, 2.5843e-04,\n                        2.4469e-04, 1.9096e-04, 2.5144e-04, 3.9122e-04, 2.4157e-04, 3.2492e-04,\n                        2.9357e-04, 3.4679e-04, 2.6488e-04, 3.0521e-04, 2.2160e-04, 3.1836e-04,\n                        4.6600e-04, 3.3416e-04, 2.9203e-04, 2.9778e-04, 2.1045e-04, 6.2246e-04,\n                        2.6470e-04, 3.0873e-04, 2.9868e-04, 2.7402e-04, 3.4761e-04, 2.6387e-04,\n                        2.3617e-04, 4.3748e-04, 2.8629e-04, 2.5890e-04, 4.6869e-04, 2.8734e-04,\n                        8.0748e-04, 3.1695e-04, 2.5169e-04, 2.8184e-04, 2.9562e-04, 3.3062e-04,\n                        3.2172e-04, 3.8229e-04, 2.7093e-04, 6.1904e-04, 2.8598e-04, 4.3092e-04,\n                        3.2400e-04, 2.3605e-04, 3.0291e-04, 2.1532e-04, 2.5930e-04, 2.4466e-04,\n                        8.2466e-04, 2.4113e-04, 2.9164e-04, 2.8858e-04, 2.3406e-04, 2.2271e-04,\n                        2.8389e-04, 2.7487e-04, 2.9126e-04, 3.1901e-04, 2.5481e-04, 2.6815e-04,\n                        2.5387e-04, 2.7851e-04, 2.2265e-04, 3.4891e-04, 2.4510e-04, 3.9997e-04,\n                        3.2394e-04, 4.3971e-04, 2.3547e-04, 2.9300e-04, 2.5511e-04, 3.0760e-04,\n                        2.6686e-04, 4.2122e-04, 3.1471e-04, 3.2505e-04, 2.5626e-04, 4.1186e-04,\n                        2.2690e-04, 3.2977e-04, 2.4745e-04, 2.6759e-04, 3.5449e-04, 2.9601e-04,\n                        5.5223e-04, 3.1747e-04, 2.2640e-04, 5.0646e-04, 2.7904e-04, 3.9696e-04,\n                        2.7960e-04, 2.3878e-04, 2.6511e-04, 2.6087e-04, 3.0427e-04, 2.5850e-04,\n                        2.5967e-04, 3.2591e-04, 2.8907e-04, 3.4207e-04, 2.4725e-04, 2.4548e-04,\n                        2.5237e-04, 4.3755e-04, 2.6779e-04, 3.7726e-04, 5.3182e-04, 4.4051e-04,\n                        3.0362e-04, 2.8800e-04, 4.1287e-04, 4.3871e-04, 2.4280e-04, 3.6331e-04,\n                        2.6193e-04, 4.1276e-04, 2.5831e-04, 3.3477e-04, 2.9292e-04, 3.6628e-04,\n                        5.3160e-04, 2.9771e-04, 2.4128e-04, 3.5639e-04, 3.1765e-04, 3.3068e-04,\n                        2.5711e-04, 3.3150e-04, 2.4887e-04, 2.5627e-04, 3.1532e-04, 2.9154e-04,\n                        2.6687e-04, 3.3633e-04, 3.2622e-04, 5.8133e-04, 3.1996e-04, 4.7386e-04,\n                        4.5345e-04, 3.6571e-04, 2.6542e-04, 2.5200e-04, 2.9372e-04, 3.6002e-04,\n                        3.2303e-04, 2.8973e-04, 4.0259e-04, 2.8257e-04, 2.1097e-04, 2.8196e-04,\n                        2.1337e-04, 2.9418e-04, 3.2041e-04, 2.5385e-04, 5.8283e-04, 2.6575e-04,\n                        2.6789e-04, 2.4644e-04, 2.8313e-04, 3.6763e-04, 2.3836e-04, 2.2475e-04,\n                        2.6800e-04, 2.5053e-04, 2.6584e-04, 3.9372e-04, 3.3288e-04, 3.1086e-04,\n                        2.3797e-04, 3.7407e-04, 2.2597e-04, 3.7725e-04, 2.6491e-04, 2.3415e-04,\n                        3.4244e-04, 3.2485e-04, 2.6550e-04, 5.6603e-04, 4.5209e-04, 3.2283e-04,\n                        2.7324e-04, 3.8537e-04, 5.4614e-04, 2.9156e-04, 2.6848e-04, 3.4902e-04,\n                        4.6044e-04, 4.0100e-04, 2.4618e-04, 2.6285e-04, 2.5929e-04, 1.8615e-04,\n                        2.7461e-04, 2.4654e-04, 2.5841e-04, 2.2457e-04, 5.8846e-04, 2.6959e-04,\n                        2.6827e-04, 2.4253e-04, 4.6806e-04, 3.5401e-04, 2.7014e-04, 2.6745e-04,\n                        3.1462e-04, 2.8178e-04, 3.2718e-04, 3.0511e-04, 3.6806e-04, 2.5112e-04,\n                        4.1674e-04, 3.5653e-04, 2.9727e-04, 2.6902e-04, 3.5654e-04, 4.0240e-04,\n                        3.0044e-04, 4.3693e-04, 3.3656e-04, 3.1097e-04, 2.6606e-04, 3.1455e-04,\n                        4.8043e-04, 2.1650e-04, 4.6255e-04, 4.7141e-04, 2.6254e-04, 2.5138e-04,\n                        2.7692e-04, 2.7228e-04, 2.7284e-04, 2.7293e-04, 2.6957e-04, 2.7004e-04,\n                        4.5229e-04, 4.5600e-04, 3.4163e-04, 3.0988e-04, 2.7575e-04, 2.7574e-04,\n                        3.0622e-04, 5.2652e-04, 2.4026e-04, 2.6393e-04, 2.7068e-04, 5.7156e-04,\n                        3.0074e-04, 2.3128e-04, 3.4530e-04, 3.8930e-04, 2.8868e-04, 2.5471e-04,\n                        3.3771e-04, 2.5100e-04, 2.6477e-04, 2.5935e-04, 5.6483e-04, 3.0366e-04,\n                        3.5433e-04, 2.7302e-04, 2.2806e-04, 1.8661e-04, 2.5619e-04, 2.5326e-04]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                       dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n                (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n                  min_val=tensor([-0.0236, -0.0417, -0.0375, -0.0357, -0.0302, -0.0351, -0.0269, -0.0359,\n                          -0.0316, -0.0437, -0.0503, -0.0493, -0.0353, -0.0314, -0.0233, -0.0360,\n                          -0.0244, -0.0414, -0.0326, -0.0356, -0.0344, -0.0373, -0.0357, -0.0363,\n                          -0.0453, -0.0434, -0.0376, -0.0333, -0.0290, -0.0283, -0.0254, -0.0379,\n                          -0.0419, -0.0001, -0.0368, -0.0420, -0.0330, -0.0238, -0.0381, -0.0550,\n                          -0.0334, -0.0303, -0.0333, -0.0286, -0.0297, -0.0630, -0.0356, -0.0565,\n                          -0.0298, -0.0347, -0.0392, -0.0406, -0.0277, -0.0356, -0.0350, -0.0292,\n                          -0.0372, -0.0353, -0.0369, -0.0309, -0.0318, -0.0328, -0.0287, -0.0360,\n                          -0.0263, -0.0338, -0.0334, -0.0319, -0.0260, -0.0427, -0.0449, -0.0332,\n                          -0.0306, -0.0313, -0.0348, -0.0286, -0.0329, -0.0246, -0.0381, -0.0611,\n                          -0.0358, -0.0360, -0.0361, -0.0406, -0.0302, -0.0400, -0.0291, -0.0612,\n                          -0.0263, -0.0245, -0.0307, -0.0427, -0.0408, -0.0301, -0.0441, -0.0542,\n                          -0.0299, -0.0292, -0.0416, -0.0348, -0.0397, -0.0381, -0.0302, -0.0285,\n                          -0.0534, -0.0306, -0.0494, -0.0320, -0.0269, -0.0600, -0.0294, -0.0518,\n                          -0.0277, -0.0348, -0.0328, -0.0298, -0.0304, -0.0237, -0.0264, -0.0310,\n                          -0.0380, -0.0567, -0.0359, -0.0284, -0.0326, -0.0522, -0.0374, -0.0261,\n                          -0.0279, -0.0402, -0.0347, -0.0381, -0.0255, -0.0315, -0.0285, -0.0346,\n                          -0.0332, -0.0374, -0.0448, -0.0323, -0.0248, -0.0502, -0.0332, -0.0364,\n                          -0.0412, -0.0332, -0.0328, -0.0326, -0.0274, -0.0345, -0.0420, -0.0315,\n                          -0.0304, -0.0282, -0.0308, -0.0407, -0.0340, -0.0361, -0.0422, -0.0379,\n                          -0.0298, -0.0334, -0.0324, -0.0337, -0.0291, -0.0314, -0.0309, -0.0482,\n                          -0.0282, -0.0285, -0.0316, -0.0394, -0.0330, -0.0276, -0.0527, -0.0235,\n                          -0.0413, -0.0449, -0.0292, -0.0257, -0.0360, -0.0270, -0.0309, -0.0392,\n                          -0.0349, -0.0278, -0.0359, -0.0248, -0.0271, -0.0328, -0.0285, -0.0308,\n                          -0.0301, -0.0289, -0.0386, -0.0337, -0.0303, -0.0329, -0.0233, -0.0417,\n                          -0.0353, -0.0407, -0.0386, -0.0274, -0.0283, -0.0361, -0.0402, -0.0418,\n                          -0.0343, -0.0323, -0.0257, -0.0331, -0.0369, -0.0369, -0.0310, -0.0368,\n                          -0.0590, -0.0325, -0.0366, -0.0380, -0.0350, -0.0321, -0.0269, -0.0260,\n                          -0.0268, -0.0238, -0.0411, -0.0286, -0.0286, -0.0355, -0.0380, -0.0405,\n                          -0.0313, -0.0505, -0.0268, -0.0455, -0.0378, -0.0444, -0.0345, -0.0275,\n                          -0.0317, -0.0298, -0.0284, -0.0410, -0.0401, -0.0343, -0.0354, -0.0425,\n                          -0.0361, -0.0445, -0.0502, -0.0290, -0.0314, -0.0312, -0.0276, -0.0266,\n                          -0.0293, -0.0397, -0.0387, -0.0358, -0.0351, -0.0433, -0.0322, -0.0430,\n                          -0.0274, -0.0328, -0.0605, -0.0325, -0.0275, -0.0269, -0.0308, -0.0294,\n                          -0.0247, -0.0362, -0.0357, -0.0345, -0.0337, -0.0349, -0.0254, -0.0423,\n                          -0.0511, -0.0312, -0.0293, -0.0342, -0.0279, -0.0341, -0.0349, -0.0281,\n                          -0.0375, -0.0361, -0.0323, -0.0358, -0.0426, -0.0404, -0.0408, -0.0360,\n                          -0.0492, -0.0353, -0.0556, -0.0485, -0.0402, -0.0465, -0.0296, -0.0281,\n                          -0.0259, -0.0379, -0.0574, -0.0494, -0.0411, -0.0324, -0.0246, -0.0262,\n                          -0.0361, -0.0284, -0.0389, -0.0299, -0.0447, -0.0353, -0.0289, -0.0324,\n                          -0.0344, -0.0561, -0.0450, -0.0649, -0.0280, -0.0418, -0.0305, -0.0352,\n                          -0.0426, -0.0338, -0.0241, -0.0265, -0.0272, -0.0510, -0.0560, -0.0297,\n                          -0.0305, -0.0285, -0.0251, -0.0298, -0.0336, -0.0548, -0.0250, -0.0352,\n                          -0.0312, -0.0496, -0.0383, -0.0320, -0.0378, -0.0307, -0.0419, -0.0270,\n                          -0.0434, -0.0304, -0.0608, -0.0374, -0.0262, -0.0392, -0.0267, -0.0310,\n                          -0.0281, -0.0318, -0.0341, -0.0357, -0.0506, -0.0527, -0.0443, -0.0648,\n                          -0.0357, -0.0347, -0.0368, -0.0362, -0.0461, -0.0339, -0.0350, -0.0347,\n                          -0.0326, -0.0628, -0.0325, -0.0352, -0.0410, -0.0312, -0.0323, -0.0382,\n                          -0.0413, -0.0297, -0.0264, -0.0449, -0.0305, -0.0343, -0.0231, -0.0625,\n                          -0.0556, -0.0271, -0.0461, -0.0335, -0.0318, -0.0318, -0.0424, -0.0269,\n                          -0.0549, -0.0562, -0.0322, -0.0490, -0.0350, -0.0267, -0.0827, -0.0460,\n                          -0.0354, -0.0286, -0.0483, -0.0287, -0.0270, -0.0442, -0.0391, -0.0503,\n                          -0.0402, -0.0482, -0.0380, -0.0349, -0.0304, -0.0437, -0.0431, -0.0394,\n                          -0.0277, -0.0307, -0.0293, -0.0345, -0.0324, -0.0584, -0.0242, -0.0271,\n                          -0.0355, -0.0274, -0.0330, -0.0376, -0.0421, -0.0261, -0.0308, -0.0368,\n                          -0.0380, -0.0188, -0.0271, -0.0300, -0.0340, -0.0365, -0.0379, -0.0405,\n                          -0.0459, -0.0396, -0.0374, -0.0337, -0.0675, -0.0357, -0.0312, -0.0412,\n                          -0.0307, -0.0270, -0.0375, -0.0248, -0.0305, -0.0365, -0.0432, -0.0352,\n                          -0.0238, -0.0385, -0.0380, -0.0324, -0.0261, -0.0285, -0.0286, -0.0357,\n                          -0.0392, -0.0299, -0.0361, -0.0308, -0.0305, -0.0424, -0.0539, -0.0411,\n                          -0.0384, -0.0408, -0.0422, -0.0549, -0.0366, -0.0347, -0.0281, -0.0289,\n                          -0.0446, -0.0423, -0.0284, -0.0317, -0.0340, -0.0604, -0.0385, -0.0310,\n                          -0.0434, -0.0407, -0.0319, -0.0333, -0.0296, -0.0559, -0.0374, -0.0741,\n                          -0.0289, -0.0386, -0.0299, -0.0484, -0.0509, -0.0441, -0.0351, -0.0344,\n                          -0.0370, -0.0305, -0.0394, -0.0364, -0.0322, -0.0265, -0.0444, -0.0237,\n                          -0.0264, -0.0228, -0.0302, -0.0333, -0.0447, -0.0340, -0.0313, -0.0299,\n                          -0.0347, -0.0252, -0.0477, -0.0249, -0.0425, -0.0323, -0.0344, -0.0295,\n                          -0.0306, -0.0414, -0.0227, -0.0291, -0.0358, -0.0264, -0.0536, -0.0286,\n                          -0.0335, -0.0462, -0.0400, -0.0407, -0.0244, -0.0413, -0.0542, -0.0363,\n                          -0.0722, -0.0339, -0.0334, -0.0297, -0.0414, -0.0287, -0.0395, -0.0433,\n                          -0.0407, -0.0291, -0.0496, -0.0362, -0.0474, -0.0266, -0.0253, -0.0278,\n                          -0.0297, -0.0309, -0.0283, -0.0318, -0.0319, -0.0309, -0.0295, -0.0336,\n                          -0.0313, -0.0354, -0.0338, -0.0486, -0.0287, -0.0356, -0.0325, -0.0278,\n                          -0.0319, -0.0242, -0.0243, -0.0302, -0.0333, -0.0324, -0.0401, -0.0306,\n                          -0.0446, -0.0383, -0.0297, -0.0349, -0.0245, -0.0400, -0.0566, -0.0229,\n                          -0.0250, -0.0293, -0.0274, -0.0250, -0.0307, -0.0504, -0.0272, -0.0299,\n                          -0.0376, -0.0423, -0.0323, -0.0344, -0.0353, -0.0317, -0.0272, -0.0374,\n                          -0.0376, -0.0352, -0.0272, -0.0363, -0.0319, -0.0274, -0.0305, -0.0401,\n                          -0.0710, -0.0292, -0.0334, -0.0340, -0.0524, -0.0690, -0.0340, -0.0594,\n                          -0.0322, -0.0265, -0.0357, -0.0450, -0.0336, -0.0478, -0.0302, -0.0335,\n                          -0.0417, -0.0488, -0.0344, -0.0401, -0.0268, -0.0379, -0.0294, -0.0300,\n                          -0.0391, -0.0335, -0.0496, -0.0324, -0.0676, -0.0263, -0.0327, -0.0245,\n                          -0.0304, -0.0380, -0.0287, -0.0271, -0.0320, -0.0330, -0.0388, -0.0475,\n                          -0.0344, -0.0401, -0.0293, -0.0302, -0.0336, -0.0588, -0.0448, -0.0404,\n                          -0.0329, -0.0346, -0.0369, -0.0379, -0.0478, -0.0473, -0.0275, -0.0440,\n                          -0.0122, -0.0253, -0.0371, -0.0368, -0.0302, -0.0499, -0.0298, -0.0239,\n                          -0.0415, -0.0250, -0.0557, -0.0351, -0.0326, -0.0332, -0.0386, -0.0331,\n                          -0.0313, -0.0195, -0.0322, -0.0501, -0.0252, -0.0363, -0.0376, -0.0425,\n                          -0.0321, -0.0260, -0.0284, -0.0271, -0.0596, -0.0428, -0.0312, -0.0352,\n                          -0.0269, -0.0797, -0.0339, -0.0348, -0.0321, -0.0326, -0.0445, -0.0322,\n                          -0.0302, -0.0560, -0.0316, -0.0286, -0.0600, -0.0284, -0.1034, -0.0406,\n                          -0.0287, -0.0361, -0.0378, -0.0317, -0.0412, -0.0440, -0.0294, -0.0682,\n                          -0.0366, -0.0552, -0.0415, -0.0264, -0.0243, -0.0276, -0.0237, -0.0306,\n                          -0.0874, -0.0274, -0.0342, -0.0369, -0.0268, -0.0273, -0.0363, -0.0352,\n                          -0.0323, -0.0361, -0.0326, -0.0303, -0.0265, -0.0323, -0.0285, -0.0373,\n                          -0.0314, -0.0327, -0.0272, -0.0470, -0.0301, -0.0310, -0.0327, -0.0361,\n                          -0.0342, -0.0539, -0.0378, -0.0283, -0.0289, -0.0468, -0.0290, -0.0380,\n                          -0.0317, -0.0328, -0.0454, -0.0379, -0.0560, -0.0406, -0.0273, -0.0648,\n                          -0.0357, -0.0508, -0.0358, -0.0306, -0.0250, -0.0317, -0.0255, -0.0314,\n                          -0.0268, -0.0307, -0.0370, -0.0438, -0.0301, -0.0263, -0.0323, -0.0560,\n                          -0.0343, -0.0322, -0.0654, -0.0509, -0.0314, -0.0369, -0.0465, -0.0562,\n                          -0.0311, -0.0465, -0.0335, -0.0528, -0.0331, -0.0429, -0.0269, -0.0437,\n                          -0.0680, -0.0381, -0.0255, -0.0456, -0.0407, -0.0423, -0.0329, -0.0424,\n                          -0.0319, -0.0326, -0.0365, -0.0349, -0.0253, -0.0300, -0.0313, -0.0541,\n                          -0.0366, -0.0607, -0.0580, -0.0429, -0.0291, -0.0315, -0.0376, -0.0461,\n                          -0.0371, -0.0286, -0.0515, -0.0362, -0.0229, -0.0342, -0.0273, -0.0377,\n                          -0.0410, -0.0325, -0.0746, -0.0328, -0.0343, -0.0270, -0.0290, -0.0383,\n                          -0.0291, -0.0275, -0.0293, -0.0266, -0.0288, -0.0504, -0.0414, -0.0398,\n                          -0.0305, -0.0479, -0.0289, -0.0483, -0.0339, -0.0300, -0.0401, -0.0333,\n                          -0.0295, -0.0492, -0.0579, -0.0243, -0.0350, -0.0493, -0.0544, -0.0373,\n                          -0.0344, -0.0447, -0.0589, -0.0513, -0.0315, -0.0285, -0.0303, -0.0191,\n                          -0.0325, -0.0316, -0.0289, -0.0223, -0.0753, -0.0345, -0.0318, -0.0284,\n                          -0.0428, -0.0371, -0.0290, -0.0342, -0.0282, -0.0260, -0.0311, -0.0391,\n                          -0.0471, -0.0319, -0.0450, -0.0346, -0.0343, -0.0304, -0.0456, -0.0406,\n                          -0.0368, -0.0423, -0.0286, -0.0398, -0.0341, -0.0337, -0.0615, -0.0265,\n                          -0.0592, -0.0603, -0.0334, -0.0250, -0.0312, -0.0316, -0.0330, -0.0343,\n                          -0.0286, -0.0343, -0.0579, -0.0584, -0.0304, -0.0397, -0.0266, -0.0353,\n                          -0.0392, -0.0674, -0.0308, -0.0303, -0.0346, -0.0499, -0.0385, -0.0291,\n                          -0.0400, -0.0498, -0.0243, -0.0326, -0.0399, -0.0321, -0.0261, -0.0295,\n                          -0.0723, -0.0342, -0.0454, -0.0317, -0.0235, -0.0202, -0.0328, -0.0324]), max_val=tensor([0.0261, 0.0507, 0.0385, 0.0356, 0.0248, 0.0395, 0.0372, 0.0357, 0.0387,\n                          0.0339, 0.0441, 0.0523, 0.0389, 0.0276, 0.0318, 0.0361, 0.0354, 0.0350,\n                          0.0405, 0.0310, 0.0285, 0.0339, 0.0352, 0.0435, 0.0670, 0.0429, 0.0298,\n                          0.0306, 0.0309, 0.0348, 0.0232, 0.0304, 0.0656, 0.0002, 0.0351, 0.0383,\n                          0.0304, 0.0403, 0.0382, 0.0706, 0.0457, 0.0281, 0.0262, 0.0344, 0.0323,\n                          0.0532, 0.0394, 0.0571, 0.0262, 0.0312, 0.0449, 0.0361, 0.0350, 0.0304,\n                          0.0343, 0.0293, 0.0407, 0.0330, 0.0285, 0.0302, 0.0280, 0.0349, 0.0304,\n                          0.0287, 0.0298, 0.0370, 0.0296, 0.0281, 0.0366, 0.0598, 0.0372, 0.0367,\n                          0.0325, 0.0277, 0.0341, 0.0289, 0.0327, 0.0273, 0.0360, 0.1236, 0.0353,\n                          0.0379, 0.0348, 0.0346, 0.0379, 0.0365, 0.0323, 0.0593, 0.0352, 0.0277,\n                          0.0286, 0.0680, 0.0524, 0.0309, 0.0702, 0.0539, 0.0313, 0.0344, 0.0260,\n                          0.0227, 0.0459, 0.0322, 0.0322, 0.0349, 0.0379, 0.0400, 0.0421, 0.0240,\n                          0.0294, 0.0562, 0.0399, 0.0586, 0.0423, 0.0355, 0.0310, 0.0282, 0.0284,\n                          0.0322, 0.0367, 0.0422, 0.0380, 0.0528, 0.0340, 0.0362, 0.0433, 0.0384,\n                          0.0341, 0.0232, 0.0333, 0.0317, 0.0272, 0.0387, 0.0255, 0.0286, 0.0323,\n                          0.0421, 0.0294, 0.0306, 0.0485, 0.0394, 0.0229, 0.0743, 0.0340, 0.0349,\n                          0.0305, 0.0329, 0.0356, 0.0293, 0.0261, 0.0403, 0.0295, 0.0383, 0.0338,\n                          0.0259, 0.0329, 0.0405, 0.0307, 0.0314, 0.0282, 0.0290, 0.0315, 0.0330,\n                          0.0394, 0.0360, 0.0399, 0.0313, 0.0462, 0.0416, 0.0340, 0.0290, 0.0408,\n                          0.0328, 0.0467, 0.0321, 0.0547, 0.0315, 0.0496, 0.0535, 0.0321, 0.0341,\n                          0.0382, 0.0304, 0.0312, 0.0266, 0.0306, 0.0365, 0.0312, 0.0505, 0.0320,\n                          0.0325, 0.0344, 0.0326, 0.0226, 0.0285, 0.0504, 0.0313, 0.0279, 0.0292,\n                          0.0333, 0.0333, 0.0502, 0.0447, 0.0626, 0.0308, 0.0321, 0.0292, 0.0311,\n                          0.0323, 0.0363, 0.0292, 0.0336, 0.0317, 0.0338, 0.0397, 0.0268, 0.0271,\n                          0.0681, 0.0305, 0.0385, 0.0316, 0.0332, 0.0248, 0.0320, 0.0326, 0.0278,\n                          0.0268, 0.0409, 0.0383, 0.0420, 0.0285, 0.0305, 0.0363, 0.0264, 0.0477,\n                          0.0300, 0.0315, 0.0311, 0.0457, 0.0391, 0.0291, 0.0286, 0.0393, 0.0453,\n                          0.0398, 0.0371, 0.0321, 0.0312, 0.0312, 0.0369, 0.0446, 0.0394, 0.0365,\n                          0.0367, 0.0271, 0.0293, 0.0357, 0.0244, 0.0336, 0.0317, 0.0298, 0.0448,\n                          0.0364, 0.0332, 0.0525, 0.0264, 0.0296, 0.0523, 0.0323, 0.0285, 0.0411,\n                          0.0363, 0.0334, 0.0319, 0.0326, 0.0340, 0.0386, 0.0448, 0.0555, 0.0279,\n                          0.0441, 0.0604, 0.0323, 0.0241, 0.0430, 0.0274, 0.0305, 0.0289, 0.0301,\n                          0.0276, 0.0318, 0.0265, 0.0405, 0.0399, 0.0394, 0.0359, 0.0309, 0.0507,\n                          0.0276, 0.0534, 0.0312, 0.0270, 0.0284, 0.0346, 0.0333, 0.0205, 0.0484,\n                          0.0600, 0.0335, 0.0282, 0.0271, 0.0272, 0.0354, 0.0343, 0.0490, 0.0422,\n                          0.0300, 0.0338, 0.0324, 0.0332, 0.0406, 0.0332, 0.0465, 0.0311, 0.0541,\n                          0.0337, 0.0893, 0.0316, 0.0373, 0.0425, 0.0373, 0.0293, 0.0356, 0.0338,\n                          0.0527, 0.0533, 0.0287, 0.0400, 0.0371, 0.0311, 0.0250, 0.0336, 0.0356,\n                          0.0376, 0.0381, 0.0344, 0.0440, 0.0376, 0.0390, 0.0293, 0.0372, 0.0387,\n                          0.0339, 0.0268, 0.0389, 0.0508, 0.0285, 0.0441, 0.0336, 0.0378, 0.0344,\n                          0.0280, 0.0332, 0.0374, 0.0283, 0.0377, 0.0348, 0.0422, 0.0612, 0.0332,\n                          0.0384, 0.0332, 0.0292, 0.0692, 0.0366, 0.0344, 0.0338, 0.0342, 0.0622,\n                          0.0388, 0.0309, 0.0379, 0.0370, 0.0322, 0.0368, 0.0343, 0.0227, 0.0260,\n                          0.0392, 0.0349, 0.0282, 0.0242, 0.0538, 0.0401, 0.0370, 0.0352, 0.0313,\n                          0.0347, 0.0241, 0.0314, 0.0357, 0.0515, 0.0536, 0.0428, 0.0447, 0.0604,\n                          0.0366, 0.0767, 0.0647, 0.0362, 0.0277, 0.0440, 0.0314, 0.0324, 0.0336,\n                          0.0365, 0.0441, 0.0363, 0.0438, 0.0323, 0.0368, 0.0308, 0.0635, 0.0368,\n                          0.0429, 0.0300, 0.0274, 0.0287, 0.0348, 0.0377, 0.0563, 0.0328, 0.0293,\n                          0.0341, 0.0261, 0.0467, 0.0241, 0.0366, 0.0408, 0.0340, 0.0354, 0.0408,\n                          0.0183, 0.0254, 0.0422, 0.0353, 0.0249, 0.0363, 0.0291, 0.0311, 0.0295,\n                          0.0453, 0.0368, 0.0600, 0.0356, 0.0303, 0.0274, 0.0263, 0.0313, 0.0346,\n                          0.0276, 0.0335, 0.0336, 0.0509, 0.0287, 0.0286, 0.0401, 0.0312, 0.0291,\n                          0.0356, 0.0362, 0.0313, 0.0453, 0.0354, 0.0389, 0.0312, 0.0360, 0.0237,\n                          0.0288, 0.0332, 0.0424, 0.0281, 0.0359, 0.0454, 0.0328, 0.0383, 0.0406,\n                          0.0389, 0.0302, 0.0302, 0.0722, 0.0320, 0.0382, 0.0376, 0.0628, 0.0298,\n                          0.0295, 0.0426, 0.0596, 0.0271, 0.0380, 0.0327, 0.0460, 0.0393, 0.0591,\n                          0.0348, 0.0333, 0.0282, 0.0551, 0.0404, 0.0473, 0.0379, 0.0341, 0.0335,\n                          0.0355, 0.0429, 0.0352, 0.0275, 0.0230, 0.0431, 0.0301, 0.0343, 0.0304,\n                          0.0324, 0.0404, 0.0524, 0.0334, 0.0329, 0.0407, 0.0270, 0.0348, 0.0530,\n                          0.0273, 0.0341, 0.0319, 0.0309, 0.0315, 0.0348, 0.0361, 0.0341, 0.0310,\n                          0.0378, 0.0273, 0.0534, 0.0258, 0.0319, 0.0531, 0.0322, 0.0461, 0.0397,\n                          0.0392, 0.0716, 0.0302, 0.0665, 0.0287, 0.0303, 0.0466, 0.0350, 0.0296,\n                          0.0605, 0.0364, 0.0296, 0.0385, 0.0323, 0.0328, 0.0430, 0.0232, 0.0347,\n                          0.0393, 0.0302, 0.0317, 0.0415, 0.0329, 0.0315, 0.0374, 0.0297, 0.0333,\n                          0.0325, 0.0343, 0.0321, 0.0481, 0.0275, 0.0441, 0.0248, 0.0248, 0.0336,\n                          0.0353, 0.0386, 0.0344, 0.0379, 0.0344, 0.0329, 0.0262, 0.0366, 0.0255,\n                          0.0432, 0.0254, 0.0461, 0.0358, 0.0380, 0.0364, 0.0353, 0.0414, 0.0314,\n                          0.0347, 0.0299, 0.0573, 0.0374, 0.0266, 0.0299, 0.0293, 0.0322, 0.0299,\n                          0.0306, 0.0351, 0.0317, 0.0435, 0.0468, 0.0352, 0.0330, 0.0369, 0.0365,\n                          0.0297, 0.0341, 0.0496, 0.0665, 0.0343, 0.0326, 0.0394, 0.0605, 0.0568,\n                          0.0428, 0.0461, 0.0391, 0.0259, 0.0337, 0.0341, 0.0326, 0.0380, 0.0285,\n                          0.0309, 0.0413, 0.0535, 0.0452, 0.0289, 0.0252, 0.0291, 0.0383, 0.0341,\n                          0.0346, 0.0382, 0.0384, 0.0304, 0.0725, 0.0325, 0.0299, 0.0285, 0.0342,\n                          0.0621, 0.0264, 0.0358, 0.0344, 0.0355, 0.0417, 0.0515, 0.0340, 0.0423,\n                          0.0263, 0.0305, 0.0350, 0.0443, 0.0530, 0.0342, 0.0393, 0.0309, 0.0311,\n                          0.0378, 0.0484, 0.0412, 0.0281, 0.0492, 0.0125, 0.0247, 0.0337, 0.0364,\n                          0.0304, 0.0474, 0.0359, 0.0254, 0.0403, 0.0316, 0.0480, 0.0389, 0.0315,\n                          0.0289, 0.0424, 0.0319, 0.0309, 0.0243, 0.0311, 0.0388, 0.0307, 0.0413,\n                          0.0317, 0.0440, 0.0336, 0.0388, 0.0268, 0.0404, 0.0333, 0.0297, 0.0371,\n                          0.0378, 0.0252, 0.0623, 0.0329, 0.0392, 0.0379, 0.0348, 0.0372, 0.0335,\n                          0.0252, 0.0537, 0.0364, 0.0329, 0.0490, 0.0365, 0.0705, 0.0369, 0.0320,\n                          0.0325, 0.0364, 0.0420, 0.0381, 0.0486, 0.0344, 0.0786, 0.0334, 0.0527,\n                          0.0334, 0.0300, 0.0385, 0.0270, 0.0329, 0.0311, 0.1047, 0.0306, 0.0370,\n                          0.0320, 0.0297, 0.0283, 0.0345, 0.0300, 0.0370, 0.0405, 0.0294, 0.0341,\n                          0.0322, 0.0354, 0.0258, 0.0443, 0.0275, 0.0508, 0.0411, 0.0558, 0.0274,\n                          0.0372, 0.0262, 0.0391, 0.0273, 0.0513, 0.0400, 0.0413, 0.0325, 0.0523,\n                          0.0232, 0.0419, 0.0294, 0.0340, 0.0335, 0.0297, 0.0701, 0.0389, 0.0288,\n                          0.0615, 0.0323, 0.0484, 0.0249, 0.0297, 0.0337, 0.0331, 0.0386, 0.0328,\n                          0.0330, 0.0414, 0.0244, 0.0341, 0.0314, 0.0312, 0.0319, 0.0464, 0.0319,\n                          0.0479, 0.0675, 0.0559, 0.0386, 0.0296, 0.0524, 0.0390, 0.0263, 0.0346,\n                          0.0300, 0.0432, 0.0291, 0.0372, 0.0372, 0.0465, 0.0620, 0.0289, 0.0306,\n                          0.0354, 0.0334, 0.0385, 0.0284, 0.0266, 0.0285, 0.0325, 0.0400, 0.0370,\n                          0.0339, 0.0427, 0.0414, 0.0738, 0.0406, 0.0521, 0.0574, 0.0464, 0.0337,\n                          0.0320, 0.0318, 0.0301, 0.0410, 0.0368, 0.0497, 0.0298, 0.0268, 0.0358,\n                          0.0250, 0.0319, 0.0346, 0.0289, 0.0456, 0.0338, 0.0284, 0.0313, 0.0360,\n                          0.0467, 0.0303, 0.0285, 0.0340, 0.0318, 0.0338, 0.0423, 0.0423, 0.0278,\n                          0.0257, 0.0416, 0.0277, 0.0387, 0.0321, 0.0294, 0.0435, 0.0413, 0.0337,\n                          0.0719, 0.0321, 0.0410, 0.0346, 0.0308, 0.0694, 0.0310, 0.0298, 0.0408,\n                          0.0443, 0.0381, 0.0284, 0.0334, 0.0329, 0.0236, 0.0349, 0.0307, 0.0328,\n                          0.0285, 0.0611, 0.0291, 0.0341, 0.0308, 0.0594, 0.0450, 0.0343, 0.0268,\n                          0.0400, 0.0358, 0.0416, 0.0319, 0.0392, 0.0319, 0.0529, 0.0453, 0.0378,\n                          0.0342, 0.0365, 0.0511, 0.0382, 0.0555, 0.0427, 0.0375, 0.0257, 0.0399,\n                          0.0535, 0.0275, 0.0528, 0.0516, 0.0333, 0.0319, 0.0352, 0.0346, 0.0347,\n                          0.0347, 0.0342, 0.0343, 0.0556, 0.0523, 0.0434, 0.0281, 0.0350, 0.0304,\n                          0.0293, 0.0471, 0.0284, 0.0335, 0.0289, 0.0726, 0.0344, 0.0294, 0.0439,\n                          0.0298, 0.0367, 0.0257, 0.0429, 0.0300, 0.0336, 0.0329, 0.0466, 0.0386,\n                          0.0356, 0.0347, 0.0290, 0.0237, 0.0316, 0.0234])\n                )\n              )\n            )\n            (2): ReLU6(inplace=True)\n          )\n          (1): Module(\n            (0): ConvBn2d(\n              960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False\n              (bn): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n                fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0127, 0.0019, 0.0056, 0.0074, 0.0061, 0.0071, 0.0088, 0.0041, 0.0051,\n                        0.0055, 0.0019, 0.0014, 0.0060, 0.0060, 0.0045, 0.0055, 0.0087, 0.0078,\n                        0.0066, 0.0072, 0.0069, 0.0025, 0.0051, 0.0019, 0.0017, 0.0062, 0.0047,\n                        0.0069, 0.0036, 0.0033, 0.0060, 0.0045, 0.0013, 0.0003, 0.0022, 0.0075,\n                        0.0051, 0.0067, 0.0022, 0.0024, 0.0057, 0.0054, 0.0031, 0.0058, 0.0034,\n                        0.0020, 0.0056, 0.0017, 0.0046, 0.0068, 0.0024, 0.0054, 0.0057, 0.0069,\n                        0.0027, 0.0058, 0.0023, 0.0022, 0.0038, 0.0047, 0.0036, 0.0052, 0.0048,\n                        0.0067, 0.0054, 0.0088, 0.0029, 0.0054, 0.0031, 0.0015, 0.0017, 0.0055,\n                        0.0060, 0.0050, 0.0066, 0.0058, 0.0063, 0.0034, 0.0081, 0.0012, 0.0027,\n                        0.0052, 0.0053, 0.0059, 0.0050, 0.0049, 0.0062, 0.0017, 0.0054, 0.0039,\n                        0.0047, 0.0028, 0.0024, 0.0064, 0.0016, 0.0011, 0.0061, 0.0062, 0.0029,\n                        0.0040, 0.0116, 0.0046, 0.0057, 0.0050, 0.0014, 0.0032, 0.0023, 0.0037,\n                        0.0075, 0.0015, 0.0060, 0.0020, 0.0064, 0.0024, 0.0062, 0.0061, 0.0061,\n                        0.0060, 0.0042, 0.0065, 0.0026, 0.0017, 0.0036, 0.0026, 0.0052, 0.0022,\n                        0.0069, 0.0040, 0.0066, 0.0056, 0.0054, 0.0072, 0.0056, 0.0059, 0.0025,\n                        0.0070, 0.0057, 0.0040, 0.0104, 0.0035, 0.0042, 0.0017, 0.0060, 0.0062,\n                        0.0037, 0.0037, 0.0031, 0.0058, 0.0037, 0.0076, 0.0049, 0.0059, 0.0056,\n                        0.0060, 0.0085, 0.0021, 0.0053, 0.0070, 0.0038, 0.0055, 0.0027, 0.0041,\n                        0.0082, 0.0069, 0.0059, 0.0025, 0.0032, 0.0026, 0.0048, 0.0062, 0.0034,\n                        0.0039, 0.0074, 0.0033, 0.0018, 0.0039, 0.0031, 0.0015, 0.0055, 0.0045,\n                        0.0041, 0.0059, 0.0052, 0.0075, 0.0031, 0.0045, 0.0029, 0.0063, 0.0070,\n                        0.0027, 0.0052, 0.0048, 0.0041, 0.0048, 0.0046, 0.0055, 0.0028, 0.0029,\n                        0.0060, 0.0068, 0.0076, 0.0066, 0.0092, 0.0059, 0.0054, 0.0078, 0.0058,\n                        0.0071, 0.0048, 0.0068, 0.0033, 0.0062, 0.0050, 0.0061, 0.0041, 0.0076,\n                        0.0017, 0.0062, 0.0031, 0.0025, 0.0017, 0.0029, 0.0030, 0.0045, 0.0035,\n                        0.0047, 0.0024, 0.0021, 0.0025, 0.0057, 0.0092, 0.0030, 0.0031, 0.0018,\n                        0.0053, 0.0077, 0.0062, 0.0116, 0.0093, 0.0068, 0.0065, 0.0055, 0.0051,\n                        0.0027, 0.0023, 0.0057, 0.0089, 0.0058, 0.0051, 0.0026, 0.0026, 0.0054,\n                        0.0068, 0.0056, 0.0048, 0.0059, 0.0074, 0.0075, 0.0072, 0.0074, 0.0028,\n                        0.0018, 0.0030, 0.0013, 0.0027, 0.0069, 0.0015, 0.0081, 0.0039, 0.0061,\n                        0.0062, 0.0061, 0.0033, 0.0064, 0.0064, 0.0051, 0.0078, 0.0027, 0.0044,\n                        0.0023, 0.0018, 0.0062, 0.0058, 0.0080, 0.0055, 0.0063, 0.0048, 0.0033,\n                        0.0052, 0.0030, 0.0056, 0.0028, 0.0018, 0.0062, 0.0027, 0.0021, 0.0016,\n                        0.0068, 0.0015, 0.0064, 0.0029, 0.0030, 0.0065, 0.0040, 0.0042, 0.0059,\n                        0.0020, 0.0075, 0.0059, 0.0059, 0.0059, 0.0044, 0.0025, 0.0024, 0.0031,\n                        0.0069, 0.0065, 0.0058, 0.0066, 0.0033, 0.0069, 0.0018, 0.0097, 0.0025,\n                        0.0032, 0.0015, 0.0069, 0.0030, 0.0139, 0.0038, 0.0034, 0.0064, 0.0048,\n                        0.0016, 0.0019, 0.0036, 0.0062, 0.0046, 0.0066, 0.0038, 0.0053, 0.0017,\n                        0.0041, 0.0032, 0.0066, 0.0018, 0.0060, 0.0052, 0.0034, 0.0071, 0.0062,\n                        0.0032, 0.0030, 0.0060, 0.0026, 0.0075, 0.0059, 0.0027, 0.0067, 0.0065,\n                        0.0043, 0.0092, 0.0024, 0.0029, 0.0087, 0.0024, 0.0087, 0.0010, 0.0061,\n                        0.0019, 0.0129, 0.0067, 0.0017, 0.0045, 0.0050, 0.0056, 0.0052, 0.0020,\n                        0.0026, 0.0021, 0.0080, 0.0033, 0.0065, 0.0028, 0.0017, 0.0055, 0.0056,\n                        0.0058, 0.0042, 0.0047, 0.0057, 0.0017, 0.0106, 0.0044, 0.0061, 0.0019,\n                        0.0067, 0.0037, 0.0058, 0.0061, 0.0020, 0.0019, 0.0056, 0.0114, 0.0016,\n                        0.0053, 0.0016, 0.0018, 0.0106, 0.0060, 0.0014, 0.0026, 0.0056, 0.0075,\n                        0.0067, 0.0016, 0.0030, 0.0021, 0.0039, 0.0050, 0.0038, 0.0015, 0.0079,\n                        0.0061, 0.0039, 0.0036, 0.0062, 0.0069, 0.0068, 0.0021, 0.0033, 0.0035,\n                        0.0042, 0.0056, 0.0025, 0.0055, 0.0015, 0.0061, 0.0066, 0.0016, 0.0019,\n                        0.0054, 0.0056, 0.0060, 0.0029, 0.0056, 0.0022, 0.0054, 0.0059, 0.0018,\n                        0.0056, 0.0051, 0.0020, 0.0065, 0.0025, 0.0034, 0.0052, 0.0043, 0.0043,\n                        0.0062, 0.0064, 0.0031, 0.0016, 0.0076, 0.0032, 0.0016, 0.0066, 0.0068,\n                        0.0056, 0.0049, 0.0049, 0.0020, 0.0032, 0.0049, 0.0067, 0.0060, 0.0041,\n                        0.0030, 0.0079, 0.0075, 0.0058, 0.0093, 0.0016, 0.0092, 0.0025, 0.0065,\n                        0.0061, 0.0070, 0.0053, 0.0015, 0.0050, 0.0036, 0.0029, 0.0014, 0.0036,\n                        0.0034, 0.0015, 0.0020, 0.0041, 0.0043, 0.0066, 0.0014, 0.0082, 0.0016,\n                        0.0035, 0.0061, 0.0057, 0.0018, 0.0018, 0.0015, 0.0068, 0.0021, 0.0061,\n                        0.0058, 0.0124, 0.0037, 0.0069, 0.0056, 0.0018, 0.0048, 0.0040, 0.0068,\n                        0.0056, 0.0018, 0.0020, 0.0060, 0.0056, 0.0038, 0.0047, 0.0028, 0.0021,\n                        0.0044, 0.0093, 0.0034, 0.0062, 0.0034, 0.0054, 0.0053, 0.0064, 0.0033,\n                        0.0069, 0.0047, 0.0014, 0.0046, 0.0070, 0.0021, 0.0018, 0.0026, 0.0029,\n                        0.0064, 0.0019, 0.0069, 0.0017, 0.0047, 0.0058, 0.0056, 0.0063, 0.0077,\n                        0.0018, 0.0018, 0.0026, 0.0061, 0.0016, 0.0068, 0.0074, 0.0030, 0.0064,\n                        0.0065, 0.0053, 0.0061, 0.0070, 0.0059, 0.0063, 0.0075, 0.0074, 0.0059,\n                        0.0061, 0.0028, 0.0029, 0.0037, 0.0025, 0.0060, 0.0055, 0.0042, 0.0065,\n                        0.0033, 0.0033, 0.0065, 0.0072, 0.0063, 0.0047, 0.0061, 0.0017, 0.0050,\n                        0.0076, 0.0028, 0.0045, 0.0060, 0.0018, 0.0067, 0.0072, 0.0069, 0.0062,\n                        0.0064, 0.0059, 0.0013, 0.0036, 0.0060, 0.0042, 0.0081, 0.0035, 0.0058,\n                        0.0032, 0.0076, 0.0029, 0.0017, 0.0027, 0.0040, 0.0066, 0.0058, 0.0063,\n                        0.0025, 0.0048, 0.0062, 0.0018, 0.0063, 0.0061, 0.0017, 0.0012, 0.0015,\n                        0.0072, 0.0014, 0.0070, 0.0049, 0.0047, 0.0024, 0.0055, 0.0020, 0.0046,\n                        0.0063, 0.0026, 0.0020, 0.0106, 0.0056, 0.0061, 0.0042, 0.0068, 0.0047,\n                        0.0074, 0.0062, 0.0060, 0.0036, 0.0017, 0.0059, 0.0045, 0.0061, 0.0062,\n                        0.0018, 0.0054, 0.0049, 0.0034, 0.0070, 0.0079, 0.0020, 0.0031, 0.0025,\n                        0.0045, 0.0052, 0.0083, 0.0017, 0.0021, 0.0086, 0.0037, 0.0077, 0.0062,\n                        0.0060, 0.0174, 0.0017, 0.0061, 0.0015, 0.0450, 0.0051, 0.0069, 0.0027,\n                        0.0066, 0.0021, 0.0028, 0.0128, 0.0027, 0.0047, 0.0026, 0.0029, 0.0046,\n                        0.0063, 0.0121, 0.0065, 0.0059, 0.0050, 0.0055, 0.0092, 0.0060, 0.0061,\n                        0.0036, 0.0019, 0.0070, 0.0069, 0.0055, 0.0074, 0.0016, 0.0049, 0.0057,\n                        0.0062, 0.0037, 0.0016, 0.0085, 0.0072, 0.0062, 0.0059, 0.0084, 0.0061,\n                        0.0038, 0.0015, 0.0035, 0.0066, 0.0014, 0.0074, 0.0011, 0.0032, 0.0083,\n                        0.0071, 0.0026, 0.0043, 0.0056, 0.0020, 0.0056, 0.0015, 0.0065, 0.0115,\n                        0.0037, 0.0058, 0.0063, 0.0048, 0.0075, 0.0065, 0.0173, 0.0050, 0.0067,\n                        0.0056, 0.0033, 0.0055, 0.0047, 0.0056, 0.0038, 0.0089, 0.0031, 0.0072,\n                        0.0062, 0.0060, 0.0071, 0.0068, 0.0048, 0.0061, 0.0035, 0.0096, 0.0076,\n                        0.0069, 0.0068, 0.0054, 0.0059, 0.0014, 0.0061, 0.0085, 0.0028, 0.0015,\n                        0.0044, 0.0076, 0.0049, 0.0079, 0.0056, 0.0050, 0.0017, 0.0029, 0.0056,\n                        0.0015, 0.0054, 0.0016, 0.0061, 0.0063, 0.0036, 0.0036, 0.0064, 0.0052,\n                        0.0036, 0.0029, 0.0052, 0.0032, 0.0024, 0.0046, 0.0056, 0.0019, 0.0032,\n                        0.0054, 0.0018, 0.0029, 0.0056, 0.0058, 0.0017, 0.0016, 0.0038, 0.0017,\n                        0.0055, 0.0017, 0.0031, 0.0070, 0.0056, 0.0022, 0.0021, 0.0067, 0.0067,\n                        0.0071, 0.0068, 0.0022, 0.0049, 0.0042, 0.0031, 0.0080, 0.0076, 0.0074,\n                        0.0052, 0.0070, 0.0073, 0.0010, 0.0055, 0.0017, 0.0020, 0.0064, 0.0060,\n                        0.0079, 0.0057, 0.0056, 0.0068, 0.0068, 0.0018, 0.0062, 0.0070, 0.0055,\n                        0.0038, 0.0063, 0.0016, 0.0056, 0.0013, 0.0063, 0.0061, 0.0070, 0.0058,\n                        0.0036, 0.0049, 0.0064, 0.0068, 0.0038, 0.0044, 0.0019, 0.0017, 0.0045,\n                        0.0071, 0.0016, 0.0064, 0.0016, 0.0059, 0.0031, 0.0011, 0.0064, 0.0067,\n                        0.0018, 0.0014, 0.0062, 0.0064, 0.0074, 0.0013, 0.0070, 0.0045, 0.0016,\n                        0.0017, 0.0015, 0.0053, 0.0072, 0.0034, 0.0658, 0.0026, 0.0056, 0.0062,\n                        0.0047, 0.0018, 0.0090, 0.0062, 0.0034, 0.0020, 0.0022, 0.0063, 0.0052,\n                        0.0055, 0.0049, 0.0048, 0.0071, 0.0017, 0.0066, 0.0015, 0.0062, 0.0048,\n                        0.0056, 0.0020, 0.0023, 0.0053, 0.0019, 0.0052, 0.0026, 0.0048, 0.0031,\n                        0.0022, 0.0038, 0.0017, 0.0016, 0.0060, 0.0029, 0.0048, 0.0052, 0.0027,\n                        0.0056, 0.0056, 0.0071, 0.0017, 0.0017, 0.0035, 0.0056, 0.0050, 0.0060,\n                        0.0037, 0.0020, 0.0037, 0.0053, 0.0030, 0.0019, 0.0062, 0.0066, 0.0022,\n                        0.0034, 0.0049, 0.0045, 0.0101, 0.0061, 0.0051, 0.0060, 0.0018, 0.0062,\n                        0.0019, 0.0071, 0.0040, 0.0036, 0.0058, 0.0059]), zero_point=tensor([   0,  127, -128, -128, -128, -128, -128, -128, -128, -128,  127,  127,\n                        -128, -128, -128, -128, -128, -128, -128, -128, -128,  127, -128,  127,\n                         127, -128, -128, -128,  127,  127, -128, -128,  127,    0,  127, -128,\n                        -128, -128,  127,  127, -128, -128,  127, -128,  127,  127, -128,  127,\n                         127, -128,  127, -128, -128, -128,  127, -128,  127,  127, -128, -128,\n                         127, -128,  127, -128, -128, -128,  127, -128,  127,  127,  127, -128,\n                        -128, -128, -128, -128, -128,  127, -128,  127,  127, -128, -128, -128,\n                        -128, -128, -128,  127,  127,  127, -128,  127,  127, -128,  127,  127,\n                        -128, -128,  127,  127, -128, -128, -128, -128,  127,  127,  127,  127,\n                        -128,  127, -128,  127, -128,  127, -128, -128, -128, -128, -128, -128,\n                         127,  127,  127,  127, -128,  127, -128,  127, -128, -128, -128, -128,\n                        -128, -128,  127, -128, -128,  127, -128,  127,  127,  127, -128, -128,\n                         127,  127,  127, -128,  127, -128, -128, -128, -128, -128, -128,  127,\n                         127, -128,  127, -128,  127,  127, -128, -128, -128,  127,  127,  127,\n                        -128, -128,  127,  127, -128,  127,  127,  127,  127,  127, -128, -128,\n                         127, -128, -128, -128,  127, -128,  127, -128, -128,  127, -128, -128,\n                         127, -128, -128, -128,  127,  127, -128, -128, -128, -128,    0, -128,\n                        -128, -128, -128, -128, -128, -128,  127, -128, -128, -128,  127, -128,\n                         127, -128,  127,  127,  127,  127,  127,  127,  127, -128,  127,  127,\n                         127, -128, -128,  127,  127,  127, -128, -128, -128, -128, -128, -128,\n                        -128, -128, -128,  127,  127, -128, -128, -128, -128,  127,  127, -128,\n                        -128, -128, -128, -128, -128, -128, -128, -128,  127,  127,  127,  127,\n                         127, -128,  127, -128,  127, -128, -128, -128,  127, -128, -128, -128,\n                        -128,  127,  127,  127,  127, -128, -128, -128, -128, -128, -128,  127,\n                        -128,  127, -128,  127,  127, -128,  127,  127,  127, -128,  127, -128,\n                         127,  127, -128,  127,  127, -128,  127, -128, -128, -128, -128, -128,\n                         127,  127,  127, -128, -128, -128, -128,  127, -128,  127, -128,  127,\n                         127,  127, -128,  127,    0,  127,  127, -128, -128,  127,  127,  127,\n                        -128, -128, -128,  127, -128,  127,  127,  127, -128,  127, -128, -128,\n                         127, -128, -128,  127,  127, -128,  127, -128, -128,  127, -128, -128,\n                         127, -128,  127,  127, -128,  127, -128,  127, -128,  127,    0, -128,\n                         127, -128, -128, -128, -128,  127,  127,  127, -128,  127, -128,  127,\n                         127, -128, -128, -128, -128, -128, -128,  127, -128,  127, -128,  127,\n                        -128,  127, -128, -128,  127,  127, -128, -128,  127, -128,  127,  127,\n                        -128, -128,  127,  127, -128, -128, -128,  127,  127,  127, -128, -128,\n                         127,  127, -128, -128,  127,  127, -128, -128, -128,  127,  127,  127,\n                        -128, -128,  127, -128,  127, -128, -128,  127,  127, -128, -128, -128,\n                         127, -128,  127, -128, -128,  127, -128, -128,  127, -128,  127,  127,\n                        -128,  127, -128, -128, -128,  127,  127, -128,  127,  127, -128, -128,\n                        -128, -128, -128,  127,  127, -128, -128, -128,  127,  127, -128, -128,\n                        -128, -128,  127, -128,  127, -128, -128, -128, -128,  127, -128,  127,\n                         127,  127,  127,  127,  127,  127,  127,  127, -128,  127, -128,  127,\n                         127, -128, -128,  127,  127,  127, -128,  127, -128, -128, -128,  127,\n                        -128, -128,  127,  127,  127, -128, -128,  127,  127, -128, -128,  127,\n                         127,  127,  127, -128, -128,  127, -128,  127, -128, -128, -128,  127,\n                        -128, -128,  127, -128, -128,  127,  127,  127,  127, -128,  127, -128,\n                         127, -128, -128, -128, -128, -128,  127,  127,  127, -128,  127, -128,\n                        -128,  127, -128, -128,  127, -128, -128, -128, -128, -128, -128, -128,\n                        -128,  127,  127, -128,  127, -128, -128,  127, -128,  127,  127, -128,\n                        -128, -128, -128, -128,  127, -128, -128,  127, -128, -128,  127, -128,\n                        -128, -128, -128, -128, -128,  127,  127, -128,  127, -128,  127, -128,\n                         127, -128,  127,  127,  127, -128, -128, -128, -128,  127, -128, -128,\n                         127, -128, -128,  127,  127,  127, -128,  127, -128, -128, -128,  127,\n                        -128,  127, -128, -128,  127,  127, -128, -128, -128, -128, -128, -128,\n                        -128, -128, -128,  127,  127, -128, -128, -128, -128,  127, -128, -128,\n                         127, -128, -128,  127,  127,  127, -128, -128, -128,  127,  127, -128,\n                         127, -128, -128, -128, -128,  127, -128,  127,    0, -128, -128,  127,\n                        -128,  127,  127,    0,  127, -128,  127,  127, -128, -128, -128,    0,\n                        -128,  127, -128, -128, -128, -128,  127,  127, -128, -128, -128, -128,\n                         127, -128, -128, -128,  127,  127, -128, -128, -128, -128, -128, -128,\n                         127,  127,  127, -128,  127, -128,  127,  127, -128, -128,  127,  127,\n                        -128,  127, -128,  127, -128, -128,  127, -128, -128,  127, -128, -128,\n                         127, -128, -128, -128,  127, -128, -128, -128,  127, -128,  127, -128,\n                        -128, -128, -128, -128,  127, -128,  127, -128, -128, -128, -128, -128,\n                        -128,  127, -128, -128,  127,  127,  127, -128, -128, -128, -128, -128,\n                         127,  127, -128,  127, -128,  127, -128, -128,  127, -128, -128, -128,\n                         127,  127,  127,  127,  127, -128, -128,  127,  127, -128,  127,  127,\n                        -128, -128,  127,  127,  127,  127, -128,  127,  127, -128, -128,  127,\n                         127, -128, -128, -128, -128,  127, -128,  127,  127, -128, -128, -128,\n                        -128, -128, -128,  127, -128,  127,  127, -128, -128, -128, -128, -128,\n                        -128, -128,  127, -128, -128, -128,  127, -128,  127, -128,  127, -128,\n                        -128, -128, -128,  127, -128, -128, -128,  127, -128,  127,  127, -128,\n                        -128,  127, -128,  127, -128,  127,  127, -128, -128,  127,  127, -128,\n                        -128, -128,  127, -128, -128,  127,  127,  127, -128, -128,  127,    0,\n                         127, -128, -128, -128,  127, -128, -128,  127,  127,  127, -128, -128,\n                        -128, -128, -128, -128,  127, -128,  127, -128, -128, -128,  127,  127,\n                        -128,  127, -128,  127, -128,  127,  127,  127,  127,  127, -128, -128,\n                        -128, -128,  127, -128, -128, -128,  127,  127,  127, -128,  127, -128,\n                         127,  127,  127, -128,  127,  127, -128, -128,  127, -128, -128, -128,\n                        -128, -128, -128, -128,  127, -128,  127, -128, -128,  127, -128, -128],\n                       dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n                (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n                  min_val=tensor([-1.6216, -0.4748,  0.5766,  0.8473,  0.7283,  0.8683,  1.1215,  0.4216,\n                           0.5330,  0.7218, -0.4722, -0.3572,  0.6978,  0.6640,  0.5042,  0.6564,\n                           0.9121,  0.8576,  0.7372,  0.7354,  0.7223, -0.6431,  0.4240, -0.4741,\n                          -0.4227,  0.6218,  0.5352,  0.8141, -0.9093, -0.8446,  0.6788,  0.4266,\n                          -0.3254, -0.0431, -0.5547,  1.0096,  0.6090,  0.7100, -0.5595, -0.6005,\n                           0.6550,  0.5467, -0.7799,  0.6647, -0.8631, -0.5176,  0.4967, -0.4227,\n                          -1.1832,  0.8453, -0.6162,  0.5285,  0.4842,  0.8971, -0.6979,  0.5283,\n                          -0.5764, -0.5576,  0.3522,  0.4570, -0.9102,  0.6422, -1.2267,  0.6854,\n                           0.4364,  1.2375, -0.7443,  0.4769, -0.7866, -0.3797, -0.4409,  0.5939,\n                           0.8352,  0.5302,  1.1027,  0.9117,  0.6338, -0.8668,  0.8317, -0.3131,\n                          -0.6797,  0.5333,  0.5284,  0.6066,  0.6032,  0.5419,  0.7699, -0.4393,\n                          -1.3691, -0.9915,  0.5658, -0.7021, -0.6239,  0.5375, -0.4200, -0.2815,\n                           0.6629,  0.6612, -0.7324, -1.0258,  2.0286,  0.4674,  0.6875,  0.5672,\n                          -0.3664, -0.8144, -0.5772, -0.9387,  0.7700, -0.3755,  0.6328, -0.5047,\n                           0.7845, -0.6228,  0.5725,  0.7345,  0.6951,  0.6264,  0.4519,  0.7509,\n                          -0.6595, -0.4252, -0.9289, -0.6525,  0.6165, -0.5696,  0.8361, -1.0262,\n                           0.8474,  0.5739,  0.4839,  0.6194,  0.5390,  0.7069, -0.6483,  0.9638,\n                           0.6458, -1.0133,  1.5796, -0.8802, -1.0808, -0.4434,  0.7041,  0.6880,\n                          -0.9561, -0.9435, -0.7866,  0.7917, -0.9416,  1.1300,  0.5085,  0.6706,\n                           0.6215,  0.6645,  1.3420, -0.5233, -1.3432,  0.7546, -0.9683,  0.6380,\n                          -0.6881, -1.0351,  1.0309,  0.5841,  0.6932, -0.6264, -0.8113, -0.6516,\n                           0.5190,  0.6824, -0.8679, -0.9909,  0.8998, -0.8343, -0.4680, -0.9854,\n                          -0.7958, -0.3726,  0.6146,  0.4833, -1.0567,  0.6985,  0.4643,  0.8490,\n                          -0.7798,  0.5582, -0.7281,  0.6823,  0.8385, -0.6831,  0.5705,  0.6476,\n                          -1.0496,  0.3795,  0.3921,  0.5805, -0.7221, -0.7299,  0.7516,  0.8037,\n                           0.9966,  0.7494, -1.1840,  0.6218,  0.5696,  1.1468,  0.6465,  0.8018,\n                           0.3958,  0.6222, -0.8394,  0.7081,  0.4684,  0.6299, -1.0558,  1.0184,\n                          -0.4438,  0.6705, -0.7798, -0.6387, -0.4398, -0.7302, -0.7607, -1.1529,\n                          -0.9020,  0.5524, -0.6215, -0.5281, -0.6388,  0.4997,  1.4031, -0.7568,\n                          -0.7891, -0.4523,  0.5272,  0.9669,  0.5752,  1.7951,  1.4263,  0.7495,\n                           0.7735,  0.6622,  0.5564, -0.6923, -0.5871,  0.5906,  0.7520,  0.6002,\n                           0.4756, -0.6641, -0.6558,  0.6348,  0.7627,  0.6296,  0.4275,  0.5696,\n                           1.0497,  0.8735,  0.8849,  1.0262, -0.7260, -0.4519, -0.7748, -0.3338,\n                          -0.6936,  0.7741, -0.3885,  0.8495, -0.9980,  0.6993,  0.7361,  0.6780,\n                          -0.8298,  0.7458,  0.8162,  0.5191,  0.8110, -0.6837, -1.1286, -0.5935,\n                          -0.4474,  0.8334,  0.7887,  0.9791,  0.3972,  0.7807,  0.4367, -0.8531,\n                           0.5300, -0.7609,  0.4708, -0.7164, -0.4691,  0.6262, -0.7002, -0.5360,\n                          -0.4158,  0.6779, -0.3945,  0.6267, -0.7331, -0.7751,  0.6368, -1.0191,\n                          -1.0763,  0.8593, -0.5167,  1.0106,  0.5689,  0.6077,  0.7688,  0.5488,\n                          -0.6386, -0.6029, -0.7883,  0.9624,  0.9966,  0.5481,  0.8539, -0.8447,\n                           0.8730, -0.4483,  1.3113, -0.6331, -0.8170, -0.3908,  0.8910, -0.7592,\n                          -1.7722, -0.9572, -0.8585,  0.7522,  0.5355, -0.4201, -0.4877, -0.9069,\n                           0.6841,  0.3844,  0.7084, -0.9817,  0.5778, -0.4409, -1.0582, -0.8166,\n                           0.8665, -0.4529,  0.6866,  0.6276, -0.8582,  0.7668,  0.7667, -0.8115,\n                          -0.7698,  0.8564, -0.6609,  1.0026,  0.5897, -0.6804,  0.7824,  0.7389,\n                          -1.0977,  1.1317, -0.6078, -0.7454,  1.3315, -0.6032,  1.3230, -0.2483,\n                           0.7288, -0.4886, -1.6516,  0.8292, -0.4212,  0.4879,  0.6569,  0.5673,\n                           0.5374, -0.5000, -0.6583, -0.5430,  0.8721, -0.8439,  0.9523, -0.7032,\n                          -0.4254,  0.6940,  0.6212,  0.5894,  0.5099,  0.5289,  0.3993, -0.4336,\n                           1.7127, -1.1125,  0.7086, -0.4727,  0.7024, -0.9353,  0.6147,  0.8560,\n                          -0.5044, -0.4917,  0.6951,  2.0296, -0.4104,  0.5718, -0.4179, -0.4597,\n                           1.7448,  0.5743, -0.3515, -0.6542,  0.6370,  0.9000,  0.8095, -0.3995,\n                          -0.7740, -0.5457,  0.4627,  0.4364, -0.9611, -0.3936,  0.9766,  0.5985,\n                          -0.9824, -0.9217,  0.6470,  0.7229,  0.7565, -0.5318, -0.8469, -0.8869,\n                           0.3908,  0.6020, -0.6426,  0.5068, -0.3910,  0.6465,  0.7187, -0.4148,\n                          -0.4870,  0.9737,  0.5456,  0.6001, -0.7379,  0.6136, -0.5495,  0.6820,\n                           0.6510, -0.4693,  0.6579,  0.6633, -0.5018,  0.6341, -0.6437, -0.8564,\n                           0.6700, -1.0966,  0.4225,  0.6171,  0.7094, -0.7893, -0.4108,  0.7698,\n                          -0.8168, -0.4147,  0.8179,  0.8027,  0.5944,  0.5773,  0.4413, -0.5112,\n                          -0.8097,  0.6237,  0.7729,  0.5976, -1.0550, -0.7651,  1.2563,  0.9915,\n                           0.5705,  1.2625, -0.4068,  1.1186, -0.6326,  0.7519,  0.7341,  0.8431,\n                           0.4930, -0.3940,  0.4576, -0.9187, -0.7508, -0.3687, -0.9053, -0.8741,\n                          -0.3847, -0.5185, -1.0559, -1.0878,  0.5674, -0.3649,  1.2381, -0.4168,\n                          -0.8896,  0.8959,  0.6877, -0.4592, -0.4498, -0.3773,  0.9530, -0.5420,\n                           0.7077,  0.5378,  1.7499, -0.9507,  0.9811,  0.5727, -0.4652, -1.2263,\n                          -1.0258,  0.8132,  0.7015, -0.4587, -0.5086,  0.8349,  0.5744, -0.9612,\n                          -1.1937, -0.7032, -0.5460,  0.5135,  1.3285, -0.8661,  0.6940, -0.8569,\n                           0.5452,  0.6375,  0.6090, -0.8536,  0.6423,  0.5351, -0.3689,  0.4229,\n                           0.8345, -0.5306, -0.4635, -0.6552, -0.7323,  0.7064, -0.4902,  0.7634,\n                          -0.4384,  0.5514,  0.7045,  0.6128,  0.6372,  0.8090, -0.4515, -0.4505,\n                          -0.6518,  0.6103, -0.4044,  0.8165,  0.9280, -0.7738,  0.6921,  0.6916,\n                          -1.3432,  0.6243,  0.9459,  0.6926,  0.8867,  0.9013,  0.9914,  0.6569,\n                           0.6855, -0.7153, -0.7521,  0.4415, -0.6278,  0.7909,  0.5100, -1.0739,\n                           0.7895, -0.8357, -0.8486,  0.6305,  0.9065,  0.8820,  0.6183,  0.6949,\n                          -0.4312,  0.4750,  1.0459, -0.7097,  0.5113,  0.6541, -0.4575,  0.8390,\n                           0.8687,  0.8612,  0.8152,  0.6801,  0.7548, -0.3329, -0.9162,  0.6518,\n                          -1.0778,  0.7931, -0.9051,  0.7168, -0.8157,  0.8600, -0.7349, -0.4217,\n                          -0.6823,  0.4677,  0.7009,  0.5969,  0.6919, -0.6251,  0.4580,  0.6707,\n                          -0.4545,  0.8294,  0.6465, -0.4274, -0.3126, -0.3723,  0.9668, -0.3504,\n                           0.8181,  0.5548,  0.5321, -0.6049,  0.5249, -0.5008,  0.5822,  0.6993,\n                          -0.6716, -0.5019,  2.1690,  0.5473,  0.7532,  0.3730,  0.6287,  0.5266,\n                           0.8016,  0.6874,  0.8577, -0.9087, -0.4460,  0.7429,  0.4975,  0.6893,\n                           0.5174, -0.4609,  0.5798,  0.4356, -0.8683,  0.8221,  1.1058, -0.5025,\n                          -0.7919, -0.6407,  0.4647,  0.5284,  0.8976, -0.4290, -0.5382,  1.2987,\n                          -0.9373,  0.7305,  0.7665,  0.6285,  3.4411, -0.4376,  0.7243, -0.3844,\n                          -5.7617,  0.3811,  1.0548, -0.6893,  0.8312, -0.5409, -0.7062, -1.6341,\n                          -0.6972,  0.4820, -0.6743, -0.7477,  0.4408,  0.8849,  2.0077, -0.8313,\n                           0.6084, -1.2766,  0.7496,  1.4310,  0.7237,  0.6223, -0.9139, -0.4949,\n                           0.6303,  0.6674,  0.6401,  0.8640, -0.4148,  0.6247,  0.5575,  0.6587,\n                          -0.9534, -0.4070,  1.3054,  0.8713,  0.7834,  0.6314,  1.2516,  0.6992,\n                          -0.9746, -0.3730, -0.8907,  0.7341, -0.3570,  0.8866, -0.2909, -0.8156,\n                           0.9872,  0.9119, -0.6727, -1.0930,  0.5835, -0.5167,  0.6780, -0.3737,\n                           0.7560,  1.6664, -0.9541,  0.5910,  0.7819, -1.2233,  1.0160,  0.6249,\n                          -4.4041,  0.5309,  0.7136,  0.5304, -0.8342,  0.5392,  0.6158,  0.6852,\n                          -0.9640,  1.5998, -0.7818,  0.8594,  0.7366,  0.5989,  0.7711,  0.7605,\n                          -1.2258,  0.7826, -0.8920,  1.6093,  1.0168,  0.8570,  0.8278,  0.6425,\n                           0.6722, -0.3616,  0.6164,  1.1596, -0.7075, -0.3839, -1.1289,  1.0915,\n                           0.4637,  0.8655,  0.5988,  0.5794, -0.4367, -0.7421,  0.5797, -0.3759,\n                           0.4891, -0.4001,  0.7125,  0.7775, -0.9104,  0.3574,  0.6400,  0.6181,\n                          -0.9058, -0.7515, -1.3140, -0.8060, -0.6059,  0.5098,  0.6716, -0.4826,\n                          -0.8064,  0.4419, -0.4592, -0.7492,  0.7012,  0.6526, -0.4458, -0.3966,\n                          -0.9754, -0.4377,  0.5983, -0.4265, -0.7804,  0.8080,  0.5858, -0.5514,\n                          -0.5440,  0.9077,  0.7383,  1.0483,  0.7475, -0.5558,  0.6023, -1.0659,\n                          -0.7866,  0.8429,  1.0974,  0.8816,  0.5480,  0.8614,  1.0232, -0.2499,\n                           0.5738, -0.4442, -0.5212,  0.6988,  0.5901,  1.0661,  0.6503,  0.4991,\n                           0.8064,  0.9838, -0.4506,  0.7284,  0.7075,  0.6407, -0.9776,  0.7297,\n                          -0.4190,  0.5942, -0.3338,  0.7365,  0.6417,  0.7703,  0.6197, -0.9073,\n                           0.4857,  0.5731,  0.8616, -0.9599,  0.4586, -0.4793, -0.4431,  0.4175,\n                           0.6585, -0.4118,  0.6972, -0.4138,  0.6550, -0.7937, -0.2881,  0.8152,\n                           0.9214, -0.4598, -0.3543,  0.6768,  0.7234,  0.8015, -0.3274,  0.8707,\n                           0.4965, -0.4072, -0.4251, -0.3709,  0.5655,  0.6942, -0.8760, -4.6963,\n                          -0.6705,  0.6682,  0.6192,  0.5688, -0.4572,  1.3539,  0.7211, -0.8713,\n                          -0.5170, -0.5626,  0.5930,  0.6303,  0.5383,  0.4942,  0.6435,  0.6270,\n                          -0.4247,  0.6643, -0.3736,  0.6739,  0.6718,  0.6494, -0.5073, -0.5802,\n                           0.4775, -0.4721,  0.5554, -0.6513,  0.4455, -0.8026, -0.5512, -0.9570,\n                          -0.4318, -0.4112,  0.7093,  0.4646,  0.5857,  0.5732, -0.6801,  0.6168,\n                           0.6473,  0.6907, -0.4390, -0.4251, -0.8924,  0.7385, -1.2861,  0.6062,\n                          -0.9371, -0.5000, -0.9328,  0.6488, -0.7751, -0.4947,  0.6230,  0.7787,\n                          -0.5631,  0.4073,  0.5949,  0.5256,  1.6560,  0.7150,  0.4712,  0.5805,\n                          -0.4482,  0.7313, -0.4807,  0.7596,  0.4820, -0.9253,  0.5433,  0.6116]), max_val=tensor([ 9.9971e-01, -1.5420e-01,  1.4219e+00,  1.8986e+00,  1.5550e+00,\n                           1.8207e+00,  2.2523e+00,  1.0577e+00,  1.2948e+00,  1.4045e+00,\n                          -1.8428e-01, -2.0541e-01,  1.5273e+00,  1.5372e+00,  1.1466e+00,\n                           1.4151e+00,  2.2227e+00,  1.9966e+00,  1.6732e+00,  1.8459e+00,\n                           1.7555e+00, -3.1335e-01,  1.2985e+00, -1.6685e-01, -1.6792e-01,\n                           1.5863e+00,  1.2034e+00,  1.7470e+00, -3.4519e-01, -2.0647e-01,\n                           1.5195e+00,  1.1369e+00, -1.3513e-01,  3.4705e-02, -1.5930e-01,\n                           1.9005e+00,  1.2902e+00,  1.7125e+00, -1.4696e-01, -1.0322e-01,\n                           1.4419e+00,  1.3827e+00, -3.1319e-01,  1.4734e+00, -3.6638e-01,\n                          -1.2757e-01,  1.4219e+00, -2.2596e-01, -5.1975e-01,  1.7453e+00,\n                          -1.8062e-01,  1.3750e+00,  1.4456e+00,  1.7701e+00, -1.3974e-01,\n                           1.4733e+00, -2.0717e-01, -1.4754e-01,  9.8062e-01,  1.2048e+00,\n                          -1.5908e-01,  1.3257e+00, -4.0981e-01,  1.7083e+00,  1.3752e+00,\n                           2.2417e+00, -3.1158e-01,  1.3729e+00, -3.0184e-01, -1.5753e-01,\n                          -1.8936e-01,  1.4004e+00,  1.5250e+00,  1.2657e+00,  1.6779e+00,\n                           1.4705e+00,  1.6036e+00, -3.5152e-01,  2.0528e+00, -9.8993e-02,\n                          -3.1324e-01,  1.3136e+00,  1.3624e+00,  1.4978e+00,  1.2816e+00,\n                           1.2507e+00,  1.5906e+00, -9.3856e-02, -3.2312e-01, -4.3651e-01,\n                           1.1864e+00, -2.0427e-01, -9.3199e-02,  1.6409e+00, -1.2524e-01,\n                          -1.0901e-01,  1.5596e+00,  1.5837e+00, -2.5905e-01, -3.2116e-01,\n                           2.9609e+00,  1.1789e+00,  1.4499e+00,  1.2818e+00, -1.6326e-01,\n                          -3.2351e-01, -1.8060e-01, -1.7813e-01,  1.9158e+00, -1.8829e-01,\n                           1.5229e+00, -2.2058e-01,  1.6361e+00, -2.2287e-01,  1.5829e+00,\n                           1.5571e+00,  1.5621e+00,  1.5357e+00,  1.0592e+00,  1.6591e+00,\n                          -1.4857e-01, -1.2274e-01, -2.8199e-01, -2.5720e-01,  1.3210e+00,\n                          -2.5138e-01,  1.7473e+00, -3.8758e-01,  1.6884e+00,  1.4154e+00,\n                           1.3828e+00,  1.8367e+00,  1.4283e+00,  1.5054e+00, -2.6136e-01,\n                           1.7871e+00,  1.4611e+00, -3.5069e-01,  2.6546e+00, -1.2732e-01,\n                          -2.0338e-01, -1.5941e-01,  1.5243e+00,  1.5885e+00, -3.1581e-01,\n                          -3.7419e-01, -2.1469e-01,  1.4832e+00, -2.4128e-01,  1.9468e+00,\n                           1.2408e+00,  1.4927e+00,  1.4233e+00,  1.5320e+00,  2.1594e+00,\n                          -1.6915e-01, -3.3733e-01,  1.7957e+00, -4.1425e-01,  1.4064e+00,\n                          -2.4591e-01, -1.5326e-01,  2.1007e+00,  1.7563e+00,  1.5144e+00,\n                          -2.4006e-01, -2.7903e-01, -2.1377e-01,  1.2184e+00,  1.5772e+00,\n                          -1.4928e-01, -3.6214e-01,  1.8778e+00, -2.8110e-01, -2.0684e-01,\n                          -4.1103e-01, -1.6148e-01, -1.5073e-01,  1.3976e+00,  1.1521e+00,\n                          -3.0352e-01,  1.5172e+00,  1.3238e+00,  1.9149e+00, -2.5371e-01,\n                           1.1400e+00, -1.7400e-01,  1.6157e+00,  1.7832e+00, -2.2539e-01,\n                           1.3221e+00,  1.2209e+00, -3.5127e-01,  1.2155e+00,  1.1717e+00,\n                           1.3935e+00, -3.4071e-01, -2.6881e-01,  1.5278e+00,  1.7425e+00,\n                           1.9344e+00,  1.6820e+00,  6.1460e-01,  1.4955e+00,  1.3772e+00,\n                           1.9890e+00,  1.4752e+00,  1.8226e+00,  1.2294e+00,  1.7294e+00,\n                          -3.0342e-01,  1.5882e+00,  1.2864e+00,  1.5682e+00, -4.2504e-01,\n                           1.9487e+00, -1.4866e-01,  1.5923e+00, -2.6244e-01, -2.2068e-01,\n                          -1.6130e-01, -3.0766e-01, -1.7395e-01, -3.0722e-01, -2.2885e-01,\n                           1.2015e+00, -1.7091e-01, -2.9013e-01, -3.0310e-01,  1.4544e+00,\n                           2.3553e+00, -2.2666e-01, -1.8328e-01, -1.5722e-01,  1.3405e+00,\n                           1.9576e+00,  1.5873e+00,  2.9500e+00,  2.3657e+00,  1.7327e+00,\n                           1.6555e+00,  1.3989e+00,  1.3095e+00, -2.3383e-01, -1.7614e-01,\n                           1.4593e+00,  2.2574e+00,  1.4762e+00,  1.2937e+00, -2.1554e-01,\n                          -9.6455e-02,  1.3756e+00,  1.7446e+00,  1.4182e+00,  1.2243e+00,\n                           1.4939e+00,  1.8839e+00,  1.9160e+00,  1.8388e+00,  1.8883e+00,\n                          -1.9222e-01, -2.1863e-01, -3.8403e-01, -1.6376e-01, -2.7628e-01,\n                           1.7485e+00, -1.6059e-01,  2.0762e+00, -2.3387e-01,  1.5616e+00,\n                           1.5751e+00,  1.5442e+00, -1.7720e-01,  1.6287e+00,  1.6273e+00,\n                           1.2878e+00,  1.9949e+00, -2.3099e-01, -4.1191e-01, -7.9256e-02,\n                          -1.6567e-01,  1.5916e+00,  1.4792e+00,  2.0281e+00,  1.4098e+00,\n                           1.6012e+00,  1.2282e+00, -2.9784e-01,  1.3175e+00, -3.3316e-01,\n                           1.4397e+00, -3.2477e-01, -1.6484e-01,  1.5835e+00, -3.0555e-01,\n                          -1.9640e-01, -1.1605e-01,  1.7337e+00, -1.1257e-01,  1.6344e+00,\n                          -2.3750e-01, -2.5391e-01,  1.6613e+00, -2.4011e-01, -1.8089e-01,\n                           1.5025e+00, -9.0321e-02,  1.9175e+00,  1.4952e+00,  1.4956e+00,\n                           1.4941e+00,  1.1167e+00, -2.1686e-01, -3.0853e-01, -1.8625e-01,\n                           1.7709e+00,  1.6479e+00,  1.4885e+00,  1.6865e+00, -3.8072e-01,\n                           1.7593e+00, -1.5565e-01,  2.4656e+00, -1.2337e-01, -3.4028e-01,\n                          -1.8788e-01,  1.7665e+00, -2.7173e-01,  1.7599e+00, -3.0889e-01,\n                          -2.4466e-01,  1.6286e+00,  1.2345e+00, -1.3961e-01, -1.4710e-01,\n                          -1.7820e-01,  1.5902e+00,  1.1636e+00,  1.6886e+00, -2.4519e-01,\n                           1.3601e+00, -1.6356e-01, -2.5358e-01, -2.2130e-01,  1.6730e+00,\n                          -1.5159e-01,  1.5422e+00,  1.3186e+00, -2.5421e-01,  1.8066e+00,\n                           1.5791e+00, -2.4769e-01, -2.9140e-01,  1.5245e+00, -1.4937e-01,\n                           1.9067e+00,  1.5055e+00, -3.0187e-01,  1.7176e+00,  1.6554e+00,\n                          -2.9798e-01,  2.3435e+00, -2.5295e-01, -3.4083e-01,  2.2224e+00,\n                          -2.6533e-01,  2.2060e+00, -1.7816e-01,  1.5614e+00, -1.8318e-01,\n                           1.2166e+00,  1.7120e+00, -1.7828e-01,  1.1494e+00,  1.2706e+00,\n                           1.4388e+00,  1.3328e+00, -1.4273e-01, -2.0039e-01, -2.2298e-01,\n                           2.0365e+00, -3.0206e-01,  1.6641e+00, -2.9606e-01, -1.4587e-01,\n                           1.4120e+00,  1.4183e+00,  1.4677e+00,  1.0805e+00,  1.1945e+00,\n                           1.4470e+00, -1.3610e-01,  2.7077e+00, -4.0687e-01,  1.5633e+00,\n                          -1.5891e-01,  1.6992e+00, -3.1731e-01,  1.4784e+00,  1.5560e+00,\n                          -1.3940e-01, -1.2219e-01,  1.4386e+00,  2.8958e+00, -1.5435e-01,\n                           1.3615e+00, -1.1659e-01, -1.7258e-01,  2.6983e+00,  1.5284e+00,\n                          -1.4220e-01, -2.0026e-01,  1.4319e+00,  1.9011e+00,  1.6997e+00,\n                          -1.7028e-01, -1.9468e-01, -1.1780e-01,  9.8392e-01,  1.2712e+00,\n                          -3.2432e-01, -1.4280e-01,  2.0139e+00,  1.5679e+00, -3.2737e-01,\n                          -1.8647e-01,  1.5824e+00,  1.7651e+00,  1.7350e+00, -1.3051e-01,\n                          -2.8536e-01, -2.4645e-01,  1.0713e+00,  1.4288e+00, -2.8492e-01,\n                           1.4104e+00, -1.5187e-01,  1.5579e+00,  1.6802e+00, -2.1085e-01,\n                          -1.9299e-01,  1.3892e+00,  1.4163e+00,  1.5391e+00, -2.2638e-01,\n                           1.4304e+00, -2.5434e-01,  1.3674e+00,  1.5166e+00, -1.5480e-01,\n                           1.4286e+00,  1.2971e+00, -9.6240e-02,  1.6569e+00, -2.8325e-01,\n                          -2.4324e-01,  1.3237e+00, -2.2661e-01,  1.0891e+00,  1.5935e+00,\n                           1.6302e+00, -1.9789e-01, -2.0289e-01,  1.9389e+00, -3.4009e-01,\n                          -1.9683e-01,  1.6789e+00,  1.7411e+00,  1.4372e+00,  1.2532e+00,\n                           1.2471e+00, -2.3893e-01, -3.6216e-01,  1.2621e+00,  1.6974e+00,\n                           1.5378e+00, -1.8651e-01, -1.9032e-01,  2.0160e+00,  1.9096e+00,\n                           1.4899e+00,  2.3827e+00, -1.0823e-01,  2.3371e+00, -2.5683e-01,\n                           1.6648e+00,  1.5605e+00,  1.7772e+00,  1.3456e+00, -1.6960e-01,\n                           1.2691e+00, -3.1194e-01, -2.3155e-01, -1.4023e-01, -2.2098e-01,\n                          -2.8905e-01, -1.3071e-01, -2.6661e-01, -3.0309e-01, -3.8308e-01,\n                           1.6942e+00, -1.7577e-01,  2.0875e+00, -3.1765e-01, -1.9509e-01,\n                           1.5662e+00,  1.4467e+00, -1.9404e-01, -7.4695e-02, -1.6600e-01,\n                           1.7295e+00, -2.1037e-01,  1.5529e+00,  1.4717e+00,  3.1518e+00,\n                          -3.3628e-01,  1.7551e+00,  1.4395e+00, -1.7559e-01, -3.8531e-01,\n                          -3.5886e-01,  1.7391e+00,  1.4353e+00, -1.7978e-01, -2.0605e-01,\n                           1.5329e+00,  1.4247e+00, -1.1713e-01, -4.0201e-01, -3.0506e-01,\n                          -1.5603e-01,  1.1217e+00,  2.3708e+00, -2.2761e-01,  1.5834e+00,\n                          -3.4696e-01,  1.3715e+00,  1.3577e+00,  1.6384e+00, -2.8787e-01,\n                           1.7625e+00,  1.2084e+00, -1.7942e-01,  1.1728e+00,  1.7833e+00,\n                          -1.1704e-01, -1.5513e-01, -2.2524e-01, -1.5282e-01,  1.6350e+00,\n                          -1.2669e-01,  1.7582e+00, -1.6209e-01,  1.1870e+00,  1.4669e+00,\n                           1.4370e+00,  1.6092e+00,  1.9573e+00, -1.6572e-01, -1.4686e-01,\n                          -2.6945e-01,  1.5468e+00, -1.1805e-01,  1.7404e+00,  1.8749e+00,\n                          -2.8350e-01,  1.6443e+00,  1.6482e+00, -2.0768e-01,  1.5467e+00,\n                           1.7730e+00,  1.5127e+00,  1.6114e+00,  1.9160e+00,  1.8870e+00,\n                           1.5081e+00,  1.5675e+00, -1.9797e-01, -3.6335e-01,  9.4639e-01,\n                          -3.1314e-01,  1.5216e+00,  1.4110e+00, -3.2158e-01,  1.6540e+00,\n                          -3.4479e-01, -2.8992e-01,  1.6585e+00,  1.8294e+00,  1.6159e+00,\n                           1.2083e+00,  1.5612e+00, -1.2587e-01,  1.2840e+00,  1.9331e+00,\n                          -3.2357e-01,  1.1360e+00,  1.5192e+00, -1.3553e-01,  1.6994e+00,\n                           1.8359e+00,  1.7511e+00,  1.5695e+00,  1.6392e+00,  1.5122e+00,\n                          -1.6758e-01, -4.7492e-01,  1.5282e+00, -1.2641e-01,  2.0621e+00,\n                          -2.5436e-01,  1.4734e+00, -1.4681e-01,  1.9420e+00, -2.5778e-01,\n                          -1.7352e-01, -2.2997e-01,  1.0312e+00,  1.6759e+00,  1.4676e+00,\n                           1.6098e+00, -2.5118e-01,  1.2198e+00,  1.5756e+00, -1.7415e-01,\n                           1.6037e+00,  1.5520e+00, -2.4506e-01, -1.4442e-01, -1.4662e-01,\n                           1.8275e+00, -1.4248e-01,  1.7949e+00,  1.2561e+00,  1.1996e+00,\n                          -2.5960e-01,  1.3933e+00, -1.9533e-01,  1.1621e+00,  1.6074e+00,\n                          -1.5290e-01, -1.4262e-01,  2.7048e+00,  1.4291e+00,  1.5651e+00,\n                           1.0762e+00,  1.7289e+00,  1.1897e+00,  1.8969e+00,  1.5764e+00,\n                           1.5422e+00, -3.4682e-01, -1.4062e-01,  1.4935e+00,  1.1362e+00,\n                           1.5653e+00,  1.5873e+00, -1.4177e-01,  1.3765e+00,  1.2421e+00,\n                          -1.6177e-01,  1.7849e+00,  2.0233e+00, -1.7249e-01, -2.1294e-01,\n                          -2.0179e-01,  1.1388e+00,  1.3333e+00,  2.1290e+00, -1.6826e-01,\n                          -1.7585e-01,  2.2043e+00, -3.7128e-01,  1.9605e+00,  1.5753e+00,\n                           1.5291e+00,  4.4487e+00, -2.9227e-01,  1.5592e+00, -1.4459e-01,\n                           3.6653e+00,  1.3126e+00,  1.7691e+00, -2.8196e-01,  1.6922e+00,\n                          -1.7179e-01, -2.9262e-01,  5.1523e-02, -2.0200e-01,  1.1930e+00,\n                          -1.8016e-01, -2.0581e-01,  1.1851e+00,  1.6110e+00,  3.0951e+00,\n                           6.5558e-03,  1.5138e+00, -3.2775e-01,  1.3914e+00,  2.3571e+00,\n                           1.5426e+00,  1.5577e+00, -4.6988e-01, -1.4449e-01,  1.7831e+00,\n                           1.7581e+00,  1.4067e+00,  1.8906e+00, -1.7175e-01,  1.2415e+00,\n                           1.4629e+00,  1.5870e+00, -3.1513e-01, -1.2052e-01,  2.1637e+00,\n                           1.8339e+00,  1.5687e+00,  1.5133e+00,  2.1412e+00,  1.5434e+00,\n                          -2.5367e-01, -1.5369e-01, -1.1842e-01,  1.6781e+00, -1.3512e-01,\n                           1.8866e+00, -2.0974e-01, -2.5431e-01,  2.1079e+00,  1.8224e+00,\n                          -3.3001e-01, -1.7810e-01,  1.4320e+00, -1.3351e-01,  1.4344e+00,\n                          -1.8814e-02,  1.6513e+00,  2.9239e+00, -1.6832e-01,  1.4714e+00,\n                           1.6026e+00, -2.4819e-01,  1.9192e+00,  1.6598e+00, -1.1605e-01,\n                           1.2710e+00,  1.6977e+00,  1.4317e+00, -2.1954e-01,  1.3980e+00,\n                           1.1929e+00,  1.4279e+00, -3.8550e-01,  2.2602e+00, -2.6147e-01,\n                           1.8342e+00,  1.5864e+00,  1.5218e+00,  1.8139e+00,  1.7237e+00,\n                          -2.5446e-01,  1.5609e+00, -3.7324e-01,  2.4517e+00,  1.9472e+00,\n                           1.7690e+00,  1.7388e+00,  1.3658e+00,  1.5161e+00, -2.0894e-01,\n                           1.5463e+00,  2.1637e+00, -2.8040e-01, -1.5537e-01, -2.1717e-01,\n                           1.9374e+00,  1.2480e+00,  2.0019e+00,  1.4336e+00,  1.2837e+00,\n                          -1.2892e-01, -1.1063e-01,  1.4364e+00, -4.7497e-02,  1.3869e+00,\n                          -1.9322e-01,  1.5466e+00,  1.5952e+00, -2.9237e-01,  9.1216e-01,\n                           1.6259e+00,  1.3343e+00, -3.3843e-01, -2.5677e-01, -6.5086e-01,\n                          -1.2011e-01, -2.4767e-01,  1.1709e+00,  1.4396e+00, -2.3278e-01,\n                          -1.9572e-01,  1.3799e+00, -7.0699e-02, -1.5942e-01,  1.4364e+00,\n                           1.4739e+00, -1.4319e-01, -1.8679e-01, -3.3553e-01, -1.7234e-01,\n                           1.3990e+00, -1.3970e-01, -2.4721e-01,  1.7920e+00,  1.4191e+00,\n                          -2.0572e-01, -1.4890e-01,  1.7032e+00,  1.7197e+00,  1.8089e+00,\n                           1.7465e+00, -1.7261e-01,  1.2499e+00, -2.2923e-01, -2.2148e-01,\n                           2.0410e+00,  1.9387e+00,  1.8908e+00,  1.3364e+00,  1.7779e+00,\n                           1.8740e+00, -1.2042e-01,  1.4123e+00, -1.5983e-01, -1.1573e-01,\n                           1.6293e+00,  1.5292e+00,  2.0234e+00,  1.4641e+00,  1.4385e+00,\n                           1.7438e+00,  1.7430e+00, -9.6801e-02,  1.5900e+00,  1.7768e+00,\n                           1.3910e+00, -4.2893e-01,  1.6024e+00, -1.5487e-01,  1.4302e+00,\n                          -1.3557e-01,  1.5973e+00,  1.5470e+00,  1.7798e+00,  1.4758e+00,\n                          -2.2685e-01,  1.2477e+00,  1.6229e+00,  1.7415e+00, -3.8853e-01,\n                           1.1296e+00, -9.9378e-02, -1.9450e-01,  1.1478e+00,  1.8084e+00,\n                          -1.7210e-01,  1.6331e+00, -1.7372e-01,  1.5020e+00, -3.5564e-01,\n                          -1.8927e-01,  1.6367e+00,  1.7198e+00, -1.8986e-01, -1.9595e-01,\n                           1.5712e+00,  1.6372e+00,  1.8986e+00, -1.3522e-01,  1.7760e+00,\n                           1.1599e+00, -9.6351e-02, -1.7345e-01, -1.8432e-01,  1.3568e+00,\n                           1.8396e+00, -2.8228e-01,  8.3602e+00, -2.9256e-01,  1.4235e+00,\n                           1.5836e+00,  1.2034e+00, -1.0852e-01,  2.3077e+00,  1.5729e+00,\n                          -3.1214e-01, -1.4900e-01, -1.9546e-01,  1.6008e+00,  1.3267e+00,\n                           1.4003e+00,  1.2556e+00,  1.2246e+00,  1.8163e+00, -2.0707e-01,\n                           1.6707e+00, -2.1940e-01,  1.5811e+00,  1.2141e+00,  1.4345e+00,\n                          -2.2184e-01, -1.5946e-01,  1.3400e+00, -1.4874e-01,  1.3150e+00,\n                          -2.1888e-01,  1.2271e+00, -2.8589e-01, -1.9873e-01, -3.9979e-01,\n                          -1.5905e-01, -1.1103e-01,  1.5217e+00,  7.3747e-01,  1.2211e+00,\n                           1.3139e+00, -1.1954e-01,  1.4214e+00,  1.4333e+00,  1.7999e+00,\n                          -1.7407e-01, -1.7523e-01, -2.8474e-01,  1.4283e+00, -4.4054e-01,\n                           1.5365e+00, -1.7644e-01, -1.3502e-01, -2.7158e-01,  1.3613e+00,\n                          -2.5617e-01, -2.0319e-01,  1.5815e+00,  1.6728e+00, -1.5055e-01,\n                           8.5935e-01,  1.2618e+00,  1.1516e+00,  2.5730e+00,  1.5622e+00,\n                           1.3117e+00,  1.5185e+00, -1.3042e-01,  1.5729e+00, -1.7331e-01,\n                           1.8132e+00,  1.0131e+00, -2.0516e-01,  1.4902e+00,  1.5027e+00])\n                )\n              )\n            )\n            (2): ReLU6(inplace=True)\n          )\n          (2): ConvBn2d(\n            960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0024, 0.0032, 0.0031, 0.0031, 0.0025, 0.0024, 0.0028, 0.0029, 0.0024,\n                      0.0025, 0.0029, 0.0029, 0.0027, 0.0035, 0.0025, 0.0032, 0.0026, 0.0028,\n                      0.0033, 0.0028, 0.0030, 0.0030, 0.0043, 0.0030, 0.0024, 0.0030, 0.0029,\n                      0.0032, 0.0026, 0.0033, 0.0027, 0.0031, 0.0038, 0.0027, 0.0025, 0.0023,\n                      0.0029, 0.0028, 0.0029, 0.0028, 0.0028, 0.0026, 0.0030, 0.0035, 0.0027,\n                      0.0028, 0.0031, 0.0029, 0.0030, 0.0026, 0.0028, 0.0026, 0.0027, 0.0023,\n                      0.0029, 0.0031, 0.0030, 0.0026, 0.0031, 0.0026, 0.0027, 0.0032, 0.0027,\n                      0.0027, 0.0029, 0.0031, 0.0028, 0.0027, 0.0025, 0.0028, 0.0024, 0.0028,\n                      0.0018, 0.0033, 0.0027, 0.0026, 0.0037, 0.0023, 0.0026, 0.0036, 0.0033,\n                      0.0027, 0.0026, 0.0028, 0.0028, 0.0036, 0.0025, 0.0030, 0.0025, 0.0028,\n                      0.0025, 0.0032, 0.0028, 0.0030, 0.0024, 0.0029, 0.0030, 0.0033, 0.0030,\n                      0.0032, 0.0028, 0.0031, 0.0028, 0.0029, 0.0025, 0.0026, 0.0027, 0.0030,\n                      0.0024, 0.0028, 0.0028, 0.0030, 0.0029, 0.0025, 0.0026, 0.0027, 0.0026,\n                      0.0030, 0.0023, 0.0027, 0.0028, 0.0031, 0.0026, 0.0034, 0.0029, 0.0028,\n                      0.0030, 0.0025, 0.0027, 0.0027, 0.0029, 0.0029, 0.0026, 0.0026, 0.0024,\n                      0.0030, 0.0027, 0.0029, 0.0027, 0.0029, 0.0030, 0.0026, 0.0032, 0.0026,\n                      0.0029, 0.0028, 0.0031, 0.0028, 0.0026, 0.0029, 0.0027, 0.0030, 0.0027,\n                      0.0023, 0.0032, 0.0036, 0.0037, 0.0028, 0.0028, 0.0031, 0.0032, 0.0035,\n                      0.0034, 0.0028, 0.0027, 0.0030, 0.0027, 0.0028, 0.0026, 0.0031, 0.0027,\n                      0.0032, 0.0033, 0.0030, 0.0030, 0.0029, 0.0028, 0.0028, 0.0033, 0.0030,\n                      0.0028, 0.0027, 0.0026, 0.0026, 0.0034, 0.0026, 0.0025, 0.0024, 0.0034,\n                      0.0025, 0.0030, 0.0026, 0.0034, 0.0022, 0.0028, 0.0028, 0.0025, 0.0025,\n                      0.0026, 0.0033, 0.0026, 0.0030, 0.0028, 0.0027, 0.0028, 0.0030, 0.0026,\n                      0.0026, 0.0034, 0.0027, 0.0029, 0.0026, 0.0028, 0.0028, 0.0024, 0.0030,\n                      0.0028, 0.0029, 0.0032, 0.0032, 0.0024, 0.0028, 0.0029, 0.0034, 0.0029,\n                      0.0026, 0.0023, 0.0028, 0.0028, 0.0037, 0.0030, 0.0025, 0.0026, 0.0030,\n                      0.0027, 0.0029, 0.0029, 0.0025, 0.0027, 0.0027, 0.0033, 0.0030, 0.0025,\n                      0.0027, 0.0032, 0.0030, 0.0030, 0.0023, 0.0028, 0.0028, 0.0026, 0.0032,\n                      0.0023, 0.0031, 0.0029, 0.0033, 0.0030, 0.0033, 0.0029, 0.0025, 0.0030,\n                      0.0024, 0.0027, 0.0030, 0.0029, 0.0028, 0.0033, 0.0026, 0.0027, 0.0030,\n                      0.0036, 0.0035, 0.0030, 0.0030, 0.0029, 0.0029, 0.0037, 0.0024, 0.0029,\n                      0.0038, 0.0031, 0.0029, 0.0032, 0.0026, 0.0029, 0.0025, 0.0029, 0.0032,\n                      0.0038, 0.0028, 0.0034, 0.0026, 0.0028, 0.0028, 0.0026, 0.0028, 0.0037,\n                      0.0024, 0.0026, 0.0033, 0.0030, 0.0032, 0.0026, 0.0028, 0.0028, 0.0021,\n                      0.0032, 0.0029, 0.0024, 0.0028, 0.0033, 0.0023, 0.0029, 0.0026, 0.0028,\n                      0.0032, 0.0022, 0.0030, 0.0031, 0.0026]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n              (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n                min_val=tensor([-0.3017, -0.4035, -0.4004, -0.3173, -0.2506, -0.2983, -0.3546, -0.3051,\n                        -0.3102, -0.3173, -0.3206, -0.3155, -0.3173, -0.3445, -0.3249, -0.3946,\n                        -0.3010, -0.3645, -0.3717, -0.3193, -0.3866, -0.3433, -0.5527, -0.3778,\n                        -0.2952, -0.3612, -0.3348, -0.3595, -0.3352, -0.3710, -0.3344, -0.4001,\n                        -0.2717, -0.3473, -0.2610, -0.2995, -0.3754, -0.3389, -0.3104, -0.3645,\n                        -0.3232, -0.3327, -0.3842, -0.3523, -0.2919, -0.3541, -0.3987, -0.2806,\n                        -0.3480, -0.3330, -0.2550, -0.3192, -0.3395, -0.2879, -0.3345, -0.3418,\n                        -0.3095, -0.2986, -0.3523, -0.3007, -0.3437, -0.3866, -0.3164, -0.3396,\n                        -0.3654, -0.3494, -0.3034, -0.3420, -0.3113, -0.3378, -0.2826, -0.3552,\n                        -0.2079, -0.4161, -0.3469, -0.3265, -0.4723, -0.2987, -0.3317, -0.4587,\n                        -0.4034, -0.3458, -0.3135, -0.3178, -0.3346, -0.3009, -0.3225, -0.3899,\n                        -0.3161, -0.3271, -0.3242, -0.3961, -0.3474, -0.3881, -0.2870, -0.2809,\n                        -0.3830, -0.3238, -0.3215, -0.3094, -0.3626, -0.4026, -0.3481, -0.3225,\n                        -0.3183, -0.3228, -0.3506, -0.2845, -0.2958, -0.2689, -0.3522, -0.3812,\n                        -0.3342, -0.2734, -0.3339, -0.3506, -0.2965, -0.3661, -0.2780, -0.3305,\n                        -0.3212, -0.3440, -0.3358, -0.3419, -0.3485, -0.3606, -0.3492, -0.3039,\n                        -0.3441, -0.2667, -0.3194, -0.3463, -0.3343, -0.3189, -0.2839, -0.3548,\n                        -0.3131, -0.3649, -0.3339, -0.3684, -0.3726, -0.3275, -0.4114, -0.3315,\n                        -0.3729, -0.3608, -0.3639, -0.3576, -0.3285, -0.3658, -0.2443, -0.3853,\n                        -0.3347, -0.2787, -0.3823, -0.4587, -0.3262, -0.3551, -0.3381, -0.3915,\n                        -0.4054, -0.4467, -0.3748, -0.3584, -0.3470, -0.3562, -0.3426, -0.3572,\n                        -0.3006, -0.2827, -0.3468, -0.4143, -0.3240, -0.2872, -0.3227, -0.3687,\n                        -0.3601, -0.2803, -0.4242, -0.3173, -0.3585, -0.3453, -0.3102, -0.3025,\n                        -0.4372, -0.3342, -0.3226, -0.2882, -0.3606, -0.3230, -0.2787, -0.3328,\n                        -0.3772, -0.2754, -0.3615, -0.3559, -0.3261, -0.3259, -0.3270, -0.3887,\n                        -0.3280, -0.3301, -0.3572, -0.3457, -0.3646, -0.3781, -0.3338, -0.3151,\n                        -0.4396, -0.3420, -0.3680, -0.3325, -0.3568, -0.3359, -0.3003, -0.3019,\n                        -0.3607, -0.3758, -0.4089, -0.4118, -0.2733, -0.2920, -0.3215, -0.2951,\n                        -0.3698, -0.3317, -0.3006, -0.3558, -0.3532, -0.2946, -0.3788, -0.3179,\n                        -0.3348, -0.3721, -0.3270, -0.3324, -0.3182, -0.2994, -0.3417, -0.3256,\n                        -0.3561, -0.3192, -0.3210, -0.3212, -0.3444, -0.3260, -0.3791, -0.2921,\n                        -0.3551, -0.3608, -0.3312, -0.3282, -0.2974, -0.3769, -0.3675, -0.3826,\n                        -0.3552, -0.3616, -0.3740, -0.3187, -0.3800, -0.3010, -0.3469, -0.3263,\n                        -0.3062, -0.3555, -0.4247, -0.2947, -0.2964, -0.3614, -0.3497, -0.4492,\n                        -0.3869, -0.3544, -0.3765, -0.3293, -0.4768, -0.3022, -0.3508, -0.3977,\n                        -0.3917, -0.3163, -0.4141, -0.3103, -0.3244, -0.3251, -0.3747, -0.4094,\n                        -0.4864, -0.3119, -0.4312, -0.2897, -0.3579, -0.3157, -0.3231, -0.3619,\n                        -0.4734, -0.2997, -0.3291, -0.4167, -0.3869, -0.3911, -0.3351, -0.3623,\n                        -0.3552, -0.2714, -0.4041, -0.3685, -0.2796, -0.3636, -0.4170, -0.2909,\n                        -0.3770, -0.3034, -0.3096, -0.3328, -0.2674, -0.3803, -0.3963, -0.3214]), max_val=tensor([0.2930, 0.3301, 0.3254, 0.3915, 0.3212, 0.3025, 0.3003, 0.3698, 0.2878,\n                        0.3165, 0.3729, 0.3702, 0.3420, 0.4437, 0.3051, 0.4079, 0.3255, 0.3372,\n                        0.4233, 0.3607, 0.3568, 0.3758, 0.3077, 0.3008, 0.3086, 0.3796, 0.3689,\n                        0.4093, 0.3156, 0.4201, 0.3380, 0.3499, 0.4872, 0.3177, 0.3142, 0.2900,\n                        0.3011, 0.3616, 0.3628, 0.2857, 0.3507, 0.3319, 0.3362, 0.4388, 0.3471,\n                        0.2991, 0.3505, 0.3695, 0.3839, 0.3311, 0.3507, 0.3356, 0.3189, 0.2981,\n                        0.3641, 0.3898, 0.3757, 0.3343, 0.3879, 0.3270, 0.3362, 0.4052, 0.3424,\n                        0.3097, 0.3037, 0.3993, 0.3509, 0.2942, 0.3139, 0.3608, 0.2997, 0.3368,\n                        0.2275, 0.3990, 0.3398, 0.3057, 0.3306, 0.2837, 0.3097, 0.3536, 0.4240,\n                        0.3155, 0.3332, 0.3516, 0.3498, 0.4574, 0.2916, 0.3129, 0.3215, 0.3558,\n                        0.3064, 0.4120, 0.3522, 0.3726, 0.3026, 0.3622, 0.3710, 0.4146, 0.3844,\n                        0.4011, 0.2911, 0.3369, 0.3604, 0.3638, 0.3205, 0.3251, 0.3455, 0.3863,\n                        0.2988, 0.3540, 0.3350, 0.3823, 0.3670, 0.3149, 0.2907, 0.3453, 0.3282,\n                        0.3768, 0.2938, 0.3452, 0.3538, 0.3950, 0.3231, 0.4278, 0.3688, 0.2956,\n                        0.3864, 0.3191, 0.3460, 0.3489, 0.3665, 0.3681, 0.3192, 0.3264, 0.2990,\n                        0.3872, 0.3477, 0.3553, 0.3391, 0.2975, 0.3764, 0.2941, 0.3000, 0.3058,\n                        0.3645, 0.3379, 0.3924, 0.3465, 0.2964, 0.3061, 0.3441, 0.3554, 0.3476,\n                        0.2906, 0.4061, 0.3589, 0.4735, 0.3077, 0.3512, 0.3414, 0.3512, 0.3308,\n                        0.4346, 0.2713, 0.3125, 0.3846, 0.3141, 0.3432, 0.3270, 0.3950, 0.3347,\n                        0.3140, 0.4236, 0.3822, 0.3848, 0.3563, 0.3010, 0.3496, 0.3692, 0.3867,\n                        0.3021, 0.3008, 0.3247, 0.3252, 0.3588, 0.3162, 0.3025, 0.3025, 0.4298,\n                        0.3210, 0.3787, 0.3049, 0.4259, 0.2538, 0.3502, 0.3282, 0.2098, 0.3232,\n                        0.3348, 0.4222, 0.2963, 0.3829, 0.3237, 0.3133, 0.3488, 0.3595, 0.3190,\n                        0.3320, 0.3200, 0.3327, 0.3436, 0.3244, 0.3370, 0.3520, 0.3010, 0.3751,\n                        0.3130, 0.3163, 0.3582, 0.3432, 0.3111, 0.3501, 0.3688, 0.4339, 0.3482,\n                        0.3042, 0.2891, 0.3322, 0.3611, 0.4706, 0.3055, 0.2556, 0.2980, 0.3773,\n                        0.3388, 0.3621, 0.3676, 0.3172, 0.3331, 0.3397, 0.4175, 0.3854, 0.3184,\n                        0.3388, 0.4068, 0.3816, 0.3113, 0.2756, 0.3172, 0.3037, 0.3111, 0.4115,\n                        0.2899, 0.3960, 0.3155, 0.4201, 0.3869, 0.4129, 0.3091, 0.3047, 0.3195,\n                        0.3098, 0.3114, 0.3787, 0.3703, 0.3495, 0.2420, 0.3330, 0.3391, 0.3839,\n                        0.4573, 0.3430, 0.3629, 0.3835, 0.3134, 0.3726, 0.3392, 0.3092, 0.3634,\n                        0.4837, 0.3957, 0.3711, 0.3275, 0.3296, 0.3620, 0.2762, 0.3032, 0.3525,\n                        0.3648, 0.3550, 0.3372, 0.3251, 0.3239, 0.3582, 0.3249, 0.3084, 0.3869,\n                        0.3093, 0.3155, 0.3121, 0.3807, 0.4007, 0.3001, 0.3150, 0.3302, 0.2722,\n                        0.3341, 0.3181, 0.3027, 0.3226, 0.3808, 0.2778, 0.3684, 0.3264, 0.3501,\n                        0.4119, 0.2849, 0.3552, 0.2854, 0.3343])\n              )\n            )\n          )\n        )\n      )\n      (18): Module(\n        (0): ConvBn2d(\n          320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (bn): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n            fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0079, 0.0068, 0.0113,  ..., 0.0106, 0.0080, 0.0077]), zero_point=tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n            (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([-1.0176, -0.8717, -1.4429,  ..., -1.3546, -1.0232, -0.9361]), max_val=tensor([0.9358, 0.8352, 0.9954,  ..., 0.7170, 0.9495, 0.9758]))\n          )\n        )\n        (2): ReLU6(inplace=True)\n      )\n    )\n    (classifier): Module(\n      (0): LinearReLU(\n        in_features=1280, out_features=1024, bias=True\n        (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0011, 0.0013, 0.0014,  ..., 0.0003, 0.0007, 0.0008]), zero_point=tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([-0.1403, -0.1259, -0.1742,  ..., -0.0414, -0.0838, -0.0993]), max_val=tensor([0.1240, 0.1684, 0.0906,  ..., 0.0384, 0.0703, 0.0911]))\n        )\n      )\n      (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (3): LinearReLU(\n        in_features=1024, out_features=512, bias=True\n        (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0012, 0.0014, 0.0013, 0.0015, 0.0012, 0.0015, 0.0012, 0.0017, 0.0011,\n                  0.0014, 0.0013, 0.0013, 0.0015, 0.0014, 0.0013, 0.0015, 0.0013, 0.0014,\n                  0.0012, 0.0013, 0.0015, 0.0012, 0.0015, 0.0013, 0.0014, 0.0016, 0.0013,\n                  0.0013, 0.0013, 0.0020, 0.0012, 0.0013, 0.0023, 0.0011, 0.0012, 0.0013,\n                  0.0011, 0.0015, 0.0013, 0.0016, 0.0018, 0.0020, 0.0011, 0.0014, 0.0012,\n                  0.0014, 0.0012, 0.0012, 0.0013, 0.0013, 0.0011, 0.0014, 0.0012, 0.0013,\n                  0.0014, 0.0014, 0.0012, 0.0014, 0.0012, 0.0012, 0.0013, 0.0014, 0.0015,\n                  0.0012, 0.0011, 0.0014, 0.0017, 0.0014, 0.0018, 0.0013, 0.0017, 0.0013,\n                  0.0015, 0.0013, 0.0014, 0.0019, 0.0013, 0.0012, 0.0013, 0.0014, 0.0016,\n                  0.0013, 0.0011, 0.0011, 0.0014, 0.0017, 0.0011, 0.0011, 0.0012, 0.0016,\n                  0.0020, 0.0015, 0.0013, 0.0014, 0.0013, 0.0012, 0.0013, 0.0012, 0.0016,\n                  0.0019, 0.0015, 0.0013, 0.0017, 0.0011, 0.0010, 0.0013, 0.0016, 0.0011,\n                  0.0012, 0.0015, 0.0020, 0.0012, 0.0017, 0.0011, 0.0011, 0.0013, 0.0017,\n                  0.0017, 0.0015, 0.0013, 0.0012, 0.0020, 0.0015, 0.0018, 0.0011, 0.0014,\n                  0.0013, 0.0012, 0.0013, 0.0013, 0.0011, 0.0016, 0.0011, 0.0012, 0.0018,\n                  0.0014, 0.0020, 0.0012, 0.0014, 0.0012, 0.0014, 0.0012, 0.0014, 0.0015,\n                  0.0014, 0.0013, 0.0018, 0.0015, 0.0012, 0.0012, 0.0018, 0.0013, 0.0012,\n                  0.0014, 0.0014, 0.0011, 0.0013, 0.0013, 0.0016, 0.0010, 0.0017, 0.0012,\n                  0.0014, 0.0029, 0.0011, 0.0018, 0.0013, 0.0017, 0.0012, 0.0015, 0.0012,\n                  0.0015, 0.0015, 0.0013, 0.0014, 0.0013, 0.0015, 0.0012, 0.0013, 0.0013,\n                  0.0014, 0.0012, 0.0013, 0.0012, 0.0013, 0.0013, 0.0011, 0.0012, 0.0011,\n                  0.0015, 0.0011, 0.0013, 0.0013, 0.0017, 0.0013, 0.0012, 0.0014, 0.0012,\n                  0.0019, 0.0014, 0.0015, 0.0012, 0.0011, 0.0025, 0.0012, 0.0014, 0.0014,\n                  0.0017, 0.0013, 0.0016, 0.0010, 0.0014, 0.0012, 0.0013, 0.0011, 0.0011,\n                  0.0012, 0.0011, 0.0014, 0.0016, 0.0012, 0.0013, 0.0017, 0.0017, 0.0015,\n                  0.0015, 0.0014, 0.0012, 0.0011, 0.0015, 0.0013, 0.0014, 0.0015, 0.0012,\n                  0.0011, 0.0012, 0.0013, 0.0012, 0.0014, 0.0012, 0.0014, 0.0013, 0.0019,\n                  0.0011, 0.0014, 0.0016, 0.0016, 0.0015, 0.0012, 0.0013, 0.0012, 0.0013,\n                  0.0013, 0.0017, 0.0012, 0.0013, 0.0017, 0.0014, 0.0016, 0.0015, 0.0011,\n                  0.0012, 0.0011, 0.0012, 0.0016, 0.0015, 0.0010, 0.0012, 0.0012, 0.0014,\n                  0.0014, 0.0014, 0.0012, 0.0014, 0.0010, 0.0014, 0.0016, 0.0013, 0.0013,\n                  0.0011, 0.0014, 0.0014, 0.0018, 0.0016, 0.0012, 0.0012, 0.0017, 0.0012,\n                  0.0013, 0.0024, 0.0014, 0.0012, 0.0010, 0.0012, 0.0016, 0.0011, 0.0013,\n                  0.0014, 0.0016, 0.0012, 0.0013, 0.0017, 0.0012, 0.0014, 0.0016, 0.0013,\n                  0.0014, 0.0015, 0.0013, 0.0014, 0.0012, 0.0013, 0.0011, 0.0013, 0.0013,\n                  0.0013, 0.0013, 0.0015, 0.0013, 0.0013, 0.0015, 0.0013, 0.0015, 0.0016,\n                  0.0016, 0.0012, 0.0015, 0.0014, 0.0015, 0.0015, 0.0013, 0.0012, 0.0012,\n                  0.0014, 0.0012, 0.0012, 0.0015, 0.0015, 0.0018, 0.0013, 0.0013, 0.0015,\n                  0.0014, 0.0016, 0.0012, 0.0011, 0.0013, 0.0012, 0.0013, 0.0018, 0.0012,\n                  0.0013, 0.0015, 0.0016, 0.0013, 0.0014, 0.0012, 0.0014, 0.0022, 0.0016,\n                  0.0015, 0.0013, 0.0013, 0.0025, 0.0020, 0.0012, 0.0014, 0.0013, 0.0012,\n                  0.0013, 0.0014, 0.0013, 0.0011, 0.0016, 0.0014, 0.0014, 0.0011, 0.0015,\n                  0.0011, 0.0014, 0.0012, 0.0017, 0.0013, 0.0012, 0.0014, 0.0013, 0.0013,\n                  0.0011, 0.0013, 0.0014, 0.0014, 0.0014, 0.0012, 0.0014, 0.0018, 0.0012,\n                  0.0024, 0.0011, 0.0016, 0.0016, 0.0016, 0.0012, 0.0013, 0.0011, 0.0016,\n                  0.0011, 0.0014, 0.0013, 0.0011, 0.0012, 0.0011, 0.0014, 0.0013, 0.0011,\n                  0.0013, 0.0012, 0.0013, 0.0014, 0.0012, 0.0017, 0.0020, 0.0019, 0.0013,\n                  0.0010, 0.0015, 0.0011, 0.0013, 0.0018, 0.0014, 0.0013, 0.0015, 0.0017,\n                  0.0013, 0.0014, 0.0015, 0.0016, 0.0015, 0.0017, 0.0013, 0.0014, 0.0018,\n                  0.0015, 0.0012, 0.0013, 0.0020, 0.0011, 0.0014, 0.0014, 0.0013, 0.0012,\n                  0.0012, 0.0014, 0.0011, 0.0012, 0.0013, 0.0015, 0.0014, 0.0012, 0.0016,\n                  0.0012, 0.0015, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0013,\n                  0.0013, 0.0013, 0.0017, 0.0013, 0.0015, 0.0013, 0.0015, 0.0016, 0.0011,\n                  0.0014, 0.0015, 0.0012, 0.0014, 0.0011, 0.0013, 0.0012, 0.0013, 0.0013,\n                  0.0012, 0.0015, 0.0012, 0.0016, 0.0015, 0.0016, 0.0017, 0.0014, 0.0012,\n                  0.0017, 0.0013, 0.0013, 0.0012, 0.0015, 0.0013, 0.0013, 0.0015, 0.0014,\n                  0.0012, 0.0014, 0.0012, 0.0016, 0.0013, 0.0013, 0.0012, 0.0014]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                  0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n          (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n            min_val=tensor([-0.1554, -0.1735, -0.1696, -0.1621, -0.1424, -0.1514, -0.1479, -0.1781,\n                    -0.1439, -0.1674, -0.1652, -0.1612, -0.1636, -0.1822, -0.1679, -0.1599,\n                    -0.1717, -0.1749, -0.1524, -0.1542, -0.1891, -0.1491, -0.1548, -0.1423,\n                    -0.1763, -0.2072, -0.1720, -0.1664, -0.1530, -0.2533, -0.1532, -0.1700,\n                    -0.2947, -0.1430, -0.1596, -0.1366, -0.1413, -0.1845, -0.1199, -0.2052,\n                    -0.2315, -0.1956, -0.1426, -0.1731, -0.1528, -0.1840, -0.1589, -0.1493,\n                    -0.1696, -0.1689, -0.1179, -0.1601, -0.1552, -0.1642, -0.1816, -0.1624,\n                    -0.1469, -0.1847, -0.1519, -0.1459, -0.1611, -0.1759, -0.1876, -0.1512,\n                    -0.1422, -0.1741, -0.2131, -0.1623, -0.2058, -0.1706, -0.1970, -0.1723,\n                    -0.1743, -0.1605, -0.1333, -0.2036, -0.1717, -0.1499, -0.1607, -0.1587,\n                    -0.2056, -0.1685, -0.1118, -0.1359, -0.1849, -0.2050, -0.1319, -0.1240,\n                    -0.1489, -0.1615, -0.2093, -0.1495, -0.1675, -0.1506, -0.1449, -0.1542,\n                    -0.1395, -0.1441, -0.1518, -0.2274, -0.1940, -0.1695, -0.2113, -0.1472,\n                    -0.1341, -0.1632, -0.1912, -0.1362, -0.1223, -0.1977, -0.2039, -0.1375,\n                    -0.2228, -0.1114, -0.1384, -0.1529, -0.1757, -0.2223, -0.1586, -0.1677,\n                    -0.1525, -0.2547, -0.1404, -0.1666, -0.1174, -0.1260, -0.1349, -0.1576,\n                    -0.1629, -0.1698, -0.1447, -0.1691, -0.1257, -0.1578, -0.2280, -0.1732,\n                    -0.2167, -0.1479, -0.1634, -0.1360, -0.1456, -0.1494, -0.1830, -0.1778,\n                    -0.1829, -0.1611, -0.1916, -0.1962, -0.1481, -0.1512, -0.2256, -0.1367,\n                    -0.1557, -0.1729, -0.1799, -0.1447, -0.1644, -0.1510, -0.2097, -0.1336,\n                    -0.1910, -0.1501, -0.1781, -0.3439, -0.1375, -0.2354, -0.1725, -0.2156,\n                    -0.1492, -0.1503, -0.1267, -0.1831, -0.1882, -0.1454, -0.1718, -0.1656,\n                    -0.1865, -0.1386, -0.1492, -0.1667, -0.1297, -0.1505, -0.1641, -0.1419,\n                    -0.1412, -0.1663, -0.1430, -0.1296, -0.1290, -0.1652, -0.1403, -0.1673,\n                    -0.1531, -0.1723, -0.1637, -0.1553, -0.1801, -0.1512, -0.1671, -0.1852,\n                    -0.1885, -0.1484, -0.1442, -0.2588, -0.1571, -0.1847, -0.1188, -0.2140,\n                    -0.1306, -0.1945, -0.1193, -0.1738, -0.1584, -0.1706, -0.1365, -0.1466,\n                    -0.1589, -0.1406, -0.1763, -0.2022, -0.1553, -0.1611, -0.2012, -0.1993,\n                    -0.1912, -0.1865, -0.1823, -0.1509, -0.1433, -0.1941, -0.1706, -0.1677,\n                    -0.1325, -0.1573, -0.1452, -0.1476, -0.1369, -0.1380, -0.1758, -0.1524,\n                    -0.1774, -0.1591, -0.2202, -0.1329, -0.1748, -0.1951, -0.2050, -0.1947,\n                    -0.1338, -0.1704, -0.1518, -0.1611, -0.1624, -0.1766, -0.1575, -0.1584,\n                    -0.2221, -0.1782, -0.1809, -0.1981, -0.1143, -0.1573, -0.1173, -0.1336,\n                    -0.2011, -0.1896, -0.1296, -0.1479, -0.1550, -0.1749, -0.1454, -0.1567,\n                    -0.1551, -0.1474, -0.1219, -0.1766, -0.2069, -0.1639, -0.1619, -0.1413,\n                    -0.1622, -0.1782, -0.2274, -0.1631, -0.1536, -0.1475, -0.2193, -0.1484,\n                    -0.1711, -0.2549, -0.1419, -0.1326, -0.1252, -0.1480, -0.2012, -0.1442,\n                    -0.1294, -0.1780, -0.2029, -0.1488, -0.1452, -0.1759, -0.1483, -0.1771,\n                    -0.1300, -0.1678, -0.1538, -0.1726, -0.1657, -0.1756, -0.1511, -0.1613,\n                    -0.1348, -0.1675, -0.1549, -0.1717, -0.1672, -0.1974, -0.1616, -0.1161,\n                    -0.1951, -0.1487, -0.1913, -0.1858, -0.2037, -0.1588, -0.1939, -0.1425,\n                    -0.1972, -0.1895, -0.1481, -0.1571, -0.1548, -0.1584, -0.1545, -0.1544,\n                    -0.1579, -0.1785, -0.2294, -0.1283, -0.1684, -0.1618, -0.1453, -0.1865,\n                    -0.1435, -0.1443, -0.1384, -0.1553, -0.1726, -0.2265, -0.1494, -0.1600,\n                    -0.1885, -0.2023, -0.1354, -0.1637, -0.1539, -0.1812, -0.2802, -0.1710,\n                    -0.1941, -0.1647, -0.1442, -0.2494, -0.1587, -0.1347, -0.1760, -0.1696,\n                    -0.1402, -0.1700, -0.1817, -0.1347, -0.1387, -0.2037, -0.1754, -0.1767,\n                    -0.1222, -0.1385, -0.1345, -0.1823, -0.1236, -0.2033, -0.1444, -0.1460,\n                    -0.1646, -0.1518, -0.1634, -0.1268, -0.1650, -0.1779, -0.1668, -0.1507,\n                    -0.1576, -0.1208, -0.2185, -0.1461, -0.2912, -0.1198, -0.1304, -0.1345,\n                    -0.1869, -0.1520, -0.1237, -0.1461, -0.2002, -0.1409, -0.1810, -0.1390,\n                    -0.1348, -0.1249, -0.1461, -0.1730, -0.1542, -0.1460, -0.1658, -0.1491,\n                    -0.1649, -0.1494, -0.1536, -0.1600, -0.2055, -0.1974, -0.1614, -0.1281,\n                    -0.1937, -0.1391, -0.1646, -0.1950, -0.1768, -0.1666, -0.1332, -0.1904,\n                    -0.1629, -0.1721, -0.1895, -0.2030, -0.1949, -0.2188, -0.1623, -0.1820,\n                    -0.2320, -0.1871, -0.1184, -0.1642, -0.2503, -0.1244, -0.1302, -0.1757,\n                    -0.1672, -0.1597, -0.1474, -0.1411, -0.1442, -0.1587, -0.1662, -0.1969,\n                    -0.1800, -0.1445, -0.1998, -0.1552, -0.1653, -0.1542, -0.1494, -0.1286,\n                    -0.1523, -0.1559, -0.1585, -0.1599, -0.1643, -0.1623, -0.2217, -0.1610,\n                    -0.1589, -0.1720, -0.1896, -0.2035, -0.1270, -0.1784, -0.1496, -0.1404,\n                    -0.1726, -0.1419, -0.1466, -0.1373, -0.1658, -0.1455, -0.1311, -0.1646,\n                    -0.1517, -0.1939, -0.1899, -0.1631, -0.2062, -0.1823, -0.1474, -0.2198,\n                    -0.1626, -0.1698, -0.1569, -0.1547, -0.1671, -0.1705, -0.1839, -0.1651,\n                    -0.1484, -0.1581, -0.1485, -0.1900, -0.1656, -0.1365, -0.1511, -0.1344]), max_val=tensor([0.1032, 0.1397, 0.1432, 0.1844, 0.1522, 0.1862, 0.1473, 0.2208, 0.1396,\n                    0.1821, 0.1539, 0.1407, 0.1852, 0.1777, 0.1308, 0.1930, 0.1362, 0.1506,\n                    0.1494, 0.1686, 0.1508, 0.1388, 0.1963, 0.1604, 0.1766, 0.2083, 0.1702,\n                    0.1536, 0.1635, 0.2372, 0.1159, 0.1639, 0.2442, 0.1138, 0.1322, 0.1634,\n                    0.1347, 0.1867, 0.1648, 0.1500, 0.2227, 0.2561, 0.1358, 0.1627, 0.1543,\n                    0.1776, 0.1569, 0.1370, 0.1215, 0.1683, 0.1442, 0.1836, 0.1531, 0.1620,\n                    0.1819, 0.1748, 0.1580, 0.1668, 0.1429, 0.1564, 0.1617, 0.1432, 0.1883,\n                    0.1511, 0.1421, 0.1720, 0.2110, 0.1761, 0.2317, 0.1304, 0.2139, 0.1281,\n                    0.1845, 0.1471, 0.1772, 0.2435, 0.1406, 0.1504, 0.1634, 0.1747, 0.1535,\n                    0.1354, 0.1390, 0.1393, 0.1681, 0.2170, 0.1447, 0.1421, 0.1285, 0.2011,\n                    0.2571, 0.1850, 0.1649, 0.1785, 0.1630, 0.1381, 0.1622, 0.1518, 0.2041,\n                    0.2411, 0.1575, 0.1238, 0.1593, 0.1363, 0.1302, 0.1324, 0.1978, 0.1408,\n                    0.1492, 0.1750, 0.2582, 0.1479, 0.1945, 0.1372, 0.1305, 0.1617, 0.2131,\n                    0.1951, 0.1848, 0.1431, 0.1484, 0.2536, 0.1893, 0.2269, 0.1450, 0.1841,\n                    0.1605, 0.1241, 0.1623, 0.1633, 0.1333, 0.2059, 0.1449, 0.1549, 0.2236,\n                    0.1729, 0.2537, 0.1408, 0.1720, 0.1555, 0.1774, 0.1112, 0.1330, 0.1863,\n                    0.1445, 0.1650, 0.2250, 0.1553, 0.1440, 0.1323, 0.1808, 0.1617, 0.1582,\n                    0.1708, 0.1764, 0.1279, 0.1471, 0.1713, 0.2069, 0.1304, 0.2119, 0.1443,\n                    0.1508, 0.3693, 0.1360, 0.1954, 0.1697, 0.1726, 0.1536, 0.1895, 0.1537,\n                    0.1882, 0.1553, 0.1637, 0.1755, 0.1333, 0.1421, 0.1573, 0.1685, 0.1400,\n                    0.1735, 0.1560, 0.1629, 0.1568, 0.1642, 0.1649, 0.1089, 0.1521, 0.1364,\n                    0.1867, 0.1275, 0.1572, 0.1597, 0.2121, 0.1330, 0.1482, 0.1786, 0.1519,\n                    0.2442, 0.1689, 0.1613, 0.1218, 0.1412, 0.3133, 0.1147, 0.1258, 0.1810,\n                    0.1564, 0.1649, 0.2050, 0.1307, 0.1274, 0.1340, 0.1688, 0.1291, 0.1223,\n                    0.1550, 0.1405, 0.1756, 0.2018, 0.1532, 0.1550, 0.2201, 0.2176, 0.1897,\n                    0.1843, 0.1472, 0.1450, 0.1166, 0.1913, 0.1390, 0.1826, 0.1953, 0.1533,\n                    0.1233, 0.1441, 0.1685, 0.1463, 0.1452, 0.1341, 0.1322, 0.1681, 0.2368,\n                    0.1442, 0.1741, 0.2023, 0.1940, 0.1247, 0.1570, 0.1370, 0.1159, 0.1652,\n                    0.1615, 0.2117, 0.1508, 0.1603, 0.1499, 0.1706, 0.2049, 0.1537, 0.1402,\n                    0.1533, 0.1442, 0.1585, 0.1426, 0.1575, 0.1293, 0.1231, 0.1446, 0.1809,\n                    0.1767, 0.1729, 0.1262, 0.1765, 0.1253, 0.1409, 0.1911, 0.1163, 0.1655,\n                    0.1390, 0.1795, 0.1618, 0.1828, 0.2003, 0.1549, 0.1562, 0.2020, 0.1559,\n                    0.1702, 0.3062, 0.1719, 0.1509, 0.1186, 0.1103, 0.1932, 0.1452, 0.1698,\n                    0.1636, 0.1867, 0.1291, 0.1627, 0.2165, 0.1424, 0.1779, 0.2009, 0.1674,\n                    0.1795, 0.1908, 0.1495, 0.1706, 0.1126, 0.1542, 0.1310, 0.1376, 0.1695,\n                    0.1687, 0.1658, 0.1768, 0.1434, 0.1594, 0.1711, 0.1637, 0.1522, 0.2009,\n                    0.1843, 0.1581, 0.1688, 0.1728, 0.1968, 0.1294, 0.1640, 0.0925, 0.1508,\n                    0.1808, 0.1340, 0.1179, 0.1954, 0.1883, 0.1979, 0.1642, 0.1633, 0.1935,\n                    0.1795, 0.2059, 0.1502, 0.1348, 0.1696, 0.1564, 0.1345, 0.2001, 0.1493,\n                    0.1576, 0.1709, 0.1278, 0.1639, 0.1782, 0.1456, 0.1789, 0.1592, 0.2005,\n                    0.1770, 0.1481, 0.1619, 0.3175, 0.2493, 0.1490, 0.1739, 0.1336, 0.1502,\n                    0.1652, 0.1807, 0.1604, 0.1443, 0.1845, 0.1323, 0.1428, 0.1352, 0.1853,\n                    0.1394, 0.1528, 0.1501, 0.2134, 0.1636, 0.1486, 0.1764, 0.1681, 0.1635,\n                    0.1340, 0.1125, 0.1541, 0.1765, 0.1793, 0.1543, 0.1821, 0.2259, 0.1490,\n                    0.2998, 0.1454, 0.2014, 0.2076, 0.2077, 0.1527, 0.1654, 0.1377, 0.1628,\n                    0.0944, 0.1723, 0.1696, 0.1357, 0.1538, 0.1449, 0.1654, 0.1643, 0.1457,\n                    0.1511, 0.1364, 0.1642, 0.1784, 0.1114, 0.2162, 0.2517, 0.2423, 0.1593,\n                    0.1286, 0.1869, 0.1431, 0.1639, 0.2269, 0.1741, 0.1593, 0.1856, 0.2105,\n                    0.1675, 0.1760, 0.1875, 0.1836, 0.1923, 0.1698, 0.1621, 0.1744, 0.1696,\n                    0.1575, 0.1469, 0.1579, 0.2277, 0.1450, 0.1794, 0.1337, 0.1644, 0.1469,\n                    0.1530, 0.1765, 0.1285, 0.1483, 0.1689, 0.1948, 0.1656, 0.1484, 0.1859,\n                    0.1511, 0.1842, 0.1477, 0.1301, 0.1496, 0.1162, 0.1577, 0.1586, 0.1603,\n                    0.1634, 0.1550, 0.1262, 0.1385, 0.1931, 0.1631, 0.1806, 0.1747, 0.1423,\n                    0.1450, 0.1918, 0.1483, 0.1789, 0.1317, 0.1624, 0.1536, 0.1349, 0.1609,\n                    0.1520, 0.1952, 0.1411, 0.1980, 0.1889, 0.1975, 0.2123, 0.1804, 0.1327,\n                    0.2009, 0.1321, 0.1406, 0.1309, 0.1949, 0.1652, 0.1405, 0.1869, 0.1760,\n                    0.1489, 0.1808, 0.1567, 0.2045, 0.1649, 0.1677, 0.1526, 0.1790])\n          )\n        )\n      )\n      (5): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (6): Linear(\n        in_features=512, out_features=10, bias=True\n        (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0011, 0.0011, 0.0010, 0.0011, 0.0011, 0.0012, 0.0009, 0.0012, 0.0012,\n                  0.0014]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n          (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n            min_val=tensor([-0.1372, -0.1433, -0.1274, -0.1100, -0.1328, -0.1599, -0.1186, -0.1496,\n                    -0.1504, -0.1802]), max_val=tensor([0.1053, 0.1436, 0.0933, 0.1387, 0.1414, 0.1331, 0.1128, 0.1239, 0.0864,\n                    0.1490])\n          )\n        )\n      )\n      (7): LogSoftmax(dim=1)\n    )\n  )\n  (activation_post_process_1): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0673]), zero_point=tensor([30], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.9912073612213135, max_val=6.558025360107422)\n  )\n  (activation_post_process_2): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0673]), zero_point=tensor([30], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.9912073612213135, max_val=6.558025360107422)\n  )\n  (activation_post_process_3): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.2140]), zero_point=tensor([62], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-13.23567008972168, max_val=13.941986083984375)\n  )\n  (activation_post_process_4): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.2140]), zero_point=tensor([62], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-13.23567008972168, max_val=13.941986083984375)\n  )\n  (activation_post_process_5): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.1324]), zero_point=tensor([63], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-8.405353546142578, max_val=8.415411949157715)\n  )\n  (activation_post_process_6): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0828]), zero_point=tensor([51], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-4.197344779968262, max_val=6.31725549697876)\n  )\n  (activation_post_process_7): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0828]), zero_point=tensor([51], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-4.197344779968262, max_val=6.31725549697876)\n  )\n  (activation_post_process_8): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0917]), zero_point=tensor([61], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-5.600224494934082, max_val=6.048417568206787)\n  )\n  (activation_post_process_9): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0917]), zero_point=tensor([61], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-5.600224494934082, max_val=6.048417568206787)\n  )\n  (activation_post_process_10): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.1019]), zero_point=tensor([54], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-5.462436199188232, max_val=7.476212024688721)\n  )\n  (activation_post_process_11): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0356]), zero_point=tensor([59], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.1074728965759277, max_val=2.418851613998413)\n  )\n  (activation_post_process_12): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0356]), zero_point=tensor([59], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.1074728965759277, max_val=2.418851613998413)\n  )\n  (activation_post_process_13): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0497]), zero_point=tensor([46], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.2760863304138184, max_val=4.036453723907471)\n  )\n  (activation_post_process_14): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0497]), zero_point=tensor([46], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.2760863304138184, max_val=4.036453723907471)\n  )\n  (activation_post_process_15): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0785]), zero_point=tensor([59], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-4.64752197265625, max_val=5.315978050231934)\n  )\n  (activation_post_process_16): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.1189]), zero_point=tensor([54], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-6.461780548095703, max_val=8.641191482543945)\n  )\n  (activation_post_process_17): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0413]), zero_point=tensor([54], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.2149484157562256, max_val=3.026954412460327)\n  )\n  (activation_post_process_18): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0413]), zero_point=tensor([54], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.2149484157562256, max_val=3.026954412460327)\n  )\n  (activation_post_process_19): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0470]), zero_point=tensor([51], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.3961308002471924, max_val=3.5780975818634033)\n  )\n  (activation_post_process_20): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0470]), zero_point=tensor([51], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.3961308002471924, max_val=3.5780975818634033)\n  )\n  (activation_post_process_21): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0670]), zero_point=tensor([67], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-4.511716365814209, max_val=3.9998433589935303)\n  )\n  (activation_post_process_22): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0167]), zero_point=tensor([46], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.7724939584732056, max_val=1.3478087186813354)\n  )\n  (activation_post_process_23): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0167]), zero_point=tensor([46], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.7724939584732056, max_val=1.3478087186813354)\n  )\n  (activation_post_process_24): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0298]), zero_point=tensor([50], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.48408043384552, max_val=2.2963902950286865)\n  )\n  (activation_post_process_25): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0298]), zero_point=tensor([50], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.48408043384552, max_val=2.2963902950286865)\n  )\n  (activation_post_process_26): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0527]), zero_point=tensor([65], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.4081485271453857, max_val=3.2792205810546875)\n  )\n  (activation_post_process_27): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0783]), zero_point=tensor([64], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-5.0248260498046875, max_val=4.91534948348999)\n  )\n  (activation_post_process_28): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0168]), zero_point=tensor([39], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6575756669044495, max_val=1.4755045175552368)\n  )\n  (activation_post_process_29): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0168]), zero_point=tensor([39], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6575756669044495, max_val=1.4755045175552368)\n  )\n  (activation_post_process_30): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0294]), zero_point=tensor([45], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.3143341541290283, max_val=2.420137643814087)\n  )\n  (activation_post_process_31): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0294]), zero_point=tensor([45], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.3143341541290283, max_val=2.420137643814087)\n  )\n  (activation_post_process_32): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0478]), zero_point=tensor([63], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.9968388080596924, max_val=3.077651262283325)\n  )\n  (activation_post_process_33): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0896]), zero_point=tensor([62], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-5.5915350914001465, max_val=5.793413162231445)\n  )\n  (activation_post_process_34): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0259]), zero_point=tensor([41], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0526056289672852, max_val=2.2397634983062744)\n  )\n  (activation_post_process_35): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0259]), zero_point=tensor([41], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0526056289672852, max_val=2.2397634983062744)\n  )\n  (activation_post_process_36): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0305]), zero_point=tensor([37], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.11781907081604, max_val=2.7611680030822754)\n  )\n  (activation_post_process_37): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0305]), zero_point=tensor([37], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.11781907081604, max_val=2.7611680030822754)\n  )\n  (activation_post_process_38): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0558]), zero_point=tensor([70], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.934875965118408, max_val=3.1556177139282227)\n  )\n  (activation_post_process_39): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0124]), zero_point=tensor([43], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5382543206214905, max_val=1.0371376276016235)\n  )\n  (activation_post_process_40): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0124]), zero_point=tensor([43], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5382543206214905, max_val=1.0371376276016235)\n  )\n  (activation_post_process_41): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0280]), zero_point=tensor([41], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.1430102586746216, max_val=2.4163875579833984)\n  )\n  (activation_post_process_42): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0280]), zero_point=tensor([41], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.1430102586746216, max_val=2.4163875579833984)\n  )\n  (activation_post_process_43): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0420]), zero_point=tensor([66], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.7594153881073, max_val=2.578143358230591)\n  )\n  (activation_post_process_44): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0563]), zero_point=tensor([65], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.6426503658294678, max_val=3.508071184158325)\n  )\n  (activation_post_process_45): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0116]), zero_point=tensor([46], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5302771329879761, max_val=0.9385779500007629)\n  )\n  (activation_post_process_46): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0116]), zero_point=tensor([46], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5302771329879761, max_val=0.9385779500007629)\n  )\n  (activation_post_process_47): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0224]), zero_point=tensor([49], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.1004540920257568, max_val=1.7490841150283813)\n  )\n  (activation_post_process_48): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0224]), zero_point=tensor([49], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.1004540920257568, max_val=1.7490841150283813)\n  )\n  (activation_post_process_49): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0303]), zero_point=tensor([69], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.0795934200286865, max_val=1.7635278701782227)\n  )\n  (activation_post_process_50): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0585]), zero_point=tensor([67], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.9433445930480957, max_val=3.4848835468292236)\n  )\n  (activation_post_process_51): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0143]), zero_point=tensor([38], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5479546189308167, max_val=1.2693192958831787)\n  )\n  (activation_post_process_52): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0143]), zero_point=tensor([38], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5479546189308167, max_val=1.2693192958831787)\n  )\n  (activation_post_process_53): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0579]), zero_point=tensor([15], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.8932462930679321, max_val=6.462332725524902)\n  )\n  (activation_post_process_54): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0579]), zero_point=tensor([15], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.8932462930679321, max_val=6.462332725524902)\n  )\n  (activation_post_process_55): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0725]), zero_point=tensor([76], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-5.5383148193359375, max_val=3.6724064350128174)\n  )\n  (activation_post_process_56): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0837]), zero_point=tensor([69], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-5.775081157684326, max_val=4.859138488769531)\n  )\n  (activation_post_process_57): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0207]), zero_point=tensor([44], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.9107241034507751, max_val=1.7198708057403564)\n  )\n  (activation_post_process_58): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0207]), zero_point=tensor([44], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.9107241034507751, max_val=1.7198708057403564)\n  )\n  (activation_post_process_59): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0463]), zero_point=tensor([32], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.4969818592071533, max_val=4.3768768310546875)\n  )\n  (activation_post_process_60): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0463]), zero_point=tensor([32], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.4969818592071533, max_val=4.3768768310546875)\n  )\n  (activation_post_process_61): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0477]), zero_point=tensor([64], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.0510783195495605, max_val=3.012347936630249)\n  )\n  (activation_post_process_62): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0211]), zero_point=tensor([37], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.7750471830368042, max_val=1.9028416872024536)\n  )\n  (activation_post_process_63): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0211]), zero_point=tensor([37], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.7750471830368042, max_val=1.9028416872024536)\n  )\n  (activation_post_process_64): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0979]), zero_point=tensor([53], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-5.226074695587158, max_val=7.2036452293396)\n  )\n  (activation_post_process_65): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0979]), zero_point=tensor([53], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-5.226074695587158, max_val=7.2036452293396)\n  )\n  (activation_post_process_66): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0710]), zero_point=tensor([56], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.959630250930786, max_val=5.054750919342041)\n  )\n  (activation_post_process_67): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0705]), zero_point=tensor([52], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.6465299129486084, max_val=5.304708480834961)\n  )\n  (activation_post_process_68): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0541]), zero_point=tensor([25], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.377771019935608, max_val=5.490816116333008)\n  )\n  (activation_post_process_69): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0541]), zero_point=tensor([25], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.377771019935608, max_val=5.490816116333008)\n  )\n  (activation_post_process_70): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0678]), zero_point=tensor([32], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.1605722904205322, max_val=6.449800968170166)\n  )\n  (activation_post_process_71): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0678]), zero_point=tensor([32], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.1605722904205322, max_val=6.449800968170166)\n  )\n  (activation_post_process_72): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.1249]), zero_point=tensor([71], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-8.905035972595215, max_val=6.958243370056152)\n  )\n  (activation_post_process_73): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.1384]), zero_point=tensor([61], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-8.402813911437988, max_val=9.172750473022461)\n  )\n  (activation_post_process_74): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0391]), zero_point=tensor([23], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.8955557942390442, max_val=4.066318511962891)\n  )\n  (activation_post_process_75): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0391]), zero_point=tensor([23], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.8955557942390442, max_val=4.066318511962891)\n  )\n  (activation_post_process_76): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0688]), zero_point=tensor([29], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.029092788696289, max_val=6.708486557006836)\n  )\n  (activation_post_process_77): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0688]), zero_point=tensor([29], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.029092788696289, max_val=6.708486557006836)\n  )\n  (activation_post_process_78): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.1466]), zero_point=tensor([74], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-10.853048324584961, max_val=7.769497394561768)\n  )\n  (activation_post_process_79): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0586]), zero_point=tensor([36], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.0884227752685547, max_val=5.3533830642700195)\n  )\n  (activation_post_process_80): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0586]), zero_point=tensor([36], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.0884227752685547, max_val=5.3533830642700195)\n  )\n  (activation_post_process_81): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0780]), zero_point=tensor([47], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.645120859146118, max_val=6.261792182922363)\n  )\n  (activation_post_process_82): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0780]), zero_point=tensor([47], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.645120859146118, max_val=6.261792182922363)\n  )\n  (activation_post_process_83): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.1317]), zero_point=tensor([53], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-6.937586307525635, max_val=9.790032386779785)\n  )\n  (activation_post_process_84): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.2482]), zero_point=tensor([59], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-14.740655899047852, max_val=16.784685134887695)\n  )\n  (activation_post_process_85): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0544]), zero_point=tensor([37], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.0055596828460693, max_val=4.898824214935303)\n  )\n  (activation_post_process_86): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0544]), zero_point=tensor([37], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.0055596828460693, max_val=4.898824214935303)\n  )\n  (activation_post_process_87): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0882]), zero_point=tensor([34], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.9977667331695557, max_val=8.207756996154785)\n  )\n  (activation_post_process_88): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0882]), zero_point=tensor([34], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.9977667331695557, max_val=8.207756996154785)\n  )\n  (activation_post_process_89): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.2924]), zero_point=tensor([66], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-19.17190170288086, max_val=17.958045959472656)\n  )\n  (activation_post_process_90): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.4744]), zero_point=tensor([55], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-26.1285343170166, max_val=34.122413635253906)\n  )\n  (activation_post_process_91): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0673]), zero_point=tensor([72], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-4.870312690734863, max_val=3.6720190048217773)\n  )\n  (activation_post_process_92): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0673]), zero_point=tensor([72], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-4.870312690734863, max_val=3.6720190048217773)\n  )\n  (activation_post_process_93): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0285]), zero_point=tensor([67], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.8983958959579468, max_val=1.7176510095596313)\n  )\n  (activation_post_process_94): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0285]), zero_point=tensor([67], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.8983958959579468, max_val=1.7176510095596313)\n  )\n  (activation_post_process_95): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0229]), zero_point=tensor([64], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.4644443988800049, max_val=1.448568344116211)\n  )\n  (activation_post_process_96): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0665]), zero_point=tensor([35], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.303508996963501, max_val=6.1477251052856445)\n  )\n  (activation_post_process_97): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0665]), zero_point=tensor([35], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.303508996963501, max_val=6.1477251052856445)\n  )\n  (activation_post_process_98): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0665]), zero_point=tensor([35], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.303508996963501, max_val=6.1477251052856445)\n  )\n  (activation_post_process_99): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0665]), zero_point=tensor([35], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.303508996963501, max_val=6.1477251052856445)\n  )\n  (activation_post_process_100): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.1614]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=20.497081756591797)\n  )\n  (activation_post_process_101): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.1081]), zero_point=tensor([27], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.947880268096924, max_val=10.774870872497559)\n  )\n  (activation_post_process_102): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.1162]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=14.754679679870605)\n  )\n  (activation_post_process_103): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0561]), zero_point=tensor([18], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0180859565734863, max_val=6.107339859008789)\n  )\n  (activation_post_process_104): FusedMovingAvgObsFakeQuantize(\n    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.1648]), zero_point=tensor([52], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n    (activation_post_process): MovingAverageMinMaxObserver(min_val=-8.493448257446289, max_val=12.438427925109863)\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"model_quantized_trained = quantize_fx.convert_fx(model_prepared)","metadata":{"execution":{"iopub.status.busy":"2024-04-07T12:31:54.700009Z","iopub.execute_input":"2024-04-07T12:31:54.700543Z","iopub.status.idle":"2024-04-07T12:31:55.999990Z","shell.execute_reply.started":"2024-04-07T12:31:54.700505Z","shell.execute_reply":"2024-04-07T12:31:55.999199Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"model_quantized_trained.to('cpu')","metadata":{"execution":{"iopub.status.busy":"2024-04-07T12:31:56.001097Z","iopub.execute_input":"2024-04-07T12:31:56.001456Z","iopub.status.idle":"2024-04-07T12:31:56.036417Z","shell.execute_reply.started":"2024-04-07T12:31:56.001426Z","shell.execute_reply":"2024-04-07T12:31:56.035543Z"},"trusted":true},"execution_count":45,"outputs":[{"execution_count":45,"output_type":"execute_result","data":{"text/plain":"GraphModule(\n  (mnet): Module(\n    (features): Module(\n      (0): Module(\n        (0): QuantizedConv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), scale=0.06731679290533066, zero_point=30, padding=(1, 1))\n        (2): ReLU6(inplace=True)\n      )\n      (1): Module(\n        (conv): Module(\n          (0): Module(\n            (0): QuantizedConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), scale=0.21399728953838348, zero_point=62, padding=(1, 1), groups=32)\n            (2): ReLU6(inplace=True)\n          )\n          (1): QuantizedConv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), scale=0.13244697451591492, zero_point=63)\n        )\n      )\n      (2): Module(\n        (conv): Module(\n          (0): Module(\n            (0): QuantizedConv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), scale=0.08279213309288025, zero_point=51)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Module(\n            (0): QuantizedConv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), scale=0.09172158688306808, zero_point=61, padding=(1, 1), groups=96)\n            (2): ReLU6(inplace=True)\n          )\n          (2): QuantizedConv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), scale=0.10187911987304688, zero_point=54)\n        )\n      )\n      (3): Module(\n        (conv): Module(\n          (0): Module(\n            (0): QuantizedConv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), scale=0.03564034774899483, zero_point=59)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Module(\n            (0): QuantizedConv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), scale=0.04970503970980644, zero_point=46, padding=(1, 1), groups=144)\n            (2): ReLU6(inplace=True)\n          )\n          (2): QuantizedConv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), scale=0.07845275849103928, zero_point=59)\n        )\n      )\n      (4): Module(\n        (conv): Module(\n          (0): Module(\n            (0): QuantizedConv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), scale=0.04127482697367668, zero_point=54)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Module(\n            (0): QuantizedConv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), scale=0.047041166573762894, zero_point=51, padding=(1, 1), groups=144)\n            (2): ReLU6(inplace=True)\n          )\n          (2): QuantizedConv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), scale=0.06702015548944473, zero_point=67)\n        )\n      )\n      (5): Module(\n        (conv): Module(\n          (0): Module(\n            (0): QuantizedConv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), scale=0.01669529639184475, zero_point=46)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Module(\n            (0): QuantizedConv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), scale=0.029767487198114395, zero_point=50, padding=(1, 1), groups=192)\n            (2): ReLU6(inplace=True)\n          )\n          (2): QuantizedConv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), scale=0.05265645310282707, zero_point=65)\n        )\n      )\n      (6): Module(\n        (conv): Module(\n          (0): Module(\n            (0): QuantizedConv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), scale=0.01679590716958046, zero_point=39)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Module(\n            (0): QuantizedConv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), scale=0.029405290260910988, zero_point=45, padding=(1, 1), groups=192)\n            (2): ReLU6(inplace=True)\n          )\n          (2): QuantizedConv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), scale=0.04783063009381294, zero_point=63)\n        )\n      )\n      (7): Module(\n        (conv): Module(\n          (0): Module(\n            (0): QuantizedConv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), scale=0.025924166664481163, zero_point=41)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Module(\n            (0): QuantizedConv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), scale=0.03054320439696312, zero_point=37, padding=(1, 1), groups=192)\n            (2): ReLU6(inplace=True)\n          )\n          (2): QuantizedConv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), scale=0.05583065748214722, zero_point=70)\n        )\n      )\n      (8): Module(\n        (conv): Module(\n          (0): Module(\n            (0): QuantizedConv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), scale=0.012404661625623703, zero_point=43)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Module(\n            (0): QuantizedConv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), scale=0.028026754036545753, zero_point=41, padding=(1, 1), groups=384)\n            (2): ReLU6(inplace=True)\n          )\n          (2): QuantizedConv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), scale=0.042028021067380905, zero_point=66)\n        )\n      )\n      (9): Module(\n        (conv): Module(\n          (0): Module(\n            (0): QuantizedConv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), scale=0.01156578864902258, zero_point=46)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Module(\n            (0): QuantizedConv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), scale=0.022437309846282005, zero_point=49, padding=(1, 1), groups=384)\n            (2): ReLU6(inplace=True)\n          )\n          (2): QuantizedConv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), scale=0.03026079759001732, zero_point=69)\n        )\n      )\n      (10): Module(\n        (conv): Module(\n          (0): Module(\n            (0): QuantizedConv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), scale=0.01430924329906702, zero_point=38)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Module(\n            (0): QuantizedConv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), scale=0.057917945086956024, zero_point=15, padding=(1, 1), groups=384)\n            (2): ReLU6(inplace=True)\n          )\n          (2): QuantizedConv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), scale=0.07252535969018936, zero_point=76)\n        )\n      )\n      (11): Module(\n        (conv): Module(\n          (0): Module(\n            (0): QuantizedConv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), scale=0.02071334607899189, zero_point=44)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Module(\n            (0): QuantizedConv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), scale=0.046250853687524796, zero_point=32, padding=(1, 1), groups=384)\n            (2): ReLU6(inplace=True)\n          )\n          (2): QuantizedConv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), scale=0.04774351045489311, zero_point=64)\n        )\n      )\n      (12): Module(\n        (conv): Module(\n          (0): Module(\n            (0): QuantizedConv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), scale=0.021085739135742188, zero_point=37)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Module(\n            (0): QuantizedConv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), scale=0.0978718101978302, zero_point=53, padding=(1, 1), groups=576)\n            (2): ReLU6(inplace=True)\n          )\n          (2): QuantizedConv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), scale=0.07097937911748886, zero_point=56)\n        )\n      )\n      (13): Module(\n        (conv): Module(\n          (0): Module(\n            (0): QuantizedConv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), scale=0.054083362221717834, zero_point=25)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Module(\n            (0): QuantizedConv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), scale=0.06779821962118149, zero_point=32, padding=(1, 1), groups=576)\n            (2): ReLU6(inplace=True)\n          )\n          (2): QuantizedConv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), scale=0.1249077096581459, zero_point=71)\n        )\n      )\n      (14): Module(\n        (conv): Module(\n          (0): Module(\n            (0): QuantizedConv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), scale=0.039069876074790955, zero_point=23)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Module(\n            (0): QuantizedConv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), scale=0.06879983842372894, zero_point=29, padding=(1, 1), groups=576)\n            (2): ReLU6(inplace=True)\n          )\n          (2): QuantizedConv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), scale=0.14663422107696533, zero_point=74)\n        )\n      )\n      (15): Module(\n        (conv): Module(\n          (0): Module(\n            (0): QuantizedConv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), scale=0.0585968978703022, zero_point=36)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Module(\n            (0): QuantizedConv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), scale=0.07800718396902084, zero_point=47, padding=(1, 1), groups=960)\n            (2): ReLU6(inplace=True)\n          )\n          (2): QuantizedConv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), scale=0.13171353936195374, zero_point=53)\n        )\n      )\n      (16): Module(\n        (conv): Module(\n          (0): Module(\n            (0): QuantizedConv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), scale=0.05436522513628006, zero_point=37)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Module(\n            (0): QuantizedConv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), scale=0.08823246508836746, zero_point=34, padding=(1, 1), groups=960)\n            (2): ReLU6(inplace=True)\n          )\n          (2): QuantizedConv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), scale=0.2923617959022522, zero_point=66)\n        )\n      )\n      (17): Module(\n        (conv): Module(\n          (0): Module(\n            (0): QuantizedConv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), scale=0.06726245582103729, zero_point=72)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Module(\n            (0): QuantizedConv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), scale=0.028472810983657837, zero_point=67, padding=(1, 1), groups=960)\n            (2): ReLU6(inplace=True)\n          )\n          (2): QuantizedConv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), scale=0.0229371078312397, zero_point=64)\n        )\n      )\n      (18): Module(\n        (0): QuantizedConv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), scale=0.06654515117406845, zero_point=35)\n        (2): ReLU6(inplace=True)\n      )\n    )\n    (classifier): Module(\n      (0): QuantizedLinearReLU(in_features=1280, out_features=1024, scale=0.16139434278011322, zero_point=0, qscheme=torch.per_channel_affine)\n      (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (3): QuantizedLinearReLU(in_features=1024, out_features=512, scale=0.11617857962846756, zero_point=0, qscheme=torch.per_channel_affine)\n      (5): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (6): QuantizedLinear(in_features=512, out_features=10, scale=0.1648179292678833, zero_point=52, qscheme=torch.per_channel_affine)\n      (7): LogSoftmax(dim=1)\n    )\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"all_predictions_int8 = []\nall_labels_int8 = []\ncorrect_pred = 0\ntotal_pred = 0\nstart_time_int8 = time()\nwith torch.no_grad():\n    model_quantized_trained.eval()\n    for data in testloader:\n        images, labels = data\n        all_labels_int8.extend(labels.numpy())\n        #images, labels = images.to(device), labels.to(device)\n        outputs = model_quantized_trained(images.to('cpu'))\n        _, predicted = torch.max(outputs.data, 1)\n        total_pred += labels.size(0)\n        correct_pred += (predicted == labels).sum().item()\n        predicted_tensor_cpu = predicted.to('cpu')\n        all_predictions_int8.extend(predicted_tensor_cpu.numpy())\nend_time_int8 = time()\nprint(\"Time: \",end_time_int8 - start_time_int8)\nprint('Accuracy achieved by the network on test images is: %d%%' % (100 * correct_pred / total_pred))","metadata":{"execution":{"iopub.status.busy":"2024-04-07T12:31:56.037336Z","iopub.execute_input":"2024-04-07T12:31:56.037577Z","iopub.status.idle":"2024-04-07T12:33:03.867232Z","shell.execute_reply.started":"2024-04-07T12:31:56.037556Z","shell.execute_reply":"2024-04-07T12:33:03.866274Z"},"trusted":true},"execution_count":46,"outputs":[{"name":"stdout","text":"Time:  67.82008290290833\nAccuracy achieved by the network on test images is: 82%\n","output_type":"stream"}]},{"cell_type":"code","source":"metrics(all_predictions_int8,all_labels_int8)","metadata":{"execution":{"iopub.status.busy":"2024-04-07T12:33:03.868487Z","iopub.execute_input":"2024-04-07T12:33:03.869140Z","iopub.status.idle":"2024-04-07T12:33:04.714598Z","shell.execute_reply.started":"2024-04-07T12:33:03.869105Z","shell.execute_reply":"2024-04-07T12:33:04.713525Z"},"trusted":true},"execution_count":47,"outputs":[{"name":"stdout","text":"Confusion Matrix:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 800x600 with 2 Axes>","image/png":"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"},"metadata":{"image/png":{"width":793,"height":689}}},{"name":"stdout","text":"----------------------------------------------------------------\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.90      0.79      0.84       726\n           1       0.82      0.95      0.88       491\n           2       0.91      0.70      0.79       779\n           3       0.57      0.83      0.67       338\n           4       0.90      0.89      0.90       498\n           5       0.73      0.86      0.79       340\n           6       0.62      0.86      0.72       370\n           7       0.96      0.90      0.93       664\n           8       0.74      0.71      0.73       510\n           9       0.97      0.83      0.89       684\n\n    accuracy                           0.82      5400\n   macro avg       0.81      0.83      0.81      5400\nweighted avg       0.85      0.82      0.83      5400\n\n","output_type":"stream"}]},{"cell_type":"code","source":"print(\"Size of fp16 model:\",end='')\nprint_model_size(model_fp16)\nprint(\"Size of PTQ model:\",end='')\nprint_model_size(model_quantized_static)\nprint(\"Size of QAT model:\",end='')\nprint_model_size(model_quantized_trained)","metadata":{"execution":{"iopub.status.busy":"2024-04-07T12:33:04.715885Z","iopub.execute_input":"2024-04-07T12:33:04.716290Z","iopub.status.idle":"2024-04-07T12:33:04.880387Z","shell.execute_reply.started":"2024-04-07T12:33:04.716254Z","shell.execute_reply":"2024-04-07T12:33:04.879301Z"},"trusted":true},"execution_count":48,"outputs":[{"name":"stdout","text":"Size of fp16 model:8.31 MB\nSize of PTQ model:4.53 MB\nSize of QAT model:4.53 MB\n","output_type":"stream"}]}]}