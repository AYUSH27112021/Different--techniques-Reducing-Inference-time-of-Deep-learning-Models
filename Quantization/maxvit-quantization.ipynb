{"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1663377,"sourceType":"datasetVersion","datasetId":918039},{"sourceId":2546969,"sourceType":"datasetVersion","datasetId":1544742},{"sourceId":6810129,"sourceType":"datasetVersion","datasetId":3917752}],"dockerImageVersionId":30683,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Imports here\nimport torch\nimport PIL\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nfrom torchvision import datasets, transforms, models\nfrom torch import nn\nfrom torch import optim\nfrom collections import OrderedDict\nfrom time import time\nimport copy\nfrom sklearn.metrics import confusion_matrix,classification_report\nimport numpy as np\nimport skimage.io as io\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2024-04-09T17:31:56.113326Z","iopub.execute_input":"2024-04-09T17:31:56.114331Z","iopub.status.idle":"2024-04-09T17:32:00.569062Z","shell.execute_reply.started":"2024-04-09T17:31:56.114285Z","shell.execute_reply":"2024-04-09T17:32:00.567965Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"data_dir = '/kaggle/input/eurosat-dataset'\ntrain_dir = data_dir + '/EuroSAT'\n\n# Define your transformations\ntrain_transforms = transforms.Compose([\n    #transforms.RandomRotation(30),\n    transforms.RandomResizedCrop(224),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\n# Load all the images from the train folder\nall_data = datasets.ImageFolder(train_dir, transform=train_transforms)\n\n# Calculate the sizes for train, validation, and test sets\ntotal_size = len(all_data)\ntrain_size = int(0.7 * total_size)\ntest_size = int(0.2 * total_size)\nvalid_size = total_size - train_size - test_size\n\n# Use random_split to split the dataset\ntrain_data, valid_data, test_data = torch.utils.data.random_split(all_data, [train_size, valid_size, test_size])\n\n# Create data loaders\ntrainloader = torch.utils.data.DataLoader(train_data, batch_size=50, shuffle=False)\nvalidloader = torch.utils.data.DataLoader(valid_data, batch_size=50)\ntestloader = torch.utils.data.DataLoader(test_data, batch_size=50)","metadata":{"execution":{"iopub.status.busy":"2024-04-09T17:32:00.570947Z","iopub.execute_input":"2024-04-09T17:32:00.571380Z","iopub.status.idle":"2024-04-09T17:32:15.530488Z","shell.execute_reply.started":"2024-04-09T17:32:00.571355Z","shell.execute_reply":"2024-04-09T17:32:15.529316Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Load a pre-trained network \nmodel = models.maxvit_t(weights='DEFAULT')\nmodel.name = \"maxvit\"\nmodel","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-04-09T17:32:15.531870Z","iopub.execute_input":"2024-04-09T17:32:15.532182Z","iopub.status.idle":"2024-04-09T17:32:16.406750Z","shell.execute_reply.started":"2024-04-09T17:32:15.532157Z","shell.execute_reply":"2024-04-09T17:32:16.405657Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/native/TensorShape.cpp:3526.)\n  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"MaxVit(\n  (stem): Sequential(\n    (0): Conv2dNormActivation(\n      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (1): BatchNorm2d(64, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n      (2): GELU(approximate='none')\n    )\n    (1): Conv2dNormActivation(\n      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    )\n  )\n  (blocks): ModuleList(\n    (0): MaxVitBlock(\n      (layers): ModuleList(\n        (0): MaxVitLayer(\n          (layers): Sequential(\n            (MBconv): MBConv(\n              (proj): Sequential(\n                (0): AvgPool2d(kernel_size=3, stride=2, padding=1)\n                (1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n              )\n              (stochastic_depth): Identity()\n              (layers): Sequential(\n                (pre_norm): BatchNorm2d(64, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n                (conv_a): Conv2dNormActivation(\n                  (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                  (1): BatchNorm2d(256, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n                  (2): GELU(approximate='none')\n                )\n                (conv_b): Conv2dNormActivation(\n                  (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)\n                  (1): BatchNorm2d(256, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n                  (2): GELU(approximate='none')\n                )\n                (squeeze_excitation): SqueezeExcitation(\n                  (avgpool): AdaptiveAvgPool2d(output_size=1)\n                  (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1))\n                  (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n                  (activation): SiLU()\n                  (scale_activation): Sigmoid()\n                )\n                (conv_c): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n              )\n            )\n            (window_attention): PartitionAttentionLayer(\n              (partition_op): WindowPartition()\n              (departition_op): WindowDepartition()\n              (partition_swap): Identity()\n              (departition_swap): Identity()\n              (attn_layer): Sequential(\n                (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n                (1): RelativePositionalMultiHeadAttention(\n                  (to_qkv): Linear(in_features=64, out_features=192, bias=True)\n                  (merge): Linear(in_features=64, out_features=64, bias=True)\n                )\n                (2): Dropout(p=0.0, inplace=False)\n              )\n              (mlp_layer): Sequential(\n                (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n                (1): Linear(in_features=64, out_features=256, bias=True)\n                (2): GELU(approximate='none')\n                (3): Linear(in_features=256, out_features=64, bias=True)\n                (4): Dropout(p=0.0, inplace=False)\n              )\n              (stochastic_dropout): StochasticDepth(p=0.0, mode=row)\n            )\n            (grid_attention): PartitionAttentionLayer(\n              (partition_op): WindowPartition()\n              (departition_op): WindowDepartition()\n              (partition_swap): SwapAxes()\n              (departition_swap): SwapAxes()\n              (attn_layer): Sequential(\n                (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n                (1): RelativePositionalMultiHeadAttention(\n                  (to_qkv): Linear(in_features=64, out_features=192, bias=True)\n                  (merge): Linear(in_features=64, out_features=64, bias=True)\n                )\n                (2): Dropout(p=0.0, inplace=False)\n              )\n              (mlp_layer): Sequential(\n                (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n                (1): Linear(in_features=64, out_features=256, bias=True)\n                (2): GELU(approximate='none')\n                (3): Linear(in_features=256, out_features=64, bias=True)\n                (4): Dropout(p=0.0, inplace=False)\n              )\n              (stochastic_dropout): StochasticDepth(p=0.0, mode=row)\n            )\n          )\n        )\n        (1): MaxVitLayer(\n          (layers): Sequential(\n            (MBconv): MBConv(\n              (proj): Identity()\n              (stochastic_depth): StochasticDepth(p=0.02, mode=row)\n              (layers): Sequential(\n                (pre_norm): BatchNorm2d(64, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n                (conv_a): Conv2dNormActivation(\n                  (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                  (1): BatchNorm2d(256, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n                  (2): GELU(approximate='none')\n                )\n                (conv_b): Conv2dNormActivation(\n                  (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n                  (1): BatchNorm2d(256, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n                  (2): GELU(approximate='none')\n                )\n                (squeeze_excitation): SqueezeExcitation(\n                  (avgpool): AdaptiveAvgPool2d(output_size=1)\n                  (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1))\n                  (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n                  (activation): SiLU()\n                  (scale_activation): Sigmoid()\n                )\n                (conv_c): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n              )\n            )\n            (window_attention): PartitionAttentionLayer(\n              (partition_op): WindowPartition()\n              (departition_op): WindowDepartition()\n              (partition_swap): Identity()\n              (departition_swap): Identity()\n              (attn_layer): Sequential(\n                (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n                (1): RelativePositionalMultiHeadAttention(\n                  (to_qkv): Linear(in_features=64, out_features=192, bias=True)\n                  (merge): Linear(in_features=64, out_features=64, bias=True)\n                )\n                (2): Dropout(p=0.0, inplace=False)\n              )\n              (mlp_layer): Sequential(\n                (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n                (1): Linear(in_features=64, out_features=256, bias=True)\n                (2): GELU(approximate='none')\n                (3): Linear(in_features=256, out_features=64, bias=True)\n                (4): Dropout(p=0.0, inplace=False)\n              )\n              (stochastic_dropout): StochasticDepth(p=0.02, mode=row)\n            )\n            (grid_attention): PartitionAttentionLayer(\n              (partition_op): WindowPartition()\n              (departition_op): WindowDepartition()\n              (partition_swap): SwapAxes()\n              (departition_swap): SwapAxes()\n              (attn_layer): Sequential(\n                (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n                (1): RelativePositionalMultiHeadAttention(\n                  (to_qkv): Linear(in_features=64, out_features=192, bias=True)\n                  (merge): Linear(in_features=64, out_features=64, bias=True)\n                )\n                (2): Dropout(p=0.0, inplace=False)\n              )\n              (mlp_layer): Sequential(\n                (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n                (1): Linear(in_features=64, out_features=256, bias=True)\n                (2): GELU(approximate='none')\n                (3): Linear(in_features=256, out_features=64, bias=True)\n                (4): Dropout(p=0.0, inplace=False)\n              )\n              (stochastic_dropout): StochasticDepth(p=0.02, mode=row)\n            )\n          )\n        )\n      )\n    )\n    (1): MaxVitBlock(\n      (layers): ModuleList(\n        (0): MaxVitLayer(\n          (layers): Sequential(\n            (MBconv): MBConv(\n              (proj): Sequential(\n                (0): AvgPool2d(kernel_size=3, stride=2, padding=1)\n                (1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n              )\n              (stochastic_depth): StochasticDepth(p=0.04, mode=row)\n              (layers): Sequential(\n                (pre_norm): BatchNorm2d(64, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n                (conv_a): Conv2dNormActivation(\n                  (0): Conv2d(64, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                  (1): BatchNorm2d(512, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n                  (2): GELU(approximate='none')\n                )\n                (conv_b): Conv2dNormActivation(\n                  (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512, bias=False)\n                  (1): BatchNorm2d(512, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n                  (2): GELU(approximate='none')\n                )\n                (squeeze_excitation): SqueezeExcitation(\n                  (avgpool): AdaptiveAvgPool2d(output_size=1)\n                  (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n                  (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n                  (activation): SiLU()\n                  (scale_activation): Sigmoid()\n                )\n                (conv_c): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n              )\n            )\n            (window_attention): PartitionAttentionLayer(\n              (partition_op): WindowPartition()\n              (departition_op): WindowDepartition()\n              (partition_swap): Identity()\n              (departition_swap): Identity()\n              (attn_layer): Sequential(\n                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n                (1): RelativePositionalMultiHeadAttention(\n                  (to_qkv): Linear(in_features=128, out_features=384, bias=True)\n                  (merge): Linear(in_features=128, out_features=128, bias=True)\n                )\n                (2): Dropout(p=0.0, inplace=False)\n              )\n              (mlp_layer): Sequential(\n                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n                (1): Linear(in_features=128, out_features=512, bias=True)\n                (2): GELU(approximate='none')\n                (3): Linear(in_features=512, out_features=128, bias=True)\n                (4): Dropout(p=0.0, inplace=False)\n              )\n              (stochastic_dropout): StochasticDepth(p=0.04, mode=row)\n            )\n            (grid_attention): PartitionAttentionLayer(\n              (partition_op): WindowPartition()\n              (departition_op): WindowDepartition()\n              (partition_swap): SwapAxes()\n              (departition_swap): SwapAxes()\n              (attn_layer): Sequential(\n                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n                (1): RelativePositionalMultiHeadAttention(\n                  (to_qkv): Linear(in_features=128, out_features=384, bias=True)\n                  (merge): Linear(in_features=128, out_features=128, bias=True)\n                )\n                (2): Dropout(p=0.0, inplace=False)\n              )\n              (mlp_layer): Sequential(\n                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n                (1): Linear(in_features=128, out_features=512, bias=True)\n                (2): GELU(approximate='none')\n                (3): Linear(in_features=512, out_features=128, bias=True)\n                (4): Dropout(p=0.0, inplace=False)\n              )\n              (stochastic_dropout): StochasticDepth(p=0.04, mode=row)\n            )\n          )\n        )\n        (1): MaxVitLayer(\n          (layers): Sequential(\n            (MBconv): MBConv(\n              (proj): Identity()\n              (stochastic_depth): StochasticDepth(p=0.06, mode=row)\n              (layers): Sequential(\n                (pre_norm): BatchNorm2d(128, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n                (conv_a): Conv2dNormActivation(\n                  (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                  (1): BatchNorm2d(512, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n                  (2): GELU(approximate='none')\n                )\n                (conv_b): Conv2dNormActivation(\n                  (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n                  (1): BatchNorm2d(512, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n                  (2): GELU(approximate='none')\n                )\n                (squeeze_excitation): SqueezeExcitation(\n                  (avgpool): AdaptiveAvgPool2d(output_size=1)\n                  (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n                  (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n                  (activation): SiLU()\n                  (scale_activation): Sigmoid()\n                )\n                (conv_c): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n              )\n            )\n            (window_attention): PartitionAttentionLayer(\n              (partition_op): WindowPartition()\n              (departition_op): WindowDepartition()\n              (partition_swap): Identity()\n              (departition_swap): Identity()\n              (attn_layer): Sequential(\n                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n                (1): RelativePositionalMultiHeadAttention(\n                  (to_qkv): Linear(in_features=128, out_features=384, bias=True)\n                  (merge): Linear(in_features=128, out_features=128, bias=True)\n                )\n                (2): Dropout(p=0.0, inplace=False)\n              )\n              (mlp_layer): Sequential(\n                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n                (1): Linear(in_features=128, out_features=512, bias=True)\n                (2): GELU(approximate='none')\n                (3): Linear(in_features=512, out_features=128, bias=True)\n                (4): Dropout(p=0.0, inplace=False)\n              )\n              (stochastic_dropout): StochasticDepth(p=0.06, mode=row)\n            )\n            (grid_attention): PartitionAttentionLayer(\n              (partition_op): WindowPartition()\n              (departition_op): WindowDepartition()\n              (partition_swap): SwapAxes()\n              (departition_swap): SwapAxes()\n              (attn_layer): Sequential(\n                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n                (1): RelativePositionalMultiHeadAttention(\n                  (to_qkv): Linear(in_features=128, out_features=384, bias=True)\n                  (merge): Linear(in_features=128, out_features=128, bias=True)\n                )\n                (2): Dropout(p=0.0, inplace=False)\n              )\n              (mlp_layer): Sequential(\n                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n                (1): Linear(in_features=128, out_features=512, bias=True)\n                (2): GELU(approximate='none')\n                (3): Linear(in_features=512, out_features=128, bias=True)\n                (4): Dropout(p=0.0, inplace=False)\n              )\n              (stochastic_dropout): StochasticDepth(p=0.06, mode=row)\n            )\n          )\n        )\n      )\n    )\n    (2): MaxVitBlock(\n      (layers): ModuleList(\n        (0): MaxVitLayer(\n          (layers): Sequential(\n            (MBconv): MBConv(\n              (proj): Sequential(\n                (0): AvgPool2d(kernel_size=3, stride=2, padding=1)\n                (1): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n              )\n              (stochastic_depth): StochasticDepth(p=0.08, mode=row)\n              (layers): Sequential(\n                (pre_norm): BatchNorm2d(128, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n                (conv_a): Conv2dNormActivation(\n                  (0): Conv2d(128, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                  (1): BatchNorm2d(1024, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n                  (2): GELU(approximate='none')\n                )\n                (conv_b): Conv2dNormActivation(\n                  (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=1024, bias=False)\n                  (1): BatchNorm2d(1024, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n                  (2): GELU(approximate='none')\n                )\n                (squeeze_excitation): SqueezeExcitation(\n                  (avgpool): AdaptiveAvgPool2d(output_size=1)\n                  (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n                  (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n                  (activation): SiLU()\n                  (scale_activation): Sigmoid()\n                )\n                (conv_c): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n              )\n            )\n            (window_attention): PartitionAttentionLayer(\n              (partition_op): WindowPartition()\n              (departition_op): WindowDepartition()\n              (partition_swap): Identity()\n              (departition_swap): Identity()\n              (attn_layer): Sequential(\n                (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n                (1): RelativePositionalMultiHeadAttention(\n                  (to_qkv): Linear(in_features=256, out_features=768, bias=True)\n                  (merge): Linear(in_features=256, out_features=256, bias=True)\n                )\n                (2): Dropout(p=0.0, inplace=False)\n              )\n              (mlp_layer): Sequential(\n                (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n                (1): Linear(in_features=256, out_features=1024, bias=True)\n                (2): GELU(approximate='none')\n                (3): Linear(in_features=1024, out_features=256, bias=True)\n                (4): Dropout(p=0.0, inplace=False)\n              )\n              (stochastic_dropout): StochasticDepth(p=0.08, mode=row)\n            )\n            (grid_attention): PartitionAttentionLayer(\n              (partition_op): WindowPartition()\n              (departition_op): WindowDepartition()\n              (partition_swap): SwapAxes()\n              (departition_swap): SwapAxes()\n              (attn_layer): Sequential(\n                (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n                (1): RelativePositionalMultiHeadAttention(\n                  (to_qkv): Linear(in_features=256, out_features=768, bias=True)\n                  (merge): Linear(in_features=256, out_features=256, bias=True)\n                )\n                (2): Dropout(p=0.0, inplace=False)\n              )\n              (mlp_layer): Sequential(\n                (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n                (1): Linear(in_features=256, out_features=1024, bias=True)\n                (2): GELU(approximate='none')\n                (3): Linear(in_features=1024, out_features=256, bias=True)\n                (4): Dropout(p=0.0, inplace=False)\n              )\n              (stochastic_dropout): StochasticDepth(p=0.08, mode=row)\n            )\n          )\n        )\n        (1): MaxVitLayer(\n          (layers): Sequential(\n            (MBconv): MBConv(\n              (proj): Identity()\n              (stochastic_depth): StochasticDepth(p=0.1, mode=row)\n              (layers): Sequential(\n                (pre_norm): BatchNorm2d(256, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n                (conv_a): Conv2dNormActivation(\n                  (0): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                  (1): BatchNorm2d(1024, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n                  (2): GELU(approximate='none')\n                )\n                (conv_b): Conv2dNormActivation(\n                  (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n                  (1): BatchNorm2d(1024, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n                  (2): GELU(approximate='none')\n                )\n                (squeeze_excitation): SqueezeExcitation(\n                  (avgpool): AdaptiveAvgPool2d(output_size=1)\n                  (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n                  (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n                  (activation): SiLU()\n                  (scale_activation): Sigmoid()\n                )\n                (conv_c): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n              )\n            )\n            (window_attention): PartitionAttentionLayer(\n              (partition_op): WindowPartition()\n              (departition_op): WindowDepartition()\n              (partition_swap): Identity()\n              (departition_swap): Identity()\n              (attn_layer): Sequential(\n                (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n                (1): RelativePositionalMultiHeadAttention(\n                  (to_qkv): Linear(in_features=256, out_features=768, bias=True)\n                  (merge): Linear(in_features=256, out_features=256, bias=True)\n                )\n                (2): Dropout(p=0.0, inplace=False)\n              )\n              (mlp_layer): Sequential(\n                (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n                (1): Linear(in_features=256, out_features=1024, bias=True)\n                (2): GELU(approximate='none')\n                (3): Linear(in_features=1024, out_features=256, bias=True)\n                (4): Dropout(p=0.0, inplace=False)\n              )\n              (stochastic_dropout): StochasticDepth(p=0.1, mode=row)\n            )\n            (grid_attention): PartitionAttentionLayer(\n              (partition_op): WindowPartition()\n              (departition_op): WindowDepartition()\n              (partition_swap): SwapAxes()\n              (departition_swap): SwapAxes()\n              (attn_layer): Sequential(\n                (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n                (1): RelativePositionalMultiHeadAttention(\n                  (to_qkv): Linear(in_features=256, out_features=768, bias=True)\n                  (merge): Linear(in_features=256, out_features=256, bias=True)\n                )\n                (2): Dropout(p=0.0, inplace=False)\n              )\n              (mlp_layer): Sequential(\n                (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n                (1): Linear(in_features=256, out_features=1024, bias=True)\n                (2): GELU(approximate='none')\n                (3): Linear(in_features=1024, out_features=256, bias=True)\n                (4): Dropout(p=0.0, inplace=False)\n              )\n              (stochastic_dropout): StochasticDepth(p=0.1, mode=row)\n            )\n          )\n        )\n        (2): MaxVitLayer(\n          (layers): Sequential(\n            (MBconv): MBConv(\n              (proj): Identity()\n              (stochastic_depth): StochasticDepth(p=0.12, mode=row)\n              (layers): Sequential(\n                (pre_norm): BatchNorm2d(256, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n                (conv_a): Conv2dNormActivation(\n                  (0): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                  (1): BatchNorm2d(1024, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n                  (2): GELU(approximate='none')\n                )\n                (conv_b): Conv2dNormActivation(\n                  (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n                  (1): BatchNorm2d(1024, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n                  (2): GELU(approximate='none')\n                )\n                (squeeze_excitation): SqueezeExcitation(\n                  (avgpool): AdaptiveAvgPool2d(output_size=1)\n                  (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n                  (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n                  (activation): SiLU()\n                  (scale_activation): Sigmoid()\n                )\n                (conv_c): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n              )\n            )\n            (window_attention): PartitionAttentionLayer(\n              (partition_op): WindowPartition()\n              (departition_op): WindowDepartition()\n              (partition_swap): Identity()\n              (departition_swap): Identity()\n              (attn_layer): Sequential(\n                (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n                (1): RelativePositionalMultiHeadAttention(\n                  (to_qkv): Linear(in_features=256, out_features=768, bias=True)\n                  (merge): Linear(in_features=256, out_features=256, bias=True)\n                )\n                (2): Dropout(p=0.0, inplace=False)\n              )\n              (mlp_layer): Sequential(\n                (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n                (1): Linear(in_features=256, out_features=1024, bias=True)\n                (2): GELU(approximate='none')\n                (3): Linear(in_features=1024, out_features=256, bias=True)\n                (4): Dropout(p=0.0, inplace=False)\n              )\n              (stochastic_dropout): StochasticDepth(p=0.12, mode=row)\n            )\n            (grid_attention): PartitionAttentionLayer(\n              (partition_op): WindowPartition()\n              (departition_op): WindowDepartition()\n              (partition_swap): SwapAxes()\n              (departition_swap): SwapAxes()\n              (attn_layer): Sequential(\n                (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n                (1): RelativePositionalMultiHeadAttention(\n                  (to_qkv): Linear(in_features=256, out_features=768, bias=True)\n                  (merge): Linear(in_features=256, out_features=256, bias=True)\n                )\n                (2): Dropout(p=0.0, inplace=False)\n              )\n              (mlp_layer): Sequential(\n                (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n                (1): Linear(in_features=256, out_features=1024, bias=True)\n                (2): GELU(approximate='none')\n                (3): Linear(in_features=1024, out_features=256, bias=True)\n                (4): Dropout(p=0.0, inplace=False)\n              )\n              (stochastic_dropout): StochasticDepth(p=0.12, mode=row)\n            )\n          )\n        )\n        (3): MaxVitLayer(\n          (layers): Sequential(\n            (MBconv): MBConv(\n              (proj): Identity()\n              (stochastic_depth): StochasticDepth(p=0.14, mode=row)\n              (layers): Sequential(\n                (pre_norm): BatchNorm2d(256, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n                (conv_a): Conv2dNormActivation(\n                  (0): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                  (1): BatchNorm2d(1024, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n                  (2): GELU(approximate='none')\n                )\n                (conv_b): Conv2dNormActivation(\n                  (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n                  (1): BatchNorm2d(1024, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n                  (2): GELU(approximate='none')\n                )\n                (squeeze_excitation): SqueezeExcitation(\n                  (avgpool): AdaptiveAvgPool2d(output_size=1)\n                  (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n                  (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n                  (activation): SiLU()\n                  (scale_activation): Sigmoid()\n                )\n                (conv_c): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n              )\n            )\n            (window_attention): PartitionAttentionLayer(\n              (partition_op): WindowPartition()\n              (departition_op): WindowDepartition()\n              (partition_swap): Identity()\n              (departition_swap): Identity()\n              (attn_layer): Sequential(\n                (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n                (1): RelativePositionalMultiHeadAttention(\n                  (to_qkv): Linear(in_features=256, out_features=768, bias=True)\n                  (merge): Linear(in_features=256, out_features=256, bias=True)\n                )\n                (2): Dropout(p=0.0, inplace=False)\n              )\n              (mlp_layer): Sequential(\n                (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n                (1): Linear(in_features=256, out_features=1024, bias=True)\n                (2): GELU(approximate='none')\n                (3): Linear(in_features=1024, out_features=256, bias=True)\n                (4): Dropout(p=0.0, inplace=False)\n              )\n              (stochastic_dropout): StochasticDepth(p=0.14, mode=row)\n            )\n            (grid_attention): PartitionAttentionLayer(\n              (partition_op): WindowPartition()\n              (departition_op): WindowDepartition()\n              (partition_swap): SwapAxes()\n              (departition_swap): SwapAxes()\n              (attn_layer): Sequential(\n                (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n                (1): RelativePositionalMultiHeadAttention(\n                  (to_qkv): Linear(in_features=256, out_features=768, bias=True)\n                  (merge): Linear(in_features=256, out_features=256, bias=True)\n                )\n                (2): Dropout(p=0.0, inplace=False)\n              )\n              (mlp_layer): Sequential(\n                (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n                (1): Linear(in_features=256, out_features=1024, bias=True)\n                (2): GELU(approximate='none')\n                (3): Linear(in_features=1024, out_features=256, bias=True)\n                (4): Dropout(p=0.0, inplace=False)\n              )\n              (stochastic_dropout): StochasticDepth(p=0.14, mode=row)\n            )\n          )\n        )\n        (4): MaxVitLayer(\n          (layers): Sequential(\n            (MBconv): MBConv(\n              (proj): Identity()\n              (stochastic_depth): StochasticDepth(p=0.16, mode=row)\n              (layers): Sequential(\n                (pre_norm): BatchNorm2d(256, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n                (conv_a): Conv2dNormActivation(\n                  (0): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                  (1): BatchNorm2d(1024, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n                  (2): GELU(approximate='none')\n                )\n                (conv_b): Conv2dNormActivation(\n                  (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n                  (1): BatchNorm2d(1024, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n                  (2): GELU(approximate='none')\n                )\n                (squeeze_excitation): SqueezeExcitation(\n                  (avgpool): AdaptiveAvgPool2d(output_size=1)\n                  (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n                  (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n                  (activation): SiLU()\n                  (scale_activation): Sigmoid()\n                )\n                (conv_c): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n              )\n            )\n            (window_attention): PartitionAttentionLayer(\n              (partition_op): WindowPartition()\n              (departition_op): WindowDepartition()\n              (partition_swap): Identity()\n              (departition_swap): Identity()\n              (attn_layer): Sequential(\n                (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n                (1): RelativePositionalMultiHeadAttention(\n                  (to_qkv): Linear(in_features=256, out_features=768, bias=True)\n                  (merge): Linear(in_features=256, out_features=256, bias=True)\n                )\n                (2): Dropout(p=0.0, inplace=False)\n              )\n              (mlp_layer): Sequential(\n                (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n                (1): Linear(in_features=256, out_features=1024, bias=True)\n                (2): GELU(approximate='none')\n                (3): Linear(in_features=1024, out_features=256, bias=True)\n                (4): Dropout(p=0.0, inplace=False)\n              )\n              (stochastic_dropout): StochasticDepth(p=0.16, mode=row)\n            )\n            (grid_attention): PartitionAttentionLayer(\n              (partition_op): WindowPartition()\n              (departition_op): WindowDepartition()\n              (partition_swap): SwapAxes()\n              (departition_swap): SwapAxes()\n              (attn_layer): Sequential(\n                (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n                (1): RelativePositionalMultiHeadAttention(\n                  (to_qkv): Linear(in_features=256, out_features=768, bias=True)\n                  (merge): Linear(in_features=256, out_features=256, bias=True)\n                )\n                (2): Dropout(p=0.0, inplace=False)\n              )\n              (mlp_layer): Sequential(\n                (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n                (1): Linear(in_features=256, out_features=1024, bias=True)\n                (2): GELU(approximate='none')\n                (3): Linear(in_features=1024, out_features=256, bias=True)\n                (4): Dropout(p=0.0, inplace=False)\n              )\n              (stochastic_dropout): StochasticDepth(p=0.16, mode=row)\n            )\n          )\n        )\n      )\n    )\n    (3): MaxVitBlock(\n      (layers): ModuleList(\n        (0): MaxVitLayer(\n          (layers): Sequential(\n            (MBconv): MBConv(\n              (proj): Sequential(\n                (0): AvgPool2d(kernel_size=3, stride=2, padding=1)\n                (1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n              )\n              (stochastic_depth): StochasticDepth(p=0.18, mode=row)\n              (layers): Sequential(\n                (pre_norm): BatchNorm2d(256, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n                (conv_a): Conv2dNormActivation(\n                  (0): Conv2d(256, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                  (1): BatchNorm2d(2048, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n                  (2): GELU(approximate='none')\n                )\n                (conv_b): Conv2dNormActivation(\n                  (0): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=2048, bias=False)\n                  (1): BatchNorm2d(2048, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n                  (2): GELU(approximate='none')\n                )\n                (squeeze_excitation): SqueezeExcitation(\n                  (avgpool): AdaptiveAvgPool2d(output_size=1)\n                  (fc1): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1))\n                  (fc2): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1))\n                  (activation): SiLU()\n                  (scale_activation): Sigmoid()\n                )\n                (conv_c): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))\n              )\n            )\n            (window_attention): PartitionAttentionLayer(\n              (partition_op): WindowPartition()\n              (departition_op): WindowDepartition()\n              (partition_swap): Identity()\n              (departition_swap): Identity()\n              (attn_layer): Sequential(\n                (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n                (1): RelativePositionalMultiHeadAttention(\n                  (to_qkv): Linear(in_features=512, out_features=1536, bias=True)\n                  (merge): Linear(in_features=512, out_features=512, bias=True)\n                )\n                (2): Dropout(p=0.0, inplace=False)\n              )\n              (mlp_layer): Sequential(\n                (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n                (1): Linear(in_features=512, out_features=2048, bias=True)\n                (2): GELU(approximate='none')\n                (3): Linear(in_features=2048, out_features=512, bias=True)\n                (4): Dropout(p=0.0, inplace=False)\n              )\n              (stochastic_dropout): StochasticDepth(p=0.18, mode=row)\n            )\n            (grid_attention): PartitionAttentionLayer(\n              (partition_op): WindowPartition()\n              (departition_op): WindowDepartition()\n              (partition_swap): SwapAxes()\n              (departition_swap): SwapAxes()\n              (attn_layer): Sequential(\n                (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n                (1): RelativePositionalMultiHeadAttention(\n                  (to_qkv): Linear(in_features=512, out_features=1536, bias=True)\n                  (merge): Linear(in_features=512, out_features=512, bias=True)\n                )\n                (2): Dropout(p=0.0, inplace=False)\n              )\n              (mlp_layer): Sequential(\n                (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n                (1): Linear(in_features=512, out_features=2048, bias=True)\n                (2): GELU(approximate='none')\n                (3): Linear(in_features=2048, out_features=512, bias=True)\n                (4): Dropout(p=0.0, inplace=False)\n              )\n              (stochastic_dropout): StochasticDepth(p=0.18, mode=row)\n            )\n          )\n        )\n        (1): MaxVitLayer(\n          (layers): Sequential(\n            (MBconv): MBConv(\n              (proj): Identity()\n              (stochastic_depth): StochasticDepth(p=0.2, mode=row)\n              (layers): Sequential(\n                (pre_norm): BatchNorm2d(512, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n                (conv_a): Conv2dNormActivation(\n                  (0): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                  (1): BatchNorm2d(2048, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n                  (2): GELU(approximate='none')\n                )\n                (conv_b): Conv2dNormActivation(\n                  (0): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048, bias=False)\n                  (1): BatchNorm2d(2048, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n                  (2): GELU(approximate='none')\n                )\n                (squeeze_excitation): SqueezeExcitation(\n                  (avgpool): AdaptiveAvgPool2d(output_size=1)\n                  (fc1): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1))\n                  (fc2): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1))\n                  (activation): SiLU()\n                  (scale_activation): Sigmoid()\n                )\n                (conv_c): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))\n              )\n            )\n            (window_attention): PartitionAttentionLayer(\n              (partition_op): WindowPartition()\n              (departition_op): WindowDepartition()\n              (partition_swap): Identity()\n              (departition_swap): Identity()\n              (attn_layer): Sequential(\n                (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n                (1): RelativePositionalMultiHeadAttention(\n                  (to_qkv): Linear(in_features=512, out_features=1536, bias=True)\n                  (merge): Linear(in_features=512, out_features=512, bias=True)\n                )\n                (2): Dropout(p=0.0, inplace=False)\n              )\n              (mlp_layer): Sequential(\n                (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n                (1): Linear(in_features=512, out_features=2048, bias=True)\n                (2): GELU(approximate='none')\n                (3): Linear(in_features=2048, out_features=512, bias=True)\n                (4): Dropout(p=0.0, inplace=False)\n              )\n              (stochastic_dropout): StochasticDepth(p=0.2, mode=row)\n            )\n            (grid_attention): PartitionAttentionLayer(\n              (partition_op): WindowPartition()\n              (departition_op): WindowDepartition()\n              (partition_swap): SwapAxes()\n              (departition_swap): SwapAxes()\n              (attn_layer): Sequential(\n                (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n                (1): RelativePositionalMultiHeadAttention(\n                  (to_qkv): Linear(in_features=512, out_features=1536, bias=True)\n                  (merge): Linear(in_features=512, out_features=512, bias=True)\n                )\n                (2): Dropout(p=0.0, inplace=False)\n              )\n              (mlp_layer): Sequential(\n                (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n                (1): Linear(in_features=512, out_features=2048, bias=True)\n                (2): GELU(approximate='none')\n                (3): Linear(in_features=2048, out_features=512, bias=True)\n                (4): Dropout(p=0.0, inplace=False)\n              )\n              (stochastic_dropout): StochasticDepth(p=0.2, mode=row)\n            )\n          )\n        )\n      )\n    )\n  )\n  (classifier): Sequential(\n    (0): AdaptiveAvgPool2d(output_size=1)\n    (1): Flatten(start_dim=1, end_dim=-1)\n    (2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n    (3): Linear(in_features=512, out_features=512, bias=True)\n    (4): Tanh()\n    (5): Linear(in_features=512, out_features=1000, bias=False)\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"for param in model.parameters():\n    param.requires_grad = False","metadata":{"execution":{"iopub.status.busy":"2024-04-09T17:32:16.409401Z","iopub.execute_input":"2024-04-09T17:32:16.410182Z","iopub.status.idle":"2024-04-09T17:32:16.418783Z","shell.execute_reply.started":"2024-04-09T17:32:16.410145Z","shell.execute_reply":"2024-04-09T17:32:16.417891Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"num_ftrs = model.classifier[-1].in_features\nmodel.classifier[-1] = nn.Linear(num_ftrs, 10)  ","metadata":{"execution":{"iopub.status.busy":"2024-04-09T17:32:16.419948Z","iopub.execute_input":"2024-04-09T17:32:16.420226Z","iopub.status.idle":"2024-04-09T17:32:16.428950Z","shell.execute_reply.started":"2024-04-09T17:32:16.420202Z","shell.execute_reply":"2024-04-09T17:32:16.427927Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ndevice","metadata":{"execution":{"iopub.status.busy":"2024-04-09T17:32:16.430152Z","iopub.execute_input":"2024-04-09T17:32:16.430421Z","iopub.status.idle":"2024-04-09T17:32:16.490870Z","shell.execute_reply.started":"2024-04-09T17:32:16.430381Z","shell.execute_reply":"2024-04-09T17:32:16.489803Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"device(type='cuda', index=0)"},"metadata":{}}]},{"cell_type":"code","source":"model.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-04-09T17:32:16.492249Z","iopub.execute_input":"2024-04-09T17:32:16.492584Z","iopub.status.idle":"2024-04-09T17:32:16.709772Z","shell.execute_reply.started":"2024-04-09T17:32:16.492558Z","shell.execute_reply":"2024-04-09T17:32:16.708868Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"MaxVit(\n  (stem): Sequential(\n    (0): Conv2dNormActivation(\n      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (1): BatchNorm2d(64, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n      (2): GELU(approximate='none')\n    )\n    (1): Conv2dNormActivation(\n      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    )\n  )\n  (blocks): ModuleList(\n    (0): MaxVitBlock(\n      (layers): ModuleList(\n        (0): MaxVitLayer(\n          (layers): Sequential(\n            (MBconv): MBConv(\n              (proj): Sequential(\n                (0): AvgPool2d(kernel_size=3, stride=2, padding=1)\n                (1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n              )\n              (stochastic_depth): Identity()\n              (layers): Sequential(\n                (pre_norm): BatchNorm2d(64, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n                (conv_a): Conv2dNormActivation(\n                  (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                  (1): BatchNorm2d(256, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n                  (2): GELU(approximate='none')\n                )\n                (conv_b): Conv2dNormActivation(\n                  (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)\n                  (1): BatchNorm2d(256, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n                  (2): GELU(approximate='none')\n                )\n                (squeeze_excitation): SqueezeExcitation(\n                  (avgpool): AdaptiveAvgPool2d(output_size=1)\n                  (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1))\n                  (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n                  (activation): SiLU()\n                  (scale_activation): Sigmoid()\n                )\n                (conv_c): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n              )\n            )\n            (window_attention): PartitionAttentionLayer(\n              (partition_op): WindowPartition()\n              (departition_op): WindowDepartition()\n              (partition_swap): Identity()\n              (departition_swap): Identity()\n              (attn_layer): Sequential(\n                (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n                (1): RelativePositionalMultiHeadAttention(\n                  (to_qkv): Linear(in_features=64, out_features=192, bias=True)\n                  (merge): Linear(in_features=64, out_features=64, bias=True)\n                )\n                (2): Dropout(p=0.0, inplace=False)\n              )\n              (mlp_layer): Sequential(\n                (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n                (1): Linear(in_features=64, out_features=256, bias=True)\n                (2): GELU(approximate='none')\n                (3): Linear(in_features=256, out_features=64, bias=True)\n                (4): Dropout(p=0.0, inplace=False)\n              )\n              (stochastic_dropout): StochasticDepth(p=0.0, mode=row)\n            )\n            (grid_attention): PartitionAttentionLayer(\n              (partition_op): WindowPartition()\n              (departition_op): WindowDepartition()\n              (partition_swap): SwapAxes()\n              (departition_swap): SwapAxes()\n              (attn_layer): Sequential(\n                (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n                (1): RelativePositionalMultiHeadAttention(\n                  (to_qkv): Linear(in_features=64, out_features=192, bias=True)\n                  (merge): Linear(in_features=64, out_features=64, bias=True)\n                )\n                (2): Dropout(p=0.0, inplace=False)\n              )\n              (mlp_layer): Sequential(\n                (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n                (1): Linear(in_features=64, out_features=256, bias=True)\n                (2): GELU(approximate='none')\n                (3): Linear(in_features=256, out_features=64, bias=True)\n                (4): Dropout(p=0.0, inplace=False)\n              )\n              (stochastic_dropout): StochasticDepth(p=0.0, mode=row)\n            )\n          )\n        )\n        (1): MaxVitLayer(\n          (layers): Sequential(\n            (MBconv): MBConv(\n              (proj): Identity()\n              (stochastic_depth): StochasticDepth(p=0.02, mode=row)\n              (layers): Sequential(\n                (pre_norm): BatchNorm2d(64, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n                (conv_a): Conv2dNormActivation(\n                  (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                  (1): BatchNorm2d(256, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n                  (2): GELU(approximate='none')\n                )\n                (conv_b): Conv2dNormActivation(\n                  (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n                  (1): BatchNorm2d(256, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n                  (2): GELU(approximate='none')\n                )\n                (squeeze_excitation): SqueezeExcitation(\n                  (avgpool): AdaptiveAvgPool2d(output_size=1)\n                  (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1))\n                  (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n                  (activation): SiLU()\n                  (scale_activation): Sigmoid()\n                )\n                (conv_c): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n              )\n            )\n            (window_attention): PartitionAttentionLayer(\n              (partition_op): WindowPartition()\n              (departition_op): WindowDepartition()\n              (partition_swap): Identity()\n              (departition_swap): Identity()\n              (attn_layer): Sequential(\n                (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n                (1): RelativePositionalMultiHeadAttention(\n                  (to_qkv): Linear(in_features=64, out_features=192, bias=True)\n                  (merge): Linear(in_features=64, out_features=64, bias=True)\n                )\n                (2): Dropout(p=0.0, inplace=False)\n              )\n              (mlp_layer): Sequential(\n                (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n                (1): Linear(in_features=64, out_features=256, bias=True)\n                (2): GELU(approximate='none')\n                (3): Linear(in_features=256, out_features=64, bias=True)\n                (4): Dropout(p=0.0, inplace=False)\n              )\n              (stochastic_dropout): StochasticDepth(p=0.02, mode=row)\n            )\n            (grid_attention): PartitionAttentionLayer(\n              (partition_op): WindowPartition()\n              (departition_op): WindowDepartition()\n              (partition_swap): SwapAxes()\n              (departition_swap): SwapAxes()\n              (attn_layer): Sequential(\n                (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n                (1): RelativePositionalMultiHeadAttention(\n                  (to_qkv): Linear(in_features=64, out_features=192, bias=True)\n                  (merge): Linear(in_features=64, out_features=64, bias=True)\n                )\n                (2): Dropout(p=0.0, inplace=False)\n              )\n              (mlp_layer): Sequential(\n                (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n                (1): Linear(in_features=64, out_features=256, bias=True)\n                (2): GELU(approximate='none')\n                (3): Linear(in_features=256, out_features=64, bias=True)\n                (4): Dropout(p=0.0, inplace=False)\n              )\n              (stochastic_dropout): StochasticDepth(p=0.02, mode=row)\n            )\n          )\n        )\n      )\n    )\n    (1): MaxVitBlock(\n      (layers): ModuleList(\n        (0): MaxVitLayer(\n          (layers): Sequential(\n            (MBconv): MBConv(\n              (proj): Sequential(\n                (0): AvgPool2d(kernel_size=3, stride=2, padding=1)\n                (1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n              )\n              (stochastic_depth): StochasticDepth(p=0.04, mode=row)\n              (layers): Sequential(\n                (pre_norm): BatchNorm2d(64, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n                (conv_a): Conv2dNormActivation(\n                  (0): Conv2d(64, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                  (1): BatchNorm2d(512, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n                  (2): GELU(approximate='none')\n                )\n                (conv_b): Conv2dNormActivation(\n                  (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512, bias=False)\n                  (1): BatchNorm2d(512, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n                  (2): GELU(approximate='none')\n                )\n                (squeeze_excitation): SqueezeExcitation(\n                  (avgpool): AdaptiveAvgPool2d(output_size=1)\n                  (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n                  (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n                  (activation): SiLU()\n                  (scale_activation): Sigmoid()\n                )\n                (conv_c): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n              )\n            )\n            (window_attention): PartitionAttentionLayer(\n              (partition_op): WindowPartition()\n              (departition_op): WindowDepartition()\n              (partition_swap): Identity()\n              (departition_swap): Identity()\n              (attn_layer): Sequential(\n                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n                (1): RelativePositionalMultiHeadAttention(\n                  (to_qkv): Linear(in_features=128, out_features=384, bias=True)\n                  (merge): Linear(in_features=128, out_features=128, bias=True)\n                )\n                (2): Dropout(p=0.0, inplace=False)\n              )\n              (mlp_layer): Sequential(\n                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n                (1): Linear(in_features=128, out_features=512, bias=True)\n                (2): GELU(approximate='none')\n                (3): Linear(in_features=512, out_features=128, bias=True)\n                (4): Dropout(p=0.0, inplace=False)\n              )\n              (stochastic_dropout): StochasticDepth(p=0.04, mode=row)\n            )\n            (grid_attention): PartitionAttentionLayer(\n              (partition_op): WindowPartition()\n              (departition_op): WindowDepartition()\n              (partition_swap): SwapAxes()\n              (departition_swap): SwapAxes()\n              (attn_layer): Sequential(\n                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n                (1): RelativePositionalMultiHeadAttention(\n                  (to_qkv): Linear(in_features=128, out_features=384, bias=True)\n                  (merge): Linear(in_features=128, out_features=128, bias=True)\n                )\n                (2): Dropout(p=0.0, inplace=False)\n              )\n              (mlp_layer): Sequential(\n                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n                (1): Linear(in_features=128, out_features=512, bias=True)\n                (2): GELU(approximate='none')\n                (3): Linear(in_features=512, out_features=128, bias=True)\n                (4): Dropout(p=0.0, inplace=False)\n              )\n              (stochastic_dropout): StochasticDepth(p=0.04, mode=row)\n            )\n          )\n        )\n        (1): MaxVitLayer(\n          (layers): Sequential(\n            (MBconv): MBConv(\n              (proj): Identity()\n              (stochastic_depth): StochasticDepth(p=0.06, mode=row)\n              (layers): Sequential(\n                (pre_norm): BatchNorm2d(128, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n                (conv_a): Conv2dNormActivation(\n                  (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                  (1): BatchNorm2d(512, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n                  (2): GELU(approximate='none')\n                )\n                (conv_b): Conv2dNormActivation(\n                  (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n                  (1): BatchNorm2d(512, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n                  (2): GELU(approximate='none')\n                )\n                (squeeze_excitation): SqueezeExcitation(\n                  (avgpool): AdaptiveAvgPool2d(output_size=1)\n                  (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n                  (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n                  (activation): SiLU()\n                  (scale_activation): Sigmoid()\n                )\n                (conv_c): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n              )\n            )\n            (window_attention): PartitionAttentionLayer(\n              (partition_op): WindowPartition()\n              (departition_op): WindowDepartition()\n              (partition_swap): Identity()\n              (departition_swap): Identity()\n              (attn_layer): Sequential(\n                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n                (1): RelativePositionalMultiHeadAttention(\n                  (to_qkv): Linear(in_features=128, out_features=384, bias=True)\n                  (merge): Linear(in_features=128, out_features=128, bias=True)\n                )\n                (2): Dropout(p=0.0, inplace=False)\n              )\n              (mlp_layer): Sequential(\n                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n                (1): Linear(in_features=128, out_features=512, bias=True)\n                (2): GELU(approximate='none')\n                (3): Linear(in_features=512, out_features=128, bias=True)\n                (4): Dropout(p=0.0, inplace=False)\n              )\n              (stochastic_dropout): StochasticDepth(p=0.06, mode=row)\n            )\n            (grid_attention): PartitionAttentionLayer(\n              (partition_op): WindowPartition()\n              (departition_op): WindowDepartition()\n              (partition_swap): SwapAxes()\n              (departition_swap): SwapAxes()\n              (attn_layer): Sequential(\n                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n                (1): RelativePositionalMultiHeadAttention(\n                  (to_qkv): Linear(in_features=128, out_features=384, bias=True)\n                  (merge): Linear(in_features=128, out_features=128, bias=True)\n                )\n                (2): Dropout(p=0.0, inplace=False)\n              )\n              (mlp_layer): Sequential(\n                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n                (1): Linear(in_features=128, out_features=512, bias=True)\n                (2): GELU(approximate='none')\n                (3): Linear(in_features=512, out_features=128, bias=True)\n                (4): Dropout(p=0.0, inplace=False)\n              )\n              (stochastic_dropout): StochasticDepth(p=0.06, mode=row)\n            )\n          )\n        )\n      )\n    )\n    (2): MaxVitBlock(\n      (layers): ModuleList(\n        (0): MaxVitLayer(\n          (layers): Sequential(\n            (MBconv): MBConv(\n              (proj): Sequential(\n                (0): AvgPool2d(kernel_size=3, stride=2, padding=1)\n                (1): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n              )\n              (stochastic_depth): StochasticDepth(p=0.08, mode=row)\n              (layers): Sequential(\n                (pre_norm): BatchNorm2d(128, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n                (conv_a): Conv2dNormActivation(\n                  (0): Conv2d(128, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                  (1): BatchNorm2d(1024, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n                  (2): GELU(approximate='none')\n                )\n                (conv_b): Conv2dNormActivation(\n                  (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=1024, bias=False)\n                  (1): BatchNorm2d(1024, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n                  (2): GELU(approximate='none')\n                )\n                (squeeze_excitation): SqueezeExcitation(\n                  (avgpool): AdaptiveAvgPool2d(output_size=1)\n                  (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n                  (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n                  (activation): SiLU()\n                  (scale_activation): Sigmoid()\n                )\n                (conv_c): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n              )\n            )\n            (window_attention): PartitionAttentionLayer(\n              (partition_op): WindowPartition()\n              (departition_op): WindowDepartition()\n              (partition_swap): Identity()\n              (departition_swap): Identity()\n              (attn_layer): Sequential(\n                (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n                (1): RelativePositionalMultiHeadAttention(\n                  (to_qkv): Linear(in_features=256, out_features=768, bias=True)\n                  (merge): Linear(in_features=256, out_features=256, bias=True)\n                )\n                (2): Dropout(p=0.0, inplace=False)\n              )\n              (mlp_layer): Sequential(\n                (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n                (1): Linear(in_features=256, out_features=1024, bias=True)\n                (2): GELU(approximate='none')\n                (3): Linear(in_features=1024, out_features=256, bias=True)\n                (4): Dropout(p=0.0, inplace=False)\n              )\n              (stochastic_dropout): StochasticDepth(p=0.08, mode=row)\n            )\n            (grid_attention): PartitionAttentionLayer(\n              (partition_op): WindowPartition()\n              (departition_op): WindowDepartition()\n              (partition_swap): SwapAxes()\n              (departition_swap): SwapAxes()\n              (attn_layer): Sequential(\n                (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n                (1): RelativePositionalMultiHeadAttention(\n                  (to_qkv): Linear(in_features=256, out_features=768, bias=True)\n                  (merge): Linear(in_features=256, out_features=256, bias=True)\n                )\n                (2): Dropout(p=0.0, inplace=False)\n              )\n              (mlp_layer): Sequential(\n                (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n                (1): Linear(in_features=256, out_features=1024, bias=True)\n                (2): GELU(approximate='none')\n                (3): Linear(in_features=1024, out_features=256, bias=True)\n                (4): Dropout(p=0.0, inplace=False)\n              )\n              (stochastic_dropout): StochasticDepth(p=0.08, mode=row)\n            )\n          )\n        )\n        (1): MaxVitLayer(\n          (layers): Sequential(\n            (MBconv): MBConv(\n              (proj): Identity()\n              (stochastic_depth): StochasticDepth(p=0.1, mode=row)\n              (layers): Sequential(\n                (pre_norm): BatchNorm2d(256, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n                (conv_a): Conv2dNormActivation(\n                  (0): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                  (1): BatchNorm2d(1024, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n                  (2): GELU(approximate='none')\n                )\n                (conv_b): Conv2dNormActivation(\n                  (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n                  (1): BatchNorm2d(1024, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n                  (2): GELU(approximate='none')\n                )\n                (squeeze_excitation): SqueezeExcitation(\n                  (avgpool): AdaptiveAvgPool2d(output_size=1)\n                  (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n                  (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n                  (activation): SiLU()\n                  (scale_activation): Sigmoid()\n                )\n                (conv_c): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n              )\n            )\n            (window_attention): PartitionAttentionLayer(\n              (partition_op): WindowPartition()\n              (departition_op): WindowDepartition()\n              (partition_swap): Identity()\n              (departition_swap): Identity()\n              (attn_layer): Sequential(\n                (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n                (1): RelativePositionalMultiHeadAttention(\n                  (to_qkv): Linear(in_features=256, out_features=768, bias=True)\n                  (merge): Linear(in_features=256, out_features=256, bias=True)\n                )\n                (2): Dropout(p=0.0, inplace=False)\n              )\n              (mlp_layer): Sequential(\n                (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n                (1): Linear(in_features=256, out_features=1024, bias=True)\n                (2): GELU(approximate='none')\n                (3): Linear(in_features=1024, out_features=256, bias=True)\n                (4): Dropout(p=0.0, inplace=False)\n              )\n              (stochastic_dropout): StochasticDepth(p=0.1, mode=row)\n            )\n            (grid_attention): PartitionAttentionLayer(\n              (partition_op): WindowPartition()\n              (departition_op): WindowDepartition()\n              (partition_swap): SwapAxes()\n              (departition_swap): SwapAxes()\n              (attn_layer): Sequential(\n                (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n                (1): RelativePositionalMultiHeadAttention(\n                  (to_qkv): Linear(in_features=256, out_features=768, bias=True)\n                  (merge): Linear(in_features=256, out_features=256, bias=True)\n                )\n                (2): Dropout(p=0.0, inplace=False)\n              )\n              (mlp_layer): Sequential(\n                (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n                (1): Linear(in_features=256, out_features=1024, bias=True)\n                (2): GELU(approximate='none')\n                (3): Linear(in_features=1024, out_features=256, bias=True)\n                (4): Dropout(p=0.0, inplace=False)\n              )\n              (stochastic_dropout): StochasticDepth(p=0.1, mode=row)\n            )\n          )\n        )\n        (2): MaxVitLayer(\n          (layers): Sequential(\n            (MBconv): MBConv(\n              (proj): Identity()\n              (stochastic_depth): StochasticDepth(p=0.12, mode=row)\n              (layers): Sequential(\n                (pre_norm): BatchNorm2d(256, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n                (conv_a): Conv2dNormActivation(\n                  (0): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                  (1): BatchNorm2d(1024, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n                  (2): GELU(approximate='none')\n                )\n                (conv_b): Conv2dNormActivation(\n                  (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n                  (1): BatchNorm2d(1024, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n                  (2): GELU(approximate='none')\n                )\n                (squeeze_excitation): SqueezeExcitation(\n                  (avgpool): AdaptiveAvgPool2d(output_size=1)\n                  (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n                  (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n                  (activation): SiLU()\n                  (scale_activation): Sigmoid()\n                )\n                (conv_c): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n              )\n            )\n            (window_attention): PartitionAttentionLayer(\n              (partition_op): WindowPartition()\n              (departition_op): WindowDepartition()\n              (partition_swap): Identity()\n              (departition_swap): Identity()\n              (attn_layer): Sequential(\n                (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n                (1): RelativePositionalMultiHeadAttention(\n                  (to_qkv): Linear(in_features=256, out_features=768, bias=True)\n                  (merge): Linear(in_features=256, out_features=256, bias=True)\n                )\n                (2): Dropout(p=0.0, inplace=False)\n              )\n              (mlp_layer): Sequential(\n                (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n                (1): Linear(in_features=256, out_features=1024, bias=True)\n                (2): GELU(approximate='none')\n                (3): Linear(in_features=1024, out_features=256, bias=True)\n                (4): Dropout(p=0.0, inplace=False)\n              )\n              (stochastic_dropout): StochasticDepth(p=0.12, mode=row)\n            )\n            (grid_attention): PartitionAttentionLayer(\n              (partition_op): WindowPartition()\n              (departition_op): WindowDepartition()\n              (partition_swap): SwapAxes()\n              (departition_swap): SwapAxes()\n              (attn_layer): Sequential(\n                (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n                (1): RelativePositionalMultiHeadAttention(\n                  (to_qkv): Linear(in_features=256, out_features=768, bias=True)\n                  (merge): Linear(in_features=256, out_features=256, bias=True)\n                )\n                (2): Dropout(p=0.0, inplace=False)\n              )\n              (mlp_layer): Sequential(\n                (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n                (1): Linear(in_features=256, out_features=1024, bias=True)\n                (2): GELU(approximate='none')\n                (3): Linear(in_features=1024, out_features=256, bias=True)\n                (4): Dropout(p=0.0, inplace=False)\n              )\n              (stochastic_dropout): StochasticDepth(p=0.12, mode=row)\n            )\n          )\n        )\n        (3): MaxVitLayer(\n          (layers): Sequential(\n            (MBconv): MBConv(\n              (proj): Identity()\n              (stochastic_depth): StochasticDepth(p=0.14, mode=row)\n              (layers): Sequential(\n                (pre_norm): BatchNorm2d(256, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n                (conv_a): Conv2dNormActivation(\n                  (0): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                  (1): BatchNorm2d(1024, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n                  (2): GELU(approximate='none')\n                )\n                (conv_b): Conv2dNormActivation(\n                  (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n                  (1): BatchNorm2d(1024, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n                  (2): GELU(approximate='none')\n                )\n                (squeeze_excitation): SqueezeExcitation(\n                  (avgpool): AdaptiveAvgPool2d(output_size=1)\n                  (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n                  (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n                  (activation): SiLU()\n                  (scale_activation): Sigmoid()\n                )\n                (conv_c): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n              )\n            )\n            (window_attention): PartitionAttentionLayer(\n              (partition_op): WindowPartition()\n              (departition_op): WindowDepartition()\n              (partition_swap): Identity()\n              (departition_swap): Identity()\n              (attn_layer): Sequential(\n                (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n                (1): RelativePositionalMultiHeadAttention(\n                  (to_qkv): Linear(in_features=256, out_features=768, bias=True)\n                  (merge): Linear(in_features=256, out_features=256, bias=True)\n                )\n                (2): Dropout(p=0.0, inplace=False)\n              )\n              (mlp_layer): Sequential(\n                (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n                (1): Linear(in_features=256, out_features=1024, bias=True)\n                (2): GELU(approximate='none')\n                (3): Linear(in_features=1024, out_features=256, bias=True)\n                (4): Dropout(p=0.0, inplace=False)\n              )\n              (stochastic_dropout): StochasticDepth(p=0.14, mode=row)\n            )\n            (grid_attention): PartitionAttentionLayer(\n              (partition_op): WindowPartition()\n              (departition_op): WindowDepartition()\n              (partition_swap): SwapAxes()\n              (departition_swap): SwapAxes()\n              (attn_layer): Sequential(\n                (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n                (1): RelativePositionalMultiHeadAttention(\n                  (to_qkv): Linear(in_features=256, out_features=768, bias=True)\n                  (merge): Linear(in_features=256, out_features=256, bias=True)\n                )\n                (2): Dropout(p=0.0, inplace=False)\n              )\n              (mlp_layer): Sequential(\n                (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n                (1): Linear(in_features=256, out_features=1024, bias=True)\n                (2): GELU(approximate='none')\n                (3): Linear(in_features=1024, out_features=256, bias=True)\n                (4): Dropout(p=0.0, inplace=False)\n              )\n              (stochastic_dropout): StochasticDepth(p=0.14, mode=row)\n            )\n          )\n        )\n        (4): MaxVitLayer(\n          (layers): Sequential(\n            (MBconv): MBConv(\n              (proj): Identity()\n              (stochastic_depth): StochasticDepth(p=0.16, mode=row)\n              (layers): Sequential(\n                (pre_norm): BatchNorm2d(256, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n                (conv_a): Conv2dNormActivation(\n                  (0): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                  (1): BatchNorm2d(1024, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n                  (2): GELU(approximate='none')\n                )\n                (conv_b): Conv2dNormActivation(\n                  (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n                  (1): BatchNorm2d(1024, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n                  (2): GELU(approximate='none')\n                )\n                (squeeze_excitation): SqueezeExcitation(\n                  (avgpool): AdaptiveAvgPool2d(output_size=1)\n                  (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n                  (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n                  (activation): SiLU()\n                  (scale_activation): Sigmoid()\n                )\n                (conv_c): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n              )\n            )\n            (window_attention): PartitionAttentionLayer(\n              (partition_op): WindowPartition()\n              (departition_op): WindowDepartition()\n              (partition_swap): Identity()\n              (departition_swap): Identity()\n              (attn_layer): Sequential(\n                (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n                (1): RelativePositionalMultiHeadAttention(\n                  (to_qkv): Linear(in_features=256, out_features=768, bias=True)\n                  (merge): Linear(in_features=256, out_features=256, bias=True)\n                )\n                (2): Dropout(p=0.0, inplace=False)\n              )\n              (mlp_layer): Sequential(\n                (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n                (1): Linear(in_features=256, out_features=1024, bias=True)\n                (2): GELU(approximate='none')\n                (3): Linear(in_features=1024, out_features=256, bias=True)\n                (4): Dropout(p=0.0, inplace=False)\n              )\n              (stochastic_dropout): StochasticDepth(p=0.16, mode=row)\n            )\n            (grid_attention): PartitionAttentionLayer(\n              (partition_op): WindowPartition()\n              (departition_op): WindowDepartition()\n              (partition_swap): SwapAxes()\n              (departition_swap): SwapAxes()\n              (attn_layer): Sequential(\n                (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n                (1): RelativePositionalMultiHeadAttention(\n                  (to_qkv): Linear(in_features=256, out_features=768, bias=True)\n                  (merge): Linear(in_features=256, out_features=256, bias=True)\n                )\n                (2): Dropout(p=0.0, inplace=False)\n              )\n              (mlp_layer): Sequential(\n                (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n                (1): Linear(in_features=256, out_features=1024, bias=True)\n                (2): GELU(approximate='none')\n                (3): Linear(in_features=1024, out_features=256, bias=True)\n                (4): Dropout(p=0.0, inplace=False)\n              )\n              (stochastic_dropout): StochasticDepth(p=0.16, mode=row)\n            )\n          )\n        )\n      )\n    )\n    (3): MaxVitBlock(\n      (layers): ModuleList(\n        (0): MaxVitLayer(\n          (layers): Sequential(\n            (MBconv): MBConv(\n              (proj): Sequential(\n                (0): AvgPool2d(kernel_size=3, stride=2, padding=1)\n                (1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n              )\n              (stochastic_depth): StochasticDepth(p=0.18, mode=row)\n              (layers): Sequential(\n                (pre_norm): BatchNorm2d(256, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n                (conv_a): Conv2dNormActivation(\n                  (0): Conv2d(256, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                  (1): BatchNorm2d(2048, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n                  (2): GELU(approximate='none')\n                )\n                (conv_b): Conv2dNormActivation(\n                  (0): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=2048, bias=False)\n                  (1): BatchNorm2d(2048, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n                  (2): GELU(approximate='none')\n                )\n                (squeeze_excitation): SqueezeExcitation(\n                  (avgpool): AdaptiveAvgPool2d(output_size=1)\n                  (fc1): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1))\n                  (fc2): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1))\n                  (activation): SiLU()\n                  (scale_activation): Sigmoid()\n                )\n                (conv_c): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))\n              )\n            )\n            (window_attention): PartitionAttentionLayer(\n              (partition_op): WindowPartition()\n              (departition_op): WindowDepartition()\n              (partition_swap): Identity()\n              (departition_swap): Identity()\n              (attn_layer): Sequential(\n                (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n                (1): RelativePositionalMultiHeadAttention(\n                  (to_qkv): Linear(in_features=512, out_features=1536, bias=True)\n                  (merge): Linear(in_features=512, out_features=512, bias=True)\n                )\n                (2): Dropout(p=0.0, inplace=False)\n              )\n              (mlp_layer): Sequential(\n                (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n                (1): Linear(in_features=512, out_features=2048, bias=True)\n                (2): GELU(approximate='none')\n                (3): Linear(in_features=2048, out_features=512, bias=True)\n                (4): Dropout(p=0.0, inplace=False)\n              )\n              (stochastic_dropout): StochasticDepth(p=0.18, mode=row)\n            )\n            (grid_attention): PartitionAttentionLayer(\n              (partition_op): WindowPartition()\n              (departition_op): WindowDepartition()\n              (partition_swap): SwapAxes()\n              (departition_swap): SwapAxes()\n              (attn_layer): Sequential(\n                (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n                (1): RelativePositionalMultiHeadAttention(\n                  (to_qkv): Linear(in_features=512, out_features=1536, bias=True)\n                  (merge): Linear(in_features=512, out_features=512, bias=True)\n                )\n                (2): Dropout(p=0.0, inplace=False)\n              )\n              (mlp_layer): Sequential(\n                (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n                (1): Linear(in_features=512, out_features=2048, bias=True)\n                (2): GELU(approximate='none')\n                (3): Linear(in_features=2048, out_features=512, bias=True)\n                (4): Dropout(p=0.0, inplace=False)\n              )\n              (stochastic_dropout): StochasticDepth(p=0.18, mode=row)\n            )\n          )\n        )\n        (1): MaxVitLayer(\n          (layers): Sequential(\n            (MBconv): MBConv(\n              (proj): Identity()\n              (stochastic_depth): StochasticDepth(p=0.2, mode=row)\n              (layers): Sequential(\n                (pre_norm): BatchNorm2d(512, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n                (conv_a): Conv2dNormActivation(\n                  (0): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                  (1): BatchNorm2d(2048, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n                  (2): GELU(approximate='none')\n                )\n                (conv_b): Conv2dNormActivation(\n                  (0): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048, bias=False)\n                  (1): BatchNorm2d(2048, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n                  (2): GELU(approximate='none')\n                )\n                (squeeze_excitation): SqueezeExcitation(\n                  (avgpool): AdaptiveAvgPool2d(output_size=1)\n                  (fc1): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1))\n                  (fc2): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1))\n                  (activation): SiLU()\n                  (scale_activation): Sigmoid()\n                )\n                (conv_c): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))\n              )\n            )\n            (window_attention): PartitionAttentionLayer(\n              (partition_op): WindowPartition()\n              (departition_op): WindowDepartition()\n              (partition_swap): Identity()\n              (departition_swap): Identity()\n              (attn_layer): Sequential(\n                (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n                (1): RelativePositionalMultiHeadAttention(\n                  (to_qkv): Linear(in_features=512, out_features=1536, bias=True)\n                  (merge): Linear(in_features=512, out_features=512, bias=True)\n                )\n                (2): Dropout(p=0.0, inplace=False)\n              )\n              (mlp_layer): Sequential(\n                (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n                (1): Linear(in_features=512, out_features=2048, bias=True)\n                (2): GELU(approximate='none')\n                (3): Linear(in_features=2048, out_features=512, bias=True)\n                (4): Dropout(p=0.0, inplace=False)\n              )\n              (stochastic_dropout): StochasticDepth(p=0.2, mode=row)\n            )\n            (grid_attention): PartitionAttentionLayer(\n              (partition_op): WindowPartition()\n              (departition_op): WindowDepartition()\n              (partition_swap): SwapAxes()\n              (departition_swap): SwapAxes()\n              (attn_layer): Sequential(\n                (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n                (1): RelativePositionalMultiHeadAttention(\n                  (to_qkv): Linear(in_features=512, out_features=1536, bias=True)\n                  (merge): Linear(in_features=512, out_features=512, bias=True)\n                )\n                (2): Dropout(p=0.0, inplace=False)\n              )\n              (mlp_layer): Sequential(\n                (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n                (1): Linear(in_features=512, out_features=2048, bias=True)\n                (2): GELU(approximate='none')\n                (3): Linear(in_features=2048, out_features=512, bias=True)\n                (4): Dropout(p=0.0, inplace=False)\n              )\n              (stochastic_dropout): StochasticDepth(p=0.2, mode=row)\n            )\n          )\n        )\n      )\n    )\n  )\n  (classifier): Sequential(\n    (0): AdaptiveAvgPool2d(output_size=1)\n    (1): Flatten(start_dim=1, end_dim=-1)\n    (2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n    (3): Linear(in_features=512, out_features=512, bias=True)\n    (4): Tanh()\n    (5): Linear(in_features=512, out_features=10, bias=True)\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\nepochs = 5\nprint_every = 30 # Prints every 30 images out of batch of 50 images\nsteps = 0","metadata":{"execution":{"iopub.status.busy":"2024-04-09T17:35:40.810884Z","iopub.execute_input":"2024-04-09T17:35:40.811701Z","iopub.status.idle":"2024-04-09T17:35:40.821059Z","shell.execute_reply.started":"2024-04-09T17:35:40.811672Z","shell.execute_reply":"2024-04-09T17:35:40.820024Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def validation(model, testloader, criterion,device):\n    test_loss = 0\n    accuracy = 0\n    \n    for ii, (inputs, labels) in enumerate(testloader):\n        \n        inputs, labels = inputs.to(device), labels.to(device)\n        \n        output = model.forward(inputs)\n        test_loss += criterion(output, labels).item()\n        \n        ps = torch.exp(output)\n        equality = (labels.data == ps.max(dim=1)[1])\n        accuracy += equality.type(torch.FloatTensor).mean()\n    \n    return test_loss, accuracy","metadata":{"execution":{"iopub.status.busy":"2024-04-09T17:35:40.823330Z","iopub.execute_input":"2024-04-09T17:35:40.824041Z","iopub.status.idle":"2024-04-09T17:35:40.832867Z","shell.execute_reply.started":"2024-04-09T17:35:40.824004Z","shell.execute_reply":"2024-04-09T17:35:40.831887Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def train(model, trainloader, validloader, criterion, optimizer, device, epochs, steps,print_every):\n    print(\"Training process initializing .....\\n\")\n\n    for e in range(epochs):\n        running_loss = 0\n        model.train() \n    \n        for ii, (inputs, labels) in enumerate(trainloader):\n            steps += 1\n        \n            inputs, labels = inputs.to(device), labels.to(device)\n        \n            optimizer.zero_grad()\n        \n            # Forward and backward passes\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n        \n            running_loss += loss.item()\n        \n            if steps % print_every == 0:\n                model.eval()\n\n                with torch.no_grad():\n                    valid_loss, accuracy = validation(model, validloader, criterion, device)\n            \n                print(\"Epoch: {}/{} | \".format(e+1, epochs),\n                      \"Training Loss: {:.4f} | \".format(running_loss/print_every),\n                      \"Validation Loss: {:.4f} | \".format(valid_loss/len(validloader)),\n                      \"Validation Accuracy: {:.4f}\".format(accuracy/len(validloader)))\n            \n                running_loss = 0\n                model.train()\n\n    print(\"\\nTraining process is now complete!!\")\n    return model\n","metadata":{"execution":{"iopub.status.busy":"2024-04-09T17:35:40.834155Z","iopub.execute_input":"2024-04-09T17:35:40.834515Z","iopub.status.idle":"2024-04-09T17:35:40.846224Z","shell.execute_reply.started":"2024-04-09T17:35:40.834481Z","shell.execute_reply":"2024-04-09T17:35:40.845168Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"model = train(model,trainloader, validloader,criterion, optimizer,device, epochs,steps, print_every)","metadata":{"execution":{"iopub.status.busy":"2024-04-09T17:35:40.847208Z","iopub.execute_input":"2024-04-09T17:35:40.847475Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Training process initializing .....\n\nEpoch: 1/5 |  Training Loss: 1.9568 |  Validation Loss: 1.6491 |  Validation Accuracy: 0.5611\nEpoch: 1/5 |  Training Loss: 1.4695 |  Validation Loss: 1.3024 |  Validation Accuracy: 0.6607\nEpoch: 1/5 |  Training Loss: 1.2074 |  Validation Loss: 1.1206 |  Validation Accuracy: 0.7119\nEpoch: 1/5 |  Training Loss: 1.0628 |  Validation Loss: 0.9974 |  Validation Accuracy: 0.7330\nEpoch: 1/5 |  Training Loss: 0.9909 |  Validation Loss: 0.8920 |  Validation Accuracy: 0.7663\nEpoch: 1/5 |  Training Loss: 0.9142 |  Validation Loss: 0.8773 |  Validation Accuracy: 0.7604\nEpoch: 1/5 |  Training Loss: 0.8591 |  Validation Loss: 0.8083 |  Validation Accuracy: 0.7744\nEpoch: 1/5 |  Training Loss: 0.7969 |  Validation Loss: 0.7857 |  Validation Accuracy: 0.7733\nEpoch: 1/5 |  Training Loss: 0.7696 |  Validation Loss: 0.7431 |  Validation Accuracy: 0.7874\nEpoch: 1/5 |  Training Loss: 0.7089 |  Validation Loss: 0.7619 |  Validation Accuracy: 0.7637\nEpoch: 1/5 |  Training Loss: 0.7241 |  Validation Loss: 0.6810 |  Validation Accuracy: 0.7989\nEpoch: 1/5 |  Training Loss: 0.6969 |  Validation Loss: 0.6917 |  Validation Accuracy: 0.7904\nEpoch: 2/5 |  Training Loss: 0.2668 |  Validation Loss: 0.6510 |  Validation Accuracy: 0.8037\nEpoch: 2/5 |  Training Loss: 0.6579 |  Validation Loss: 0.6371 |  Validation Accuracy: 0.8141\nEpoch: 2/5 |  Training Loss: 0.6482 |  Validation Loss: 0.6946 |  Validation Accuracy: 0.7815\nEpoch: 2/5 |  Training Loss: 0.6389 |  Validation Loss: 0.6468 |  Validation Accuracy: 0.7967\nEpoch: 2/5 |  Training Loss: 0.6168 |  Validation Loss: 0.6178 |  Validation Accuracy: 0.8096\nEpoch: 2/5 |  Training Loss: 0.6194 |  Validation Loss: 0.6023 |  Validation Accuracy: 0.8126\nEpoch: 2/5 |  Training Loss: 0.6280 |  Validation Loss: 0.5820 |  Validation Accuracy: 0.8219\nEpoch: 2/5 |  Training Loss: 0.6097 |  Validation Loss: 0.6017 |  Validation Accuracy: 0.8089\nEpoch: 2/5 |  Training Loss: 0.5615 |  Validation Loss: 0.5587 |  Validation Accuracy: 0.8307\nEpoch: 2/5 |  Training Loss: 0.5960 |  Validation Loss: 0.5811 |  Validation Accuracy: 0.8119\nEpoch: 2/5 |  Training Loss: 0.5882 |  Validation Loss: 0.5723 |  Validation Accuracy: 0.8219\nEpoch: 2/5 |  Training Loss: 0.6061 |  Validation Loss: 0.5576 |  Validation Accuracy: 0.8296\nEpoch: 2/5 |  Training Loss: 0.5589 |  Validation Loss: 0.5482 |  Validation Accuracy: 0.8307\nEpoch: 3/5 |  Training Loss: 0.4259 |  Validation Loss: 0.5548 |  Validation Accuracy: 0.8307\nEpoch: 3/5 |  Training Loss: 0.5807 |  Validation Loss: 0.5726 |  Validation Accuracy: 0.8159\nEpoch: 3/5 |  Training Loss: 0.5535 |  Validation Loss: 0.5524 |  Validation Accuracy: 0.8256\nEpoch: 3/5 |  Training Loss: 0.5676 |  Validation Loss: 0.5974 |  Validation Accuracy: 0.8004\nEpoch: 3/5 |  Training Loss: 0.5715 |  Validation Loss: 0.5630 |  Validation Accuracy: 0.8248\nEpoch: 3/5 |  Training Loss: 0.5907 |  Validation Loss: 0.5357 |  Validation Accuracy: 0.8315\nEpoch: 3/5 |  Training Loss: 0.5859 |  Validation Loss: 0.5593 |  Validation Accuracy: 0.8200\nEpoch: 3/5 |  Training Loss: 0.5276 |  Validation Loss: 0.5140 |  Validation Accuracy: 0.8367\nEpoch: 3/5 |  Training Loss: 0.5189 |  Validation Loss: 0.5655 |  Validation Accuracy: 0.8152\nEpoch: 3/5 |  Training Loss: 0.5220 |  Validation Loss: 0.5458 |  Validation Accuracy: 0.8278\nEpoch: 3/5 |  Training Loss: 0.5483 |  Validation Loss: 0.5350 |  Validation Accuracy: 0.8315\nEpoch: 3/5 |  Training Loss: 0.5458 |  Validation Loss: 0.5303 |  Validation Accuracy: 0.8200\nEpoch: 4/5 |  Training Loss: 0.1114 |  Validation Loss: 0.5269 |  Validation Accuracy: 0.8304\nEpoch: 4/5 |  Training Loss: 0.5263 |  Validation Loss: 0.5199 |  Validation Accuracy: 0.8263\nEpoch: 4/5 |  Training Loss: 0.5431 |  Validation Loss: 0.5020 |  Validation Accuracy: 0.8304\nEpoch: 4/5 |  Training Loss: 0.5319 |  Validation Loss: 0.4999 |  Validation Accuracy: 0.8393\nEpoch: 4/5 |  Training Loss: 0.5249 |  Validation Loss: 0.5126 |  Validation Accuracy: 0.8422\nEpoch: 4/5 |  Training Loss: 0.5609 |  Validation Loss: 0.5190 |  Validation Accuracy: 0.8326\n","output_type":"stream"}]},{"cell_type":"code","source":"def pred(Model,Testloader):\n    all_labels = []\n    all_predictions = []\n    correct = 0\n    total = 0\n    start_time = time()\n    with torch.no_grad():\n        Model.eval()\n        for images, labels in Testloader:\n            all_labels.extend(labels.numpy())\n            images, labels = images.to(device), labels.to(device)\n            outputs = Model(images)\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n            predicted_tensor_cpu = predicted.to('cpu')\n            all_predictions.extend(predicted_tensor_cpu.numpy())\n    end_time = time()\n    print(\"Time: \",end_time - start_time)\n    print('Accuracy achieved by the network on test images is: %d%%' % (100 * correct / total))\n    \n    return all_labels,all_predictions","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels_fp32,predictions_fp32 = pred(model,testloader)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\ndef metrics(labels,predictions):\n    classes = train_data.dataset.classes\n    cm = confusion_matrix(np.array(labels), np.array(predictions))\n    print(\"Confusion Matrix:\")\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n    plt.xlabel('Predicted')\n    plt.ylabel('Actual')\n    plt.title('Confusion Matrix')\n    plt.show()\n    print('----------------------------------------------------------------')\n    print(\"Classification Report:\")\n    report = classification_report(np.array(labels),np.array(predictions))\n    print(report)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"metrics(labels_fp32,predictions_fp32)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\ndef print_model_size(mdl):\n    torch.save(mdl.state_dict(), \"tmp.pt\")\n    print(\"%.2f MB\" %(os.path.getsize(\"tmp.pt\")/1e6))\n    os.remove('tmp.pt')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Size of fp32 model:\",end='')\nprint_model_size(model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Quantization","metadata":{}},{"cell_type":"markdown","source":"# FP-16","metadata":{}},{"cell_type":"code","source":"model_fp16 = copy.deepcopy(model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_fp16.half()\nmodel_fp16.to(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def pred_fp16(Model,Testloader):\n    all_labels = []\n    all_predictions = []\n    correct = 0\n    total = 0\n    start_time = time()\n    with torch.no_grad():\n        Model.eval()\n        for images, labels in Testloader:\n            all_labels.extend(labels.numpy())\n            images, labels = images.to(device), labels.to(device)\n            outputs = Model(images.half())\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n            predicted_tensor_cpu = predicted.to('cpu')\n            all_predictions.extend(predicted_tensor_cpu.numpy())\n    end_time = time()\n    print(\"Time: \",end_time - start_time)\n    print('Accuracy achieved by the network on test images is: %d%%' % (100 * correct / total))\n    \n    return all_labels,all_predictions","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels_fp16,predictions_fp16 = pred_fp16(model_fp16,testloader)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"metrics(labels_fp16,predictions_fp16)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# FP-64","metadata":{}},{"cell_type":"code","source":"model_64 = copy.deepcopy(model)\nmodel_64.double()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"weights_64 = model_64.state_dict()\nfor name, param in weights_64.items():\n    print(name)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"weights_64 = model_64.state_dict()\nprint(weights_64['stem.0.0.weight'].dtype)\n# print(weights_64['conv1.weight'].dtype)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_64.to(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def pred_fp64(Model,Testloader): \n    all_labels = [] \n    all_predictions = [] \n    correct = 0 \n    total = 0 \n    start_time = time() \n    with torch.no_grad(): \n        Model.eval() \n        for images, labels in Testloader: \n            all_labels.extend(labels.numpy()) \n            images, labels = images.to(device), labels.to(device) \n            outputs = Model(images.double())\n            _,predicted = torch.max(outputs.data, 1) \n            total += labels.size(0) \n            correct += (predicted == labels).sum().item() \n            predicted_tensor_cpu = predicted.to('cpu') \n            all_predictions.extend(predicted_tensor_cpu.numpy()) \n        end_time = time() \n        print(\"Time: \",end_time - start_time) \n        print('Accuracy achieved by the network on test images is: %d%%' % (100 * correct / total))\n        return all_labels,all_predictions","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels_64,predictions_64 = pred_fp64(model_64,testloader)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"metrics(labels_64,predictions_64)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## INT-8","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.ao.quantization import (\n  get_default_qconfig_mapping,\n  get_default_qat_qconfig_mapping,\n  QConfigMapping,\n)\nimport torch.ao.quantization.quantize_fx as quantize_fx","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### PTQ","metadata":{}},{"cell_type":"code","source":"model.to('cpu')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_data = next(iter(trainloader))[0] \ncalibrate_data = input_data.to(\"cpu\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_int8 = copy.deepcopy(model)\n\nqconfig_mapping = get_default_qconfig_mapping(\"x86\")\nmodel_int8.eval()\n# prepare\nmodel_prepared = quantize_fx.prepare_fx(model_int8, qconfig_mapping, calibrate_data)\n# calibrate","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with torch.no_grad():\n    for i in range(20):\n        batch = next(iter(trainloader))[0]\n        output = model_prepared(batch.to('cpu'))\n        print(i)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_quantized_static = quantize_fx.convert_fx(model_prepared)\nmodel_quantized_static.state_dict()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_quantized_static","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_quantized_static.to('cpu')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_predictions_int8 = []\nall_labels_int8 = []\ncorrect_pred = 0\ntotal_pred = 0\nstart_time_int8 = time()\nwith torch.no_grad():\n    model_quantized_static.eval()\n    for data in testloader:\n        images, labels = data\n        all_labels_int8.extend(labels.numpy())\n        #images, labels = images.to(device), labels.to(device)\n        outputs = model_quantized_static(images.to('cpu'))\n        _, predicted = torch.max(outputs.data, 1)\n        total_pred += labels.size(0)\n        correct_pred += (predicted == labels).sum().item()\n        predicted_tensor_cpu = predicted.to('cpu')\n        all_predictions_int8.extend(predicted_tensor_cpu.numpy())\nend_time_int8 = time()\nprint(\"Time: \",end_time_int8 - start_time_int8)\nprint('Accuracy achieved by the network on test images is: %d%%' % (100 * correct_pred / total_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"metrics(all_predictions_int8,all_labels_int8)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### QAT","metadata":{}},{"cell_type":"markdown","source":"Load a new vgg model which is not trained before starting qat","metadata":{}},{"cell_type":"code","source":"input_data = next(iter(trainloader))[0]\ncalibrate_data = input_data.to(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.eval()\nmodel.qconfig = torch.ao.quantization.get_default_qat_qconfig('x86')\nmodel.train()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"qconfig_mapping = get_default_qat_qconfig_mapping(\"x86\")\nmodel_prepared = quantize_fx.prepare_qat_fx(model, qconfig_mapping, calibrate_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_prepared.to(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_prepared = train(model_prepared,trainloader, validloader,criterion, optimizer,device,epochs,steps,print_every)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_prepared.to('cpu')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_quantized_trained = quantize_fx.convert_fx(model_prepared)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_quantized_trained.to('cpu')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_predictions_int8 = []\nall_labels_int8 = []\ncorrect_pred = 0\ntotal_pred = 0\nstart_time_int8 = time()\nwith torch.no_grad():\n    model_quantized_trained.eval()\n    for data in testloader:\n        images, labels = data\n        all_labels_int8.extend(labels.numpy())\n        #images, labels = images.to(device), labels.to(device)\n        outputs = model_quantized_trained(images.to('cpu'))\n        _, predicted = torch.max(outputs.data, 1)\n        total_pred += labels.size(0)\n        correct_pred += (predicted == labels).sum().item()\n        predicted_tensor_cpu = predicted.to('cpu')\n        all_predictions_int8.extend(predicted_tensor_cpu.numpy())\nend_time_int8 = time()\nprint(\"Time: \",end_time_int8 - start_time_int8)\nprint('Accuracy achieved by the network on test images is: %d%%' % (100 * correct_pred / total_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"metrics(all_predictions_int8,all_labels_int8)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Size of fp16 model:\",end='')\nprint_model_size(model_fp16)\nprint(\"Size of PTQ model:\",end='')\nprint_model_size(model_quantized_static)\nprint(\"Size of QAT model:\",end='')\nprint_model_size(model_quantized_trained)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}